[
  {
    "number": 155,
    "title": "Add siiRL",
    "user": "liao1995",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T04:48:22Z",
    "closed_at": "2025-07-29T05:57:37Z",
    "merged_at": "2025-07-29T05:57:37Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/155"
  },
  {
    "number": 154,
    "title": "add two papers",
    "user": "JoursBleu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T09:36:56Z",
    "closed_at": "2025-07-10T01:32:02Z",
    "merged_at": "2025-07-10T01:32:02Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/154"
  },
  {
    "number": 153,
    "title": "Add SDMPrune paper",
    "user": "sccbhxc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T09:27:25Z",
    "closed_at": "2025-06-30T11:38:11Z",
    "merged_at": "2025-06-30T11:38:11Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/153"
  },
  {
    "number": 152,
    "title": "Add Inference-Time Hyper-Scaling",
    "user": "CStanKonrad",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T12:08:15Z",
    "closed_at": "2025-06-16T12:12:38Z",
    "merged_at": "2025-06-16T12:12:38Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/152"
  },
  {
    "number": 151,
    "title": "Add STAND",
    "user": "woominsong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T06:52:24Z",
    "closed_at": "2025-06-06T07:46:11Z",
    "merged_at": "2025-06-06T07:46:11Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/151"
  },
  {
    "number": 150,
    "title": "Add a new paper (GuidedQuant)",
    "user": "jusjinuk",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T11:57:56Z",
    "closed_at": "2025-06-06T03:28:41Z",
    "merged_at": "2025-06-06T03:28:41Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/150"
  },
  {
    "number": 149,
    "title": "Update new paper (KVzip)",
    "user": "Janghyun1230",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T07:33:38Z",
    "closed_at": "2025-06-05T09:45:56Z",
    "merged_at": "2025-06-05T09:45:56Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/149"
  },
  {
    "number": 148,
    "title": "Add 4 papers",
    "user": "woominsong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T02:09:23Z",
    "closed_at": "2025-06-04T02:50:06Z",
    "merged_at": "2025-06-04T02:50:06Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/148"
  },
  {
    "number": 147,
    "title": "ðŸ”¥[SageAttention-3] SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-bit Training",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-24T02:42:13Z",
    "closed_at": "2025-05-24T02:42:26Z",
    "merged_at": "2025-05-24T02:42:26Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/147"
  },
  {
    "number": 146,
    "title": "Flex Attention: a Programming Model for Generating Optimized Attention Kernels",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T02:07:23Z",
    "closed_at": "2025-05-12T02:07:58Z",
    "merged_at": "2025-05-12T02:07:58Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/146"
  },
  {
    "number": 145,
    "title": "Add The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
    "user": "PiotrNawrot",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T11:02:16Z",
    "closed_at": "2025-05-05T11:06:57Z",
    "merged_at": "2025-05-05T11:06:57Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/145"
  },
  {
    "number": 144,
    "title": "ðŸ”¥[BitNet v2] Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T09:07:21Z",
    "closed_at": "2025-05-05T09:07:29Z",
    "merged_at": "2025-05-05T09:07:29Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/144"
  },
  {
    "number": 142,
    "title": "ðŸ”¥[Triton-distributed] TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T08:22:00Z",
    "closed_at": "2025-04-27T08:22:06Z",
    "merged_at": "2025-04-27T08:22:06Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/142"
  },
  {
    "number": 141,
    "title": "Update Multi-GPUs/Multi-Nodes Parallelism",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-26T02:19:42Z",
    "closed_at": "2025-04-26T02:19:50Z",
    "merged_at": "2025-04-26T02:19:50Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/141"
  },
  {
    "number": 140,
    "title": "ðŸ”¥[MMInference] MMInference: Accelerating Pre-filling for Long-Context Visual Language Models via Modality-Aware Permutation Sparse Attention",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T12:10:20Z",
    "closed_at": "2025-04-25T12:10:32Z",
    "merged_at": "2025-04-25T12:10:32Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/140"
  },
  {
    "number": 139,
    "title": "ðŸ”¥[FSDP 1/2] PyTorch FSDP: Getting Started with Fully Sharded Data Parallel(FSDP)",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T08:03:03Z",
    "closed_at": "2025-04-25T08:03:10Z",
    "merged_at": "2025-04-25T08:03:10Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/139"
  },
  {
    "number": 138,
    "title": "ðŸ”¥ðŸ”¥[SGLang] Efficiently Programming Large Language Models using SGLang",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T06:37:31Z",
    "closed_at": "2025-04-18T06:37:41Z",
    "merged_at": "2025-04-18T06:37:41Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/138"
  },
  {
    "number": 135,
    "title": "Add SeerAttention and SlimAttention Paper",
    "user": "sunshinemyson",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T15:21:35Z",
    "closed_at": "2025-04-16T16:26:33Z",
    "merged_at": "2025-04-16T16:26:33Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/135"
  },
  {
    "number": 133,
    "title": "ðŸ”¥[KV Cache Prefetch] Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-12T03:38:55Z",
    "closed_at": "2025-04-12T03:39:02Z",
    "merged_at": "2025-04-12T03:39:02Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/133"
  },
  {
    "number": 132,
    "title": "TRITONBENCH: Benchmarking Large Language Model Capabilities for Generating Triton Operator",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-06T11:12:18Z",
    "closed_at": "2025-04-06T11:12:25Z",
    "merged_at": "2025-04-06T11:12:24Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/132"
  },
  {
    "number": 131,
    "title": "MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-06T11:04:28Z",
    "closed_at": "2025-04-06T11:04:35Z",
    "merged_at": "2025-04-06T11:04:35Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/131"
  },
  {
    "number": 130,
    "title": "Update Mooncake-v3 paper link",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-30T13:47:29Z",
    "closed_at": "2025-03-30T13:47:47Z",
    "merged_at": "2025-03-30T13:47:47Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/130"
  },
  {
    "number": 129,
    "title": "Update README.md",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-30T05:01:09Z",
    "closed_at": "2025-03-30T05:01:17Z",
    "merged_at": "2025-03-30T05:01:17Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/129"
  },
  {
    "number": 128,
    "title": "Add download_pdfs.py",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-30T04:59:02Z",
    "closed_at": "2025-03-30T04:59:09Z",
    "merged_at": "2025-03-30T04:59:09Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/128"
  },
  {
    "number": 127,
    "title": "ðŸ”¥[X-EcoMLA] Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-29T15:42:07Z",
    "closed_at": "2025-03-29T15:44:21Z",
    "merged_at": "2025-03-29T15:44:21Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/127"
  },
  {
    "number": 126,
    "title": "Request to Add CacheCraft: A Relevant Work on Chunk-Aware KV Cache Reuse",
    "user": "skejriwal44",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-03T18:22:04Z",
    "closed_at": "2025-03-04T02:57:49Z",
    "merged_at": "2025-03-04T02:57:49Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/126"
  },
  {
    "number": 125,
    "title": "Update DeepSeek/MLA Topics",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-03T02:01:38Z",
    "closed_at": "2025-03-03T02:01:44Z",
    "merged_at": "2025-03-03T02:01:44Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/125"
  },
  {
    "number": 124,
    "title": "Add DeepSeek Open Sources modules",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-02T07:23:27Z",
    "closed_at": "2025-03-02T07:23:35Z",
    "merged_at": "2025-03-02T07:23:35Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/124"
  },
  {
    "number": 123,
    "title": "update the title of SageAttention2 and add SpargeAttn",
    "user": "jt-zhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-01T04:22:55Z",
    "closed_at": "2025-03-01T04:38:03Z",
    "merged_at": "2025-03-01T04:38:03Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/123"
  },
  {
    "number": 122,
    "title": "ðŸ”¥[MHA2MLA] Towards Economical Inference: Enabling DeepSeekâ€™s Multi-Head Latent Attention in Any Transformer-based LLMs",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-01T04:02:21Z",
    "closed_at": "2025-03-01T04:02:31Z",
    "merged_at": "2025-03-01T04:02:31Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/122"
  },
  {
    "number": 121,
    "title": "Add our ICLR2025 work Dynamic-LLaVA",
    "user": "Blank-z0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-27T04:55:22Z",
    "closed_at": "2025-02-27T05:07:20Z",
    "merged_at": "2025-02-27T05:07:20Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/121"
  },
  {
    "number": 120,
    "title": "[feat] add deepseek FlashMLA",
    "user": "shaoyuyoung",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-24T04:20:24Z",
    "closed_at": "2025-02-24T05:10:20Z",
    "merged_at": "2025-02-24T05:10:19Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/120"
  },
  {
    "number": 119,
    "title": "ðŸ”¥[DeepSeek-NSA] Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-19T11:45:30Z",
    "closed_at": "2025-02-19T11:45:38Z",
    "merged_at": "2025-02-19T11:45:38Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/119"
  },
  {
    "number": 118,
    "title": "Add Multi-head Latent Attention(MLA) topic",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-13T04:20:00Z",
    "closed_at": "2025-02-13T04:20:07Z",
    "merged_at": "2025-02-13T04:20:07Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/118"
  },
  {
    "number": 117,
    "title": "ðŸ”¥ðŸ”¥[Mooncake] Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-31T06:38:57Z",
    "closed_at": "2025-01-31T06:39:03Z",
    "merged_at": "2025-01-31T06:39:03Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/117"
  },
  {
    "number": 116,
    "title": "ðŸ”¥ðŸ”¥[DeServe] DESERVE: TOWARDS AFFORDABLE OFFLINE LLM INFERENCE VIA DECENTRALIZATION",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-31T06:36:58Z",
    "closed_at": "2025-01-31T06:37:12Z",
    "merged_at": "2025-01-31T06:37:12Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/116"
  },
  {
    "number": 115,
    "title": "ðŸ”¥ðŸ”¥[KVDirect] KVDirect: Distributed Disaggregated LLM Inference",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-31T06:33:40Z",
    "closed_at": "2025-01-31T06:33:58Z",
    "merged_at": "2025-01-31T06:33:58Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/115"
  },
  {
    "number": 114,
    "title": "ðŸ”¥ðŸ”¥[DistServe] DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-31T06:28:55Z",
    "closed_at": "2025-01-31T06:29:04Z",
    "merged_at": "2025-01-31T06:29:04Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/114"
  },
  {
    "number": 113,
    "title": "[feat] add deepseek-r1",
    "user": "shaoyuyoung",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-24T02:58:59Z",
    "closed_at": "2025-01-24T06:38:30Z",
    "merged_at": "2025-01-24T06:38:30Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/113"
  },
  {
    "number": 112,
    "title": "add `MiniMax-01` in Trending LLM/VLM Topics and Long Context Attention",
    "user": "shaoyuyoung",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-15T05:29:51Z",
    "closed_at": "2025-01-15T05:41:52Z",
    "merged_at": "2025-01-15T05:41:52Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/112"
  },
  {
    "number": 111,
    "title": "ðŸ”¥ðŸ”¥[FFPA] FFPA: Yet another Faster Flash Prefill Attention with O(1) SRAM complexity for headdim > 256, ~1.5x faster than SDPA EA(@DefTruth)",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-06T06:09:47Z",
    "closed_at": "2025-01-06T06:09:55Z",
    "merged_at": "2025-01-06T06:09:55Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/111"
  },
  {
    "number": 110,
    "title": "ðŸ”¥ðŸ”¥[SP: TokenRing] TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs via Bidirectional Communication",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-06T06:06:11Z",
    "closed_at": "2025-01-06T06:06:19Z",
    "merged_at": "2025-01-06T06:06:19Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/110"
  },
  {
    "number": 109,
    "title": "ðŸ”¥ðŸ”¥ðŸ”¥[DeepSeek-V3] DeepSeek-V3 Technical Report",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-03T09:16:49Z",
    "closed_at": "2025-01-03T09:17:24Z",
    "merged_at": "2025-01-03T09:17:24Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/109"
  },
  {
    "number": 108,
    "title": "ðŸ”¥ðŸ”¥[HADACORE] HADACORE: TENSOR CORE ACCELERATED HADAMARD TRANSFORM KERNEL",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-22T02:14:11Z",
    "closed_at": "2024-12-22T02:14:19Z",
    "merged_at": "2024-12-22T02:14:19Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/108"
  },
  {
    "number": 107,
    "title": "ðŸ”¥[DynamicKV] DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-22T01:56:04Z",
    "closed_at": "2024-12-22T01:56:12Z",
    "merged_at": "2024-12-22T01:56:12Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/107"
  },
  {
    "number": 106,
    "title": "ðŸ”¥ðŸ”¥[NITRO] NITRO: LLM INFERENCE ON INTELÂ® LAPTOP NPUS",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-22T01:51:55Z",
    "closed_at": "2024-12-22T01:52:01Z",
    "merged_at": "2024-12-22T01:52:01Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/106"
  },
  {
    "number": 105,
    "title": "ðŸ”¥ðŸ”¥[TurboAttention] TURBOATTENTION: EFFICIENT ATTENTION APPROXIMATION FOR HIGH THROUGHPUTS LLMS",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-22T01:46:27Z",
    "closed_at": "2024-12-22T01:46:33Z",
    "merged_at": "2024-12-22T01:46:33Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/105"
  },
  {
    "number": 104,
    "title": "ðŸ”¥[BatchLLM] BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-08T07:39:35Z",
    "closed_at": "2024-12-08T07:40:03Z",
    "merged_at": "2024-12-08T07:40:03Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/104"
  },
  {
    "number": 103,
    "title": "ðŸ”¥[ClusterKV] ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable Compression",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-08T07:34:43Z",
    "closed_at": "2024-12-08T07:34:54Z",
    "merged_at": "2024-12-08T07:34:54Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/103"
  },
  {
    "number": 102,
    "title": "ðŸ”¥[KV Cache Recomputation] Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-01T09:09:12Z",
    "closed_at": "2024-12-01T09:09:19Z",
    "merged_at": "2024-12-01T09:09:19Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/102"
  },
  {
    "number": 101,
    "title": "ðŸ”¥[Star-Attention: 11x~ speedup] Star Attention: Efficient LLM Inference over Long Sequences",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-28T01:31:08Z",
    "closed_at": "2024-11-28T01:31:17Z",
    "merged_at": "2024-11-28T01:31:17Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/101"
  },
  {
    "number": 100,
    "title": "ðŸ”¥[SparseInfer] SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-25T03:22:24Z",
    "closed_at": "2024-11-25T03:22:30Z",
    "merged_at": "2024-11-25T03:22:30Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/100"
  },
  {
    "number": 99,
    "title": "ðŸ”¥[Squeezed Attention] SQUEEZED ATTENTION: Accelerating Long Context Length LLM Inference(@UC Berkeley)",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-24T12:15:59Z",
    "closed_at": "2024-11-24T12:16:06Z",
    "merged_at": "2024-11-24T12:16:06Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/99"
  },
  {
    "number": 98,
    "title": "ðŸ”¥[SageAttention-2] SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration(@thu-ml)",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-24T12:03:05Z",
    "closed_at": "2024-11-24T12:03:14Z",
    "merged_at": "2024-11-24T12:03:14Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/98"
  },
  {
    "number": 97,
    "title": "ðŸ”¥[SageAttention] SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE ACCELERATION(@thu-ml)",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-24T11:59:57Z",
    "closed_at": "2024-11-24T12:00:02Z",
    "merged_at": "2024-11-24T12:00:02Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/97"
  },
  {
    "number": 96,
    "title": "add vAttention code link",
    "user": "KevinZeng08",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-22T17:29:14Z",
    "closed_at": "2024-11-23T01:04:40Z",
    "merged_at": "2024-11-23T01:04:40Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/96"
  },
  {
    "number": 95,
    "title": "Add code link to BPT",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-18T08:15:35Z",
    "closed_at": "2024-11-18T08:15:49Z",
    "merged_at": "2024-11-18T08:15:49Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/95"
  },
  {
    "number": 94,
    "title": "ðŸ”¥ðŸ”¥[TP: Comm Compression] Communication Compression for Tensor Parallel LLM Inference",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-18T02:44:31Z",
    "closed_at": "2024-11-18T02:44:38Z",
    "merged_at": "2024-11-18T02:44:38Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/94"
  },
  {
    "number": 93,
    "title": "ðŸ”¥ðŸ”¥[SP: BPT] Blockwise Parallel Transformer for Large Context Models",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-18T02:38:44Z",
    "closed_at": "2024-11-18T02:38:52Z",
    "merged_at": "2024-11-18T02:38:52Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/93"
  },
  {
    "number": 92,
    "title": "Add DP/TP/SP/CP papers with codes",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-15T03:35:45Z",
    "closed_at": "2024-11-15T03:35:53Z",
    "merged_at": "2024-11-15T03:35:53Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/92"
  },
  {
    "number": 91,
    "title": "ðŸ”¥[BitNet] BitNet a4.8: 4-bit Activations for 1-bit LLMs",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-12T05:48:13Z",
    "closed_at": "2024-11-12T05:48:21Z",
    "merged_at": "2024-11-12T05:48:21Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/91"
  },
  {
    "number": 90,
    "title": "ðŸ”¥[Tensor Product] Acceleration of Tensor-Product Operations with Tensor Cores",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-31T05:57:23Z",
    "closed_at": "2024-10-31T05:57:30Z",
    "merged_at": "2024-10-31T05:57:30Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/90"
  },
  {
    "number": 89,
    "title": "ðŸ”¥[Fast Best-of-N] Fast Best-of-N Decoding via Speculative Rejection",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-31T05:46:12Z",
    "closed_at": "2024-10-31T05:46:19Z",
    "merged_at": "2024-10-31T05:46:19Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/89"
  },
  {
    "number": 88,
    "title": "ðŸ”¥[FastAttention] FastAttention: Extend FlashAttention2 to NPUs and Low-resource GPUs for Efficient Inference",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-28T02:35:33Z",
    "closed_at": "2024-10-28T02:35:40Z",
    "merged_at": "2024-10-28T02:35:40Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/88"
  },
  {
    "number": 87,
    "title": "Efficient Hybrid Inference for LLMs: Reward-Based Token Modelling with Selective Cloud Assistance",
    "user": "aharshms",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-21T10:37:17Z",
    "closed_at": "2024-10-21T13:23:48Z",
    "merged_at": "2024-10-21T13:23:48Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/87"
  },
  {
    "number": 86,
    "title": "Add paper AdaKV",
    "user": "FFY0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-15T13:15:09Z",
    "closed_at": "2024-10-15T13:51:19Z",
    "merged_at": "2024-10-15T13:51:19Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/86"
  },
  {
    "number": 85,
    "title": "early exit of LLM inference",
    "user": "boyi-liu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-14T13:31:45Z",
    "closed_at": "2024-10-15T02:15:29Z",
    "merged_at": "2024-10-15T02:15:29Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/85"
  },
  {
    "number": 84,
    "title": "ðŸ”¥[PARALLELSPEC] PARALLELSPEC: PARALLEL DRAFTER FOR EFFICIENT SPECULATIVE DECODING",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-10T01:46:17Z",
    "closed_at": "2024-10-10T01:46:25Z",
    "merged_at": "2024-10-10T01:46:25Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/84"
  },
  {
    "number": 83,
    "title": "[LLM Inference] LARGE LANGUAGE MODEL INFERENCE ACCELERATION: A COMPREHENSIVE HARDWARE PERSPECTIVE",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-10T01:28:25Z",
    "closed_at": "2024-10-10T01:28:32Z",
    "merged_at": "2024-10-10T01:28:32Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/83"
  },
  {
    "number": 82,
    "title": "Large Language Model Performance Benchmarking on Mobile Platforms: A Thorough Evaluation",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-07T04:40:12Z",
    "closed_at": "2024-10-07T04:40:27Z",
    "merged_at": "2024-10-07T04:40:27Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/82"
  },
  {
    "number": 81,
    "title": "ðŸ”¥[LORC] Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-07T04:37:02Z",
    "closed_at": "2024-10-07T04:37:08Z",
    "merged_at": "2024-10-07T04:37:08Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/81"
  },
  {
    "number": 80,
    "title": "[From Author] Link CacheGen and CacheBlend to LMCache",
    "user": "KuntaiDu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-06T05:25:13Z",
    "closed_at": "2024-10-06T10:28:48Z",
    "merged_at": "2024-10-06T10:28:48Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/80"
  },
  {
    "number": 79,
    "title": "Bump up to v2.6",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-03T01:01:46Z",
    "closed_at": "2024-10-03T01:01:52Z",
    "merged_at": "2024-10-03T01:01:52Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/79"
  },
  {
    "number": 78,
    "title": "ðŸ”¥[LayerKV] Optimizing Large Language Model Serving with Layer-wise KV Cache Management",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-03T00:56:35Z",
    "closed_at": "2024-10-03T00:56:40Z",
    "merged_at": "2024-10-03T00:56:40Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/78"
  },
  {
    "number": 77,
    "title": "ðŸ”¥[KV-COMPRESS] PAGED KV-CACHE COMPRESSION WITH VARIABLE COMPRESSION RATES PER ATTENTION HEAD",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-03T00:41:42Z",
    "closed_at": "2024-10-03T00:53:02Z",
    "merged_at": "2024-10-03T00:53:02Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/77"
  },
  {
    "number": 76,
    "title": "ðŸ”¥ðŸ”¥[Tensor Cores] Efficient Arbitrary Precision Acceleration for Large Language Models on GPU Tensor Cores",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-27T02:30:41Z",
    "closed_at": "2024-09-27T02:30:49Z",
    "merged_at": "2024-09-27T02:30:49Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/76"
  },
  {
    "number": 75,
    "title": "ðŸ”¥[AlignedKV] AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned Quantization",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-26T03:52:49Z",
    "closed_at": "2024-09-26T03:53:02Z",
    "merged_at": "2024-09-26T03:53:02Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/75"
  },
  {
    "number": 74,
    "title": "ðŸ”¥ðŸ”¥[HiFloat8] Ascend HiFloat8 Format for Deep Learning",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-26T03:47:56Z",
    "closed_at": "2024-09-26T03:48:03Z",
    "merged_at": "2024-09-26T03:48:03Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/74"
  },
  {
    "number": 73,
    "title": "[Low-bit] A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-26T03:45:14Z",
    "closed_at": "2024-09-26T03:45:21Z",
    "merged_at": "2024-09-26T03:45:21Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/73"
  },
  {
    "number": 72,
    "title": "ðŸ”¥ðŸ”¥[INT-FLASHATTENTION] INT-FLASHATTENTION: ENABLING FLASH ATTENTION FOR INT8 QUANTIZATION",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-26T03:42:18Z",
    "closed_at": "2024-09-26T03:42:26Z",
    "merged_at": "2024-09-26T03:42:26Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/72"
  },
  {
    "number": 71,
    "title": "fix typo",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-26T03:38:13Z",
    "closed_at": "2024-09-26T03:38:20Z",
    "merged_at": "2024-09-26T03:38:20Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/71"
  },
  {
    "number": 70,
    "title": "ðŸ”¥[VPTQ] VPTQ: EXTREME LOW-BIT VECTOR POST-TRAINING QUANTIZATION FOR LARGE LANGUAGE MODELS",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-26T03:37:13Z",
    "closed_at": "2024-09-26T03:37:20Z",
    "merged_at": "2024-09-26T03:37:20Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/70"
  },
  {
    "number": 69,
    "title": "Bump up to v2.5",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-26T03:25:01Z",
    "closed_at": "2024-09-26T03:25:10Z",
    "merged_at": "2024-09-26T03:25:09Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/69"
  },
  {
    "number": 68,
    "title": "ðŸ”¥ðŸ”¥[CRITIPREFILL] CRITIPREFILL: A SEGMENT-WISE CRITICALITYBASED APPROACH FOR PREFILLING ACCELERATION IN LLMS",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-25T03:24:08Z",
    "closed_at": "2024-09-25T03:24:14Z",
    "merged_at": "2024-09-25T03:24:14Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/68"
  },
  {
    "number": 67,
    "title": "move RetrievalAttention -> long context",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-23T03:03:48Z",
    "closed_at": "2024-09-23T03:03:57Z",
    "merged_at": "2024-09-23T03:03:57Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/67"
  },
  {
    "number": 66,
    "title": "Update codebase of paper \"parallel speculative decoding with adaptive draft length\"",
    "user": "smart-lty",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-19T05:02:31Z",
    "closed_at": "2024-09-19T05:23:44Z",
    "merged_at": "2024-09-19T05:23:44Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/66"
  },
  {
    "number": 65,
    "title": "ðŸ”¥[InstInfer] InstInfer: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-18T08:06:36Z",
    "closed_at": "2024-09-18T08:06:43Z",
    "merged_at": "2024-09-18T08:06:43Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/65"
  },
  {
    "number": 64,
    "title": "Bump up to v2.4",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-18T05:09:43Z",
    "closed_at": "2024-09-18T05:09:50Z",
    "merged_at": "2024-09-18T05:09:50Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/64"
  },
  {
    "number": 63,
    "title": "ðŸ”¥[Inf-MLLM] Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language Models on a Single GPU",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-17T08:10:37Z",
    "closed_at": "2024-09-17T08:10:45Z",
    "merged_at": "2024-09-17T08:10:45Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/63"
  },
  {
    "number": 62,
    "title": "ðŸ”¥[RetrievalAttention] Accelerating Long-Context LLM Inference via Vector Retrieval",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-17T05:29:43Z",
    "closed_at": "2024-09-17T05:30:09Z",
    "merged_at": "2024-09-17T05:30:09Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/62"
  },
  {
    "number": 61,
    "title": "Bump up to v2.3",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-09T01:24:49Z",
    "closed_at": "2024-09-09T01:24:57Z",
    "merged_at": "2024-09-09T01:24:56Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/61"
  },
  {
    "number": 60,
    "title": "ðŸ”¥[SpMM] High Performance Unstructured SpMM Computation Using Tensor Cores",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-05T01:40:15Z",
    "closed_at": "2024-09-05T01:40:23Z",
    "merged_at": "2024-09-05T01:40:23Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/60"
  },
  {
    "number": 59,
    "title": "ðŸ”¥[CHESS] CHESS : Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-05T01:35:24Z",
    "closed_at": "2024-09-05T01:35:32Z",
    "merged_at": "2024-09-05T01:35:32Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/59"
  },
  {
    "number": 58,
    "title": "Bump up to v2.2",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-04T06:21:31Z",
    "closed_at": "2024-09-04T06:21:38Z",
    "merged_at": "2024-09-04T06:21:38Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/58"
  },
  {
    "number": 57,
    "title": "ðŸ”¥ðŸ”¥[Context Distillation] Efficient LLM Context Distillation",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-04T06:20:10Z",
    "closed_at": "2024-09-04T06:20:17Z",
    "merged_at": "2024-09-04T06:20:17Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/57"
  },
  {
    "number": 56,
    "title": "ðŸ”¥ðŸ”¥[Prompt Compression] Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-04T06:16:49Z",
    "closed_at": "2024-09-04T06:16:58Z",
    "merged_at": "2024-09-04T06:16:58Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/56"
  },
  {
    "number": 55,
    "title": "ðŸ”¥[Speculative Decoding] Boosting Lossless Speculative Decoding via Feature Sampling and Partial Alignment Distillation",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-01T06:42:13Z",
    "closed_at": "2024-09-01T06:42:20Z",
    "merged_at": "2024-09-01T06:42:20Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/55"
  },
  {
    "number": 54,
    "title": "ðŸ”¥[SJF Scheduling] Efficient LLM Scheduling by Learning to Rank",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-01T06:37:57Z",
    "closed_at": "2024-09-01T06:38:03Z",
    "merged_at": "2024-09-01T06:38:03Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/54"
  },
  {
    "number": 53,
    "title": "ðŸ”¥[Decentralized LLM] Decentralized LLM Inference over Edge Networks with Energy Harvesting",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-01T06:33:01Z",
    "closed_at": "2024-09-01T06:33:07Z",
    "merged_at": "2024-09-01T06:33:07Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/53"
  },
  {
    "number": 52,
    "title": "ðŸ”¥[ACTIVATION SPARSITY] TRAINING-FREE ACTIVATION SPARSITY IN LARGE LANGUAGE MODELS",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-30T11:13:34Z",
    "closed_at": "2024-08-30T11:13:49Z",
    "merged_at": "2024-08-30T11:13:49Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/52"
  },
  {
    "number": 51,
    "title": "Add NanoFlow code link",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-28T03:53:57Z",
    "closed_at": "2024-08-28T03:54:05Z",
    "merged_at": "2024-08-28T03:54:05Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/51"
  },
  {
    "number": 50,
    "title": "Bump up to v2.1",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-28T01:53:14Z",
    "closed_at": "2024-08-28T01:53:21Z",
    "merged_at": "2024-08-28T01:53:21Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/50"
  },
  {
    "number": 49,
    "title": "ðŸ”¥ðŸ”¥[FLA] FLA: A Triton-Based Library for Hardware-Efficient Implementaâ€¦",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-28T01:52:16Z",
    "closed_at": "2024-08-28T01:52:23Z",
    "merged_at": "2024-08-28T01:52:23Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/49"
  },
  {
    "number": 48,
    "title": "ðŸ”¥[1-bit LLMs] Matmul or No Matmal in the Era of 1-bit LLMs",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-28T01:46:42Z",
    "closed_at": "2024-08-28T01:46:51Z",
    "merged_at": "2024-08-28T01:46:51Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/48"
  },
  {
    "number": 47,
    "title": "ðŸ”¥ðŸ”¥[MARLIN] MARLIN: Mixed-Precision Auto-Regressive Parallel Inference on Large Language Models",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-28T01:42:38Z",
    "closed_at": "2024-08-28T01:42:49Z",
    "merged_at": "2024-08-28T01:42:48Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/47"
  },
  {
    "number": 46,
    "title": "Add ABQ-LLM code link",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-28T01:34:20Z",
    "closed_at": "2024-08-28T01:34:28Z",
    "merged_at": "2024-08-28T01:34:28Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/46"
  },
  {
    "number": 44,
    "title": "ðŸ”¥[MagicDec] MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-27T01:54:34Z",
    "closed_at": "2024-08-27T01:54:41Z",
    "merged_at": "2024-08-27T01:54:41Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/44"
  },
  {
    "number": 43,
    "title": "ðŸ”¥[NanoFlow] NanoFlow: Towards Optimal Large Language Model Serving Throughput",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-27T01:40:18Z",
    "closed_at": "2024-08-27T01:40:25Z",
    "merged_at": "2024-08-27T01:40:25Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/43"
  },
  {
    "number": 42,
    "title": "ðŸ”¥[FocusLLM] FocusLLM: Scaling LLMâ€™s Context by Parallel Decoding",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-27T01:37:29Z",
    "closed_at": "2024-08-27T01:37:37Z",
    "merged_at": "2024-08-27T01:37:37Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/42"
  },
  {
    "number": 41,
    "title": "ðŸ”¥[Speculative Decoding] Parallel Speculative Decoding with Adaptive Draft Length",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-27T01:06:21Z",
    "closed_at": "2024-08-27T01:06:29Z",
    "merged_at": "2024-08-27T01:06:29Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/41"
  },
  {
    "number": 40,
    "title": "Update README.md",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-21T02:52:04Z",
    "closed_at": "2024-08-21T02:52:11Z",
    "merged_at": "2024-08-21T02:52:11Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/40"
  },
  {
    "number": 39,
    "title": "Bump up to v2.0",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-19T01:21:27Z",
    "closed_at": "2024-08-19T01:21:32Z",
    "merged_at": "2024-08-19T01:21:32Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/39"
  },
  {
    "number": 38,
    "title": "[Token Recycling] Turning Trash into Treasure: Accelerating Inferenceâ€¦",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-19T01:18:48Z",
    "closed_at": "2024-08-19T01:18:55Z",
    "merged_at": "2024-08-19T01:18:55Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/38"
  },
  {
    "number": 37,
    "title": "ðŸ”¥[ABQ-LLM] Arbitrary-Bit Quantized Inference Acceleration for Large Language Models",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-19T01:15:04Z",
    "closed_at": "2024-08-19T01:15:10Z",
    "merged_at": "2024-08-19T01:15:10Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/37"
  },
  {
    "number": 36,
    "title": "Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-17T05:29:54Z",
    "closed_at": "2024-08-17T05:30:04Z",
    "merged_at": "2024-08-17T05:30:04Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/36"
  },
  {
    "number": 35,
    "title": "KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft Heads with Adversarial Learning",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-17T05:19:34Z",
    "closed_at": "2024-08-17T05:19:45Z",
    "merged_at": "2024-08-17T05:19:45Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/35"
  },
  {
    "number": 34,
    "title": "ðŸ”¥ðŸ”¥[Eigen Attention] Attention in Low-Rank Space for KV Cache Compression",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-17T05:15:04Z",
    "closed_at": "2024-08-17T05:15:17Z",
    "merged_at": "2024-08-17T05:15:17Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/34"
  },
  {
    "number": 33,
    "title": "ðŸ”¥ðŸ”¥[LUT TENSOR CORE] Lookup Table Enables Efficient Low-Bit LLM Inference Acceleration",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-17T03:52:31Z",
    "closed_at": "2024-08-17T03:52:51Z",
    "merged_at": "2024-08-17T03:52:51Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/33"
  },
  {
    "number": 32,
    "title": "Bump up to v1.9",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-12T01:26:43Z",
    "closed_at": "2024-08-12T01:26:50Z",
    "merged_at": "2024-08-12T01:26:50Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/32"
  },
  {
    "number": 31,
    "title": "ðŸ”¥ðŸ”¥[500xCompressor] 500xCompressor: Generalized Prompt Compression forâ€¦",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-09T02:16:14Z",
    "closed_at": "2024-08-09T02:16:22Z",
    "merged_at": "2024-08-09T02:16:22Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/31"
  },
  {
    "number": 30,
    "title": "ðŸ”¥[Automatic Inference Engine Tuning] Towards SLO-Optimized LLM Servinâ€¦",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-09T02:08:58Z",
    "closed_at": "2024-08-09T02:09:05Z",
    "merged_at": "2024-08-09T02:09:05Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/30"
  },
  {
    "number": 29,
    "title": "ðŸ”¥[Zero-Delay QKV Compression] Zero-Delay QKV Compression for Mitigatiâ€¦",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-09T02:06:19Z",
    "closed_at": "2024-08-09T02:06:25Z",
    "merged_at": "2024-08-09T02:06:25Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/29"
  },
  {
    "number": 28,
    "title": "ðŸ”¥[DynamoLLM] DynamoLLM: Designing LLM Inference Clusters for Performaâ€¦",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-06T01:37:27Z",
    "closed_at": "2024-08-06T01:37:35Z",
    "merged_at": "2024-08-06T01:37:35Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/28"
  },
  {
    "number": 27,
    "title": "Bump up to v1.8",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-05T02:32:07Z",
    "closed_at": "2024-08-05T02:32:18Z",
    "merged_at": "2024-08-05T02:32:18Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/27"
  },
  {
    "number": 26,
    "title": "ðŸ”¥[SentenceVAE] SentenceVAE: Faster, Longer and More Accurate Inferencâ€¦",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-04T07:12:53Z",
    "closed_at": "2024-08-04T07:13:02Z",
    "merged_at": "2024-08-04T07:13:02Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/26"
  },
  {
    "number": 25,
    "title": "ðŸ”¥[Palu] Palu: Compressing KV-Cache with Low-Rank Projection(@nycu.eduâ€¦",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-01T06:13:05Z",
    "closed_at": "2024-08-01T06:13:13Z",
    "merged_at": "2024-08-01T06:13:13Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/25"
  },
  {
    "number": 24,
    "title": "ðŸ”¥[flashinfer] FlashInfer: Kernel Library for LLM Serving(@flashinfer-ai)",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-01T01:13:36Z",
    "closed_at": "2024-08-01T01:13:48Z",
    "merged_at": "2024-08-01T01:13:47Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/24"
  },
  {
    "number": 22,
    "title": "Update README.md",
    "user": "clevercool",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-28T07:33:20Z",
    "closed_at": "2024-07-28T08:50:54Z",
    "merged_at": "2024-07-28T08:50:54Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/22"
  },
  {
    "number": 21,
    "title": "Add paper \"Internal Consistency and Self-Feedback in Large Language Models: A Survey\"",
    "user": "fan2goa1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-23T13:22:45Z",
    "closed_at": "2024-07-23T13:57:48Z",
    "merged_at": "2024-07-23T13:57:48Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/21"
  },
  {
    "number": 20,
    "title": "add MInference 1.0 from microsoft",
    "user": "liyucheng09",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-13T06:50:25Z",
    "closed_at": "2024-07-13T06:50:36Z",
    "merged_at": "2024-07-13T06:50:36Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/20"
  },
  {
    "number": 19,
    "title": "[MoA] MoA: Mixture of Sparse Attention for Automatic LLM Compression",
    "user": "liyucheng09",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-24T14:19:56Z",
    "closed_at": "2024-06-24T14:21:01Z",
    "merged_at": "2024-06-24T14:21:01Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/19"
  },
  {
    "number": 18,
    "title": "Update README.md",
    "user": "Kthyeon",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-12T01:26:00Z",
    "closed_at": "2024-06-12T02:05:01Z",
    "merged_at": "2024-06-12T02:05:01Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/18"
  },
  {
    "number": 16,
    "title": "update [Decoding Speculative Decoding] github repo",
    "user": "KylinC",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-20T07:16:42Z",
    "closed_at": "2024-05-20T09:10:13Z",
    "merged_at": "2024-05-20T09:10:13Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/16"
  },
  {
    "number": 15,
    "title": "Update README.md",
    "user": "preminstrel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-26T17:37:42Z",
    "closed_at": "2024-04-27T01:30:53Z",
    "merged_at": "2024-04-27T01:30:53Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/15"
  },
  {
    "number": 14,
    "title": "Add Microbenchmark",
    "user": "Miroier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-26T08:10:33Z",
    "closed_at": "2024-04-26T08:12:55Z",
    "merged_at": "2024-04-26T08:12:55Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/14"
  },
  {
    "number": 13,
    "title": "[KVcache] add \"Gear\" paper and code of \"Keyformer\"",
    "user": "HarryWu99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-26T08:07:30Z",
    "closed_at": "2024-04-26T08:15:32Z",
    "merged_at": "2024-04-26T08:15:32Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/13"
  },
  {
    "number": 12,
    "title": "add SnapKV",
    "user": "liyucheng09",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-26T04:01:57Z",
    "closed_at": "2024-04-26T04:37:08Z",
    "merged_at": "2024-04-26T04:37:08Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/12"
  },
  {
    "number": 11,
    "title": "LLMLingua-2",
    "user": "liyucheng09",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-19T09:18:01Z",
    "closed_at": "2024-04-19T09:49:35Z",
    "merged_at": "2024-04-19T09:49:35Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/11"
  },
  {
    "number": 10,
    "title": "update",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-14T03:01:59Z",
    "closed_at": "2024-04-14T03:02:08Z",
    "merged_at": "2024-04-14T03:02:08Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/10"
  },
  {
    "number": 9,
    "title": "Add github link for paper FP8-Quantization[2208.09225]",
    "user": "Mr-Philo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-08T02:03:30Z",
    "closed_at": "2024-04-08T02:43:20Z",
    "merged_at": "2024-04-08T02:43:19Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/9"
  },
  {
    "number": 8,
    "title": "Add an ICLR paper for KV cache compression",
    "user": "Janghyun1230",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T14:33:50Z",
    "closed_at": "2024-03-29T15:10:12Z",
    "merged_at": "2024-03-29T15:10:12Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/8"
  },
  {
    "number": 7,
    "title": "add context compression & new papers KV compression",
    "user": "liyucheng09",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-22T18:38:10Z",
    "closed_at": "2024-03-23T01:36:19Z",
    "merged_at": "2024-03-23T01:36:19Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/7"
  },
  {
    "number": 6,
    "title": "fix typo",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-22T05:28:06Z",
    "closed_at": "2024-03-22T07:40:44Z",
    "merged_at": "2024-03-22T07:40:44Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/6"
  },
  {
    "number": 3,
    "title": "correct affiliation error",
    "user": "liyucheng09",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T16:43:21Z",
    "closed_at": "2024-03-21T01:50:49Z",
    "merged_at": "2024-03-21T01:50:49Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/3"
  },
  {
    "number": 2,
    "title": "Update README.md",
    "user": "HuangLianghong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-05T09:58:07Z",
    "closed_at": "2024-01-05T12:17:36Z",
    "merged_at": "2024-01-05T12:17:36Z",
    "state": "closed",
    "html_url": "https://github.com/xlite-dev/Awesome-LLM-Inference/pull/2"
  }
]