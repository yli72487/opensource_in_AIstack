[
  {
    "number": 8990,
    "title": "[TRTC-1922] [feat] Recipe db and its ingestion implementation for OPS",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-07T00:35:37Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8990"
  },
  {
    "number": 8989,
    "title": "[None][ci] move two dgx-b200 stages to post merge",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-07T00:32:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8989"
  },
  {
    "number": 8988,
    "title": "[TRTLLM-9198][perf] Add torch.compile + multi-stream support for k-cache scatter and weight scaling",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T23:49:40Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8988"
  },
  {
    "number": 8987,
    "title": "[None][infra] Update allowed list 2025.11.06",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T22:53:06Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8987"
  },
  {
    "number": 8986,
    "title": "[TRTLLM-9197][infra] Move thirdparty stuff to it's own listfile",
    "user": "cheshirekow",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T22:52:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8986"
  },
  {
    "number": 8985,
    "title": "[None][feat] add specdec to nemotron nas",
    "user": "NVShreyas",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T19:58:53Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8985"
  },
  {
    "number": 8984,
    "title": "[https://nvbugs/5625972][fix] Patch llama4 export for bug in torch2.9",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T19:46:57Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8984"
  },
  {
    "number": 8980,
    "title": "[None][debug] try enabling FlashInfer sampling (DO NOT MERGE)",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T19:02:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8980"
  },
  {
    "number": 8979,
    "title": "[None][fix] Switch AD AllReduce strategy to NCCL",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T18:13:29Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8979"
  },
  {
    "number": 8978,
    "title": "[https://nvbugs/5637012][fix] Bugfix when config is None for MLA",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T18:04:51Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8978"
  },
  {
    "number": 8977,
    "title": "[None][fix] add missing CLI option in multimodal example",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T14:51:34Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8977"
  },
  {
    "number": 8976,
    "title": "[None][fix] type annotations in fuse_input_embeds",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T14:24:11Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8976"
  },
  {
    "number": 8975,
    "title": "[TRTLLM-8778][feat] Add tree attention support for blackwell arch",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T13:31:27Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8975"
  },
  {
    "number": 8973,
    "title": "[https://nvbugs/5630345] [chore] skip deepseek-v3.2 fp8 kv tests on pre-Blackwell architectures",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T10:50:03Z",
    "closed_at": "2025-11-06T11:41:37Z",
    "merged_at": "2025-11-06T11:41:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8973"
  },
  {
    "number": 8972,
    "title": "[https://nvbugs/5575841] [fix] Nvbug 5575841: Remove additional test waivers for TestMoEFP4",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T10:25:36Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8972"
  },
  {
    "number": 8971,
    "title": "[TRTLLM-9183][infra] Add --waives-file in rerun pytest command",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T09:42:46Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8971"
  },
  {
    "number": 8970,
    "title": "[https://nvbugs/5633340][fix] kill processes properly after test",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T08:56:47Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8970"
  },
  {
    "number": 8969,
    "title": "[FMDL-1328][feat] Support nano v3 model with naive pytorch modules",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T08:14:55Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8969"
  },
  {
    "number": 8968,
    "title": "[TRTLLM-8957][feat] create communication related classes",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T08:04:26Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8968"
  },
  {
    "number": 8967,
    "title": "[https://nvbugs/5629790][chore] unwaive test.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T07:38:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8967"
  },
  {
    "number": 8966,
    "title": "[None][infra] Waive failed tests for release branch 11/06",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T07:25:47Z",
    "closed_at": "2025-11-07T01:01:26Z",
    "merged_at": "2025-11-07T01:01:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8966"
  },
  {
    "number": 8965,
    "title": "[None][chore] Revise the comment for AutoTuner enable flag in PyTorchConfig.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T06:56:22Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8965"
  },
  {
    "number": 8964,
    "title": "test ut waiver",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T06:53:04Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8964"
  },
  {
    "number": 8963,
    "title": "[None][feat] Unify nvfp4 gemm backend",
    "user": "Wong4j",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T06:30:30Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8963"
  },
  {
    "number": 8962,
    "title": "[None][chore] Use cached model in all ray tests",
    "user": "shuyixiong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T05:21:51Z",
    "closed_at": "2025-11-06T14:14:15Z",
    "merged_at": "2025-11-06T14:14:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8962"
  },
  {
    "number": 8960,
    "title": "[None][perf] Add custom indexer k cache scatter op",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-06T01:22:26Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8960"
  },
  {
    "number": 8958,
    "title": "[https://nvbugs/5642736][fix] fix AutoDeploy pattern matcher for torch 2.9 (#8920)",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T21:54:12Z",
    "closed_at": "2025-11-06T18:21:33Z",
    "merged_at": "2025-11-06T18:21:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8958"
  },
  {
    "number": 8957,
    "title": "[https://nvbugs/5606166][fix] AutoDeploy: unwaive test for use tuples for cudagraph shape lookup",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T21:50:04Z",
    "closed_at": "2025-11-06T00:27:01Z",
    "merged_at": "2025-11-06T00:27:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8957"
  },
  {
    "number": 8956,
    "title": "[None][Feature] Make torch.ops.trtllm.fp4_block_scale_moe_runner accept swiglu alpha, beta, limit and bias",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T21:47:40Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8956"
  },
  {
    "number": 8955,
    "title": "[None][feat] AutoDeploy: Support Latent MOE for Nemotron",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T21:32:23Z",
    "closed_at": "2025-11-06T20:40:20Z",
    "merged_at": "2025-11-06T20:40:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8955"
  },
  {
    "number": 8954,
    "title": "DO NOT REVIEW YET",
    "user": "nzmora-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T20:58:12Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8954"
  },
  {
    "number": 8952,
    "title": "[None][infra] User/yuanjingx/trigger ci gitlab scanning",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T20:35:08Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8952"
  },
  {
    "number": 8951,
    "title": "[TRTLLM-8598][feat] enable n > 1 in OpenAI API with PyTorch backend",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T19:45:25Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8951"
  },
  {
    "number": 8950,
    "title": "[None][perf] Adjust select_alltoall_method_type.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T16:44:44Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8950"
  },
  {
    "number": 8945,
    "title": "[TRTLLM-7251][test] Get submit eplb slots empty key work",
    "user": "fredricz-20070104",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T10:37:56Z",
    "closed_at": "2025-11-05T13:21:02Z",
    "merged_at": "2025-11-05T13:21:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8945"
  },
  {
    "number": 8944,
    "title": "[None][fix] fix eagle3 accuracy issue on sm120",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T10:02:04Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8944"
  },
  {
    "number": 8943,
    "title": "[TRTLLM-9001][feat] add TP support for DeepSeek-V3.2",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T09:24:36Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8943"
  },
  {
    "number": 8941,
    "title": "[None][feat] Nano-v3 stack PRs v1",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T08:39:10Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8941"
  },
  {
    "number": 8940,
    "title": "[https://nvbugs/5608930][fix] Wavie TestQwen3_8B::test_chunked_prefill for bug 5608930",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T08:16:29Z",
    "closed_at": "2025-11-05T09:52:10Z",
    "merged_at": "2025-11-05T09:52:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8940"
  },
  {
    "number": 8939,
    "title": "[https://nvbugs/5636986][fix] Fix DeepGemmMoe get_buffer calls",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T08:13:51Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8939"
  },
  {
    "number": 8938,
    "title": "[None][chore] Relocate rlhf_utils.py",
    "user": "shuyixiong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T06:31:19Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8938"
  },
  {
    "number": 8937,
    "title": "[TRTLLM-8948][test] Add long bench case",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T06:16:03Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8937"
  },
  {
    "number": 8936,
    "title": "[None][infra] Waive failed cases on main 11/05",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T05:52:43Z",
    "closed_at": "2025-11-05T06:54:45Z",
    "merged_at": "2025-11-05T06:54:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8936"
  },
  {
    "number": 8935,
    "title": "[None][chore]  Make more cases use local cached models",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T05:26:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8935"
  },
  {
    "number": 8934,
    "title": "[None][chore] Support json_schema in response_format",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T03:51:05Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8934"
  },
  {
    "number": 8933,
    "title": "[https://nvbugs/5626259][test] Fix nemotron flaky test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T03:28:16Z",
    "closed_at": "2025-11-06T02:35:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8933"
  },
  {
    "number": 8932,
    "title": "[TRTLLM-8936][test] Add disagg and wideep multi-node multi-gpu test cases",
    "user": "fredricz-20070104",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T03:26:20Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8932"
  },
  {
    "number": 8931,
    "title": "[https://nvbugs/5556998][fix] init_hf_modules in worker_main for models with trust_remote=true",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T03:04:14Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8931"
  },
  {
    "number": 8930,
    "title": "[TRTLLM-8936][test] Add disagg and wideep multi-node multi-gpu test cases",
    "user": "fredricz-20070104",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T02:48:34Z",
    "closed_at": "2025-11-05T03:02:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8930"
  },
  {
    "number": 8929,
    "title": "[TRTLLM-9079][infra] upgrade tritonserver DLFW 25.10",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T02:28:19Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8929"
  },
  {
    "number": 8928,
    "title": "[None][feat] add waive by sm version",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-05T02:20:22Z",
    "closed_at": "2025-11-06T03:20:43Z",
    "merged_at": "2025-11-06T03:20:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8928"
  },
  {
    "number": 8926,
    "title": "[https://nvbugs/5606136][ci] Remove tests for deprecating models.",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T22:18:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8926"
  },
  {
    "number": 8925,
    "title": "[TRTLLM-8679][feat] Scenario UX bulk refactor",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T22:15:37Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8925"
  },
  {
    "number": 8923,
    "title": "[TRTLLM-7967][feat] Adding Starcoder2 PyTorch Flow Support",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T21:16:07Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8923"
  },
  {
    "number": 8922,
    "title": "[feat] Allow env variable to specify spawn process IPC port",
    "user": "hvagadia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T20:53:33Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8922"
  },
  {
    "number": 8920,
    "title": "[#8924][fix] Fix AutoDeploy pattern matcher for torch 2.9",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T19:56:07Z",
    "closed_at": "2025-11-05T21:29:21Z",
    "merged_at": "2025-11-05T21:29:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8920"
  },
  {
    "number": 8919,
    "title": "[#8921][feat] Added symetric memory AllReduce strategy",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T19:28:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8919"
  },
  {
    "number": 8918,
    "title": "[None][chore] Weekly mass integration of release/1.1",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T19:20:34Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8918"
  },
  {
    "number": 8917,
    "title": "[https://nvbugs/5606136][fix] Fix torch.onnx.export with pytorch upgrade to fallback to dynamo=False.",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T18:41:18Z",
    "closed_at": "2025-11-04T22:11:48Z",
    "merged_at": "2025-11-04T22:11:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8917"
  },
  {
    "number": 8916,
    "title": "[None][fix] Multimodal InputProcessor dummy builder fix",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T16:48:09Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8916"
  },
  {
    "number": 8915,
    "title": "[None][ci] Add test on waives",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T15:36:43Z",
    "closed_at": "2025-11-05T00:42:08Z",
    "merged_at": "2025-11-05T00:42:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8915"
  },
  {
    "number": 8914,
    "title": "[None][fix] Remove duplicated test waives",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T14:56:11Z",
    "closed_at": "2025-11-04T15:04:33Z",
    "merged_at": "2025-11-04T15:04:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8914"
  },
  {
    "number": 8913,
    "title": "[None][feat] add swapsMmaAb sparseMla kernels",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T14:04:07Z",
    "closed_at": "2025-11-05T17:32:35Z",
    "merged_at": "2025-11-05T17:32:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8913"
  },
  {
    "number": 8912,
    "title": "[https://nvbugs/5631036][fix] debug the CI issue",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T14:01:54Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8912"
  },
  {
    "number": 8911,
    "title": "[https://nvbugs/5634220][fix] Add developer guide back and fix some i…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T13:04:22Z",
    "closed_at": "2025-11-05T02:17:02Z",
    "merged_at": "2025-11-05T02:17:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8911"
  },
  {
    "number": 8910,
    "title": "[https://nvbugs/5498478][fix] Fix eagle3 fp8 kv target model + bf16 draft model + chunked prefill",
    "user": "DylanChen-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T13:00:35Z",
    "closed_at": "2025-11-06T15:41:21Z",
    "merged_at": "2025-11-06T15:41:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8910"
  },
  {
    "number": 8909,
    "title": "[None][ci] Remove outdated test entries",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T11:35:45Z",
    "closed_at": "2025-11-04T13:32:46Z",
    "merged_at": "2025-11-04T13:32:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8909"
  },
  {
    "number": 8908,
    "title": "[https://nvbugs/5606266][test] move qwen3 multi-node test to the qa list",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T09:29:24Z",
    "closed_at": "2025-11-04T10:12:02Z",
    "merged_at": "2025-11-04T10:12:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8908"
  },
  {
    "number": 8907,
    "title": "[None][infra] update github token name",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T09:05:51Z",
    "closed_at": "2025-11-05T08:55:29Z",
    "merged_at": "2025-11-05T08:55:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8907"
  },
  {
    "number": 8906,
    "title": "[https://nvbugs/5543035][chore] Test on main",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T07:15:05Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8906"
  },
  {
    "number": 8905,
    "title": "[https://nvbugs/5543035][chore] Flaky test",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T06:59:02Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8905"
  },
  {
    "number": 8904,
    "title": "[https://nvbugs/5596343] [test] Waive flaky GPT-OSS cases",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T06:44:13Z",
    "closed_at": "2025-11-04T11:00:01Z",
    "merged_at": "2025-11-04T11:00:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8904"
  },
  {
    "number": 8903,
    "title": "[https://nvbugs/5628952][fix] avoid cudaFree overlap with cuda graph",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T06:31:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8903"
  },
  {
    "number": 8902,
    "title": "[None][chroe] Polish qwen3-next modeling code.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T06:29:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8902"
  },
  {
    "number": 8901,
    "title": "[https://nvbugs/5630700][chore] Unwaive Qwen3_235B_A22B test",
    "user": "shuyixiong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T06:26:54Z",
    "closed_at": "2025-11-06T07:32:39Z",
    "merged_at": "2025-11-06T07:32:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8901"
  },
  {
    "number": 8900,
    "title": "[https://nvbugs/5575920][fix] Fix cublas/cublasLt handle creation memory not sufficient error",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T06:12:04Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8900"
  },
  {
    "number": 8899,
    "title": "[TRTLLM-8263][feat] Add DSR1 and GPT-OSS Perf Tests into CI",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T05:57:34Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8899"
  },
  {
    "number": 8898,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T05:55:05Z",
    "closed_at": "2025-11-05T04:40:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8898"
  },
  {
    "number": 8897,
    "title": "[None][infra] Waive failed tests for main branch",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T05:47:21Z",
    "closed_at": "2025-11-04T06:21:28Z",
    "merged_at": "2025-11-04T06:21:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8897"
  },
  {
    "number": 8896,
    "title": "[None][infra] waive failed test on main 11/4",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T02:38:46Z",
    "closed_at": "2025-11-04T05:37:10Z",
    "merged_at": "2025-11-04T05:37:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8896"
  },
  {
    "number": 8895,
    "title": "[None][chore] Add tensorrt_llm/scripts to .gitignore",
    "user": "elvischenv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T02:22:18Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8895"
  },
  {
    "number": 8894,
    "title": "[None][feat] Integrate helix on main",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T02:06:15Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8894"
  },
  {
    "number": 8893,
    "title": "Fix links in quick start guide",
    "user": "laikhtewari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T01:40:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8893"
  },
  {
    "number": 8892,
    "title": "[None][fix] Moving transfer timeout test to test_llm_pytorch, fixing broken kv transfer timeout",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T00:41:58Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8892"
  },
  {
    "number": 8891,
    "title": "[https://nvbugs/5597647][fix] Fix MNNVL unit test failed due to accuracy issue on Hopper",
    "user": "timlee0212",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-04T00:39:31Z",
    "closed_at": "2025-11-06T17:28:07Z",
    "merged_at": "2025-11-06T17:28:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8891"
  },
  {
    "number": 8890,
    "title": "[None][fix] Add refresh blocks to Python executor",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T21:16:12Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8890"
  },
  {
    "number": 8889,
    "title": "[TRTLLM-8980][test] Clean up spec dec tests in test_llm_api_pytorch",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T20:54:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8889"
  },
  {
    "number": 8888,
    "title": "[https://nvbugs/5601682][fix] unwaive test_disaggregated_deepseek_v3_…",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T16:06:21Z",
    "closed_at": "2025-11-05T01:33:57Z",
    "merged_at": "2025-11-05T01:33:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8888"
  },
  {
    "number": 8887,
    "title": "[https://nvbugs/5630345][chore] unwaive DS-v32 nvfp4 and fp8 tests",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T15:53:28Z",
    "closed_at": "2025-11-05T11:49:23Z",
    "merged_at": "2025-11-05T11:49:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8887"
  },
  {
    "number": 8886,
    "title": "[None][feat] Enable EPLB for trtllm-gen and cutlass backend",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T13:51:07Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8886"
  },
  {
    "number": 8885,
    "title": "Update MoE kernels from flashinfer 0.5.0",
    "user": "nzmora-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T13:05:04Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8885"
  },
  {
    "number": 8884,
    "title": "[https://nvbugs/5461796][fix] Unwaive test test_llmapi_speculative_decoding_mtp",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T12:36:25Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8884"
  },
  {
    "number": 8883,
    "title": "[https://nvbugs/5467531][fix] Fix moe test and wide ep fake impl",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T12:34:53Z",
    "closed_at": "2025-11-06T03:40:50Z",
    "merged_at": "2025-11-06T03:40:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8883"
  },
  {
    "number": 8882,
    "title": "[None][feat] Add customized topk and related unit tests for DSA",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T12:20:30Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8882"
  },
  {
    "number": 8881,
    "title": "[https://nvbugs/5575920][fix] Fix cublas/cublasLt handle creation memory not sufficient error",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T12:04:26Z",
    "closed_at": "2025-11-04T06:10:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8881"
  },
  {
    "number": 8880,
    "title": "[None][feat] Integration of CuteDSL NVFP4 grouped GEMM",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T11:13:09Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8880"
  },
  {
    "number": 8879,
    "title": "[None][infra] Waive failed tests for release  branch on 11/03",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T10:09:44Z",
    "closed_at": "2025-11-03T10:59:34Z",
    "merged_at": "2025-11-03T10:59:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8879"
  },
  {
    "number": 8878,
    "title": "[https://nvbugs/5284463][fix] fix ada fp8 group gemm lacks shared memory",
    "user": "inocsin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T09:54:10Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8878"
  },
  {
    "number": 8877,
    "title": "[TRTLLM-9080][infra] upgrade tritonserver DLFW 25.10",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T09:42:44Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8877"
  },
  {
    "number": 8876,
    "title": "[https://nvbugs/5569754][chore] Adjust max batch size to prevent OOM",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T09:24:01Z",
    "closed_at": "2025-11-04T17:34:26Z",
    "merged_at": "2025-11-04T17:34:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8876"
  },
  {
    "number": 8875,
    "title": "[None][infra] Waive the failed test for main on 11/3",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T09:23:07Z",
    "closed_at": "2025-11-03T10:52:53Z",
    "merged_at": "2025-11-03T10:52:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8875"
  },
  {
    "number": 8874,
    "title": "[#8813][fix] Add missing event for block onboard for the kv cache tra…",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T07:32:34Z",
    "closed_at": "2025-11-04T08:23:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8874"
  },
  {
    "number": 8873,
    "title": "[https://nvbugs/5623960][fix] Fix the logger once key issue and further compress log in AutoTuner.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T06:52:51Z",
    "closed_at": "2025-11-05T07:25:43Z",
    "merged_at": "2025-11-05T07:25:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8873"
  },
  {
    "number": 8872,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T06:12:55Z",
    "closed_at": "2025-11-03T09:19:05Z",
    "merged_at": "2025-11-03T09:19:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8872"
  },
  {
    "number": 8870,
    "title": "[https://nvbugs/5624367][fix] Fix disagg GPT-OSS test",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T04:18:04Z",
    "closed_at": "2025-11-05T09:47:10Z",
    "merged_at": "2025-11-05T09:47:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8870"
  },
  {
    "number": 8869,
    "title": "[TRTLLM-9073/9087][doc] Add the missing content for model support section and fix valid links for long_sequence.md",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T04:14:26Z",
    "closed_at": "2025-11-03T10:06:05Z",
    "merged_at": "2025-11-03T10:06:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8869"
  },
  {
    "number": 8868,
    "title": "[None][infra] Modify wheel path from cuda13/ to dlfw/",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T03:26:41Z",
    "closed_at": "2025-11-03T08:55:42Z",
    "merged_at": "2025-11-03T08:55:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8868"
  },
  {
    "number": 8867,
    "title": "[https://nvbugs/5606266][fix] Unwaive some test ",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T03:07:22Z",
    "closed_at": "2025-11-03T05:43:59Z",
    "merged_at": "2025-11-03T05:43:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8867"
  },
  {
    "number": 8866,
    "title": "[None][update] optimized sparse mla kernels && fix unspecified cuda launch",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-03T02:42:12Z",
    "closed_at": "2025-11-03T06:27:00Z",
    "merged_at": "2025-11-03T06:26:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8866"
  },
  {
    "number": 8865,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-02T22:07:10Z",
    "closed_at": "2025-11-05T03:26:51Z",
    "merged_at": "2025-11-05T03:26:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8865"
  },
  {
    "number": 8864,
    "title": "[#8232][feat] AutoDeploy: Configurable MoE sharding dimension",
    "user": "galagam",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-02T17:46:19Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8864"
  },
  {
    "number": 8862,
    "title": "[https://nvbugs/5478158][fix] Disable paged context FMHA for INT4/AWQ + FP8 KV-cache in torch flow",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-02T12:14:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8862"
  },
  {
    "number": 8861,
    "title": "[TRTLLM-8970][infra] Fix generate report when has isolation test result",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-02T06:01:54Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8861"
  },
  {
    "number": 8860,
    "title": "[TRTLLM-8813][infra] Reduce GB200 multi-node test stages for release",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-02T03:13:19Z",
    "closed_at": "2025-11-05T07:29:29Z",
    "merged_at": "2025-11-05T07:29:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8860"
  },
  {
    "number": 8859,
    "title": "[None][infra] Check in most recent lock file for release/1.1",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-01T18:00:47Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8859"
  },
  {
    "number": 8858,
    "title": "[None][fix] WAR for tensorrt depending on the archived nvidia-cuda-runtime-cu13 package",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-01T16:27:21Z",
    "closed_at": "2025-11-02T01:21:02Z",
    "merged_at": "2025-11-02T01:21:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8858"
  },
  {
    "number": 8857,
    "title": "[None][fix] WAR for tensorrt depending on the archived nvidia-cuda-runtime-cu13 package",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-01T15:27:58Z",
    "closed_at": "2025-11-02T01:57:37Z",
    "merged_at": "2025-11-02T01:57:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8857"
  },
  {
    "number": 8856,
    "title": "[TRTLLM-9065][chore] remove PyTorchConfig completely",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-01T13:20:07Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8856"
  },
  {
    "number": 8855,
    "title": "[None][fix] Enhancing code robustness and adding boundary checks for ITensor",
    "user": "Fan-Yunfan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-01T07:58:27Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8855"
  },
  {
    "number": 8854,
    "title": "[https://nvbugs/5444687][fix] Cherrypick online EPLB CI fix from main to release 1.1",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-01T03:58:34Z",
    "closed_at": "2025-11-03T01:17:51Z",
    "merged_at": "2025-11-03T01:17:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8854"
  },
  {
    "number": 8853,
    "title": "[https://nvbugs/5625962][chore] unwaive DS-v32-fp4 tests",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-01T02:30:09Z",
    "closed_at": "2025-11-03T08:34:52Z",
    "merged_at": "2025-11-03T08:34:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8853"
  },
  {
    "number": 8852,
    "title": "Fix perf router feature for TP attention TRTLLM backend",
    "user": "jgangani",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-11-01T00:36:39Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8852"
  },
  {
    "number": 8851,
    "title": "[DO NOT MERGED, TEST ONLY] [https://nvbugs/5606136][fix] Fix regression with upgrading to torch-2.9",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T22:02:19Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8851"
  },
  {
    "number": 8850,
    "title": "[DO NOT MERGED, TEST ONLY][https://nvbugs/5606136][fix] Fix regression with upgrading to torch-2.9",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T21:57:29Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8850"
  },
  {
    "number": 8849,
    "title": "[TRTINFRA-7215][infra] - Move half of the DGX H100 premerge tests to SLURM",
    "user": "mlefeb01",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T19:29:46Z",
    "closed_at": "2025-11-03T16:11:27Z",
    "merged_at": "2025-11-03T16:11:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8849"
  },
  {
    "number": 8848,
    "title": "[None][chore] Refactor disagg integration tests",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T19:15:51Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8848"
  },
  {
    "number": 8847,
    "title": "[https://nvbugs/5474119][fix] Cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/8809",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T17:39:53Z",
    "closed_at": "2025-11-03T00:44:06Z",
    "merged_at": "2025-11-03T00:44:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8847"
  },
  {
    "number": 8846,
    "title": "[https://nvbugs/5625380][chore] Remove multimodal related fields from decoder llm input",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T17:18:45Z",
    "closed_at": "2025-11-03T01:44:08Z",
    "merged_at": "2025-11-03T01:44:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8846"
  },
  {
    "number": 8845,
    "title": "[None][feat] Add `trtllm_` prefix for exposed metrics",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T16:59:56Z",
    "closed_at": "2025-11-06T07:27:19Z",
    "merged_at": "2025-11-06T07:27:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8845"
  },
  {
    "number": 8844,
    "title": "[TRTLLM-1234][feat] Add fp8 blockscaled Gemm for sm120",
    "user": "CarstyYou",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T13:12:47Z",
    "closed_at": "2025-11-04T10:10:36Z",
    "merged_at": "2025-11-04T10:10:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8844"
  },
  {
    "number": 8842,
    "title": "[https://nvbugs/5596343] [test] Update accuracy baseline for GPT-OSS-20B",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T10:16:38Z",
    "closed_at": "2025-11-04T08:04:12Z",
    "merged_at": "2025-11-04T08:04:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8842"
  },
  {
    "number": 8841,
    "title": "[None][infra] Remove invaild waived tests which not in release branch",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T08:32:53Z",
    "closed_at": "2025-10-31T10:02:35Z",
    "merged_at": "2025-10-31T10:02:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8841"
  },
  {
    "number": 8840,
    "title": "[TRTLLM-8119][feat] Update doc/tests/chat_template for nano-v2-vlm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T08:05:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8840"
  },
  {
    "number": 8839,
    "title": "[None][test] add deepseek and qwen cases for rtx series",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T07:04:14Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8839"
  },
  {
    "number": 8838,
    "title": "[TRTLLM-8994][infra] upgrade to DLFW 25.10 and pytorch 2.9.0 / triton 3.5.0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T06:28:18Z",
    "closed_at": "2025-11-04T10:59:35Z",
    "merged_at": "2025-11-04T10:59:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8838"
  },
  {
    "number": 8837,
    "title": "[https://nvbugs/5521799][fix] add harmony channel validation",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T06:27:46Z",
    "closed_at": "2025-11-03T10:31:54Z",
    "merged_at": "2025-11-03T10:31:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8837"
  },
  {
    "number": 8836,
    "title": "[None][feat] Fix attention sink load in xqa",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T06:13:51Z",
    "closed_at": "2025-11-03T01:39:46Z",
    "merged_at": "2025-11-03T01:39:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8836"
  },
  {
    "number": 8835,
    "title": "[None][chore] Update test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T06:08:24Z",
    "closed_at": "2025-11-04T05:42:01Z",
    "merged_at": "2025-11-04T05:42:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8835"
  },
  {
    "number": 8834,
    "title": "[https://nvbugs/5606268][fix] Fix program exit segment fault triggered CublasMMWarpper deconstructor",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T06:04:34Z",
    "closed_at": "2025-11-03T06:46:01Z",
    "merged_at": "2025-11-03T06:46:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8834"
  },
  {
    "number": 8833,
    "title": "[None][infra] Skip failed tests for release branch",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T05:50:04Z",
    "closed_at": "2025-10-31T07:04:55Z",
    "merged_at": "2025-10-31T07:04:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8833"
  },
  {
    "number": 8832,
    "title": "[https://nvbugs/5461796][fix] Unwaive test test_llmapi_speculative_decoding_mtp",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T05:34:20Z",
    "closed_at": "2025-11-03T08:53:25Z",
    "merged_at": "2025-11-03T08:53:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8832"
  },
  {
    "number": 8831,
    "title": "[https://nvbugs/5608930][fix] Unwaive test 5608930",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T05:23:45Z",
    "closed_at": "2025-11-03T07:09:58Z",
    "merged_at": "2025-11-03T07:09:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8831"
  },
  {
    "number": 8830,
    "title": "Maybe fix for improper reuse",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T03:50:24Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8830"
  },
  {
    "number": 8829,
    "title": "[https://nvbugs/5625990][chore] Add test coverage for current incapability of the KV cache manager",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T03:46:33Z",
    "closed_at": "2025-11-04T08:35:46Z",
    "merged_at": "2025-11-04T08:35:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8829"
  },
  {
    "number": 8828,
    "title": "[None][chore] waive deepseek-v32 nvfp4 test",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T03:25:25Z",
    "closed_at": "2025-10-31T03:42:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8828"
  },
  {
    "number": 8827,
    "title": "[None][fix] Fix import issues in layer-wise benchmarks",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T03:22:43Z",
    "closed_at": "2025-11-03T10:32:49Z",
    "merged_at": "2025-11-03T10:32:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8827"
  },
  {
    "number": 8826,
    "title": "[None][info] Waive failed case for main",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T03:20:54Z",
    "closed_at": "2025-10-31T03:44:00Z",
    "merged_at": "2025-10-31T03:43:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8826"
  },
  {
    "number": 8825,
    "title": "[None][chore] Add sample yaml for wide-ep example and minor fixes",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T03:13:27Z",
    "closed_at": "2025-11-03T15:48:34Z",
    "merged_at": "2025-11-03T15:48:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8825"
  },
  {
    "number": 8824,
    "title": "[None][chore] Add sample yaml for wide-ep example",
    "user": "zerollzeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T02:26:27Z",
    "closed_at": "2025-10-31T05:35:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8824"
  },
  {
    "number": 8823,
    "title": "[None][fix] Waive layer-wise benchmark tests",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T02:23:56Z",
    "closed_at": "2025-10-31T05:51:32Z",
    "merged_at": "2025-10-31T05:51:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8823"
  },
  {
    "number": 8822,
    "title": "[None][perf] AutoDeploy optimize _get_unique_value",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T02:07:19Z",
    "closed_at": "2025-10-31T11:57:10Z",
    "merged_at": "2025-10-31T11:57:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8822"
  },
  {
    "number": 8820,
    "title": "[TRTLLM-8814][feat] AutoDeploy: Use TRTLLM kernels for FP8 linear",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-31T00:11:25Z",
    "closed_at": "2025-11-06T19:00:11Z",
    "merged_at": "2025-11-06T19:00:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8820"
  },
  {
    "number": 8818,
    "title": "[None] Change NIXL build type to release",
    "user": "tanmayv25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T23:05:04Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8818"
  },
  {
    "number": 8817,
    "title": "[None][fixes] Add tool call parsing fixes and Qwen3 coder parser",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T22:36:45Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8817"
  },
  {
    "number": 8816,
    "title": "[TRTLLM-8772][chore] Fuse indexer q proj into MLA Q up_proj",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T22:36:34Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8816"
  },
  {
    "number": 8815,
    "title": "[None][chore] Add error message for multiple response sampler_param with PyTorch backend",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T22:12:17Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8815"
  },
  {
    "number": 8812,
    "title": "[#8763][fix] AutoDeploy: correct mamba cache dtype extraction",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T20:56:02Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8812"
  },
  {
    "number": 8810,
    "title": "[None][feat] Make 2-model spec dec use the 1-model kernels (Hopper)",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T19:44:13Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8810"
  },
  {
    "number": 8809,
    "title": "[https://nvbugs/5474119][fix] Re-enable test",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T19:31:06Z",
    "closed_at": "2025-10-31T17:17:58Z",
    "merged_at": "2025-10-31T17:17:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8809"
  },
  {
    "number": 8808,
    "title": "[None][infra] Update allow list 20251030",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T17:56:10Z",
    "closed_at": "2025-10-30T23:41:52Z",
    "merged_at": "2025-10-30T23:41:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8808"
  },
  {
    "number": 8807,
    "title": "[#8476][chore] Update license",
    "user": "karljang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T17:34:34Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8807"
  },
  {
    "number": 8806,
    "title": "[https://nvbugs/5587574][fix] Increase server timeout to wait for weight loading",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T17:33:28Z",
    "closed_at": "2025-11-04T20:11:08Z",
    "merged_at": "2025-11-04T20:11:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8806"
  },
  {
    "number": 8805,
    "title": "[https://nvbugs/5527655][feat] Add NUMA-aware CPU affinity autoconfig",
    "user": "dhansen-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T17:22:05Z",
    "closed_at": "2025-11-06T19:59:47Z",
    "merged_at": "2025-11-06T19:59:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8805"
  },
  {
    "number": 8804,
    "title": "Trigger CI sanity check, do not merge",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T17:02:12Z",
    "closed_at": "2025-10-31T03:26:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8804"
  },
  {
    "number": 8803,
    "title": "[#8781][fix] Cache the AllReduce wrapper to avoid re-allocating workspace which caused a hang",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T15:53:15Z",
    "closed_at": "2025-11-02T13:30:39Z",
    "merged_at": "2025-11-02T13:30:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8803"
  },
  {
    "number": 8801,
    "title": "[https://nvbugs/5614506][chore] Adding e+p+d e2e test",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T14:56:58Z",
    "closed_at": "2025-10-31T16:52:43Z",
    "merged_at": "2025-10-31T16:52:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8801"
  },
  {
    "number": 8800,
    "title": "[TRTLLM-9000][feat] Add multi-node Perf Tests into CI",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T14:54:05Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8800"
  },
  {
    "number": 8799,
    "title": "[None][fix] Layer-wise benchmarks: use local models, lint",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T13:40:25Z",
    "closed_at": "2025-10-30T16:47:47Z",
    "merged_at": "2025-10-30T16:47:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8799"
  },
  {
    "number": 8798,
    "title": "[TRTLLM-9001][feat] add TP support for DeepSeek-V3.2",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T13:22:13Z",
    "closed_at": "2025-11-05T09:10:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8798"
  },
  {
    "number": 8797,
    "title": "[None][infra] Waive failed case for main branch",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T13:22:11Z",
    "closed_at": "2025-10-30T14:57:36Z",
    "merged_at": "2025-10-30T14:57:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8797"
  },
  {
    "number": 8796,
    "title": "[TRTLLM-8971][infra] Cherry-pick for Update gpu key for B300/GB300 (#8724)",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T12:40:43Z",
    "closed_at": "2025-10-30T13:12:17Z",
    "merged_at": "2025-10-30T13:12:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8796"
  },
  {
    "number": 8795,
    "title": "[https://nvbugs/5623960][fix] Compress the warning log of AutoTuner when encountering tactic failures.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T12:22:48Z",
    "closed_at": "2025-10-31T04:55:56Z",
    "merged_at": "2025-10-31T04:55:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8795"
  },
  {
    "number": 8794,
    "title": "[https://nvbugs/5508536][fix] Take Over (#8627): Reintroduce: Move stop_criteria to sample_async (#7041)",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T12:11:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8794"
  },
  {
    "number": 8793,
    "title": "[https://nvbugs/5623960][fix] Compress the warning log of AutoTuner when encountering tactic failures.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T11:53:24Z",
    "closed_at": "2025-10-31T03:09:14Z",
    "merged_at": "2025-10-31T03:09:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8793"
  },
  {
    "number": 8792,
    "title": "[None][ci] Match b300 for B300 test list",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T10:50:28Z",
    "closed_at": "2025-10-30T12:01:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8792"
  },
  {
    "number": 8791,
    "title": "[https://nvbugs/5612504][chore] refactor cluster env check logic",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T10:26:06Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8791"
  },
  {
    "number": 8790,
    "title": "[None][infra] install mooncake in docker images",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T09:59:52Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8790"
  },
  {
    "number": 8789,
    "title": "[None][fix] Fix bug of undefined py_topk_logprobs_vals",
    "user": "dcaox",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T09:46:24Z",
    "closed_at": "2025-11-04T11:32:59Z",
    "merged_at": "2025-11-04T11:32:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8789"
  },
  {
    "number": 8788,
    "title": "[None][chore] use cached vila model",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T09:41:46Z",
    "closed_at": "2025-10-31T03:26:46Z",
    "merged_at": "2025-10-31T03:26:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8788"
  },
  {
    "number": 8787,
    "title": "[#8781][fix] DRAFT - DO NOT REVIEW fixed hang mnnvl with cudagraph by deferring the ws init from the AllReduce init",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T09:24:50Z",
    "closed_at": "2025-10-30T15:31:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8787"
  },
  {
    "number": 8786,
    "title": "[https://nvbugs/5575687][fix] fix moe_gemm's preexit position that cause illegal memory access",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T08:57:15Z",
    "closed_at": "2025-10-31T01:08:23Z",
    "merged_at": "2025-10-31T01:08:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8786"
  },
  {
    "number": 8785,
    "title": "[None][feat] Return logprobs incrementally in torch backend",
    "user": "dcaox",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T08:23:12Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8785"
  },
  {
    "number": 8783,
    "title": "[None][fix] Rename: slot_count -> invalid_expert_id",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T07:58:08Z",
    "closed_at": "2025-11-01T13:36:59Z",
    "merged_at": "2025-11-01T13:36:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8783"
  },
  {
    "number": 8780,
    "title": "[https://nvbugs/5570599][fix] Set KVCache free_gpu_memory_fraction fo…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T07:23:21Z",
    "closed_at": "2025-11-06T13:58:08Z",
    "merged_at": "2025-11-06T13:58:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8780"
  },
  {
    "number": 8779,
    "title": "[TRTLLM-7963][feat] Cold L2 cache when doing autotune benchmarking.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T07:19:58Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8779"
  },
  {
    "number": 8778,
    "title": "[TRTLLM-8999][infra] Reduce gb200 multi-node test stages",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T07:06:37Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8778"
  },
  {
    "number": 8777,
    "title": "[None][feat] Add layer wise benchmarks",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T06:35:47Z",
    "closed_at": "2025-10-30T12:29:34Z",
    "merged_at": "2025-10-30T12:29:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8777"
  },
  {
    "number": 8776,
    "title": "[None][feat] Add benchmark to DeepConf",
    "user": "dcaox",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T04:57:48Z",
    "closed_at": "2025-11-03T08:05:51Z",
    "merged_at": "2025-11-03T08:05:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8776"
  },
  {
    "number": 8775,
    "title": "[None][infra] Unwaive the tests passed in latest CI and disable a perf stage",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T03:31:56Z",
    "closed_at": "2025-10-30T04:48:23Z",
    "merged_at": "2025-10-30T04:48:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8775"
  },
  {
    "number": 8774,
    "title": "[https://nvbugs/5481206][fix] update waives",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T01:50:23Z",
    "closed_at": "2025-10-30T07:43:39Z",
    "merged_at": "2025-10-30T07:43:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8774"
  },
  {
    "number": 8773,
    "title": "[None][infra] fix docker.multi",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T01:16:15Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8773"
  },
  {
    "number": 8772,
    "title": "[https://nvbugs/5606166][fix] AutoDeploy: use tuples for cudagraph shape lookup",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T00:30:16Z",
    "closed_at": "2025-10-31T12:59:49Z",
    "merged_at": "2025-10-31T12:59:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8772"
  },
  {
    "number": 8771,
    "title": "[TRTLLM-8768][chore] Fuse QK down_proj with indexer K + weight_proj for FP4 ckpt",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-30T00:15:34Z",
    "closed_at": "2025-11-05T15:57:10Z",
    "merged_at": "2025-11-05T15:57:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8771"
  },
  {
    "number": 8770,
    "title": "[TRTINFRA-7215][infra] Add support for enroot SLURM clusters",
    "user": "mlefeb01",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T23:44:25Z",
    "closed_at": "2025-10-31T19:22:22Z",
    "merged_at": "2025-10-31T19:22:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8770"
  },
  {
    "number": 8769,
    "title": "[None] [doc] Add Mixed Precision Context and Generation section to Disagg",
    "user": "timothygao8710",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T21:11:43Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8769"
  },
  {
    "number": 8768,
    "title": "[None][doc] Minor doc update to disagg-serving",
    "user": "schetlur-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T20:19:36Z",
    "closed_at": "2025-10-30T00:38:07Z",
    "merged_at": "2025-10-30T00:38:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8768"
  },
  {
    "number": 8767,
    "title": "[TRTLLM-8979][test] Improve qwen3 spec dec test coverage",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T20:01:23Z",
    "closed_at": "2025-11-03T18:12:10Z",
    "merged_at": "2025-11-03T18:12:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8767"
  },
  {
    "number": 8766,
    "title": "[TRTLLM-8978][test] Remove llama 4 spec dec tests",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T18:45:10Z",
    "closed_at": "2025-10-30T19:47:05Z",
    "merged_at": "2025-10-30T19:47:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8766"
  },
  {
    "number": 8765,
    "title": "[TRTLLM-8988][feat] Unify MPI & Ray's req/response handling with RPC Client/Server",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T17:53:58Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8765"
  },
  {
    "number": 8764,
    "title": "[https://nvbugs/5582133][fix] Fix protocol violation in AgentConnectionManager",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T16:51:24Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8764"
  },
  {
    "number": 8762,
    "title": "[https://nvbugs/5569696][fix] Fix Gemma3 accuracy test failure",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T15:51:37Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8762"
  },
  {
    "number": 8761,
    "title": "[None][fix] Fix cute dsl nvfp4 gemm autotune issue",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T13:25:10Z",
    "closed_at": "2025-11-03T14:55:46Z",
    "merged_at": "2025-11-03T14:55:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8761"
  },
  {
    "number": 8760,
    "title": "[None][infra] Waive failed tests for release branch 10/29",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T12:58:25Z",
    "closed_at": "2025-10-29T13:32:31Z",
    "merged_at": "2025-10-29T13:32:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8760"
  },
  {
    "number": 8759,
    "title": "[None][infra] Waive failed tests on main 10/29",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T12:40:42Z",
    "closed_at": "2025-10-29T13:32:24Z",
    "merged_at": "2025-10-29T13:32:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8759"
  },
  {
    "number": 8758,
    "title": "[https://nvbugs/5488118][fix] Unwaive passed tests",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T10:05:05Z",
    "closed_at": "2025-10-31T02:46:45Z",
    "merged_at": "2025-10-31T02:46:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8758"
  },
  {
    "number": 8756,
    "title": "[None][fix] Fix UnboundLocalError.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T08:36:46Z",
    "closed_at": "2025-10-30T02:41:38Z",
    "merged_at": "2025-10-30T02:41:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8756"
  },
  {
    "number": 8755,
    "title": "[https://nvbugs/5547414][fix] Use cached models",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T08:02:04Z",
    "closed_at": "2025-10-30T02:10:11Z",
    "merged_at": "2025-10-30T02:10:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8755"
  },
  {
    "number": 8754,
    "title": "[None][fix] add readme copy to wheel stage to avoid setup.py failure (cherry-pick #8736)",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T06:56:08Z",
    "closed_at": "2025-10-29T07:27:37Z",
    "merged_at": "2025-10-29T07:27:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8754"
  },
  {
    "number": 8753,
    "title": "[TRTLLM-8991][test] Add Llama 3.3 70B model with different performance config",
    "user": "yufeiwu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T06:35:48Z",
    "closed_at": "2025-11-03T05:34:06Z",
    "merged_at": "2025-11-03T05:34:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8753"
  },
  {
    "number": 8752,
    "title": "[TRTLLM-8930][infra] Force Blossom perf test stages to use 'tensorrt/test_type: perf' in the K8S template",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T06:04:22Z",
    "closed_at": "2025-10-30T13:30:11Z",
    "merged_at": "2025-10-30T13:30:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8752"
  },
  {
    "number": 8751,
    "title": "[None][infra] fix slurm results path",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T04:57:08Z",
    "closed_at": "2025-10-30T05:09:47Z",
    "merged_at": "2025-10-30T05:09:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8751"
  },
  {
    "number": 8750,
    "title": "[None][fix] Fix KV cache clearing with KV Connector API",
    "user": "jthomson04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T04:45:34Z",
    "closed_at": "2025-11-06T22:28:28Z",
    "merged_at": "2025-11-06T22:28:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8750"
  },
  {
    "number": 8749,
    "title": "[None][infra] update ci allow list 2025/10/29",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T03:55:47Z",
    "closed_at": "2025-10-29T07:34:45Z",
    "merged_at": "2025-10-29T07:34:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8749"
  },
  {
    "number": 8748,
    "title": "[None][chore] Design diagram review process change",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T03:46:01Z",
    "closed_at": "2025-11-05T00:38:35Z",
    "merged_at": "2025-11-05T00:38:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8748"
  },
  {
    "number": 8747,
    "title": "[None][doc] update ucx env description in doc",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T03:44:45Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8747"
  },
  {
    "number": 8746,
    "title": "Draft: PostNorm and multilayer options",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T03:38:28Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8746"
  },
  {
    "number": 8745,
    "title": "[None][ci] waive test_rpc.py",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T03:03:36Z",
    "closed_at": "2025-10-29T12:17:41Z",
    "merged_at": "2025-10-29T12:17:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8745"
  },
  {
    "number": 8744,
    "title": "[TRTLLM-8201][feat] Nemotron H MoE Sharding",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T02:39:45Z",
    "closed_at": "2025-11-05T20:35:29Z",
    "merged_at": "2025-11-05T20:35:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8744"
  },
  {
    "number": 8743,
    "title": "[None][ci] waive test_rpc.py temporarily",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-29T01:50:44Z",
    "closed_at": "2025-10-29T02:20:27Z",
    "merged_at": "2025-10-29T02:20:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8743"
  },
  {
    "number": 8739,
    "title": "[None][infra] Check in most recent lock file from nightly pipeline",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T20:34:33Z",
    "closed_at": "2025-10-29T19:30:36Z",
    "merged_at": "2025-10-29T19:30:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8739"
  },
  {
    "number": 8738,
    "title": "[https://nvbugs/5617275][fix] Extract py files from prebuilt wheel for editable installs",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T20:24:38Z",
    "closed_at": "2025-10-31T04:40:22Z",
    "merged_at": "2025-10-31T04:40:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8738"
  },
  {
    "number": 8737,
    "title": "[TRTLLM-8734][feat] AutoDeploy: Enable the nvfp4 for Nemotron MOE",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T19:19:14Z",
    "closed_at": "2025-10-30T19:33:09Z",
    "merged_at": "2025-10-30T19:33:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8737"
  },
  {
    "number": 8736,
    "title": "[None][fix] add readme copy to wheel stage to avoid setup.py failure",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T19:09:20Z",
    "closed_at": "2025-10-29T06:43:03Z",
    "merged_at": "2025-10-29T06:43:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8736"
  },
  {
    "number": 8735,
    "title": "[TRTLLM-8540][feat] Add support for disagg in DSv3.2",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T16:55:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8735"
  },
  {
    "number": 8731,
    "title": "test ci models",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T13:23:02Z",
    "closed_at": "2025-10-29T08:19:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8731"
  },
  {
    "number": 8729,
    "title": "[None][fix] fix config loading for DeepSeek-V3.2 in trtllm-bench",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T12:20:16Z",
    "closed_at": "2025-10-29T12:17:17Z",
    "merged_at": "2025-10-29T12:17:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8729"
  },
  {
    "number": 8728,
    "title": "[None][feat] Integrate MnnvlThroughput into TRTLLM MoE.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T11:54:34Z",
    "closed_at": "2025-11-04T13:36:30Z",
    "merged_at": "2025-11-04T13:36:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8728"
  },
  {
    "number": 8727,
    "title": "[https://nvbugs/5422621][fix] fix EPLB init hang (cherry-pick #8649)",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T11:51:40Z",
    "closed_at": "2025-10-30T02:31:35Z",
    "merged_at": "2025-10-30T02:31:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8727"
  },
  {
    "number": 8726,
    "title": "[None][test] fix a typo in perf test sampler config",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T11:44:23Z",
    "closed_at": "2025-10-29T01:53:54Z",
    "merged_at": "2025-10-29T01:53:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8726"
  },
  {
    "number": 8725,
    "title": "[None][chore] Revert \"[TRTLLM-7835][test] add default sample config for perf test (#8523)",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T11:30:44Z",
    "closed_at": "2025-10-28T13:23:39Z",
    "merged_at": "2025-10-28T13:23:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8725"
  },
  {
    "number": 8724,
    "title": "[TRTLLM-8971][infra] Update gpu key for B300/GB300",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T10:49:23Z",
    "closed_at": "2025-10-30T03:36:45Z",
    "merged_at": "2025-10-30T03:36:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8724"
  },
  {
    "number": 8723,
    "title": "[https://nvbugs/5613089][fix] Fix the rank to access all_rank_chunk_size_list when chunked MoE is used",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T10:04:58Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8723"
  },
  {
    "number": 8721,
    "title": "MXFP4 x BF16 CUTLASS MoE backend perf and profiling improvement on Hopper",
    "user": "StudyingShao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T09:27:52Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8721"
  },
  {
    "number": 8720,
    "title": "[TRTLLM-7008][fix] Enable GDRCopy and unwaive online eplb tests",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T09:18:20Z",
    "closed_at": "2025-10-31T23:39:52Z",
    "merged_at": "2025-10-31T23:39:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8720"
  },
  {
    "number": 8719,
    "title": "[None][fix] fix dockerfile: resolve installation issue caused by setup.py",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T09:07:02Z",
    "closed_at": "2025-10-29T03:37:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8719"
  },
  {
    "number": 8718,
    "title": "[None][feat] Add unit tests and revision in block_level kernel for invalid input",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T08:36:22Z",
    "closed_at": "2025-10-30T08:42:19Z",
    "merged_at": "2025-10-30T08:42:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8718"
  },
  {
    "number": 8717,
    "title": "[TRTLLM-8763][chore] Deprecate pybind based GuidedDecodingConfig usage in torch backend",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T08:18:40Z",
    "closed_at": "2025-10-29T12:37:15Z",
    "merged_at": "2025-10-29T12:37:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8717"
  },
  {
    "number": 8716,
    "title": "[None][chore] Add unittest for otlp tracing",
    "user": "zhanghaotong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T07:56:28Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8716"
  },
  {
    "number": 8714,
    "title": "[TRTLLM-8920][feat] decouple disagg service from fastapi",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T07:51:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8714"
  },
  {
    "number": 8712,
    "title": "[None][chore] Enable GPQA in CI for DeepSeek V3.2",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T06:40:13Z",
    "closed_at": "2025-10-29T11:22:22Z",
    "merged_at": "2025-10-29T11:22:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8712"
  },
  {
    "number": 8711,
    "title": "[None][fix] enhance RPC",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T06:11:22Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8711"
  },
  {
    "number": 8710,
    "title": "[https://nvbugs/5612529][fix] Fix transferAgent_test",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T05:54:53Z",
    "closed_at": "2025-10-29T01:14:34Z",
    "merged_at": "2025-10-29T01:14:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8710"
  },
  {
    "number": 8709,
    "title": "[https://nvbugs/5325296][fix] Enable relaxed acceptance test on Blackwell",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T05:27:02Z",
    "closed_at": "2025-10-31T22:02:07Z",
    "merged_at": "2025-10-31T22:02:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8709"
  },
  {
    "number": 8708,
    "title": "[https://nvbugs/5550409][fix] Disable torch compile in piecewise attention part to Avoid host overhead",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T03:00:21Z",
    "closed_at": "2025-10-29T10:12:59Z",
    "merged_at": "2025-10-29T10:12:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8708"
  },
  {
    "number": 8707,
    "title": "[https://nvbugs/5599086][fix] Fix FP8 Linear module for spark",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T01:56:58Z",
    "closed_at": "2025-10-29T20:58:20Z",
    "merged_at": "2025-10-29T20:58:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8707"
  },
  {
    "number": 8706,
    "title": "[TRTLLM-8084][feat] Enhance the overlap shceduler for two-model spec decoding",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T01:26:19Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8706"
  },
  {
    "number": 8705,
    "title": "[None][fix] InputProcessor config naming convention fix",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T01:23:23Z",
    "closed_at": "2025-11-04T06:29:22Z",
    "merged_at": "2025-11-04T06:29:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8705"
  },
  {
    "number": 8704,
    "title": "[https://nvbugs/5608461][fix] exclude InductorSubproc from thread leak check",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T01:06:01Z",
    "closed_at": "2025-10-30T07:35:15Z",
    "merged_at": "2025-10-30T07:35:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8704"
  },
  {
    "number": 8703,
    "title": "[TRTLLM-8136][feat] Dynamic draft length in spec decode (stage 2).",
    "user": "zheyuf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-28T00:31:20Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8703"
  },
  {
    "number": 8702,
    "title": "[https://nvbugs/5596343][test] Update test waive to get back some coverage",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T23:59:32Z",
    "closed_at": "2025-10-28T21:05:48Z",
    "merged_at": "2025-10-28T21:05:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8702"
  },
  {
    "number": 8701,
    "title": "[None][perf] Use fp8 quant kernel in DS3.2 indexer module",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T23:59:29Z",
    "closed_at": "2025-10-29T04:45:10Z",
    "merged_at": "2025-10-29T04:45:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8701"
  },
  {
    "number": 8699,
    "title": "[TRTLLM-8976][feat] Move indexer-k-cache to KVCacheManager",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T22:03:14Z",
    "closed_at": "2025-10-29T15:04:27Z",
    "merged_at": "2025-10-29T15:04:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8699"
  },
  {
    "number": 8698,
    "title": "[OMNIML-2932] [feat] nvfp4 awq support",
    "user": "meenchen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T21:23:21Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8698"
  },
  {
    "number": 8697,
    "title": "[None][fix] Properly raise error for nemotron H models",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T21:14:29Z",
    "closed_at": "2025-10-28T15:59:43Z",
    "merged_at": "2025-10-28T15:59:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8697"
  },
  {
    "number": 8696,
    "title": "[#8694][fix] fix AutoDeploy cuda memory access failure in NemotronHMoe",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T15:15:32Z",
    "closed_at": "2025-10-28T11:21:44Z",
    "merged_at": "2025-10-28T11:21:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8696"
  },
  {
    "number": 8695,
    "title": "[#8694][fix] fix AutoDeploy cuda memory access failure in Nemotron Moe",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T15:11:35Z",
    "closed_at": "2025-10-27T15:15:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8695"
  },
  {
    "number": 8692,
    "title": "[TRTLLM-8541][feat] Add trtllm-gen sparse MLA kernels to support per-Tensor FP8 KV Cache",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T14:31:14Z",
    "closed_at": "2025-10-31T21:38:32Z",
    "merged_at": "2025-10-31T21:38:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8692"
  },
  {
    "number": 8691,
    "title": "[TRTLLM-8690][feat] add more tensors to share buffers",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T14:27:09Z",
    "closed_at": "2025-11-04T05:08:01Z",
    "merged_at": "2025-11-04T05:08:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8691"
  },
  {
    "number": 8690,
    "title": "[None][chore] ISOLATE some cases",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T12:59:43Z",
    "closed_at": "2025-10-28T02:10:45Z",
    "merged_at": "2025-10-28T02:10:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8690"
  },
  {
    "number": 8689,
    "title": "[https://nvbugs/5570599][fix] Limit kvcache tokens to avoid OOM",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T12:55:18Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8689"
  },
  {
    "number": 8688,
    "title": "[TRTLLM-8201][feat] TP sharding of Mamba layers",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T12:41:56Z",
    "closed_at": "2025-10-29T02:40:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8688"
  },
  {
    "number": 8687,
    "title": "[https://nvbugs/5599515][fix] Fix PP bubbles.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T11:34:42Z",
    "closed_at": "2025-10-31T02:13:56Z",
    "merged_at": "2025-10-31T02:13:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8687"
  },
  {
    "number": 8686,
    "title": "[None][infra] Skip failed tests for main 10/27",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T10:44:05Z",
    "closed_at": "2025-10-28T00:04:30Z",
    "merged_at": "2025-10-28T00:04:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8686"
  },
  {
    "number": 8685,
    "title": "[https://nvbugs/5606268][fix] Separate cuda graph workspace to prevent IMA",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T09:33:46Z",
    "closed_at": "2025-10-29T08:43:31Z",
    "merged_at": "2025-10-29T08:43:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8685"
  },
  {
    "number": 8684,
    "title": "[https://nvbugs/5613456][chore] Skip test_ptp_quickstart_advanced_multi_gpus[DeepSeek-V3-671B-FP8-DeepSeek-V3-0324-8] due to Model Creation OOM",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T09:16:16Z",
    "closed_at": "2025-10-29T03:30:01Z",
    "merged_at": "2025-10-29T03:30:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8684"
  },
  {
    "number": 8683,
    "title": "[https://nvbugs/5612438][fix] Add timeout for SeedOSS test",
    "user": "zhhuang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T09:05:36Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8683"
  },
  {
    "number": 8682,
    "title": "[None] [feat] Use triton kernels for RocketKV prediction module",
    "user": "heyuhhh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T09:01:54Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8682"
  },
  {
    "number": 8681,
    "title": "[https://nvbugs/5570599][fix] Only use cuda_graph_warmup_block when cuda graph is enabled",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T08:50:17Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8681"
  },
  {
    "number": 8680,
    "title": "[https://nvbugs/5549829][fix] Qwen2.5-VL TP > 1 + Quantized weight load fix",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T08:20:29Z",
    "closed_at": "2025-10-29T04:38:43Z",
    "merged_at": "2025-10-29T04:38:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8680"
  },
  {
    "number": 8679,
    "title": "[https://nvbugs/5608489][fix] Fix output unpack issues for Llama3/4 NVFP4 models.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T07:09:44Z",
    "closed_at": "2025-10-28T06:21:47Z",
    "merged_at": "2025-10-28T06:21:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8679"
  },
  {
    "number": 8678,
    "title": "[TRTLLM-8933][chore] remove unused update_executor_config function",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T06:15:04Z",
    "closed_at": "2025-10-27T14:00:48Z",
    "merged_at": "2025-10-27T14:00:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8678"
  },
  {
    "number": 8677,
    "title": "[https://nvbugs/5546507][fix] skip test_mistral_nemo_eagle_1gpu test cases due to CMake Error in building",
    "user": "jieli-matrix",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T05:43:20Z",
    "closed_at": "2025-10-27T09:11:48Z",
    "merged_at": "2025-10-27T09:11:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8677"
  },
  {
    "number": 8676,
    "title": "[None][docs] Update Python wheel's short-/long-descriptions",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T03:10:04Z",
    "closed_at": "2025-10-27T06:58:49Z",
    "merged_at": "2025-10-27T06:58:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8676"
  },
  {
    "number": 8675,
    "title": "[TRTLLM-8827] [feat] Enable low precision alltoall for Cutlass and TRTLLMGen backends",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T02:35:04Z",
    "closed_at": "2025-10-28T23:56:48Z",
    "merged_at": "2025-10-28T23:56:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8675"
  },
  {
    "number": 8674,
    "title": "[https://nvbugs/5572099][fix] WAR MPI issue when running pytest",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T02:29:33Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8674"
  },
  {
    "number": 8673,
    "title": "[https://nvbugs/5608723][fix] Use local data on multimodal tests and unwaive tests",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T01:43:36Z",
    "closed_at": "2025-10-28T00:20:03Z",
    "merged_at": "2025-10-28T00:20:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8673"
  },
  {
    "number": 8672,
    "title": "[TRTLLM-8638][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-27T01:06:52Z",
    "closed_at": "2025-10-27T07:20:47Z",
    "merged_at": "2025-10-27T07:20:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8672"
  },
  {
    "number": 8671,
    "title": "[https://nvbugs/5572320][fix] Ported test_ad_trtllm_bench.py from main",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-26T14:34:57Z",
    "closed_at": "2025-10-28T07:41:32Z",
    "merged_at": "2025-10-28T07:41:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8671"
  },
  {
    "number": 8670,
    "title": "[https://nvbugs/5572320][fix] Ported test_ad_trtllm_bench.py from main",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-26T14:28:30Z",
    "closed_at": "2025-10-26T14:30:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8670"
  },
  {
    "number": 8669,
    "title": "[TRTLLM-8638][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-26T08:43:24Z",
    "closed_at": "2025-10-27T06:36:29Z",
    "merged_at": "2025-10-27T06:36:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8669"
  },
  {
    "number": 8668,
    "title": "[None][infra] Waive failed case on main 10/26",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-26T05:29:38Z",
    "closed_at": "2025-10-26T14:02:33Z",
    "merged_at": "2025-10-26T14:02:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8668"
  },
  {
    "number": 8667,
    "title": "[None][feat] Autodeploy: Update the ssm to use slice",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-26T05:02:47Z",
    "closed_at": "2025-10-27T16:45:21Z",
    "merged_at": "2025-10-27T16:45:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8667"
  },
  {
    "number": 8666,
    "title": "[https://nvbugs/5521253][fix] Enable Gemma3 12B & 27B on SM100",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-26T02:00:34Z",
    "closed_at": "2025-11-03T22:49:37Z",
    "merged_at": "2025-11-03T22:49:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8666"
  },
  {
    "number": 8665,
    "title": "[None][doc] Clarify the perf best practice and supported hardware for gptoss",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-25T21:18:00Z",
    "closed_at": "2025-10-31T17:11:59Z",
    "merged_at": "2025-10-31T17:11:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8665"
  },
  {
    "number": 8664,
    "title": "[https://nvbugs/5575913][fix] Use separate thresholds for 120b/20b gptoss",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-25T21:11:41Z",
    "closed_at": "2025-10-28T14:35:08Z",
    "merged_at": "2025-10-28T14:35:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8664"
  },
  {
    "number": 8663,
    "title": "[None][feat] Autotuner can iterate through all tactics for test purposes",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-25T15:53:09Z",
    "closed_at": "2025-10-30T12:11:25Z",
    "merged_at": "2025-10-30T12:11:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8663"
  },
  {
    "number": 8662,
    "title": "[None] [feat] [Do Not Review] Add C++ dependency scanning system",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-25T06:40:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8662"
  },
  {
    "number": 8661,
    "title": "[None][test] Enhance GPT-OSS CI with GPQA Diamond and additional Spec Decoding Test",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T21:41:59Z",
    "closed_at": "2025-11-03T00:44:03Z",
    "merged_at": "2025-11-03T00:44:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8661"
  },
  {
    "number": 8660,
    "title": "[None][chore] Use a cached model path for Ray integration test",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T20:55:23Z",
    "closed_at": "2025-10-28T02:16:07Z",
    "merged_at": "2025-10-28T02:16:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8660"
  },
  {
    "number": 8659,
    "title": "[None][infra] allow to choose repo when generate lock files",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T19:08:31Z",
    "closed_at": "2025-11-06T03:06:30Z",
    "merged_at": "2025-11-06T03:06:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8659"
  },
  {
    "number": 8658,
    "title": "[https://nvbugs/5606166][fix] AutoDeploy: use tuples for cudagraph shape lookup",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T16:59:38Z",
    "closed_at": "2025-10-28T17:52:44Z",
    "merged_at": "2025-10-28T17:52:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8658"
  },
  {
    "number": 8657,
    "title": "[None][autodeploy] minor refactor to rmsnorm transforms",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T16:54:16Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8657"
  },
  {
    "number": 8656,
    "title": "[None][infra] Waive failed tests for release 10/24",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T12:33:27Z",
    "closed_at": "2025-10-24T13:53:35Z",
    "merged_at": "2025-10-24T13:53:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8656"
  },
  {
    "number": 8653,
    "title": "[TRTLLM-8825][feat] Support Pytest Perf Results uploading to Database",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T11:01:55Z",
    "closed_at": "2025-11-03T08:23:13Z",
    "merged_at": "2025-11-03T08:23:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8653"
  },
  {
    "number": 8650,
    "title": "[None][feat] add flag for EPLB to force using GDRCopy",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T10:09:17Z",
    "closed_at": "2025-10-29T05:33:26Z",
    "merged_at": "2025-10-29T05:33:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8650"
  },
  {
    "number": 8649,
    "title": "[None][fix] fix EPLB init hang",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T10:06:58Z",
    "closed_at": "2025-10-28T09:22:50Z",
    "merged_at": "2025-10-28T09:22:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8649"
  },
  {
    "number": 8648,
    "title": "[https://nvbugs/5607238][test] fix working dir in disagg worker test",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T10:00:30Z",
    "closed_at": "2025-10-29T08:13:53Z",
    "merged_at": "2025-10-29T08:13:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8648"
  },
  {
    "number": 8647,
    "title": "[None][fix] Fix ModelConfig.from_pretrained get quant config file",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T09:54:45Z",
    "closed_at": "2025-10-27T03:02:24Z",
    "merged_at": "2025-10-27T03:02:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8647"
  },
  {
    "number": 8646,
    "title": "[None][infra] Test CI with lock files",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T09:33:51Z",
    "closed_at": "2025-10-24T19:31:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8646"
  },
  {
    "number": 8645,
    "title": "[None][chore] Disable GB300 stages in release branch due to nodes will be offline temporarily",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T08:52:48Z",
    "closed_at": "2025-10-24T09:21:14Z",
    "merged_at": "2025-10-24T09:21:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8645"
  },
  {
    "number": 8644,
    "title": "[https://nvbugs/5556020][fix] cherry-pick fix test_disaggregated_serving.py::TestLlama3_1_8BInstruct::test_eagle3  dimension mismatch",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T08:48:52Z",
    "closed_at": "2025-10-29T07:44:25Z",
    "merged_at": "2025-10-29T07:44:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8644"
  },
  {
    "number": 8643,
    "title": "[None][chore] Disable GB300 stages due to nodes will be offline temporarily",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T08:45:36Z",
    "closed_at": "2025-10-24T09:32:05Z",
    "merged_at": "2025-10-24T09:32:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8643"
  },
  {
    "number": 8641,
    "title": "[None][ci] move some time-consuming benchmark test cases to post merge",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T07:11:43Z",
    "closed_at": "2025-10-27T02:47:18Z",
    "merged_at": "2025-10-27T02:47:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8641"
  },
  {
    "number": 8640,
    "title": "[https://nvbugs/5552836][fix] Add flag `TLLM_SPAWN_EXTRA_MAIN_PROCESS` to disable spawning main process",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T06:59:10Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8640"
  },
  {
    "number": 8639,
    "title": "[None][infra] Waive tests on main and remove lines which missed in MI",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T06:13:44Z",
    "closed_at": "2025-10-24T06:49:24Z",
    "merged_at": "2025-10-24T06:49:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8639"
  },
  {
    "number": 8638,
    "title": "[#8389][fix] Update group attention matching to first map to custom torch attention",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T05:24:48Z",
    "closed_at": "2025-11-04T20:00:43Z",
    "merged_at": "2025-11-04T20:00:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8638"
  },
  {
    "number": 8637,
    "title": "[None] [chore] Update to cutlass 4.3",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T02:42:09Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8637"
  },
  {
    "number": 8636,
    "title": "[None][fix] Change Ray submit() to use async RPC",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-24T00:29:05Z",
    "closed_at": "2025-10-28T04:56:14Z",
    "merged_at": "2025-10-28T04:56:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8636"
  },
  {
    "number": 8635,
    "title": "[DO NOT MERGE] Trigger CI",
    "user": "jthomson04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T23:47:54Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8635"
  },
  {
    "number": 8634,
    "title": "[None][feat] Pass KvCacheRetentionConfig to torch LlmRequest",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T22:03:16Z",
    "closed_at": "2025-10-24T13:44:34Z",
    "merged_at": "2025-10-24T13:44:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8634"
  },
  {
    "number": 8632,
    "title": "[None][feat] add skip condition in AutoDeploy's triton fused moe kernel",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T20:32:50Z",
    "closed_at": "2025-10-24T12:46:18Z",
    "merged_at": "2025-10-24T12:46:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8632"
  },
  {
    "number": 8630,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T16:20:08Z",
    "closed_at": "2025-10-24T06:11:04Z",
    "merged_at": "2025-10-24T06:11:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8630"
  },
  {
    "number": 8629,
    "title": "[None][feat] Use ruff for formatting and linting new files by default",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T14:10:05Z",
    "closed_at": "2025-11-01T15:11:40Z",
    "merged_at": "2025-11-01T15:11:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8629"
  },
  {
    "number": 8628,
    "title": "[TRTLLM-8832][feat] fully async _select_generated_logits with tests",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T12:32:49Z",
    "closed_at": "2025-10-27T15:15:33Z",
    "merged_at": "2025-10-27T15:15:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8628"
  },
  {
    "number": 8627,
    "title": "[https://nvbugs/5508536][fix] Reintroduce: Move stop_criteria to sample_async (#7041)",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T11:18:38Z",
    "closed_at": "2025-11-02T14:31:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8627"
  },
  {
    "number": 8625,
    "title": "[https://nvbugs/5593199][test] Enhance beam search tests deterministic dummy model",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T10:16:18Z",
    "closed_at": "2025-10-29T05:12:23Z",
    "merged_at": "2025-10-29T05:12:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8625"
  },
  {
    "number": 8624,
    "title": "[https://nvbugs/5608461][fix] exclude InductorSubproc from thread leak check",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T09:34:01Z",
    "closed_at": "2025-10-24T05:08:43Z",
    "merged_at": "2025-10-24T05:08:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8624"
  },
  {
    "number": 8623,
    "title": "Add skip condition",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T09:32:17Z",
    "closed_at": "2025-10-23T20:34:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8623"
  },
  {
    "number": 8622,
    "title": "[None][feat] Refactor scaffolding streaming feature and fix openai wo…",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T09:25:42Z",
    "closed_at": "2025-10-30T08:02:40Z",
    "merged_at": "2025-10-30T08:02:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8622"
  },
  {
    "number": 8621,
    "title": "[TRTLLM-8658][infra] upgrade to DLFW 25.10 and pytorch 2.9.0 / triton 3.5.0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T08:44:23Z",
    "closed_at": "2025-11-03T01:24:58Z",
    "merged_at": "2025-11-03T01:24:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8621"
  },
  {
    "number": 8620,
    "title": "[None][feat] Enable nvfp4 cuda core for sm120",
    "user": "Njuapp",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T08:27:14Z",
    "closed_at": "2025-10-29T04:39:03Z",
    "merged_at": "2025-10-29T04:39:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8620"
  },
  {
    "number": 8619,
    "title": "[None][test] Clean cache for certain easily hang cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T08:12:03Z",
    "closed_at": "2025-10-24T12:17:33Z",
    "merged_at": "2025-10-24T12:17:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8619"
  },
  {
    "number": 8617,
    "title": "[https://nvbugs/5558117][fix] Allow per-layer quant config from hf_quant_config.json",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T06:54:33Z",
    "closed_at": "2025-10-31T11:41:44Z",
    "merged_at": "2025-10-31T11:41:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8617"
  },
  {
    "number": 8616,
    "title": "[None][infra] Disable rtxpro6000 stages due to nodes will be offline temporarily",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T06:31:56Z",
    "closed_at": "2025-10-23T14:21:21Z",
    "merged_at": "2025-10-23T14:21:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8616"
  },
  {
    "number": 8614,
    "title": "Add wrapper class of tensor",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T06:16:59Z",
    "closed_at": "2025-10-27T14:24:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8614"
  },
  {
    "number": 8613,
    "title": "[None][infra] Disable rtxpro6000 stages due to nodes will be offline",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T06:12:53Z",
    "closed_at": "2025-10-23T14:24:06Z",
    "merged_at": "2025-10-23T14:24:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8613"
  },
  {
    "number": 8612,
    "title": "[https://nvbugs/5597647][fix] Fix MNNVL Allreduce accuracy issue on Hopper",
    "user": "timlee0212",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T06:11:48Z",
    "closed_at": "2025-10-27T06:06:46Z",
    "merged_at": "2025-10-27T06:06:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8612"
  },
  {
    "number": 8611,
    "title": "[https://nvbugs/5587456][fix] Remove multimodal test cases using TRT backend",
    "user": "jieli-matrix",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T06:10:46Z",
    "closed_at": "2025-10-24T10:04:43Z",
    "merged_at": "2025-10-24T10:04:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8611"
  },
  {
    "number": 8610,
    "title": "[https://nvbugs/5541145][fix] Remove DeepSeekR1 test case from H20 to prevent OOM",
    "user": "jieli-matrix",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T06:04:26Z",
    "closed_at": "2025-10-24T09:20:41Z",
    "merged_at": "2025-10-24T09:20:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8610"
  },
  {
    "number": 8609,
    "title": "[https://nvbugs/5575902][fix] set max_batch_size=1 to stabilize accuracy test result",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T05:55:56Z",
    "closed_at": "2025-10-23T14:28:30Z",
    "merged_at": "2025-10-23T14:28:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8609"
  },
  {
    "number": 8608,
    "title": "[https://nvbugs/5451272][fix] unwaive the test",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T05:50:53Z",
    "closed_at": "2025-11-04T09:31:42Z",
    "merged_at": "2025-11-04T09:31:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8608"
  },
  {
    "number": 8607,
    "title": "[None][infra] Minor Update on Perf Sanity Testdb Files",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T05:43:00Z",
    "closed_at": "2025-10-28T01:54:48Z",
    "merged_at": "2025-10-28T01:54:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8607"
  },
  {
    "number": 8604,
    "title": "[None][test] Add longbench v2 for long context evaluation",
    "user": "baize97",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T04:28:18Z",
    "closed_at": "2025-10-27T12:01:14Z",
    "merged_at": "2025-10-27T12:01:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8604"
  },
  {
    "number": 8603,
    "title": "[None][fix] Fix e2e tests for phi4mm and NVILA",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T04:07:51Z",
    "closed_at": "2025-10-24T06:22:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8603"
  },
  {
    "number": 8602,
    "title": "[TRTLLM-8431][doc] update public doc and example, add etcd auto-scaling tests",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T02:53:11Z",
    "closed_at": "2025-10-29T00:04:54Z",
    "merged_at": "2025-10-29T00:04:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8602"
  },
  {
    "number": 8601,
    "title": "[None] [test] Add MNNVL AlltoAll tests to pre-merge",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T02:50:53Z",
    "closed_at": "2025-10-27T13:39:44Z",
    "merged_at": "2025-10-27T13:39:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8601"
  },
  {
    "number": 8600,
    "title": "[TRTLLM-8836][chore] Create ModelEngine from LlmArgs",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T02:21:09Z",
    "closed_at": "2025-11-01T12:26:06Z",
    "merged_at": "2025-11-01T12:26:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8600"
  },
  {
    "number": 8599,
    "title": "[None][feat] AutoDeploy: Add FP8 MOE for Nemotron",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T00:26:52Z",
    "closed_at": "2025-10-25T19:26:45Z",
    "merged_at": "2025-10-25T19:26:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8599"
  },
  {
    "number": 8598,
    "title": "[None][Feat] WIP DO NOT REVIEW Nemotron moe fp8",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-23T00:04:53Z",
    "closed_at": "2025-10-23T00:05:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8598"
  },
  {
    "number": 8597,
    "title": "[TRTLLM-8511][feat] AutoDeploy: optimize fused_mlp_moe_kernel tiles",
    "user": "nzmora-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T23:32:52Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8597"
  },
  {
    "number": 8594,
    "title": "[AutoDeploy]Add Agent ops",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T20:17:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8594"
  },
  {
    "number": 8593,
    "title": "[https://nvbugs/5604136][fix] AutoDeploy: correct import for mxfp4_moe unit test",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T19:55:01Z",
    "closed_at": "2025-10-23T02:11:19Z",
    "merged_at": "2025-10-23T02:11:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8593"
  },
  {
    "number": 8592,
    "title": "[None] [feat] Mixed precision context and generation servers",
    "user": "timothygao8710",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T19:31:40Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8592"
  },
  {
    "number": 8591,
    "title": "[None][chore] Skip failing import of mxfp4_moe",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T19:13:50Z",
    "closed_at": "2025-10-22T20:19:22Z",
    "merged_at": "2025-10-22T20:19:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8591"
  },
  {
    "number": 8590,
    "title": "[TRTLLM-8638][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T18:56:10Z",
    "closed_at": "2025-10-24T04:02:43Z",
    "merged_at": "2025-10-24T04:02:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8590"
  },
  {
    "number": 8588,
    "title": "[TRTLLM-8638][fix] Add flaky failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T15:36:12Z",
    "closed_at": "2025-10-24T03:08:53Z",
    "merged_at": "2025-10-24T03:08:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8588"
  },
  {
    "number": 8587,
    "title": "[TRTLLM-8831][feat] Enable early exit with overlap scheduler",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T15:29:32Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8587"
  },
  {
    "number": 8586,
    "title": "[TRTLLM-8160][feat] Add draft token tree runtime on CDL",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T15:12:02Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8586"
  },
  {
    "number": 8585,
    "title": "[None][infra] Fix slurm exitcode",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T13:33:23Z",
    "closed_at": "2025-10-23T13:46:00Z",
    "merged_at": "2025-10-23T13:46:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8585"
  },
  {
    "number": 8584,
    "title": "[None][chore] Print device info in trtllm-bench report",
    "user": "galagam",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T12:36:05Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8584"
  },
  {
    "number": 8583,
    "title": "[https://nvbugs/5601203] [fix]Restrict fp8 blockscale moe case",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T11:34:43Z",
    "closed_at": "2025-10-29T02:47:33Z",
    "merged_at": "2025-10-29T02:47:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8583"
  },
  {
    "number": 8582,
    "title": "[https://nvbugs/5569754][fix] trtllm llmapi launch port conflict ",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T10:58:50Z",
    "closed_at": "2025-10-23T01:14:36Z",
    "merged_at": "2025-10-23T01:14:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8582"
  },
  {
    "number": 8581,
    "title": "[TRTLLM-7723][feat] sampling using FlashInfer.sampling",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T10:41:46Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8581"
  },
  {
    "number": 8580,
    "title": "[TRTLLM-8738][test] Add end-to-end trtllm-serve negative tests",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T09:51:16Z",
    "closed_at": "2025-10-24T05:23:48Z",
    "merged_at": "2025-10-24T05:23:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8580"
  },
  {
    "number": 8579,
    "title": "[None][fix] Fix EPLB CPU thread NUMA binding",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T09:37:30Z",
    "closed_at": "2025-10-22T14:52:11Z",
    "merged_at": "2025-10-22T14:52:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8579"
  },
  {
    "number": 8578,
    "title": "[none][feat] Support nano-v2-vlm with multiple PRs v2",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T09:23:50Z",
    "closed_at": "2025-10-27T06:01:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8578"
  },
  {
    "number": 8577,
    "title": "[None][feat] add Nemotron-Ultra multi nodes eval tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T09:12:54Z",
    "closed_at": "2025-10-23T06:44:27Z",
    "merged_at": "2025-10-23T06:44:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8577"
  },
  {
    "number": 8576,
    "title": "[https://nvbugs/5575829][fix] Unwaive gpt-oss test",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T08:43:36Z",
    "closed_at": "2025-10-22T11:31:57Z",
    "merged_at": "2025-10-22T11:31:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8576"
  },
  {
    "number": 8575,
    "title": "[TRTLLM-8785][fix] create output_dir before test begin (cherry-pick #8518)",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T08:42:21Z",
    "closed_at": "2025-10-23T08:41:55Z",
    "merged_at": "2025-10-23T08:41:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8575"
  },
  {
    "number": 8574,
    "title": "[None][infra] Waive failed tests for release 10/22",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T08:10:14Z",
    "closed_at": "2025-10-22T08:41:00Z",
    "merged_at": "2025-10-22T08:41:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8574"
  },
  {
    "number": 8573,
    "title": "[None][infra] Waive failed cases for main branch 10/22",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T07:58:44Z",
    "closed_at": "2025-10-22T08:21:57Z",
    "merged_at": "2025-10-22T08:21:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8573"
  },
  {
    "number": 8568,
    "title": "[None][doc] Paragraph adjustment and fix statistic",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T06:25:36Z",
    "closed_at": "2025-10-22T07:26:10Z",
    "merged_at": "2025-10-22T07:26:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8568"
  },
  {
    "number": 8567,
    "title": "[https://nvbugs/5437384][test] cherry-pick fix trtllm llmapi launch multi tests ",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T06:24:11Z",
    "closed_at": "2025-11-01T13:49:34Z",
    "merged_at": "2025-11-01T13:49:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8567"
  },
  {
    "number": 8566,
    "title": "[https://nvbugs/5488576][fix] Propagate disable_finalize_fusion config flag in WIDEEP MoE backend (cherry-pick #8141)",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T06:12:16Z",
    "closed_at": "2025-10-22T13:47:00Z",
    "merged_at": "2025-10-22T13:47:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8566"
  },
  {
    "number": 8565,
    "title": "[https://nvbugs/5564465][test] ensure deepseek_v3_lite isl + osl < max_seq_len",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T05:45:14Z",
    "closed_at": "2025-10-28T07:25:53Z",
    "merged_at": "2025-10-28T07:25:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8565"
  },
  {
    "number": 8563,
    "title": "[None][feat] Enable rms norm fusion for Nemotron MOE",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T04:54:03Z",
    "closed_at": "2025-10-23T04:09:42Z",
    "merged_at": "2025-10-23T04:09:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8563"
  },
  {
    "number": 8562,
    "title": "[None][chore] Bump version to 1.2.0rc2",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T04:07:01Z",
    "closed_at": "2025-10-22T06:35:05Z",
    "merged_at": "2025-10-22T06:35:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8562"
  },
  {
    "number": 8561,
    "title": "[TRTLLM-8817][chore] Set default value of KvCacheConfig.free_gpu_memory_fraction explicitly",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T03:26:19Z",
    "closed_at": "2025-10-24T00:55:49Z",
    "merged_at": "2025-10-24T00:55:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8561"
  },
  {
    "number": 8560,
    "title": "[None][feat] Add vLLM KV Pool support for XQA mla kernel",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T02:40:15Z",
    "closed_at": "2025-10-22T06:12:57Z",
    "merged_at": "2025-10-22T06:12:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8560"
  },
  {
    "number": 8559,
    "title": "[None][feat] Dev scaffolding bench load_generator",
    "user": "dcaox",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T01:44:36Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8559"
  },
  {
    "number": 8558,
    "title": "[TRTLLM-8812][chore] Limit the scope of pybind based CacheTransceiverConfig",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T01:35:18Z",
    "closed_at": "2025-10-23T14:32:10Z",
    "merged_at": "2025-10-23T14:32:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8558"
  },
  {
    "number": 8557,
    "title": "[TRTLLM-8638][fix] fix test issues",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-22T01:15:49Z",
    "closed_at": "2025-10-24T06:16:55Z",
    "merged_at": "2025-10-24T06:16:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8557"
  },
  {
    "number": 8554,
    "title": "[TRTLLM-8638][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T21:48:07Z",
    "closed_at": "2025-10-22T05:26:16Z",
    "merged_at": "2025-10-22T05:26:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8554"
  },
  {
    "number": 8553,
    "title": "[https://nvbugs/5568961][fix] Fix a merge conflict (cherrypick from PR 8365)",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T21:38:32Z",
    "closed_at": "2025-10-23T06:05:16Z",
    "merged_at": "2025-10-23T06:05:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8553"
  },
  {
    "number": 8552,
    "title": "[https://nvbugs/5549081][fix] Fix device id assignment for some visio…",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T21:13:43Z",
    "closed_at": "2025-10-23T06:06:14Z",
    "merged_at": "2025-10-23T06:06:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8552"
  },
  {
    "number": 8551,
    "title": "[#8245][feat] Autodeploy: Guided Decoding Support",
    "user": "govind-ramnarayan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T20:46:17Z",
    "closed_at": "2025-10-28T01:29:57Z",
    "merged_at": "2025-10-28T01:29:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8551"
  },
  {
    "number": 8549,
    "title": "[None][fix] fixed cached model path in test",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T19:36:29Z",
    "closed_at": "2025-10-22T11:47:42Z",
    "merged_at": "2025-10-22T11:47:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8549"
  },
  {
    "number": 8548,
    "title": "[TRTLLM-8201][feat] TP sharding of Mamba layers",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T19:28:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8548"
  },
  {
    "number": 8547,
    "title": "[None][infra] enable lfs for generateLockFile pipeline",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T17:57:53Z",
    "closed_at": "2025-10-24T04:59:27Z",
    "merged_at": "2025-10-24T04:59:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8547"
  },
  {
    "number": 8546,
    "title": "[https://nvbugs/5576192][fix] Unwaive the test for test_weight_only_quant_gemm.",
    "user": "zheyuf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T17:35:34Z",
    "closed_at": "2025-10-23T22:46:10Z",
    "merged_at": "2025-10-23T22:46:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8546"
  },
  {
    "number": 8545,
    "title": "[None] [refactor] Include Python attributions in wheel packaging",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T17:16:12Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8545"
  },
  {
    "number": 8544,
    "title": "[https://nvbugs/5569719][fix] Gptoss sm120 cherrypick to release 1.1",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T17:12:57Z",
    "closed_at": "2025-11-05T17:54:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8544"
  },
  {
    "number": 8539,
    "title": "[None][fix] generate nanobind stubs for submodules",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T14:15:47Z",
    "closed_at": "2025-10-22T10:23:09Z",
    "merged_at": "2025-10-22T10:23:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8539"
  },
  {
    "number": 8538,
    "title": "[https://nvbugs/5564465][fix] Overwrite only if default_max_tokens is legal",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T12:25:37Z",
    "closed_at": "2025-10-28T09:15:27Z",
    "merged_at": "2025-10-28T09:15:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8538"
  },
  {
    "number": 8537,
    "title": "[https://nvbugs/5451272][fix] unwaive the test",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T12:21:06Z",
    "closed_at": "2025-10-22T06:28:42Z",
    "merged_at": "2025-10-22T06:28:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8537"
  },
  {
    "number": 8536,
    "title": "[None][doc] Fix the incorrect doc figure",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T12:13:08Z",
    "closed_at": "2025-10-22T02:08:56Z",
    "merged_at": "2025-10-22T02:08:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8536"
  },
  {
    "number": 8535,
    "title": "[None][fix] Allow multi-threaded copy for GDRCopy wrapper",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T12:08:57Z",
    "closed_at": "2025-10-23T02:25:04Z",
    "merged_at": "2025-10-23T02:25:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8535"
  },
  {
    "number": 8534,
    "title": "[None][chore] add precommit hook to remove redundant tab and white space",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T11:14:15Z",
    "closed_at": "2025-10-22T13:21:55Z",
    "merged_at": "2025-10-22T13:21:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8534"
  },
  {
    "number": 8533,
    "title": "[https://nvbugs/5575920][fix] Fix cublas/cublasLt handle creation memory not sufficient error",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T10:06:00Z",
    "closed_at": "2025-11-04T01:48:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8533"
  },
  {
    "number": 8532,
    "title": "[None][infra]Test multi tests in same gb200 multinode test stage",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T10:02:47Z",
    "closed_at": "2025-10-22T04:55:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8532"
  },
  {
    "number": 8531,
    "title": "[TRTLLM-8821][feat] Apply AutoTuner to AllReduce Op for strategy tuning.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T09:13:30Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8531"
  },
  {
    "number": 8530,
    "title": "[None][doc] add visualization of perf metrics in time breakdown tool doc",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T08:43:17Z",
    "closed_at": "2025-10-24T02:09:22Z",
    "merged_at": "2025-10-24T02:09:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8530"
  },
  {
    "number": 8529,
    "title": "[None][chore] Update feature combination matrix for SWA kv cache reuse",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T08:06:38Z",
    "closed_at": "2025-10-21T08:41:44Z",
    "merged_at": "2025-10-21T08:41:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8529"
  },
  {
    "number": 8528,
    "title": "[TRTLLM-8737][feat] Support media_io_kwargs on trtllm-serve",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T08:01:23Z",
    "closed_at": "2025-10-24T16:53:41Z",
    "merged_at": "2025-10-24T16:53:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8528"
  },
  {
    "number": 8527,
    "title": "[None][doc] Add doc for torch.compile & piecewise cuda graph",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T07:54:48Z",
    "closed_at": "2025-10-30T04:15:46Z",
    "merged_at": "2025-10-30T04:15:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8527"
  },
  {
    "number": 8526,
    "title": "[None][feat] Add qwen3-next nvfp4 support",
    "user": "JadoTu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T07:54:29Z",
    "closed_at": "2025-11-06T01:45:44Z",
    "merged_at": "2025-11-06T01:45:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8526"
  },
  {
    "number": 8525,
    "title": "[None][ci] reorganize disaggregated serving test cases by GPU number",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T07:26:36Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8525"
  },
  {
    "number": 8524,
    "title": "[None][infra] Waive failed tests for main 10/21",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T07:24:37Z",
    "closed_at": "2025-10-21T10:24:16Z",
    "merged_at": "2025-10-21T10:24:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8524"
  },
  {
    "number": 8523,
    "title": "[TRTLLM-7835][test] add default sample config for perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T07:08:39Z",
    "closed_at": "2025-10-28T06:22:48Z",
    "merged_at": "2025-10-28T06:22:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8523"
  },
  {
    "number": 8522,
    "title": "[None][infra] Waive tests for release 1021",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T06:50:38Z",
    "closed_at": "2025-10-21T07:21:01Z",
    "merged_at": "2025-10-21T07:21:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8522"
  },
  {
    "number": 8521,
    "title": "[None][feat] add detailed KV cache transfer time breakdown",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T06:39:19Z",
    "closed_at": "2025-10-29T02:11:10Z",
    "merged_at": "2025-10-29T02:11:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8521"
  },
  {
    "number": 8520,
    "title": "[None][test] Clean cache for certain easily hang cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T06:17:04Z",
    "closed_at": "2025-10-22T08:44:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8520"
  },
  {
    "number": 8519,
    "title": "[https://nvbugs/5582277][fix] rework DisaggPPTerminationHandler to fix hang issue",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T04:36:34Z",
    "closed_at": "2025-10-23T01:44:00Z",
    "merged_at": "2025-10-23T01:44:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8519"
  },
  {
    "number": 8518,
    "title": "[TRTLLM-8785][fix] fix conflicts between periodic-junit and store-durations",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T04:18:06Z",
    "closed_at": "2025-10-22T08:36:48Z",
    "merged_at": "2025-10-22T08:36:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8518"
  },
  {
    "number": 8517,
    "title": "[https://nvbugs/5556020][fix] test_disaggregated_serving.py::TestLlama3_1_8BInstruct::test_eagle3 dimension mismatch",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T03:54:43Z",
    "closed_at": "2025-10-22T01:58:22Z",
    "merged_at": "2025-10-22T01:58:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8517"
  },
  {
    "number": 8516,
    "title": "[None][infra] Add split algorithm for slurm",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T03:23:00Z",
    "closed_at": "2025-10-21T06:56:23Z",
    "merged_at": "2025-10-21T06:56:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8516"
  },
  {
    "number": 8515,
    "title": "[https://nvbugs/5569534][fix] Warm up with different sizes for more s…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T02:58:02Z",
    "closed_at": "2025-10-30T05:29:07Z",
    "merged_at": "2025-10-30T05:29:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8515"
  },
  {
    "number": 8514,
    "title": "[https://nvbugs/5494718][fix] Fix Single GPU Multi-node issue and OOM on DGX Spark",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-21T00:34:42Z",
    "closed_at": "2025-10-25T02:09:08Z",
    "merged_at": "2025-10-25T02:09:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8514"
  },
  {
    "number": 8513,
    "title": "[TRTLLM-8168][feat] Add multi turn flag in AD sample code",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T20:50:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8513"
  },
  {
    "number": 8510,
    "title": "[None][chore] AutoDeploy: replace HF's deprecated keyword torch_dtype --> dtype",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T19:41:21Z",
    "closed_at": "2025-10-21T21:07:07Z",
    "merged_at": "2025-10-21T21:07:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8510"
  },
  {
    "number": 8509,
    "title": "[TRTLLM-6756][feat] Add Beam Search to TorchSampler",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T16:31:31Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8509"
  },
  {
    "number": 8508,
    "title": "[None][chore] Weekly mass integration of release/1.1",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T14:44:06Z",
    "closed_at": "2025-11-04T08:42:31Z",
    "merged_at": "2025-11-04T08:42:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8508"
  },
  {
    "number": 8507,
    "title": "[None][fix] fix runtime error that bf16 input is not quantized to nvfp4 when use bf16 dispatch",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T13:42:45Z",
    "closed_at": "2025-10-30T07:06:54Z",
    "merged_at": "2025-10-30T07:06:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8507"
  },
  {
    "number": 8506,
    "title": "[TRTLLM-9003][infra] Add python OpenSearchDB query / push.",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T13:18:09Z",
    "closed_at": "2025-10-31T02:43:52Z",
    "merged_at": "2025-10-31T02:43:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8506"
  },
  {
    "number": 8505,
    "title": "[None][feat] AutoDeploy: refactor memory usage logging",
    "user": "nzmora-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T13:09:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8505"
  },
  {
    "number": 8503,
    "title": "[https://nvbugs/5481198][fix] unwavie some test cases",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T11:55:19Z",
    "closed_at": "2025-10-27T14:31:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8503"
  },
  {
    "number": 8502,
    "title": "[TRTLLM-8436][fix] restore list[list[list[int]]] in add_token",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T10:46:16Z",
    "closed_at": "2025-10-21T02:34:58Z",
    "merged_at": "2025-10-21T02:34:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8502"
  },
  {
    "number": 8501,
    "title": "[None][fix] Fix the performance issue of FP8 blockwise grouped GEMM when using attention DP",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T09:48:33Z",
    "closed_at": "2025-10-27T02:18:20Z",
    "merged_at": "2025-10-27T02:18:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8501"
  },
  {
    "number": 8500,
    "title": "[https://nvbugs/5565549][fix] unwaive test_disaggregated_spec_dec_bat…",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T09:39:20Z",
    "closed_at": "2025-10-22T07:00:00Z",
    "merged_at": "2025-10-22T07:00:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8500"
  },
  {
    "number": 8499,
    "title": "[https://nvbugs/5569081][fix] Upgrade fmha_v2. (cherry-pick from https://github.com/NVIDIA/TensorRT-LLM/pull/8364)",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T09:35:01Z",
    "closed_at": "2025-10-21T04:32:13Z",
    "merged_at": "2025-10-21T04:32:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8499"
  },
  {
    "number": 8498,
    "title": "[https://nvbugs/5596377][fix] Fix mm dummy calculation",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T08:45:55Z",
    "closed_at": "2025-10-29T00:45:21Z",
    "merged_at": "2025-10-29T00:45:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8498"
  },
  {
    "number": 8497,
    "title": "[https://nvbugs/5569754][fix] trtllm-llmapi-launch port conflict",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T08:23:55Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8497"
  },
  {
    "number": 8496,
    "title": "[None][fix] the api_stability unify default values of None and inspect._empty",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T08:01:04Z",
    "closed_at": "2025-10-21T08:57:41Z",
    "merged_at": "2025-10-21T08:57:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8496"
  },
  {
    "number": 8495,
    "title": "[TRTLLM-8803][feat] Add rope and uk-bgemm overlap for mla generation",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T08:00:29Z",
    "closed_at": "2025-11-06T09:39:58Z",
    "merged_at": "2025-11-06T09:39:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8495"
  },
  {
    "number": 8494,
    "title": "[https://nvbugs/5546510][fix] Move torch.cuda.Stream out of torch com…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T07:39:40Z",
    "closed_at": "2025-10-22T03:21:59Z",
    "merged_at": "2025-10-22T03:21:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8494"
  },
  {
    "number": 8493,
    "title": "[TRTLLM-8754][chore] Refine PyTorchModelEngine with llm args",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T06:21:14Z",
    "closed_at": "2025-10-23T00:03:19Z",
    "merged_at": "2025-10-23T00:03:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8493"
  },
  {
    "number": 8492,
    "title": "[https://nvbugs/5492250][fix] Remove isolated cases and unwaive cases",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T06:13:02Z",
    "closed_at": "2025-10-20T11:40:08Z",
    "merged_at": "2025-10-20T11:40:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8492"
  },
  {
    "number": 8491,
    "title": "[None][ci] rebalance H100 stages",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T06:07:00Z",
    "closed_at": "2025-10-21T06:03:49Z",
    "merged_at": "2025-10-21T06:03:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8491"
  },
  {
    "number": 8490,
    "title": "[None][infra] Waive a filed case for main",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T05:56:38Z",
    "closed_at": "2025-10-20T06:35:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8490"
  },
  {
    "number": 8489,
    "title": "[None][fix] Add EAXONE-4 to c++ kv_cache_manager",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T04:43:13Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8489"
  },
  {
    "number": 8488,
    "title": "[None][feat] Update 3rdparty/DeepGEMM to latest commit",
    "user": "ruoqianguo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T03:27:16Z",
    "closed_at": "2025-10-20T22:56:51Z",
    "merged_at": "2025-10-20T22:56:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8488"
  },
  {
    "number": 8487,
    "title": "[None][fix] Fix e2e tests for phi4mm and NVILA",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T03:16:25Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8487"
  },
  {
    "number": 8486,
    "title": "[TRTLLM-8638][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T03:14:26Z",
    "closed_at": "2025-10-21T05:20:30Z",
    "merged_at": "2025-10-21T05:20:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8486"
  },
  {
    "number": 8485,
    "title": "[None][docs] Update Python wheel's short-/long-descriptions",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T02:56:37Z",
    "closed_at": "2025-10-27T00:36:30Z",
    "merged_at": "2025-10-27T00:36:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8485"
  },
  {
    "number": 8484,
    "title": "[https://nvbugs/5587652][fix] Fix free block utility assertion",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T02:32:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8484"
  },
  {
    "number": 8483,
    "title": "[None][fix] Fix test_cache_transceiver should in normal test env",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T02:11:13Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8483"
  },
  {
    "number": 8482,
    "title": "[TRTLLM-8513][feat] Add back worker extension",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-20T01:53:40Z",
    "closed_at": "2025-10-25T00:30:28Z",
    "merged_at": "2025-10-25T00:30:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8482"
  },
  {
    "number": 8481,
    "title": "[None][feat] Add alltoall to trtllm-gen MoE backend.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-19T16:56:22Z",
    "closed_at": "2025-10-21T04:42:54Z",
    "merged_at": "2025-10-21T04:42:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8481"
  },
  {
    "number": 8480,
    "title": "[None][infra] Change to use pytest-forked to run isolation",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-19T13:17:00Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8480"
  },
  {
    "number": 8479,
    "title": "[None][infra] Skip a failed case in pre-merge for main on 10/19",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-19T12:32:00Z",
    "closed_at": "2025-10-19T14:19:01Z",
    "merged_at": "2025-10-19T14:19:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8479"
  },
  {
    "number": 8478,
    "title": "[TRTLLM-8638][fix] Remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-19T12:17:09Z",
    "closed_at": "2025-10-21T07:48:59Z",
    "merged_at": "2025-10-21T07:48:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8478"
  },
  {
    "number": 8477,
    "title": "[#8272][feat] Enable chunked prefill for SSMs in AutoDeploy",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-19T06:25:39Z",
    "closed_at": "2025-10-20T22:31:53Z",
    "merged_at": "2025-10-20T22:31:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8477"
  },
  {
    "number": 8475,
    "title": "[None][chore] Cleanup GDS code",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-18T21:13:29Z",
    "closed_at": "2025-10-23T19:36:31Z",
    "merged_at": "2025-10-23T19:36:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8475"
  },
  {
    "number": 8473,
    "title": "[None][chore] Waive failing transceiver test",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-18T18:58:11Z",
    "closed_at": "2025-10-18T21:22:10Z",
    "merged_at": "2025-10-18T21:22:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8473"
  },
  {
    "number": 8472,
    "title": "[None][infra] Waive test for main branch on 10/18",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-18T14:09:25Z",
    "closed_at": "2025-10-19T08:36:42Z",
    "merged_at": "2025-10-19T08:36:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8472"
  },
  {
    "number": 8471,
    "title": "[None][infra] Let CI continue running other isolation tests when an isolation test get hanging",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-18T07:02:23Z",
    "closed_at": "2025-10-22T04:07:35Z",
    "merged_at": "2025-10-22T04:07:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8471"
  },
  {
    "number": 8470,
    "title": "[https://nvbugs/5578175][fix] Fix block range index",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-18T06:10:51Z",
    "closed_at": "2025-10-28T18:42:24Z",
    "merged_at": "2025-10-28T18:42:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8470"
  },
  {
    "number": 8469,
    "title": "[None][feat] AutoDeploy: Add Nemotron MOE support for AutoDeploy",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-18T01:34:45Z",
    "closed_at": "2025-10-21T22:32:02Z",
    "merged_at": "2025-10-21T22:32:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8469"
  },
  {
    "number": 8468,
    "title": "[None] [chore] Add architecture-specific ATTRIBUTIONS files",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T23:24:33Z",
    "closed_at": "2025-10-20T20:29:16Z",
    "merged_at": "2025-10-20T20:29:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8468"
  },
  {
    "number": 8467,
    "title": "[https://nvbugs/5488576][feat] Overlap cudaMemset with FC1 in CUTLASS MoE backend",
    "user": "sklevtsov-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T23:16:56Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8467"
  },
  {
    "number": 8466,
    "title": "[None][feat] Enable time breakdown tool in disagg slurm scripts",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T21:03:47Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8466"
  },
  {
    "number": 8465,
    "title": "[None][feat] Add disagg relay time to time breakdown tool",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T20:55:31Z",
    "closed_at": "2025-10-31T01:21:45Z",
    "merged_at": "2025-10-31T01:21:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8465"
  },
  {
    "number": 8464,
    "title": "[None][feat] Mixed precision context and generation servers",
    "user": "timothygao8710",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T19:10:06Z",
    "closed_at": "2025-10-22T19:31:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8464"
  },
  {
    "number": 8463,
    "title": "[https://nvbugs/5508301][feat] Move D->H copies to a worker thread whe…",
    "user": "dhansen-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T18:43:55Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8463"
  },
  {
    "number": 8462,
    "title": "[#8461][feat] AutoDeploy: trtllm-serve bug fix + unit test",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T15:55:53Z",
    "closed_at": "2025-10-20T20:06:40Z",
    "merged_at": "2025-10-20T20:06:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8462"
  },
  {
    "number": 8459,
    "title": "[https://nvbugs/5429636][feat] Kv transfer timeout",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T13:01:20Z",
    "closed_at": "2025-10-22T13:29:02Z",
    "merged_at": "2025-10-22T13:29:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8459"
  },
  {
    "number": 8458,
    "title": "[None][feat] Support base64 video input",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T10:19:00Z",
    "closed_at": "2025-10-24T02:23:13Z",
    "merged_at": "2025-10-24T02:23:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8458"
  },
  {
    "number": 8457,
    "title": "[TRTLLM-8201][feat] Topological graph helpers",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T08:34:03Z",
    "closed_at": "2025-10-17T16:34:20Z",
    "merged_at": "2025-10-17T16:34:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8457"
  },
  {
    "number": 8456,
    "title": "[TRTLLM-8201][feat] Topological graph helpers",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T08:23:43Z",
    "closed_at": "2025-10-17T08:24:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8456"
  },
  {
    "number": 8455,
    "title": "[TRTLLM-8580][test] save runtime report periodically (#8312)",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T07:41:49Z",
    "closed_at": "2025-10-20T02:54:24Z",
    "merged_at": "2025-10-20T02:54:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8455"
  },
  {
    "number": 8454,
    "title": "[None][feat] Add fine-grained metrics",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T07:14:31Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8454"
  },
  {
    "number": 8453,
    "title": "[TRTLLM-6928][fix] Refactor multimodal unittest",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T06:52:16Z",
    "closed_at": "2025-11-03T14:01:07Z",
    "merged_at": "2025-11-03T14:01:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8453"
  },
  {
    "number": 8452,
    "title": "[None][feat] Deep Research Implemented with Scaffolding",
    "user": "Boreas618",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T06:41:29Z",
    "closed_at": "2025-11-06T02:33:29Z",
    "merged_at": "2025-11-06T02:33:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8452"
  },
  {
    "number": 8451,
    "title": "[TRTLLM-8483][chore] Refine scheduler_config and peft_cache_config in create_py_executor",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T06:34:17Z",
    "closed_at": "2025-10-22T00:33:48Z",
    "merged_at": "2025-10-22T00:33:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8451"
  },
  {
    "number": 8450,
    "title": "[https://nvbugs/5565565] [fix] Remove waiver",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T05:32:43Z",
    "closed_at": "2025-10-17T08:13:01Z",
    "merged_at": "2025-10-17T08:13:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8450"
  },
  {
    "number": 8449,
    "title": "[None][ci] move some test cases from H100 to A10",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T05:28:45Z",
    "closed_at": "2025-10-20T05:58:34Z",
    "merged_at": "2025-10-20T05:58:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8449"
  },
  {
    "number": 8448,
    "title": "[None][chore] Update commit msg for adding lock files",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T04:48:27Z",
    "closed_at": "2025-10-17T07:24:27Z",
    "merged_at": "2025-10-17T07:24:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8448"
  },
  {
    "number": 8447,
    "title": "[None][infra] install mooncake in docker images",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T03:50:01Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8447"
  },
  {
    "number": 8446,
    "title": "[https://nvbugs/5534574][fix] disable spec decoding forever once the request spec decoding is disabled",
    "user": "kris1025",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T03:21:17Z",
    "closed_at": "2025-10-29T11:28:44Z",
    "merged_at": "2025-10-29T11:28:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8446"
  },
  {
    "number": 8445,
    "title": "[TRTLLM-8638][fix] add waives tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T02:26:22Z",
    "closed_at": "2025-10-17T10:37:54Z",
    "merged_at": "2025-10-17T10:37:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8445"
  },
  {
    "number": 8444,
    "title": "[https://nvbugs/5516666][fix] cherry-pick PR 8130 to unwaive the Qwen3 CI",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T01:48:13Z",
    "closed_at": "2025-10-20T03:14:11Z",
    "merged_at": "2025-10-20T03:14:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8444"
  },
  {
    "number": 8443,
    "title": "[None][chore] add feature combination test template",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T01:45:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8443"
  },
  {
    "number": 8442,
    "title": "[None][chore] Combine two documents of feature combination matrix",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T23:26:24Z",
    "closed_at": "2025-10-17T06:31:33Z",
    "merged_at": "2025-10-17T06:31:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8442"
  },
  {
    "number": 8440,
    "title": "[https://nvbugs/5515753][ci] Add NCCL_DEBUG=INFO flag to collect more…",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T18:59:53Z",
    "closed_at": "2025-10-22T01:12:06Z",
    "merged_at": "2025-10-22T01:12:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8440"
  },
  {
    "number": 8439,
    "title": "Minor edits to the text of the Qwen3-Next guide",
    "user": "asrivas",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T17:56:26Z",
    "closed_at": "2025-10-17T03:55:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8439"
  },
  {
    "number": 8438,
    "title": "[None] [chore] Add ATTRIBUTIONS-{CPP,Python}.md + Update in wheels setup",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T17:36:21Z",
    "closed_at": "2025-10-17T13:33:06Z",
    "merged_at": "2025-10-17T13:33:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8438"
  },
  {
    "number": 8437,
    "title": "[https://nvbugs/5568676][fix] Remove test waive",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T17:23:37Z",
    "closed_at": "2025-10-20T19:03:51Z",
    "merged_at": "2025-10-20T19:03:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8437"
  },
  {
    "number": 8434,
    "title": "[None][chore] Update the Flux autodeploy example",
    "user": "ajrasane",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T16:24:23Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8434"
  },
  {
    "number": 8433,
    "title": "[TRTLLM-8650][fix] beam search request validation",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T15:02:37Z",
    "closed_at": "2025-10-21T08:50:28Z",
    "merged_at": "2025-10-21T08:50:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8433"
  },
  {
    "number": 8432,
    "title": "[https://nvbugs/5524714][fix] Fix TP sharding of fused-QKV weight scales in W4A16 AWQ",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T14:40:45Z",
    "closed_at": "2025-10-19T12:27:23Z",
    "merged_at": "2025-10-19T12:27:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8432"
  },
  {
    "number": 8431,
    "title": "Test upgrade to triton 3.5.0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T14:05:30Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8431"
  },
  {
    "number": 8430,
    "title": "[https://nvbugs/5524714][fix] Fix TP sharding of fused-QKV weight scales in W4A16 AWQ",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T13:55:09Z",
    "closed_at": "2025-10-16T14:24:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8430"
  },
  {
    "number": 8429,
    "title": "[https://nvbugs/5569713][fix] Disable fp8 deep gemm for EXAONE-4.0-32B-FP8",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T13:40:03Z",
    "closed_at": "2025-10-21T16:37:57Z",
    "merged_at": "2025-10-21T16:37:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8429"
  },
  {
    "number": 8428,
    "title": "[#8391][fix] check perf by device subtype",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T13:27:52Z",
    "closed_at": "2025-10-22T09:38:05Z",
    "merged_at": "2025-10-22T09:38:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8428"
  },
  {
    "number": 8427,
    "title": "[None][feat] Draft: Eagle3 1-model Sampling",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T13:03:25Z",
    "closed_at": "2025-10-22T00:44:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8427"
  },
  {
    "number": 8426,
    "title": "[None][fix] trtllm-gen regression in PR 8301",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T13:01:00Z",
    "closed_at": "2025-10-17T10:21:31Z",
    "merged_at": "2025-10-17T10:21:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8426"
  },
  {
    "number": 8425,
    "title": "[None][fix] Fix get_num_tokens_per_image for nano-v2-vlm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T11:57:47Z",
    "closed_at": "2025-10-18T00:51:36Z",
    "merged_at": "2025-10-18T00:51:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8425"
  },
  {
    "number": 8423,
    "title": "[TRTLLM-8638][fix] waive llam4 tests on H20 (#8416)",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T10:24:06Z",
    "closed_at": "2025-10-16T12:27:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8423"
  },
  {
    "number": 8422,
    "title": "[https://nvbugs/5575841] [test] Move test_moe.py to serial tests to improve stability + unwaive FP4 MoE torch unit tests",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T09:57:31Z",
    "closed_at": "2025-10-30T12:57:56Z",
    "merged_at": "2025-10-30T12:57:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8422"
  },
  {
    "number": 8421,
    "title": "[TRTLLM-7954][feat] Target model KV cache rellocation",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T09:55:12Z",
    "closed_at": "2025-10-23T01:36:50Z",
    "merged_at": "2025-10-23T01:36:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8421"
  },
  {
    "number": 8420,
    "title": "[None][test] Filter out all fp8 test case for A100.",
    "user": "yufeiwu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T08:09:32Z",
    "closed_at": "2025-10-17T03:42:51Z",
    "merged_at": "2025-10-17T03:42:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8420"
  },
  {
    "number": 8419,
    "title": "[https://nvbugs/5594753][fix] fix rpc unique addr related issue",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T07:53:39Z",
    "closed_at": "2025-10-22T08:47:19Z",
    "merged_at": "2025-10-22T08:47:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8419"
  },
  {
    "number": 8418,
    "title": "[None][chore] Remove duplicate log outputs in test_perf.py",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T06:57:11Z",
    "closed_at": "2025-10-17T06:11:33Z",
    "merged_at": "2025-10-17T06:11:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8418"
  },
  {
    "number": 8417,
    "title": "[None][chore] replace print_colored_debug with logger_debug",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T06:31:25Z",
    "closed_at": "2025-10-22T09:54:38Z",
    "merged_at": "2025-10-22T09:54:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8417"
  },
  {
    "number": 8416,
    "title": "[TRTLLM-8638][fix] waive llam4 tests on H20",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T06:03:21Z",
    "closed_at": "2025-10-16T10:14:58Z",
    "merged_at": "2025-10-16T10:14:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8416"
  },
  {
    "number": 8415,
    "title": "[None][chore] Optimize perf for the RPC executor and add some profile utilities to llm-api",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T05:58:16Z",
    "closed_at": "2025-11-04T01:59:49Z",
    "merged_at": "2025-11-04T01:59:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8415"
  },
  {
    "number": 8414,
    "title": "[https://nvbugs/5501820][fix] Add requirements for numba-cuda version to WAR mem corruption (#7992)",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T05:55:32Z",
    "closed_at": "2025-10-20T07:01:09Z",
    "merged_at": "2025-10-20T07:01:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8414"
  },
  {
    "number": 8413,
    "title": "[None][bug] Set NCCL_GRAPH_REGISTER to false to avoid hang",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T04:51:45Z",
    "closed_at": "2025-10-16T16:59:18Z",
    "merged_at": "2025-10-16T16:59:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8413"
  },
  {
    "number": 8412,
    "title": "[TRTLLM-8480][chore] clean create_py_executor API",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T04:07:01Z",
    "closed_at": "2025-10-18T03:52:03Z",
    "merged_at": "2025-10-18T03:52:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8412"
  },
  {
    "number": 8411,
    "title": "[https://nvbugs/5532789] [doc] Add documents about CUDA 12.9",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T03:59:11Z",
    "closed_at": "2025-10-16T07:05:18Z",
    "merged_at": "2025-10-16T07:05:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8411"
  },
  {
    "number": 8410,
    "title": "[https://nvbugs/5451280][fix] Reduce memory fraction problem by warmu…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T03:33:26Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8410"
  },
  {
    "number": 8409,
    "title": "[None][bug] Set NCCL_GRAPH_REGISTER to false to avoid hang",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T03:24:19Z",
    "closed_at": "2025-10-16T14:40:52Z",
    "merged_at": "2025-10-16T14:40:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8409"
  },
  {
    "number": 8408,
    "title": "[None][chore] Isolate several intermittent cases",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T02:32:15Z",
    "closed_at": "2025-10-16T06:48:31Z",
    "merged_at": "2025-10-16T06:48:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8408"
  },
  {
    "number": 8407,
    "title": "[https://nvbugs/5583261][ci] waive test_fetch_responses_streaming_sync",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T02:25:34Z",
    "closed_at": "2025-10-16T06:19:32Z",
    "merged_at": "2025-10-16T06:19:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8407"
  },
  {
    "number": 8406,
    "title": "[https://nvbugs/5496705][fix] Fix high IPC overhead with logprobs enabled by removing duplicate sends",
    "user": "nvxuanyuc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T01:37:56Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8406"
  },
  {
    "number": 8405,
    "title": "[TRTLLM-8535][feat] Support DeepSeek V3.2 with FP8 + BF16 KV cache/NVFP4 + BF16 KV cache",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-16T00:58:30Z",
    "closed_at": "2025-10-24T17:40:42Z",
    "merged_at": "2025-10-24T17:40:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8405"
  },
  {
    "number": 8403,
    "title": "[None][infra] Update CI allowed list 2025_10_15",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T17:03:52Z",
    "closed_at": "2025-10-16T21:17:34Z",
    "merged_at": "2025-10-16T21:17:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8403"
  },
  {
    "number": 8402,
    "title": "[TRTLLM-7136][feat] Update load_weights method to include mapping parameter in checkpoint loaders",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T15:13:13Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8402"
  },
  {
    "number": 8401,
    "title": "[None][fix] Fix the error where checkpoint_dir is assigned as NONE wh…",
    "user": "chinamaoge",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T15:06:39Z",
    "closed_at": "2025-10-16T05:37:44Z",
    "merged_at": "2025-10-16T05:37:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8401"
  },
  {
    "number": 8400,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T14:26:04Z",
    "closed_at": "2025-10-16T06:02:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8400"
  },
  {
    "number": 8399,
    "title": "[https://nvbugs/5502901][fix] Set max_seq_len and max_batch_size in TestNemotronUltra test cases to prevent OOM",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T13:34:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8399"
  },
  {
    "number": 8398,
    "title": "[TRTLLM-8436][feat] batched sampling and top-k logprobs improvements",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T13:27:58Z",
    "closed_at": "2025-10-20T09:15:42Z",
    "merged_at": "2025-10-20T09:15:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8398"
  },
  {
    "number": 8397,
    "title": "[https://nvbugs/5437384][test] fix trtllm-llmapi-launch multi tests with single launch",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T13:05:15Z",
    "closed_at": "2025-10-17T04:14:44Z",
    "merged_at": "2025-10-17T04:14:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8397"
  },
  {
    "number": 8396,
    "title": "[None][chore] Cleanup KV cache manager",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T10:58:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8396"
  },
  {
    "number": 8395,
    "title": "[none][feat] Support nano-v2-vlm with multiple PRs",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T10:22:31Z",
    "closed_at": "2025-10-23T02:55:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8395"
  },
  {
    "number": 8394,
    "title": "[TRTLLM-8669][infra] Use artifactory mirror for install python",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T09:46:06Z",
    "closed_at": "2025-10-20T04:17:10Z",
    "merged_at": "2025-10-20T04:17:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8394"
  },
  {
    "number": 8392,
    "title": "[None][feat] Add fmha_v2 kernel for head_dim=80 and sm=100 to support VLM",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T08:42:56Z",
    "closed_at": "2025-10-17T11:42:48Z",
    "merged_at": "2025-10-17T11:42:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8392"
  },
  {
    "number": 8390,
    "title": "[https://nvbugs/5540138][fix] Fix shape error when duplicating kv.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T08:36:26Z",
    "closed_at": "2025-10-17T02:07:29Z",
    "merged_at": "2025-10-17T02:07:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8390"
  },
  {
    "number": 8388,
    "title": "[None][test] cherry-pick: add test-model-suites in integration conftest.py",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T08:07:02Z",
    "closed_at": "2025-10-16T06:26:32Z",
    "merged_at": "2025-10-16T06:26:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8388"
  },
  {
    "number": 8387,
    "title": "[None][ci] move all llama4 test cases to post merge",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T07:57:16Z",
    "closed_at": "2025-10-15T08:36:38Z",
    "merged_at": "2025-10-15T08:36:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8387"
  },
  {
    "number": 8386,
    "title": "[None][infra] Waive failed tests in release post-merge 10/15",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T07:23:00Z",
    "closed_at": "2025-10-15T08:06:09Z",
    "merged_at": "2025-10-15T08:06:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8386"
  },
  {
    "number": 8385,
    "title": "[None][fix] improve mpirun hang issues",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T07:07:07Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8385"
  },
  {
    "number": 8383,
    "title": "[https://nvbugs/5567586][feat] Ampere xqa swa specdec for GPT-OSS Eagle3-one-model",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T06:45:23Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8383"
  },
  {
    "number": 8382,
    "title": "[TRTLLM-8464][infra] Upgrade to DLFW 25.10 and pytorch 2.9.0, triton 3.5.0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T06:00:56Z",
    "closed_at": "2025-10-29T02:55:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8382"
  },
  {
    "number": 8381,
    "title": "[None][fix] fix error when processing batches containing both text and mm data",
    "user": "Nekofish-L",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T04:38:38Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8381"
  },
  {
    "number": 8380,
    "title": "[None][fix] fix visual encoder attention weight loading for Qwen2.5-VL tp deployment (sm < 100)",
    "user": "Nekofish-L",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T03:54:18Z",
    "closed_at": "2025-10-29T04:49:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8380"
  },
  {
    "number": 8379,
    "title": "[https://nvbugs/5542862][fix] Upgrade fmha_v2 cubins compiled with cuda 13.0",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T03:48:01Z",
    "closed_at": "2025-10-20T02:29:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8379"
  },
  {
    "number": 8378,
    "title": "[None] [docs] Update TPOT/ITL docs",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T02:47:33Z",
    "closed_at": "2025-10-15T03:50:54Z",
    "merged_at": "2025-10-15T03:50:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8378"
  },
  {
    "number": 8377,
    "title": "[None][chore] update test duration",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-15T02:39:49Z",
    "closed_at": "2025-10-20T00:45:52Z",
    "merged_at": "2025-10-20T00:45:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8377"
  },
  {
    "number": 8375,
    "title": "[None] [chore] Add OSS compliance to CODEOWNERS",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T23:57:37Z",
    "closed_at": "2025-10-15T13:22:33Z",
    "merged_at": "2025-10-15T13:22:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8375"
  },
  {
    "number": 8373,
    "title": "[None][doc] Ray orchestrator initial doc",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T22:29:39Z",
    "closed_at": "2025-10-15T04:17:58Z",
    "merged_at": "2025-10-15T04:17:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8373"
  },
  {
    "number": 8372,
    "title": "[https://nvbugs/5552889][fix] fix: Prevent empty batch when using attention DP with disagg",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T19:49:58Z",
    "closed_at": "2025-10-16T01:11:04Z",
    "merged_at": "2025-10-16T01:11:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8372"
  },
  {
    "number": 8370,
    "title": "[None][chore] AutoDeplopy: Update expert section on yaml configuration in README",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T15:32:58Z",
    "closed_at": "2025-10-14T16:39:28Z",
    "merged_at": "2025-10-14T16:39:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8370"
  },
  {
    "number": 8369,
    "title": "[None][feat] Update devcontainer configuration to include additional extensions",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T13:53:32Z",
    "closed_at": "2025-10-20T10:29:15Z",
    "merged_at": "2025-10-20T10:29:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8369"
  },
  {
    "number": 8368,
    "title": "[None][fix] Fix request_id for best_of/n case",
    "user": "evezhier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T13:49:01Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8368"
  },
  {
    "number": 8367,
    "title": "[TRTLLM-6741] [feat] Extend lm_head_tp_size to world_size",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T13:04:04Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8367"
  },
  {
    "number": 8366,
    "title": "[TRTLLM-8160][feat] Add max_total_draft_tokens",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T11:42:45Z",
    "closed_at": "2025-10-21T15:11:05Z",
    "merged_at": "2025-10-21T15:11:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8366"
  },
  {
    "number": 8365,
    "title": "[https://nvbugs/5547435][fix] Fix a merge conflict",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T10:39:43Z",
    "closed_at": "2025-10-15T02:43:11Z",
    "merged_at": "2025-10-15T02:43:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8365"
  },
  {
    "number": 8364,
    "title": "[https://nvbugs/5542862][fix] Upgrade fmha_v2.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T10:18:39Z",
    "closed_at": "2025-10-20T02:20:23Z",
    "merged_at": "2025-10-20T02:20:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8364"
  },
  {
    "number": 8363,
    "title": "[https://nvbugs/5461761][fix] Unwaive eagle3 test",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T10:09:46Z",
    "closed_at": "2025-10-16T13:51:48Z",
    "merged_at": "2025-10-16T13:51:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8363"
  },
  {
    "number": 8362,
    "title": "[None][feat] Dev DeepConf",
    "user": "dcaox",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T09:43:29Z",
    "closed_at": "2025-10-16T03:01:32Z",
    "merged_at": "2025-10-16T03:01:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8362"
  },
  {
    "number": 8361,
    "title": "[TRTLLM-8464][infra] Use public triton 3.5.0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T09:37:13Z",
    "closed_at": "2025-10-29T05:09:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8361"
  },
  {
    "number": 8359,
    "title": "[TRTLLM-4866] [test] Support waiving unit tests by waives.txt",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T09:09:30Z",
    "closed_at": "2025-10-20T01:52:52Z",
    "merged_at": "2025-10-20T01:52:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8359"
  },
  {
    "number": 8358,
    "title": "[None][test] remove redunctant runtime backend in perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T08:10:23Z",
    "closed_at": "2025-10-24T06:01:35Z",
    "merged_at": "2025-10-24T06:01:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8358"
  },
  {
    "number": 8357,
    "title": "[TRTLLM-8113][test] Add pytorch workflow e2e tests with pp enabled",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T07:49:45Z",
    "closed_at": "2025-10-15T07:09:22Z",
    "merged_at": "2025-10-15T07:09:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8357"
  },
  {
    "number": 8356,
    "title": "[None][chore] Update nim test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T07:35:29Z",
    "closed_at": "2025-10-15T09:04:21Z",
    "merged_at": "2025-10-15T09:04:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8356"
  },
  {
    "number": 8355,
    "title": "[None][fix] Fix is_post_quant_all2all_supported for MNNVL",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T07:22:24Z",
    "closed_at": "2025-10-14T18:49:21Z",
    "merged_at": "2025-10-14T18:49:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8355"
  },
  {
    "number": 8354,
    "title": "[https://nvbugs/5565541][fix] Add timeout threshold for H100 FHMA test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T05:55:59Z",
    "closed_at": "2025-10-14T08:23:09Z",
    "merged_at": "2025-10-14T08:23:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8354"
  },
  {
    "number": 8353,
    "title": "[https://nvbugs/5541494] [fix] Remove waivers",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T05:53:24Z",
    "closed_at": "2025-10-16T02:10:36Z",
    "merged_at": "2025-10-16T02:10:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8353"
  },
  {
    "number": 8351,
    "title": "[https://nvbugs/5574556][fix] fix bug of Qwen3_235B_A22B::test_fp8 CI",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T04:30:16Z",
    "closed_at": "2025-10-14T07:26:16Z",
    "merged_at": "2025-10-14T07:26:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8351"
  },
  {
    "number": 8350,
    "title": "[None][doc] Add LLM-API API change principle",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T04:05:46Z",
    "closed_at": "2025-11-03T09:47:16Z",
    "merged_at": "2025-11-03T09:47:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8350"
  },
  {
    "number": 8349,
    "title": "[None][ci] waive several rpc tests",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T03:48:30Z",
    "closed_at": "2025-10-14T10:12:49Z",
    "merged_at": "2025-10-14T10:12:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8349"
  },
  {
    "number": 8348,
    "title": "[TRTLLM-7008][fix] unwaive online eplb multi-GPU tests",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T03:38:51Z",
    "closed_at": "2025-11-01T00:53:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8348"
  },
  {
    "number": 8347,
    "title": "[None][feat] Move StreamGeneration to scaffolding main directory",
    "user": "dcaox",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T02:47:44Z",
    "closed_at": "2025-10-14T09:16:05Z",
    "merged_at": "2025-10-14T09:16:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8347"
  },
  {
    "number": 8346,
    "title": "[None][chore] enable chunked context in PyT by default",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T02:25:43Z",
    "closed_at": "2025-10-14T02:55:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8346"
  },
  {
    "number": 8345,
    "title": "[None][fix] Move transfer remaining logits logic to handle_response",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T01:16:13Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8345"
  },
  {
    "number": 8344,
    "title": "[https://nvbugs/5534705][fix] Skip unnecessary CUDA graph capture (#8…",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T01:00:44Z",
    "closed_at": "2025-10-16T02:27:20Z",
    "merged_at": "2025-10-16T02:27:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8344"
  },
  {
    "number": 8343,
    "title": "[None][infra] Pin numexpr in requirements.txt",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-14T00:51:26Z",
    "closed_at": "2025-10-14T04:09:09Z",
    "merged_at": "2025-10-14T04:09:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8343"
  },
  {
    "number": 8333,
    "title": "[None][infra] cherry pick numexpr fix to release/1.1",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T20:44:01Z",
    "closed_at": "2025-10-14T04:20:10Z",
    "merged_at": "2025-10-14T04:20:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8333"
  },
  {
    "number": 8332,
    "title": "[None][feat] Add FP8 rowwise GEMMs for B200",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T20:41:42Z",
    "closed_at": "2025-10-27T20:33:17Z",
    "merged_at": "2025-10-27T20:33:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8332"
  },
  {
    "number": 8331,
    "title": "[TRTLLM-8686][chore] Standardize LlmArgs initialization and parsing around Pydantic",
    "user": "anish-shanbhag",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T19:56:19Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8331"
  },
  {
    "number": 8330,
    "title": "[TRTLLM-8684][chore] Migrate BuildConfig to Pydantic, add a Python wrapper for KVCacheType enum",
    "user": "anish-shanbhag",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T19:42:51Z",
    "closed_at": "2025-10-28T16:17:27Z",
    "merged_at": "2025-10-28T16:17:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8330"
  },
  {
    "number": 8329,
    "title": "[TRTLLM-8682][chore] Remove auto_parallel module",
    "user": "anish-shanbhag",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T19:37:57Z",
    "closed_at": "2025-10-23T00:53:08Z",
    "merged_at": "2025-10-23T00:53:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8329"
  },
  {
    "number": 8328,
    "title": "[TRTLLM-8685][chore] Unify CLI options for LLM API across bench, serve, and eval commands",
    "user": "anish-shanbhag",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T19:28:03Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8328"
  },
  {
    "number": 8327,
    "title": "[None][fix] workaround for numexpr issue",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T15:38:21Z",
    "closed_at": "2025-10-13T18:56:03Z",
    "merged_at": "2025-10-13T18:56:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8327"
  },
  {
    "number": 8326,
    "title": "[None] [doc] Update README",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T13:43:41Z",
    "closed_at": "2025-10-13T14:18:33Z",
    "merged_at": "2025-10-13T14:18:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8326"
  },
  {
    "number": 8325,
    "title": "[TRTLLM-7159][docs] Add documentation for additional outputs",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T13:40:37Z",
    "closed_at": "2025-10-27T08:52:04Z",
    "merged_at": "2025-10-27T08:52:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8325"
  },
  {
    "number": 8324,
    "title": "[https://nvbugs/5521949][fix] Update FP8 model with BF16 LoRA test, fix test_bielik_11b_v2_2_instruct_multi_lora",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T13:30:45Z",
    "closed_at": "2025-10-15T12:48:39Z",
    "merged_at": "2025-10-15T12:48:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8324"
  },
  {
    "number": 8323,
    "title": "[None] [blog] Scaling Expert Parallelism in TensorRT LLM (Part 3: Pushing the Performance Boundary)",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T13:04:28Z",
    "closed_at": "2025-10-13T13:37:18Z",
    "merged_at": "2025-10-13T13:37:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8323"
  },
  {
    "number": 8322,
    "title": "[https://nvbugs/5537738][fix] Add fp8 post-quant allgather support to release 1.1",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T12:52:07Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8322"
  },
  {
    "number": 8321,
    "title": "[None][test] Add post merge test for Seed-OSS-36B-Instruct",
    "user": "zhhuang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T12:11:17Z",
    "closed_at": "2025-10-17T09:30:33Z",
    "merged_at": "2025-10-17T09:30:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8321"
  },
  {
    "number": 8320,
    "title": "[None][fix] Fix cache buffer size for window",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T10:37:40Z",
    "closed_at": "2025-10-16T01:01:11Z",
    "merged_at": "2025-10-16T01:01:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8320"
  },
  {
    "number": 8319,
    "title": "[TRTLLM-8435][infra] Test existing rtxpro6000 stages on rtxpro6000d",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T10:28:49Z",
    "closed_at": "2025-11-03T13:26:18Z",
    "merged_at": "2025-11-03T13:26:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8319"
  },
  {
    "number": 8318,
    "title": "[https://nvbugs/5545522][fix] move PREEXIT in UB kernels to fix accuracy issue",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T10:06:04Z",
    "closed_at": "2025-10-16T01:50:44Z",
    "merged_at": "2025-10-16T01:50:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8318"
  },
  {
    "number": 8317,
    "title": "[TRTLLM-8551][feat] add cache_salt in LLM.generate and refactor test_return_logits.py",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T09:26:47Z",
    "closed_at": "2025-10-15T09:53:58Z",
    "merged_at": "2025-10-15T09:53:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8317"
  },
  {
    "number": 8316,
    "title": "[https://nvbugs/5504095][fix] Unwaive test_user_specify_workspace case.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T09:01:05Z",
    "closed_at": "2025-10-22T01:31:24Z",
    "merged_at": "2025-10-22T01:31:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8316"
  },
  {
    "number": 8315,
    "title": "User/guomingz/fix nvbug 5504095",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T08:55:22Z",
    "closed_at": "2025-10-13T08:55:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8315"
  },
  {
    "number": 8314,
    "title": "[None][fix] LoRA test fixes",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T08:44:28Z",
    "closed_at": "2025-10-15T13:18:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8314"
  },
  {
    "number": 8313,
    "title": "[https://nvbugs/5510879][fix] Fix pytorch & TRT-python flows fused LoRA adapter modules weight split with TP>1",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T08:35:24Z",
    "closed_at": "2025-10-15T15:24:03Z",
    "merged_at": "2025-10-15T15:24:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8313"
  },
  {
    "number": 8312,
    "title": "[TRTLLM-8580][test] save runtime report periodically",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T08:08:17Z",
    "closed_at": "2025-10-17T02:47:26Z",
    "merged_at": "2025-10-17T02:47:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8312"
  },
  {
    "number": 8311,
    "title": "[https://nvbugs/5568951][fix] Fix guided decoding disagg tests",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T07:48:19Z",
    "closed_at": "2025-10-13T10:55:28Z",
    "merged_at": "2025-10-13T10:55:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8311"
  },
  {
    "number": 8310,
    "title": "[None][fix] Add lock for request_to_session in sendReadySingal",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T07:38:38Z",
    "closed_at": "2025-10-14T11:32:38Z",
    "merged_at": "2025-10-14T11:32:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8310"
  },
  {
    "number": 8309,
    "title": "[None][feat] Support Mooncake transfer engine as a cache transceiver backend",
    "user": "wjueyao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T07:22:51Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8309"
  },
  {
    "number": 8307,
    "title": "[https://nvbugs/5550671][fix] fix disagg-serving multinodes test failure",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T07:03:10Z",
    "closed_at": "2025-10-14T06:01:01Z",
    "merged_at": "2025-10-14T06:01:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8307"
  },
  {
    "number": 8306,
    "title": "[None][chore] Bump version to 1.2.0rc0.post1",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T07:00:30Z",
    "closed_at": "2025-10-13T07:19:40Z",
    "merged_at": "2025-10-13T07:19:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8306"
  },
  {
    "number": 8305,
    "title": "[None][chore] Bump version to 1.2.0rc0.post1",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T06:45:32Z",
    "closed_at": "2025-10-13T06:59:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8305"
  },
  {
    "number": 8304,
    "title": "[TRTLLM-8579][feat] Support quantized model for nano-v2-vlm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T06:45:25Z",
    "closed_at": "2025-10-16T01:44:12Z",
    "merged_at": "2025-10-16T01:44:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8304"
  },
  {
    "number": 8303,
    "title": "[#4585][feat] Replace unified attention before export",
    "user": "h-guo18",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T05:38:47Z",
    "closed_at": "2025-10-23T22:02:05Z",
    "merged_at": "2025-10-23T22:02:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8303"
  },
  {
    "number": 8302,
    "title": "[TRTLLM-8511][feat] Add update_weights and sleep_wakeup support for rl integration",
    "user": "shuyixiong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T05:30:39Z",
    "closed_at": "2025-11-04T18:19:24Z",
    "merged_at": "2025-11-04T18:19:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8302"
  },
  {
    "number": 8301,
    "title": "[TRTLLM-8536][feat] Update trtllm gen fmha kernels to support block sparse attention",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T03:36:48Z",
    "closed_at": "2025-10-13T12:54:49Z",
    "merged_at": "2025-10-13T12:54:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8301"
  },
  {
    "number": 8300,
    "title": "[https://nvbugs/5574355][fix] convert_weights method",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T03:31:46Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8300"
  },
  {
    "number": 8299,
    "title": "[None][feat] Add fp16/bf16 no swapab support for hopper xqa",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T02:02:31Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8299"
  },
  {
    "number": 8298,
    "title": "[None][fix] Remove Qwen2 & Qwen2.5-VL tests from test list",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T01:39:53Z",
    "closed_at": "2025-10-17T07:46:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8298"
  },
  {
    "number": 8297,
    "title": "[https://nvbugs/5465642][fix] Increase server timeout to wait weight loading",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T01:31:34Z",
    "closed_at": "2025-10-14T05:55:31Z",
    "merged_at": "2025-10-14T05:55:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8297"
  },
  {
    "number": 8296,
    "title": "[None][fix] Fix bench_serving import error",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-13T01:06:56Z",
    "closed_at": "2025-10-13T05:46:32Z",
    "merged_at": "2025-10-13T05:46:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8296"
  },
  {
    "number": 8295,
    "title": "[None][chore] Waive test failing on pre-merge CI",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-12T20:43:40Z",
    "closed_at": "2025-10-12T23:54:56Z",
    "merged_at": "2025-10-12T23:54:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8295"
  },
  {
    "number": 8294,
    "title": "[https://nvbugs/5404000][fix] Ensure consistency between firstTokenTime and lastTokenTime",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-12T19:09:43Z",
    "closed_at": "2025-10-14T12:15:08Z",
    "merged_at": "2025-10-14T12:15:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8294"
  },
  {
    "number": 8293,
    "title": "[None][infra] Skip failed cases for main branch",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-12T14:02:33Z",
    "closed_at": "2025-10-12T15:04:09Z",
    "merged_at": "2025-10-12T15:04:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8293"
  },
  {
    "number": 8292,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-12T13:03:34Z",
    "closed_at": "2025-10-13T03:12:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8292"
  },
  {
    "number": 8291,
    "title": "[None][infra] Update and waive failed tests for release branch",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-12T12:52:32Z",
    "closed_at": "2025-10-12T13:51:55Z",
    "merged_at": "2025-10-12T13:51:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8291"
  },
  {
    "number": 8290,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-12T10:44:09Z",
    "closed_at": "2025-10-13T07:07:01Z",
    "merged_at": "2025-10-13T07:07:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8290"
  },
  {
    "number": 8289,
    "title": "[None][chore] Update disagg benchmark configs",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-12T08:41:19Z",
    "closed_at": "2025-10-13T10:15:47Z",
    "merged_at": "2025-10-13T10:15:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8289"
  },
  {
    "number": 8288,
    "title": "[None][doc] Add qwen3-next doc into deployment guide and test case into L0.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-11T14:23:14Z",
    "closed_at": "2025-10-13T02:25:46Z",
    "merged_at": "2025-10-13T02:25:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8288"
  },
  {
    "number": 8287,
    "title": "[None][doc] Fix several invalid ref links in deployment guide sections.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-11T13:59:54Z",
    "closed_at": "2025-10-13T02:22:33Z",
    "merged_at": "2025-10-13T02:22:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8287"
  },
  {
    "number": 8284,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-11T08:44:21Z",
    "closed_at": "2025-10-13T03:28:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8284"
  },
  {
    "number": 8282,
    "title": "[None][chore] Waive failing pre-merge test on main",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-11T04:43:28Z",
    "closed_at": "2025-10-11T06:52:06Z",
    "merged_at": "2025-10-11T06:52:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8282"
  },
  {
    "number": 8281,
    "title": "[None][infra] Update comments for pre-merge GB200 multi-node testing stage",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-11T02:52:23Z",
    "closed_at": "2025-10-13T06:56:08Z",
    "merged_at": "2025-10-13T06:56:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8281"
  },
  {
    "number": 8280,
    "title": "[None][chore] Upgrade transformers to 4.57.0.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-11T01:56:58Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8280"
  },
  {
    "number": 8279,
    "title": "[https://nvbugs/5322131][feat] Multi-LoRA serving with CUDA Graph",
    "user": "JyChang012",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-11T01:43:43Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8279"
  },
  {
    "number": 8277,
    "title": "[TRTLLM-8683][chore] Migrate PluginConfig to Pydantic",
    "user": "anish-shanbhag",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-11T01:12:15Z",
    "closed_at": "2025-10-17T20:13:23Z",
    "merged_at": "2025-10-17T20:13:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8277"
  },
  {
    "number": 8276,
    "title": "[TRTLLM-7225][chore] Refactor model loading code for gptoss",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T22:45:15Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8276"
  },
  {
    "number": 8275,
    "title": "[None][feat] Enable CUDA graph support for KvConnectorWorker API",
    "user": "nv-kmcgill53",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T22:36:18Z",
    "closed_at": "2025-10-17T22:09:03Z",
    "merged_at": "2025-10-17T22:09:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8275"
  },
  {
    "number": 8273,
    "title": "[https://nvbugs/5565530][fix] Unwaive test",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T18:46:07Z",
    "closed_at": "2025-10-13T15:59:33Z",
    "merged_at": "2025-10-13T15:59:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8273"
  },
  {
    "number": 8269,
    "title": "[https://nvbugs/5538098][fix] Checking connection to etcd server in unit test",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T12:21:31Z",
    "closed_at": "2025-10-10T21:31:37Z",
    "merged_at": "2025-10-10T21:31:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8269"
  },
  {
    "number": 8268,
    "title": "[https://nvbugs/5522746][fix] unwaive tests caused by node issues after rebooting",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T10:04:28Z",
    "closed_at": "2025-10-13T05:31:52Z",
    "merged_at": "2025-10-13T05:31:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8268"
  },
  {
    "number": 8267,
    "title": "[None][infra] Remove WAR code for GH200 node",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T10:02:06Z",
    "closed_at": "2025-10-12T03:40:17Z",
    "merged_at": "2025-10-12T03:40:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8267"
  },
  {
    "number": 8266,
    "title": "[None][infra] Remove WAR code for GH200 node",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T09:57:20Z",
    "closed_at": "2025-10-12T03:33:15Z",
    "merged_at": "2025-10-12T03:33:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8266"
  },
  {
    "number": 8265,
    "title": "[None][test] Add gpt_oss_20b Model to Sanity Perf Test",
    "user": "yufeiwu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T09:54:50Z",
    "closed_at": "2025-10-28T05:36:29Z",
    "merged_at": "2025-10-28T05:36:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8265"
  },
  {
    "number": 8264,
    "title": "[TRTLLM-8532][chore] clean warmup method of ModelEngine",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T09:19:10Z",
    "closed_at": "2025-10-15T15:40:58Z",
    "merged_at": "2025-10-15T15:40:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8264"
  },
  {
    "number": 8263,
    "title": "[None][chore] update torch_dtype -> dtype in 'transformers'",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T09:05:18Z",
    "closed_at": "2025-10-15T08:09:30Z",
    "merged_at": "2025-10-15T08:09:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8263"
  },
  {
    "number": 8262,
    "title": "[https://nvbugs/5546197][fix] Use local tokenizer",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T09:02:06Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8262"
  },
  {
    "number": 8261,
    "title": "[None][feat] Add torch compile support for cuda core GEMM OP",
    "user": "DylanChen-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T08:38:40Z",
    "closed_at": "2025-10-13T03:57:17Z",
    "merged_at": "2025-10-13T03:57:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8261"
  },
  {
    "number": 8260,
    "title": "[None][fix] Fix EventLoopShutdownError",
    "user": "dcaox",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T08:18:50Z",
    "closed_at": "2025-10-13T09:31:34Z",
    "merged_at": "2025-10-13T09:31:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8260"
  },
  {
    "number": 8259,
    "title": "[TRTLLM-8477][chore] Replace KvCacheConfigCpp with KvCacheConfig inside PyExecutor",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T07:50:24Z",
    "closed_at": "2025-10-13T06:55:37Z",
    "merged_at": "2025-10-13T06:55:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8259"
  },
  {
    "number": 8258,
    "title": "[https://nvbugs/5470769][chore] unwaive test for PR7338",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T05:57:43Z",
    "closed_at": "2025-10-14T03:17:03Z",
    "merged_at": "2025-10-14T03:17:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8258"
  },
  {
    "number": 8257,
    "title": "[https://nvbugs/5565590][fix] test_request_perf_metrics_draft",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T05:19:46Z",
    "closed_at": "2025-10-12T02:01:20Z",
    "merged_at": "2025-10-12T02:01:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8257"
  },
  {
    "number": 8256,
    "title": "[None][feat] Support Glm4MoeForCausalLM",
    "user": "dmtri35",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T04:29:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8256"
  },
  {
    "number": 8255,
    "title": "[https://nvbugs/5523315][fix] Fix serve benchmark test",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T04:23:51Z",
    "closed_at": "2025-11-03T08:30:14Z",
    "merged_at": "2025-11-03T08:30:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8255"
  },
  {
    "number": 8254,
    "title": "[None][fix] add timeout for llama4",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T03:43:39Z",
    "closed_at": "2025-10-13T04:04:20Z",
    "merged_at": "2025-10-13T04:04:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8254"
  },
  {
    "number": 8253,
    "title": "[https://nvbugs/5552132][fix] Enable LoRa for GPT OSS Torch",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T03:39:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8253"
  },
  {
    "number": 8252,
    "title": "[None][fix] cherry-pick !8217 pin flashinfer-python version (#8217)",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T03:33:04Z",
    "closed_at": "2025-10-10T06:48:23Z",
    "merged_at": "2025-10-10T06:48:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8252"
  },
  {
    "number": 8251,
    "title": "[https://nvbugs/5532023][fix] unwaive GenerationExecutor tests",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T02:54:21Z",
    "closed_at": "2025-10-11T02:43:04Z",
    "merged_at": "2025-10-11T02:43:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8251"
  },
  {
    "number": 8250,
    "title": "[https://nvbugs/5563653][infra] reduce docker image layers",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T02:24:45Z",
    "closed_at": "2025-10-13T08:38:28Z",
    "merged_at": "2025-10-13T08:38:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8250"
  },
  {
    "number": 8249,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-10T02:22:58Z",
    "closed_at": "2025-10-10T02:57:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8249"
  },
  {
    "number": 8248,
    "title": "[#8242][Autodeploy] Add int4 GPTQ support for AutoDeploy",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T22:11:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8248"
  },
  {
    "number": 8247,
    "title": "[https://nvbugs/5534837][fix] Fix KV cache split on long context",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T18:03:43Z",
    "closed_at": "2025-10-13T15:48:50Z",
    "merged_at": "2025-10-13T15:48:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8247"
  },
  {
    "number": 8246,
    "title": "[None][feat] Support KV Connector with Disagg Prefill Worker",
    "user": "jthomson04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T17:47:31Z",
    "closed_at": "2025-10-24T18:09:06Z",
    "merged_at": "2025-10-24T18:09:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8246"
  },
  {
    "number": 8231,
    "title": "[https://nvbugs/5537878][fix] Reserve an extra slot for padded batch …",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T14:25:12Z",
    "closed_at": "2025-10-14T06:34:23Z",
    "merged_at": "2025-10-14T06:34:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8231"
  },
  {
    "number": 8230,
    "title": "[None][infra] Waive failed tests on main 10/09",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T13:52:45Z",
    "closed_at": "2025-10-09T14:46:08Z",
    "merged_at": "2025-10-09T14:46:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8230"
  },
  {
    "number": 8229,
    "title": "[None][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T13:49:51Z",
    "closed_at": "2025-10-10T06:45:29Z",
    "merged_at": "2025-10-10T06:45:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8229"
  },
  {
    "number": 8228,
    "title": "[https://nvbugs/5565565] [fix] fp8 wideep support sm103",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T12:07:54Z",
    "closed_at": "2025-10-15T02:17:08Z",
    "merged_at": "2025-10-15T02:17:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8228"
  },
  {
    "number": 8227,
    "title": "[None][chore] Restore asserts in pytorch flow LoRA tests",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T11:18:53Z",
    "closed_at": "2025-10-09T14:10:39Z",
    "merged_at": "2025-10-09T14:10:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8227"
  },
  {
    "number": 8226,
    "title": "[https://nvbugs/5565590][fix] test_request_perf_metrics_draft",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T10:59:02Z",
    "closed_at": "2025-10-10T05:21:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8226"
  },
  {
    "number": 8225,
    "title": "[None][chore] Remove with-statement in RPC LLM usage",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T10:20:16Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8225"
  },
  {
    "number": 8224,
    "title": "[None][chore] Update test configs for release",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T09:31:23Z",
    "closed_at": "2025-10-13T06:07:33Z",
    "merged_at": "2025-10-13T06:07:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8224"
  },
  {
    "number": 8222,
    "title": "[https://nvbugs/5563653][infra] reduce docker image layers",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T08:40:34Z",
    "closed_at": "2025-10-10T02:17:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8222"
  },
  {
    "number": 8220,
    "title": "[None][fix] add gc for test fixture",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T07:55:37Z",
    "closed_at": "2025-10-10T09:50:26Z",
    "merged_at": "2025-10-10T09:50:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8220"
  },
  {
    "number": 8219,
    "title": "[None][fix] Avoid overwrite of `kv_cache_config.max_tokens` for VSWA scheme for the KVCacheManager",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T07:31:40Z",
    "closed_at": "2025-10-20T01:48:40Z",
    "merged_at": "2025-10-20T01:48:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8219"
  },
  {
    "number": 8218,
    "title": "[None][chore] Print log with time for starting to load safetensor weights",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T07:08:28Z",
    "closed_at": "2025-10-10T05:54:54Z",
    "merged_at": "2025-10-10T05:54:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8218"
  },
  {
    "number": 8217,
    "title": "[None][ci] pin flashinfer-python version",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T06:29:00Z",
    "closed_at": "2025-10-09T09:48:49Z",
    "merged_at": "2025-10-09T09:48:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8217"
  },
  {
    "number": 8216,
    "title": "[TRTLLM-8214][feat] Support Qwen3 tool parser",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T06:28:31Z",
    "closed_at": "2025-10-29T07:48:29Z",
    "merged_at": "2025-10-29T07:48:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8216"
  },
  {
    "number": 8215,
    "title": "[TRTLLM-7843][feat] implement disagg cluster auto-scaling",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T06:28:30Z",
    "closed_at": "2025-10-21T21:25:07Z",
    "merged_at": "2025-10-21T21:25:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8215"
  },
  {
    "number": 8214,
    "title": "[None][doc] Refine deployment guide by renaming TRT-LLM to TensorRT L…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T06:13:01Z",
    "closed_at": "2025-10-09T09:11:25Z",
    "merged_at": "2025-10-09T09:11:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8214"
  },
  {
    "number": 8213,
    "title": "[https://nvbugs/5547416][fix] unwaive no_cache test",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T05:43:04Z",
    "closed_at": "2025-10-10T08:50:13Z",
    "merged_at": "2025-10-10T08:50:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8213"
  },
  {
    "number": 8212,
    "title": "[TRTLLM-8246][test] add multimodal kvcache+chunked_prefil cases in to QA test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T05:20:03Z",
    "closed_at": "2025-10-13T03:38:39Z",
    "merged_at": "2025-10-13T03:38:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8212"
  },
  {
    "number": 8211,
    "title": "[None][chore] Update constaint for release",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T05:09:51Z",
    "closed_at": "2025-10-13T03:14:24Z",
    "merged_at": "2025-10-13T03:14:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8211"
  },
  {
    "number": 8210,
    "title": "[TRTLLM-7846][feat] implement etcd storage for disagg cluster",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T04:40:00Z",
    "closed_at": "2025-10-14T20:48:42Z",
    "merged_at": "2025-10-14T20:48:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8210"
  },
  {
    "number": 8209,
    "title": "[https://nvbugs/5560921][fix] GenerationExecutor RPC",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T03:43:58Z",
    "closed_at": "2025-10-16T01:05:22Z",
    "merged_at": "2025-10-16T01:05:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8209"
  },
  {
    "number": 8208,
    "title": "[None][chore] set the default value of max_num_tokens explicitly",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T01:44:09Z",
    "closed_at": "2025-10-15T06:03:03Z",
    "merged_at": "2025-10-15T06:03:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8208"
  },
  {
    "number": 8207,
    "title": "[https://nvbugs/5558167][fix] update canceled_req_ids correctly for canceled requests",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T01:10:45Z",
    "closed_at": "2025-10-10T10:58:26Z",
    "merged_at": "2025-10-10T10:58:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8207"
  },
  {
    "number": 8206,
    "title": "[https://nvbugs/5563469][fix] Temporarily disable test_nemotron_nano_8b_lora_torch in L0 due to Torch non-determinism",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-09T00:15:01Z",
    "closed_at": "2025-10-14T15:55:29Z",
    "merged_at": "2025-10-14T15:55:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8206"
  },
  {
    "number": 8203,
    "title": "[None][feat] AutoDeploy: VLMs with subgraphs + cudagraph/compile",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T20:57:39Z",
    "closed_at": "2025-10-14T00:34:10Z",
    "merged_at": "2025-10-14T00:34:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8203"
  },
  {
    "number": 8202,
    "title": "[None][feat] AutoDeploy: VLMs with subgraphs + cudagraph/compile",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T20:37:29Z",
    "closed_at": "2025-10-08T20:37:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8202"
  },
  {
    "number": 8200,
    "title": "[None][chore] Mass integration of release/1.1",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T15:09:22Z",
    "closed_at": "2025-10-16T14:46:20Z",
    "merged_at": "2025-10-16T14:46:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8200"
  },
  {
    "number": 8199,
    "title": "[https://nvbugs/5521949][fix] Replace test_codellama_fp8_with_bf16_lora with test_llama_3_1_8b_fp8_with_bf16_lora",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T14:51:28Z",
    "closed_at": "2025-10-13T13:01:55Z",
    "merged_at": "2025-10-13T13:01:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8199"
  },
  {
    "number": 8198,
    "title": "[None][fix] Fix the error where checkpoint_dir is assigned as NONE wh…",
    "user": "chinamaoge",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T12:59:50Z",
    "closed_at": "2025-10-15T14:36:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8198"
  },
  {
    "number": 8197,
    "title": "[None][fix] Fix NVFP4 KV size calculation",
    "user": "Tom-Zheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T12:34:11Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8197"
  },
  {
    "number": 8195,
    "title": "[None][doc] Add Qwen3 Next Quick Start Guide",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T10:12:02Z",
    "closed_at": "2025-10-13T07:49:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8195"
  },
  {
    "number": 8194,
    "title": "[TRTLLM-8136][feat] Dynamic draft length in spec decode (stage 1).",
    "user": "zheyuf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T07:27:44Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8194"
  },
  {
    "number": 8193,
    "title": "[https://nvbugs/5522746][fix] unwaive tests caused by node issues after rebooting",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T07:03:44Z",
    "closed_at": "2025-10-09T00:45:56Z",
    "merged_at": "2025-10-09T00:45:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8193"
  },
  {
    "number": 8192,
    "title": "[https://nvbugs/5532789] [doc] Add documents about CUDA 12.9",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T05:57:36Z",
    "closed_at": "2025-10-13T07:35:37Z",
    "merged_at": "2025-10-13T07:35:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8192"
  },
  {
    "number": 8191,
    "title": "[TRTLLM-8464][infra] use script for triton WAR code",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T05:57:04Z",
    "closed_at": "2025-10-29T05:09:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8191"
  },
  {
    "number": 8190,
    "title": "[https://nvbugs/5540752][fix] Support quantized Phi4 MM models",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T05:46:14Z",
    "closed_at": "2025-10-20T10:36:10Z",
    "merged_at": "2025-10-20T10:36:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8190"
  },
  {
    "number": 8189,
    "title": "[None][ci] move some llama4 test cases to pre merge",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T05:16:11Z",
    "closed_at": "2025-10-09T01:34:09Z",
    "merged_at": "2025-10-09T01:34:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8189"
  },
  {
    "number": 8188,
    "title": "[https://nvbugs/5550283][fix] update test case to call post quantization explicitly due to code refactor",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T02:43:49Z",
    "closed_at": "2025-10-09T01:41:48Z",
    "merged_at": "2025-10-09T01:41:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8188"
  },
  {
    "number": 8187,
    "title": "[https://nvbugs/5550283][fix] update test case to call post quantization explicitly due to code refactor",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T02:39:38Z",
    "closed_at": "2025-10-08T02:39:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8187"
  },
  {
    "number": 8186,
    "title": "[None][chore] Waive some tests failing on main post merge",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T01:26:25Z",
    "closed_at": "2025-10-08T13:52:31Z",
    "merged_at": "2025-10-08T13:52:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8186"
  },
  {
    "number": 8185,
    "title": "[None][chore] Waive tests failing on release/1.1 post merge",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-08T01:15:30Z",
    "closed_at": "2025-10-08T16:59:51Z",
    "merged_at": "2025-10-08T16:59:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8185"
  },
  {
    "number": 8183,
    "title": "[None][fix] Remove outdated test waives for GPTOSS",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T22:18:31Z",
    "closed_at": "2025-10-14T23:20:39Z",
    "merged_at": "2025-10-14T23:20:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8183"
  },
  {
    "number": 8182,
    "title": "[None][fix] Restrict tinygemm use to certain SMs",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T22:09:32Z",
    "closed_at": "2025-10-09T00:55:58Z",
    "merged_at": "2025-10-09T00:55:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8182"
  },
  {
    "number": 8181,
    "title": "[None][doc] Add Qwen3-Next Guide (new)",
    "user": "faradawn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T21:01:52Z",
    "closed_at": "2025-10-14T01:46:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8181"
  },
  {
    "number": 8180,
    "title": "[OMNIML-2336][feat] w4a8 nvfp4 fp8 exports scale factor properly",
    "user": "sychen52",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T20:33:56Z",
    "closed_at": "2025-10-15T05:41:28Z",
    "merged_at": "2025-10-15T05:41:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8180"
  },
  {
    "number": 8178,
    "title": "[https://nvbugs/5429636][feat] Add KV cache transfer timeout",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T17:10:24Z",
    "closed_at": "2025-10-17T18:32:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8178"
  },
  {
    "number": 8176,
    "title": "[None][infra] Skip failed cases for main",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T12:54:22Z",
    "closed_at": "2025-10-07T13:37:51Z",
    "merged_at": "2025-10-07T13:37:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8176"
  },
  {
    "number": 8175,
    "title": "[TRTLLM-8507][fix] Fix ray resource cleanup and error handling in LoRA test",
    "user": "shuyixiong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T11:22:44Z",
    "closed_at": "2025-10-14T15:46:31Z",
    "merged_at": "2025-10-14T15:46:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8175"
  },
  {
    "number": 8174,
    "title": "[TRTLLM-7769][chore] document the role of 'd2t'",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T10:10:08Z",
    "closed_at": "2025-10-09T17:13:51Z",
    "merged_at": "2025-10-09T17:13:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8174"
  },
  {
    "number": 8173,
    "title": "[TRTLLM-8680][doc] Add table with one-line deployment commands to docs",
    "user": "anish-shanbhag",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T09:18:12Z",
    "closed_at": "2025-11-04T01:42:41Z",
    "merged_at": "2025-11-04T01:42:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8173"
  },
  {
    "number": 8169,
    "title": "[https://nvbugs/5550283][fix] update to the latest MoE API",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T03:47:28Z",
    "closed_at": "2025-10-07T13:12:21Z",
    "merged_at": "2025-10-07T13:12:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8169"
  },
  {
    "number": 8167,
    "title": "[https://nvbugs/5503138] [fix] Remove compile warnings",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T03:44:23Z",
    "closed_at": "2025-10-13T05:24:23Z",
    "merged_at": "2025-10-13T05:24:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8167"
  },
  {
    "number": 8166,
    "title": "[None][feat] AutoDeploy: abstract cache interfaces",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T02:09:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8166"
  },
  {
    "number": 8165,
    "title": "[https://nvbugs/5550283][fix] update test case to the latest MoE API",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-07T02:07:54Z",
    "closed_at": "2025-10-08T05:54:35Z",
    "merged_at": "2025-10-08T05:54:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8165"
  },
  {
    "number": 8164,
    "title": "[None] [doc] Add Mixed Precision Context and Generation section to Disagg",
    "user": "timothygao8710",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T23:40:14Z",
    "closed_at": "2025-10-22T19:36:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8164"
  },
  {
    "number": 8163,
    "title": "[None] [doc] Add Mixed Precision Context and Generation section",
    "user": "timothygao8710",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T23:29:14Z",
    "closed_at": "2025-10-06T23:31:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8163"
  },
  {
    "number": 8162,
    "title": "[None][chore] Increase operations-per-run to 1000 for stale action",
    "user": "karljang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T21:12:30Z",
    "closed_at": "2025-10-06T22:02:43Z",
    "merged_at": "2025-10-06T22:02:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8162"
  },
  {
    "number": 8161,
    "title": "[None][fix] Fix MTP illegal memory access",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T19:44:00Z",
    "closed_at": "2025-10-07T18:02:55Z",
    "merged_at": "2025-10-07T18:02:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8161"
  },
  {
    "number": 8160,
    "title": "trigger CI",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T17:50:09Z",
    "closed_at": "2025-11-05T20:29:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8160"
  },
  {
    "number": 8159,
    "title": "[https://nvbugs/5455140][fix] unwaive tests related to GB200 OOM",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T16:23:21Z",
    "closed_at": "2025-10-08T05:14:12Z",
    "merged_at": "2025-10-08T05:14:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8159"
  },
  {
    "number": 8158,
    "title": "[None][feat] AutoDeploy: chunked prefill support",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T15:45:34Z",
    "closed_at": "2025-10-18T07:47:35Z",
    "merged_at": "2025-10-18T07:47:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8158"
  },
  {
    "number": 8157,
    "title": "[None][feat] AutoDeploy: per graph or whole module transform infrastructure",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T14:11:03Z",
    "closed_at": "2025-10-09T19:21:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8157"
  },
  {
    "number": 8156,
    "title": "[None][feat] Update TRTLLM MoE MxFP4 cubins; autotune tileN",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T12:51:53Z",
    "closed_at": "2025-10-23T01:14:19Z",
    "merged_at": "2025-10-23T01:14:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8156"
  },
  {
    "number": 8154,
    "title": "[None][fix] waive rpc tests in ray stage",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T02:35:27Z",
    "closed_at": "2025-10-06T18:00:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8154"
  },
  {
    "number": 8153,
    "title": "[TRTLLM-8948][feat] Support custom sharding config source",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-06T00:29:32Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8153"
  },
  {
    "number": 8152,
    "title": "[None][chore] fix llmargs conflict",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-05T13:49:17Z",
    "closed_at": "2025-10-06T09:34:28Z",
    "merged_at": "2025-10-06T09:34:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8152"
  },
  {
    "number": 8151,
    "title": "[None][chore] Remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-05T12:17:17Z",
    "closed_at": "2025-10-10T05:39:40Z",
    "merged_at": "2025-10-10T05:39:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8151"
  },
  {
    "number": 8150,
    "title": "[https://nvbugs/5556475] [fix] Fix the `tensorrt_llm_bls` model to correctly return the outputs for `num_input_tokens` and `num_output_tokens`",
    "user": "pskiran1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-05T12:04:37Z",
    "closed_at": "2025-10-28T04:06:28Z",
    "merged_at": "2025-10-28T04:06:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8150"
  },
  {
    "number": 8149,
    "title": "[None][fix] AD test_trtllm_bench to use small model config and skip loading weights",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-05T11:43:32Z",
    "closed_at": "2025-10-12T15:30:20Z",
    "merged_at": "2025-10-12T15:30:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8149"
  },
  {
    "number": 8148,
    "title": "[None][feat] Revise the calculation related to TileN in routing of MOE TRTLLM backend",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-05T11:21:32Z",
    "closed_at": "2025-10-16T01:15:46Z",
    "merged_at": "2025-10-16T01:15:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8148"
  },
  {
    "number": 8147,
    "title": "[https://nvbugs/5546202][fix] Fix concurrent bug for NIXL cache transceiver",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-05T10:32:37Z",
    "closed_at": "2025-10-13T07:40:57Z",
    "merged_at": "2025-10-13T07:40:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8147"
  },
  {
    "number": 8146,
    "title": "[https://nvbugs/5521949][fix] Re-enable test_bielik_11b_v2_2_instruct_multi_lora, fix its API use with pytorch flow LoRA",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-05T08:18:22Z",
    "closed_at": "2025-10-05T11:28:21Z",
    "merged_at": "2025-10-05T11:28:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8146"
  },
  {
    "number": 8145,
    "title": "[TRTLLM-7731][feat] Avoid over-allocation of KV cache for transmission in disagg with CP",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-05T00:02:30Z",
    "closed_at": "2025-11-01T00:32:40Z",
    "merged_at": "2025-11-01T00:32:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8145"
  },
  {
    "number": 8144,
    "title": "[fix] Fix logger reference in quantization.py",
    "user": "koush",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-04T23:54:19Z",
    "closed_at": "2025-10-22T20:53:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8144"
  },
  {
    "number": 8141,
    "title": "[https://nvbugs/5488576][fix] Propagate disable_finalize_fusion config flag in WIDEEP MoE backend",
    "user": "sklevtsov-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-04T02:10:56Z",
    "closed_at": "2025-10-07T21:44:54Z",
    "merged_at": "2025-10-07T21:44:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8141"
  },
  {
    "number": 8140,
    "title": "[None][fix] Fix and enhance MemoryCounters Singleton with compile-time safety and bounds checking",
    "user": "Fan-Yunfan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-04T01:39:45Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8140"
  },
  {
    "number": 8139,
    "title": "[None][chore]: trivial change for CI.",
    "user": "joyang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-04T00:18:10Z",
    "closed_at": "2025-10-05T02:16:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8139"
  },
  {
    "number": 8138,
    "title": "[None][fix] Adding docker folder to Dockerfile",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T22:34:27Z",
    "closed_at": "2025-10-05T17:41:40Z",
    "merged_at": "2025-10-05T17:41:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8138"
  },
  {
    "number": 8137,
    "title": "[None][feat] AutoDeploy: graph/module inputs with kwargs instead of args",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T19:27:56Z",
    "closed_at": "2025-10-03T23:53:43Z",
    "merged_at": "2025-10-03T23:53:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8137"
  },
  {
    "number": 8136,
    "title": "[None][feat] AutoDeploy: Nemotron-H accuracy testing support",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T17:41:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8136"
  },
  {
    "number": 8135,
    "title": "[None][feat] Implement support for Falcon-H1 models",
    "user": "dbari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T16:50:47Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8135"
  },
  {
    "number": 8134,
    "title": "[None][chore] AutoDeploy: clean up accuracy test configs",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T16:00:01Z",
    "closed_at": "2025-10-06T19:51:01Z",
    "merged_at": "2025-10-06T19:51:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8134"
  },
  {
    "number": 8133,
    "title": "[None][feat] AutoDeploy: Nemotron-H accuracy test",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T15:08:07Z",
    "closed_at": "2025-10-03T22:39:04Z",
    "merged_at": "2025-10-03T22:39:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8133"
  },
  {
    "number": 8132,
    "title": "[TRTLLM-8414][chore] BREAKING CHANGE: refine sampling strategy selection",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T12:56:50Z",
    "closed_at": "2025-10-08T13:46:50Z",
    "merged_at": "2025-10-08T13:46:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8132"
  },
  {
    "number": 8130,
    "title": "[https://nvbugs/5516666][fix] unwaive some Qwen3 CI tests",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T06:52:12Z",
    "closed_at": "2025-10-09T01:44:58Z",
    "merged_at": "2025-10-09T01:44:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8130"
  },
  {
    "number": 8129,
    "title": "[https://nvbugs/5548098][fix] Fix flakey unit test for dynamic spec d…",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T05:27:06Z",
    "closed_at": "2025-10-03T05:58:38Z",
    "merged_at": "2025-10-03T05:58:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8129"
  },
  {
    "number": 8128,
    "title": "[None][feat] Add request timing breakdown option in benchmark_serving",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T05:18:05Z",
    "closed_at": "2025-10-10T16:24:55Z",
    "merged_at": "2025-10-10T16:24:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8128"
  },
  {
    "number": 8127,
    "title": "[None][feat] Support ignored prompt length for penalties via new sampling config parameter",
    "user": "nvxuanyuc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-03T00:12:39Z",
    "closed_at": "2025-10-27T17:12:31Z",
    "merged_at": "2025-10-27T17:12:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8127"
  },
  {
    "number": 8126,
    "title": "[None][feat] AutoDeploy: compiler backends based on nn.Module",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-02T21:51:40Z",
    "closed_at": "2025-10-03T16:14:21Z",
    "merged_at": "2025-10-03T16:14:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8126"
  },
  {
    "number": 8124,
    "title": "draft: all reduce",
    "user": "NVShreyas",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-02T19:06:46Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8124"
  },
  {
    "number": 8123,
    "title": "[TRTLLM-6342][feat] Factory TP sharding of quantized models",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-02T16:09:13Z",
    "closed_at": "2025-10-13T21:04:47Z",
    "merged_at": "2025-10-13T21:04:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8123"
  },
  {
    "number": 8121,
    "title": "[TRTLLM-8413][chore] resolve sampling defaults in OpenAI API backend",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-02T14:20:49Z",
    "closed_at": "2025-10-06T13:09:43Z",
    "merged_at": "2025-10-06T13:09:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8121"
  },
  {
    "number": 8120,
    "title": "[None][feat] AutoDeploy add autotuning when capturing cudagraphs",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-02T08:38:29Z",
    "closed_at": "2025-10-03T15:33:22Z",
    "merged_at": "2025-10-03T15:33:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8120"
  },
  {
    "number": 8119,
    "title": "[https://nvbugs/5556020][chore] waive test_eagle3",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-02T06:44:25Z",
    "closed_at": "2025-10-02T09:33:22Z",
    "merged_at": "2025-10-02T09:33:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8119"
  },
  {
    "number": 8118,
    "title": "[TRTLLM-6342][bug] Patched incorrect starcoder tp config",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T21:21:18Z",
    "closed_at": "2025-10-02T22:42:00Z",
    "merged_at": "2025-10-02T22:42:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8118"
  },
  {
    "number": 8117,
    "title": "Test CI",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T20:40:37Z",
    "closed_at": "2025-10-02T20:42:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8117"
  },
  {
    "number": 8116,
    "title": "[None][chore] Adding install_tensorrt.sh script to pip wheel",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T17:49:09Z",
    "closed_at": "2025-10-02T19:47:12Z",
    "merged_at": "2025-10-02T19:47:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8116"
  },
  {
    "number": 8115,
    "title": "[None][fix] Fix MTP 2-model",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T17:06:33Z",
    "closed_at": "2025-10-03T17:13:51Z",
    "merged_at": "2025-10-03T17:13:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8115"
  },
  {
    "number": 8114,
    "title": "[None][feat] Support for cancelling requests with disaggregation",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T14:48:44Z",
    "closed_at": "2025-10-02T18:04:26Z",
    "merged_at": "2025-10-02T18:04:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8114"
  },
  {
    "number": 8113,
    "title": "[None][feat] Remove request ID from CacheSender/CacheReceiver",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T14:41:23Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8113"
  },
  {
    "number": 8112,
    "title": "[None][fix] fix patchelf version issue",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T13:59:47Z",
    "closed_at": "2025-10-01T20:39:23Z",
    "merged_at": "2025-10-01T20:39:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8112"
  },
  {
    "number": 8111,
    "title": "[None][test] Add accuracy test for Qwen3Next model",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T10:52:25Z",
    "closed_at": "2025-10-13T07:55:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8111"
  },
  {
    "number": 8110,
    "title": "[TRTLLM-8269][test] do not explicitly pass temperature=0 to select greedy sampling",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T08:41:20Z",
    "closed_at": "2025-10-02T08:20:32Z",
    "merged_at": "2025-10-02T08:20:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8110"
  },
  {
    "number": 8109,
    "title": "[TRTLLM-8160][TRTLLM-8161][feat][draft]Add runtime logic for draft token tree",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T07:53:44Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8109"
  },
  {
    "number": 8108,
    "title": "[None][feat] AutoDeploy: dive deeper into token generation bugs + enable_block_reuse",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-01T02:15:03Z",
    "closed_at": "2025-10-03T11:57:27Z",
    "merged_at": "2025-10-03T11:57:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8108"
  },
  {
    "number": 8106,
    "title": "[None][chore] Revert MNNVL alltoall MR",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T22:32:40Z",
    "closed_at": "2025-10-01T00:05:42Z",
    "merged_at": "2025-10-01T00:05:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8106"
  },
  {
    "number": 8105,
    "title": "Draft: [None][fix] fix: updating patchelf version",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T19:22:13Z",
    "closed_at": "2025-10-01T14:55:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8105"
  },
  {
    "number": 8104,
    "title": "[TRTLLM-5966][feat] Helix: add full MLA support for Helix",
    "user": "MatthiasKohl",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T17:01:23Z",
    "closed_at": "2025-11-04T01:06:59Z",
    "merged_at": "2025-11-04T01:06:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8104"
  },
  {
    "number": 8103,
    "title": "[TRTLLM-8269][fix] Revert \"do not explicitly pass temperature=0 to select greedy sampling\"",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T16:47:54Z",
    "closed_at": "2025-09-30T20:53:50Z",
    "merged_at": "2025-09-30T20:53:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8103"
  },
  {
    "number": 8102,
    "title": "[None][infra] Skip failed tests in post-merge for main",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T15:01:32Z",
    "closed_at": "2025-10-01T02:12:11Z",
    "merged_at": "2025-10-01T02:12:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8102"
  },
  {
    "number": 8101,
    "title": "[doc] Add Qwen3 Next Guide to Core README",
    "user": "faradawn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T14:27:08Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8101"
  },
  {
    "number": 8100,
    "title": "[https://nvbugs/5521949][fix] Fix head_size handling in ModelConfig.get_bindings_model_config",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T12:33:21Z",
    "closed_at": "2025-10-05T08:20:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8100"
  },
  {
    "number": 8099,
    "title": "[#7588][feat] lock gpu clocks in test_perf.py to reliably detect perf regressions",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T11:39:47Z",
    "closed_at": "2025-10-02T08:18:10Z",
    "merged_at": "2025-10-02T08:18:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8099"
  },
  {
    "number": 8098,
    "title": "[https://nvbugs/5541494] [fix] Fix missing sm100f/103a kernels and add tests",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T09:14:22Z",
    "closed_at": "2025-10-07T00:27:55Z",
    "merged_at": "2025-10-07T00:27:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8098"
  },
  {
    "number": 8097,
    "title": "[None][chore] Bump version to 1.2.0rc1",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T09:06:46Z",
    "closed_at": "2025-09-30T10:00:25Z",
    "merged_at": "2025-09-30T10:00:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8097"
  },
  {
    "number": 8096,
    "title": "[TRTLLM-8374][fix] workaround disagg MMLU test failures",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T08:34:06Z",
    "closed_at": "2025-09-30T16:52:18Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8096"
  },
  {
    "number": 8095,
    "title": "[None][feat] reuse cudagraph memory pool in normal forward flow",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T08:21:29Z",
    "closed_at": "2025-10-15T23:08:45Z",
    "merged_at": "2025-10-15T23:08:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8095"
  },
  {
    "number": 8094,
    "title": "[None][fix] Avoid unnecessary concat in attn_output_gate case.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T08:02:07Z",
    "closed_at": "2025-10-13T19:59:40Z",
    "merged_at": "2025-10-13T19:59:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8094"
  },
  {
    "number": 8093,
    "title": "[https://nvbugs/5550722][fix] Fix image load",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T07:49:49Z",
    "closed_at": "2025-10-13T06:12:40Z",
    "merged_at": "2025-10-13T06:12:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8093"
  },
  {
    "number": 8092,
    "title": "[#7588][fix] fixed the kv cache size parsing in test_perf.py AD backend",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T07:42:51Z",
    "closed_at": "2025-10-02T19:55:32Z",
    "merged_at": "2025-10-02T19:55:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8092"
  },
  {
    "number": 8091,
    "title": "[TRTLLM-8246][test] add multimodal kvcache+chunked_prefil cases in to QA test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T07:28:19Z",
    "closed_at": "2025-10-09T05:20:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8091"
  },
  {
    "number": 8090,
    "title": "dynamic spec fix",
    "user": "kris1025",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T07:06:57Z",
    "closed_at": "2025-09-30T07:11:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8090"
  },
  {
    "number": 8089,
    "title": "[None][doc] Add more description on EXAONE example",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T06:41:22Z",
    "closed_at": "2025-10-01T01:32:44Z",
    "merged_at": "2025-10-01T01:32:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8089"
  },
  {
    "number": 8088,
    "title": "[None][ci] Waive failing tests on release/1.1",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T06:31:10Z",
    "closed_at": "2025-09-30T08:10:23Z",
    "merged_at": "2025-09-30T08:10:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8088"
  },
  {
    "number": 8087,
    "title": "[None][fix] Disable DeepGEMM for Qwen3 MoE Attention layers",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T03:56:17Z",
    "closed_at": "2025-10-14T01:38:48Z",
    "merged_at": "2025-10-14T01:38:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8087"
  },
  {
    "number": 8086,
    "title": "[TRTLLM-8536][feat] Add the sparse attention framework and one use case--RocketKV support",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T03:45:14Z",
    "closed_at": "2025-10-14T15:23:17Z",
    "merged_at": "2025-10-14T15:23:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8086"
  },
  {
    "number": 8085,
    "title": "[None][fix] Add Lock to protect mReqeustToSession",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T03:01:28Z",
    "closed_at": "2025-10-10T13:51:50Z",
    "merged_at": "2025-10-10T13:51:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8085"
  },
  {
    "number": 8084,
    "title": "[None][feat] Spark dev branch 1.1rc3 spark gpt oss with user look up table not to be merged",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T02:32:51Z",
    "closed_at": "2025-11-06T18:24:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8084"
  },
  {
    "number": 8083,
    "title": "[None][feat] Spark dev branch 1.1rc3 spark gpt oss with user look up table not to be merged",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T02:27:52Z",
    "closed_at": "2025-09-30T02:31:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8083"
  },
  {
    "number": 8081,
    "title": "[TRTLLM-6239][feat] add test cases into QA test list",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T01:39:35Z",
    "closed_at": "2025-09-30T04:23:46Z",
    "merged_at": "2025-09-30T04:23:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8081"
  },
  {
    "number": 8080,
    "title": "[None][fix] Enable FP8 ContextMLA on GB300",
    "user": "longlee0622",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T01:01:43Z",
    "closed_at": "2025-10-10T02:20:46Z",
    "merged_at": "2025-10-10T02:20:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8080"
  },
  {
    "number": 8079,
    "title": "[None][autodeploy] small refactors on attention matching",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T00:00:02Z",
    "closed_at": "2025-10-04T05:00:27Z",
    "merged_at": "2025-10-04T05:00:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8079"
  },
  {
    "number": 8078,
    "title": "[https://nvbugs/5548098][fix] Fix flakey unit test for dynamic spec decode",
    "user": "zheyuf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T22:41:01Z",
    "closed_at": "2025-09-30T07:36:32Z",
    "merged_at": "2025-09-30T07:36:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8078"
  },
  {
    "number": 8076,
    "title": "[https://nvbugs/5549111][fix] Fix 2-model overlap scheduler accuracy on very long prompts",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T20:46:35Z",
    "closed_at": "2025-10-28T21:55:35Z",
    "merged_at": "2025-10-28T21:55:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8076"
  },
  {
    "number": 8075,
    "title": "[None][fix] Fix Qwen3 FP8 per-tensor when requesting TRTLLM-GEN MoE backend",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T20:00:52Z",
    "closed_at": "2025-10-03T14:52:52Z",
    "merged_at": "2025-10-03T14:52:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8075"
  },
  {
    "number": 8074,
    "title": "test gb200",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T19:04:05Z",
    "closed_at": "2025-10-16T18:13:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8074"
  },
  {
    "number": 8073,
    "title": "[#7312][feat] Torch.compile for transformers mode",
    "user": "h-guo18",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T19:00:39Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8073"
  },
  {
    "number": 8072,
    "title": "[#7312][feat] Torch.compile for transformers mode",
    "user": "h-guo18",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T18:56:05Z",
    "closed_at": "2025-09-29T18:58:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8072"
  },
  {
    "number": 8071,
    "title": "[https://nvbugs/5547414][fix] avoid downloading Tiny llama from HF",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T18:21:32Z",
    "closed_at": "2025-09-30T17:48:00Z",
    "merged_at": "2025-09-30T17:48:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8071"
  },
  {
    "number": 8070,
    "title": "[https://nvbugs/5549081][fix] Fix device id assignment for some vision models",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T17:56:49Z",
    "closed_at": "2025-10-02T03:28:05Z",
    "merged_at": "2025-10-02T03:28:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8070"
  },
  {
    "number": 8069,
    "title": "[https://nvbugs/5434320][fix] fix: Unwaiving disagg pp tests",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T16:59:38Z",
    "closed_at": "2025-10-01T04:34:00Z",
    "merged_at": "2025-10-01T04:34:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8069"
  },
  {
    "number": 8068,
    "title": "[#4593][feat] AutoDeploy: Linear Attention Support (SSM + causal_conv + Bamba + Nemotron-H)",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T15:32:18Z",
    "closed_at": "2025-09-30T02:41:06Z",
    "merged_at": "2025-09-30T02:41:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8068"
  },
  {
    "number": 8067,
    "title": "[None][fix] Fix chunked prefill state of draft request",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T15:26:57Z",
    "closed_at": "2025-09-30T01:51:21Z",
    "merged_at": "2025-09-30T01:51:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8067"
  },
  {
    "number": 8066,
    "title": "[https://nvbugs/5546197][fix] Use local tokenizers",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T14:56:28Z",
    "closed_at": "2025-10-10T08:25:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8066"
  },
  {
    "number": 8064,
    "title": "[None][chore] Refine qwen3-next implementation.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T13:30:12Z",
    "closed_at": "2025-09-30T19:05:13Z",
    "merged_at": "2025-09-30T19:05:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8064"
  },
  {
    "number": 8063,
    "title": "[https://nvbugs/5510879][fix] Fix pytorch & TRT-python flows fused LoRA adapter modules weight split with TP>1",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T13:06:30Z",
    "closed_at": "2025-10-12T19:29:53Z",
    "merged_at": "2025-10-12T19:29:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8063"
  },
  {
    "number": 8062,
    "title": "[https://nvbugs/5537348][fix] Use device tensor index for MTP",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T12:06:53Z",
    "closed_at": "2025-10-14T12:51:45Z",
    "merged_at": "2025-10-14T12:51:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8062"
  },
  {
    "number": 8061,
    "title": "[None][feat] Load some default SampingParams values from generation_config",
    "user": "Wokzy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T11:23:23Z",
    "closed_at": "2025-10-05T04:08:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8061"
  },
  {
    "number": 8059,
    "title": "[None][fix] Fix TRT-python multi LoRA TP=2 test arguments",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T08:59:37Z",
    "closed_at": "2025-09-29T16:20:04Z",
    "merged_at": "2025-09-29T16:20:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8059"
  },
  {
    "number": 8058,
    "title": "[https://nvbugs/5534837][fix] Fix SplitKvCache kernel",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T08:48:03Z",
    "closed_at": "2025-10-17T04:57:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8058"
  },
  {
    "number": 8057,
    "title": "[https://nvbugs/5547434][fix] Fix Qwen2.5-VL device_path error",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T07:54:43Z",
    "closed_at": "2025-10-13T06:12:27Z",
    "merged_at": "2025-10-13T06:12:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8057"
  },
  {
    "number": 8056,
    "title": "[None] [test] Add B300 cases to CI",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T07:40:58Z",
    "closed_at": "2025-10-07T02:23:32Z",
    "merged_at": "2025-10-07T02:23:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8056"
  },
  {
    "number": 8055,
    "title": "[https://nvbugs/5543770][fix] Update to Cutlass v4.2.1",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T07:21:23Z",
    "closed_at": "2025-10-14T05:39:26Z",
    "merged_at": "2025-10-14T05:39:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8055"
  },
  {
    "number": 8054,
    "title": "[None][chore] update test case constraint",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T05:44:31Z",
    "closed_at": "2025-10-09T05:10:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8054"
  },
  {
    "number": 8053,
    "title": "[None][infra] Waive failed cases for main on 0929",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T05:31:15Z",
    "closed_at": "2025-09-29T06:46:03Z",
    "merged_at": "2025-09-29T06:46:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8053"
  },
  {
    "number": 8052,
    "title": "[None][fix] : Fix OOM issue when dp padding is enabled",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T05:02:06Z",
    "closed_at": "2025-10-01T01:10:01Z",
    "merged_at": "2025-10-01T01:10:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8052"
  },
  {
    "number": 8051,
    "title": "[https://nvbugs/5541494] [fix] add back missing sm100f bmm kernels",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T02:53:34Z",
    "closed_at": "2025-09-29T09:35:45Z",
    "merged_at": "2025-09-29T09:35:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8051"
  },
  {
    "number": 8050,
    "title": "[https://nvbugs/5534705][fix] Skip unnecessary CUDA graph capture",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T02:45:52Z",
    "closed_at": "2025-10-11T05:26:55Z",
    "merged_at": "2025-10-11T05:26:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8050"
  },
  {
    "number": 8049,
    "title": "[None] [doc] Document hang issue caused by `UnpicklingError`",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T02:20:39Z",
    "closed_at": "2025-09-29T03:40:36Z",
    "merged_at": "2025-09-29T03:40:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8049"
  },
  {
    "number": 8048,
    "title": "[None][chroe] Update cron schedule for closing inactive issues",
    "user": "zhenhuaw-me",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T02:00:01Z",
    "closed_at": "2025-09-29T02:52:04Z",
    "merged_at": "2025-09-29T02:52:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8048"
  },
  {
    "number": 8047,
    "title": "[None][fix] Fix CUDA graph for Qwen2.5-VL",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-29T01:44:34Z",
    "closed_at": "2025-09-30T06:40:03Z",
    "merged_at": "2025-09-30T06:40:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8047"
  },
  {
    "number": 8046,
    "title": "[None][fix] unwaive tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-28T14:29:49Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8046"
  },
  {
    "number": 8045,
    "title": "[https://nvbugs/5542907][fix] re-enabled test_trtllm_bench",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-28T13:20:23Z",
    "closed_at": "2025-09-29T14:07:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8045"
  },
  {
    "number": 8044,
    "title": "[TRTLLM-8348][feat] Speed up concat k and copy k_nope in context phase using torch.compile",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-28T10:38:21Z",
    "closed_at": "2025-09-29T05:28:12Z",
    "merged_at": "2025-09-29T05:28:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8044"
  },
  {
    "number": 8043,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-28T08:00:02Z",
    "closed_at": "2025-09-29T07:37:40Z",
    "merged_at": "2025-09-29T07:37:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8043"
  },
  {
    "number": 8042,
    "title": "[None][doc] Scaffolding tech blog fix a typo",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-28T07:01:52Z",
    "closed_at": "2025-09-28T14:29:02Z",
    "merged_at": "2025-09-28T14:29:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8042"
  },
  {
    "number": 8041,
    "title": "[None][fix] only support deepep post quant all2all on nvfp4",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-28T03:29:32Z",
    "closed_at": "2025-09-29T06:37:50Z",
    "merged_at": "2025-09-29T06:37:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8041"
  },
  {
    "number": 8040,
    "title": "[None][infra] Skip failed test for main branch on 9/28",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-28T02:34:42Z",
    "closed_at": "2025-09-28T11:00:55Z",
    "merged_at": "2025-09-28T11:00:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8040"
  },
  {
    "number": 8039,
    "title": "[None][chore] AutoDeploy: cleanup old inference optimizer configs",
    "user": "h-guo18",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-28T00:00:57Z",
    "closed_at": "2025-10-17T19:55:58Z",
    "merged_at": "2025-10-17T19:55:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8039"
  },
  {
    "number": 8036,
    "title": "[None][doc] Refine perf overview.md and correct the error link in per…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-27T05:59:05Z",
    "closed_at": "2025-09-28T08:14:31Z",
    "merged_at": "2025-09-28T08:14:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8036"
  },
  {
    "number": 8035,
    "title": "[None][doc] Refine perf overview.md and correct the error link in per…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-27T05:56:34Z",
    "closed_at": "2025-09-28T08:14:42Z",
    "merged_at": "2025-09-28T08:14:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8035"
  },
  {
    "number": 8034,
    "title": "[None][chore] Disable concurrent weights loading for _load_weights_im…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-27T05:06:11Z",
    "closed_at": "2025-09-28T11:11:16Z",
    "merged_at": "2025-09-28T11:11:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8034"
  },
  {
    "number": 8033,
    "title": "[https://nvbugs/5542867][fix] Fix the non-determinism issue in the mm_encoder test",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T23:44:05Z",
    "closed_at": "2025-09-29T16:45:16Z",
    "merged_at": "2025-09-29T16:45:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8033"
  },
  {
    "number": 8031,
    "title": "[https://nvbugs/5541545][fix] Remove test_llama4",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T20:33:56Z",
    "closed_at": "2025-10-08T22:20:16Z",
    "merged_at": "2025-10-08T22:20:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8031"
  },
  {
    "number": 8030,
    "title": "[https://nvbugs/5461712] [fix] Use DG for Qwen3 Linear layers",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T17:43:17Z",
    "closed_at": "2025-09-28T02:33:36Z",
    "merged_at": "2025-09-28T02:33:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8030"
  },
  {
    "number": 8029,
    "title": "[https://nvbugs/5532087][ci] Enable test case",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T14:35:35Z",
    "closed_at": "2025-09-29T05:46:28Z",
    "merged_at": "2025-09-29T05:46:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8029"
  },
  {
    "number": 8028,
    "title": "[None][ci] Disable tensorRT cases in post-merge",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T14:27:57Z",
    "closed_at": "2025-09-29T06:21:52Z",
    "merged_at": "2025-09-29T06:21:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8028"
  },
  {
    "number": 8027,
    "title": "[https://nvbugs/5542878][fix] Unwaive test",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T13:56:39Z",
    "closed_at": "2025-10-14T05:58:08Z",
    "merged_at": "2025-10-14T05:58:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8027"
  },
  {
    "number": 8026,
    "title": "[None][feat] Enable trtllm gen dense gemm",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T11:56:22Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8026"
  },
  {
    "number": 8025,
    "title": "[TRTLLM-8366][feat] add kimi multi nodes case",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T10:14:53Z",
    "closed_at": "2025-10-14T04:36:04Z",
    "merged_at": "2025-10-14T04:36:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8025"
  },
  {
    "number": 8024,
    "title": "[TRTLLM-8238][feat] Add EVS support for nano-v2-vlm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T09:29:50Z",
    "closed_at": "2025-10-25T09:43:28Z",
    "merged_at": "2025-10-25T09:43:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8024"
  },
  {
    "number": 8022,
    "title": "[None][chore] Require NVIDIA developers to use their full name or NVIDIA account in GitHub profiles",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T08:49:09Z",
    "closed_at": "2025-09-26T13:16:59Z",
    "merged_at": "2025-09-26T13:16:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8022"
  },
  {
    "number": 8021,
    "title": "[None][doc] Add scaffolding tech blog to cover",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T08:20:31Z",
    "closed_at": "2025-09-26T09:22:12Z",
    "merged_at": "2025-09-26T09:22:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8021"
  },
  {
    "number": 8020,
    "title": "[None][chore] update test case constraint",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T08:04:14Z",
    "closed_at": "2025-09-29T05:25:10Z",
    "merged_at": "2025-09-29T05:25:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8020"
  },
  {
    "number": 8019,
    "title": "[None][infra] Waive failed cases in post-merge 2305",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T07:22:38Z",
    "closed_at": "2025-09-26T17:20:13Z",
    "merged_at": "2025-09-26T17:20:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8019"
  },
  {
    "number": 8018,
    "title": "[None][feat] MNNVLAllreduce Kernel Refactor",
    "user": "timlee0212",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T06:55:40Z",
    "closed_at": "2025-11-05T00:49:47Z",
    "merged_at": "2025-11-05T00:49:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8018"
  },
  {
    "number": 8017,
    "title": "[None][feat] Support task aware schedule in GuaranteedNoEvictScheduler",
    "user": "dcaox",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T06:41:06Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8017"
  },
  {
    "number": 8016,
    "title": "[None][test] add test-model-suites option in integration conftest.py",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T06:29:18Z",
    "closed_at": "2025-10-08T02:38:32Z",
    "merged_at": "2025-10-08T02:38:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8016"
  },
  {
    "number": 8015,
    "title": "test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T06:08:58Z",
    "closed_at": "2025-09-26T06:09:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8015"
  },
  {
    "number": 8014,
    "title": "[None] [perf] upgrade trtllm-gen bmm kernel",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T05:59:39Z",
    "closed_at": "2025-10-14T10:12:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8014"
  },
  {
    "number": 8013,
    "title": "Fix guided decoding bitmask synchronization bug with MTP > 1",
    "user": "alhridoy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T05:35:56Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8013"
  },
  {
    "number": 8012,
    "title": "[None][ci] debug test_moe.py::TestMoeFp4::test_autotune",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T03:51:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8012"
  },
  {
    "number": 8011,
    "title": "[TRTLLM-7287][test] add multimodal chunked_prefill cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T03:18:49Z",
    "closed_at": "2025-10-21T02:43:48Z",
    "merged_at": "2025-10-21T02:43:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8011"
  },
  {
    "number": 8010,
    "title": "[None][ci] Waive test_mm_encoder_standalone.py::test_multi_request_batch_chat[llava-v1.6-mistral-7b-hf] ",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T01:59:18Z",
    "closed_at": "2025-09-26T03:07:55Z",
    "merged_at": "2025-09-26T03:07:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8010"
  },
  {
    "number": 8009,
    "title": "[None][chore] Report NCCL error message but not OOM when NCCL error happens",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T01:22:19Z",
    "closed_at": "2025-09-26T06:07:33Z",
    "merged_at": "2025-09-26T06:07:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8009"
  },
  {
    "number": 8008,
    "title": "[https://nvbugs/5537738][fix] Add fp8 post-quant allgather support",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-26T00:21:58Z",
    "closed_at": "2025-09-28T07:32:45Z",
    "merged_at": "2025-09-28T07:32:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8008"
  },
  {
    "number": 8007,
    "title": "[None][doc] Add Qwen3 Next Quick Start Guide",
    "user": "faradawn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T23:50:49Z",
    "closed_at": "2025-10-14T01:46:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8007"
  },
  {
    "number": 8006,
    "title": "[https://nvbugs/5538098][fix] Checking connection to etcd server in unit test",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T20:49:08Z",
    "closed_at": "2025-09-30T00:53:33Z",
    "merged_at": "2025-09-30T00:53:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8006"
  },
  {
    "number": 8005,
    "title": "[None][feat] perf_metrics endpoint functionality improvement",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T18:24:20Z",
    "closed_at": "2025-10-03T00:43:26Z",
    "merged_at": "2025-10-03T00:43:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8005"
  },
  {
    "number": 8004,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T16:35:12Z",
    "closed_at": "2025-09-26T07:41:03Z",
    "merged_at": "2025-09-26T07:41:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8004"
  },
  {
    "number": 8003,
    "title": "[None][chore] Move submit.sh to python and use yaml configuration",
    "user": "zerollzeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T15:29:16Z",
    "closed_at": "2025-10-21T02:36:50Z",
    "merged_at": "2025-10-21T02:36:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8003"
  },
  {
    "number": 8002,
    "title": "[https://nvbugs/5527956][fix] AutoDeploy: fix IMA due to outdated metadata",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T15:17:36Z",
    "closed_at": "2025-09-26T05:05:56Z",
    "merged_at": "2025-09-26T05:05:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8002"
  },
  {
    "number": 8001,
    "title": "[None][infra] Waive failed tests on main 09/25",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T14:15:13Z",
    "closed_at": "2025-09-25T15:13:40Z",
    "merged_at": "2025-09-25T15:13:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8001"
  },
  {
    "number": 8000,
    "title": "[None][feat] Support Qwen3 reasoning parser",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T14:07:31Z",
    "closed_at": "2025-10-21T06:08:39Z",
    "merged_at": "2025-10-21T06:08:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8000"
  },
  {
    "number": 7999,
    "title": "[https://nvbugs/5451280][fix] Reduce memory fraction problem by warmu…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T13:06:18Z",
    "closed_at": "2025-10-04T01:14:13Z",
    "merged_at": "2025-10-04T01:14:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7999"
  },
  {
    "number": 7998,
    "title": "[https://nvbugs/5537878][fix] Reserve an extra slot for padded batch",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T12:27:50Z",
    "closed_at": "2025-10-03T15:42:52Z",
    "merged_at": "2025-10-03T15:42:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7998"
  },
  {
    "number": 7997,
    "title": "[None][Test] Add pp enabled pytorch workflow e2e tests",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T12:07:15Z",
    "closed_at": "2025-10-15T02:24:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7997"
  },
  {
    "number": 7996,
    "title": "Changes for sm_110",
    "user": "ydharavath",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T11:35:46Z",
    "closed_at": "2025-09-25T11:35:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7996"
  },
  {
    "number": 7995,
    "title": "[https://nvbugs/5495789][feat] Optionally disable server GC and worker GC",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T11:02:34Z",
    "closed_at": "2025-09-26T13:39:24Z",
    "merged_at": "2025-09-26T13:39:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7995"
  },
  {
    "number": 7994,
    "title": "[None][infra] Improve the failure message for accuracy test suite",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T10:28:29Z",
    "closed_at": "2025-09-26T02:04:47Z",
    "merged_at": "2025-09-26T02:04:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7994"
  },
  {
    "number": 7993,
    "title": "[None][fix] Fix dummy load format for key models.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T10:13:19Z",
    "closed_at": "2025-10-14T03:18:39Z",
    "merged_at": "2025-10-14T03:18:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7993"
  },
  {
    "number": 7992,
    "title": "[https://nvbugs/5501820][fix] Add requirements for numba-cuda version to WAR mem corruption",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T10:01:31Z",
    "closed_at": "2025-10-10T02:18:27Z",
    "merged_at": "2025-10-10T02:18:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7992"
  },
  {
    "number": 7991,
    "title": "[TRTLLM-8293][infra] Support creating images with CUDA 12.9",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T08:46:36Z",
    "closed_at": "2025-10-22T05:41:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7991"
  },
  {
    "number": 7990,
    "title": "[https://nvbugs/5536141][fix] fix_disagg_single_gpu_test",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T08:40:12Z",
    "closed_at": "2025-09-25T09:07:22Z",
    "merged_at": "2025-09-25T09:07:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7990"
  },
  {
    "number": 7989,
    "title": "[None][fix] Enable torch.compile for CapturableGuidedDecoder",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T08:27:14Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7989"
  },
  {
    "number": 7987,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T07:49:26Z",
    "closed_at": "2025-09-26T02:08:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7987"
  },
  {
    "number": 7986,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T07:48:49Z",
    "closed_at": "2025-09-26T06:50:09Z",
    "merged_at": "2025-09-26T06:50:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7986"
  },
  {
    "number": 7985,
    "title": "[TRTLLM-8260][feat] Add Server-Client Perf Test in pytest for B200 and B300",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T07:48:10Z",
    "closed_at": "2025-10-22T02:17:22Z",
    "merged_at": "2025-10-22T02:17:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7985"
  },
  {
    "number": 7984,
    "title": "[None][infra] Unwaive some tests since dev already have a PR to collect more info",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T07:16:26Z",
    "closed_at": "2025-09-25T08:03:13Z",
    "merged_at": "2025-09-25T08:03:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7984"
  },
  {
    "number": 7983,
    "title": "[None][doc] Add acknowledgements in scaffolding tech blog",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T07:14:57Z",
    "closed_at": "2025-09-25T15:07:14Z",
    "merged_at": "2025-09-25T15:07:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7983"
  },
  {
    "number": 7982,
    "title": "[None][test] Update get_sysinfo.py to avoid UnboundLocalError",
    "user": "yufeiwu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T07:07:25Z",
    "closed_at": "2025-09-29T09:14:39Z",
    "merged_at": "2025-09-29T09:14:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7982"
  },
  {
    "number": 7981,
    "title": "[None][chore] Remove developer name in comment",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T06:59:46Z",
    "closed_at": "2025-09-25T13:43:38Z",
    "merged_at": "2025-09-25T13:43:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7981"
  },
  {
    "number": 7980,
    "title": "[https://nvbugs/4955671][fix] update test list",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T06:44:20Z",
    "closed_at": "2025-09-25T09:58:10Z",
    "merged_at": "2025-09-25T09:58:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7980"
  },
  {
    "number": 7979,
    "title": "[None][chore] Some clean-ups for CUDA 13.0 dependencies",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T05:56:58Z",
    "closed_at": "2025-09-26T00:46:11Z",
    "merged_at": "2025-09-26T00:46:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7979"
  },
  {
    "number": 7977,
    "title": "[TRTLLM-6748][feat] add PDL support for more kernels",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T05:17:03Z",
    "closed_at": "2025-10-11T00:32:06Z",
    "merged_at": "2025-10-11T00:32:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7977"
  },
  {
    "number": 7976,
    "title": "[None][feat] Return topk logprobs in torch backend",
    "user": "dcaox",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T04:52:45Z",
    "closed_at": "2025-09-30T01:32:37Z",
    "merged_at": "2025-09-30T01:32:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7976"
  },
  {
    "number": 7975,
    "title": "[TRTLLM-7399][test] Add 8gpu test for RTX 6000",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-25T00:23:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7975"
  },
  {
    "number": 7974,
    "title": "[None][feat] Spark single gpu dev plus gptoss (won't be merged dummy PR for CI/CD on top of spark-single-gpu-dev branch)",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T23:00:11Z",
    "closed_at": "2025-11-06T18:24:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7974"
  },
  {
    "number": 7972,
    "title": "[TRTLLM-7733][feat] Executor changes to support helix parallelism",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T21:47:55Z",
    "closed_at": "2025-10-02T02:13:04Z",
    "merged_at": "2025-10-02T02:13:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7972"
  },
  {
    "number": 7971,
    "title": "[TRTLLM-8271][fix] Fix CDL overlap scheduling performance",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T21:34:30Z",
    "closed_at": "2025-09-26T20:05:11Z",
    "merged_at": "2025-09-26T20:05:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7971"
  },
  {
    "number": 7970,
    "title": "[None][feat] Update TRT-LLM Gen MoE kernels",
    "user": "nekorobov",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T21:31:47Z",
    "closed_at": "2025-10-03T01:22:45Z",
    "merged_at": "2025-10-03T01:22:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7970"
  },
  {
    "number": 7969,
    "title": "[None][fix] Revert \"[None][feat] Return topk logprobs in torch backend (#7756)\"",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T20:01:00Z",
    "closed_at": "2025-09-24T22:36:39Z",
    "merged_at": "2025-09-24T22:36:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7969"
  },
  {
    "number": 7968,
    "title": "[OMNIML-2336][feat] add W4A8 NVFP4 FP8 fused moe",
    "user": "sychen52",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T19:44:32Z",
    "closed_at": "2025-10-01T06:39:33Z",
    "merged_at": "2025-10-01T06:39:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7968"
  },
  {
    "number": 7967,
    "title": "[https://nvbugs/5473781][fix] Cherry-pick \"Fix llama 4 FP8 for PP>1 (#7220)\"",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T16:38:48Z",
    "closed_at": "2025-09-25T13:57:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7967"
  },
  {
    "number": 7966,
    "title": "[https://nvbugs/5456485][bug] unwaive triton test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T16:03:30Z",
    "closed_at": "2025-09-25T00:02:56Z",
    "merged_at": "2025-09-25T00:02:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7966"
  },
  {
    "number": 7965,
    "title": "[https://nvbugs/5451740][fix] Add DP padding back on SM120",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T15:51:46Z",
    "closed_at": "2025-09-26T05:59:54Z",
    "merged_at": "2025-09-26T05:59:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7965"
  },
  {
    "number": 7964,
    "title": "[None][bug] Fix transformers version for Triton backend",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T14:55:10Z",
    "closed_at": "2025-09-24T16:55:52Z",
    "merged_at": "2025-09-24T16:55:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7964"
  },
  {
    "number": 7963,
    "title": "[None][chroe] Update the cuda and tensorrt version in homepage icons.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T14:14:57Z",
    "closed_at": "2025-09-25T02:20:05Z",
    "merged_at": "2025-09-25T02:20:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7963"
  },
  {
    "number": 7962,
    "title": "[None][feat] Add support for expert_number=512 and topk=10 for renormalized routing",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T14:10:18Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7962"
  },
  {
    "number": 7960,
    "title": "[https://nvbugs/5536131][fix] Fix illegal access issue when scale is not provided in Llama3/4.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T10:57:23Z",
    "closed_at": "2025-10-08T06:47:00Z",
    "merged_at": "2025-10-08T06:47:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7960"
  },
  {
    "number": 7959,
    "title": "[None][doc] add Llama PP known issue to release note",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T10:37:56Z",
    "closed_at": "2025-09-24T11:39:54Z",
    "merged_at": "2025-09-24T11:39:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7959"
  },
  {
    "number": 7958,
    "title": "[None][fix] Fix access to new tokens in sampler.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T10:29:23Z",
    "closed_at": "2025-10-02T19:41:21Z",
    "merged_at": "2025-10-02T19:41:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7958"
  },
  {
    "number": 7957,
    "title": "[https://nvbugs/5536141][fix] fix_disagg_single_gpu_test",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T09:49:49Z",
    "closed_at": "2025-09-25T02:29:01Z",
    "merged_at": "2025-09-25T02:29:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7957"
  },
  {
    "number": 7956,
    "title": "[None][ci] remove duplicate test cases",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T09:41:14Z",
    "closed_at": "2025-09-24T09:47:23Z",
    "merged_at": "2025-09-24T09:47:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7956"
  },
  {
    "number": 7955,
    "title": "[None][ci] Waive some intermittent failures",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T09:05:50Z",
    "closed_at": "2025-09-24T11:00:38Z",
    "merged_at": "2025-09-24T11:00:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7955"
  },
  {
    "number": 7954,
    "title": "[None][fix] fix a bug in wideEp use DeepEP with num_chunks > 1",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T07:54:01Z",
    "closed_at": "2025-09-25T14:53:43Z",
    "merged_at": "2025-09-25T14:53:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7954"
  },
  {
    "number": 7953,
    "title": "[None][fix] trtllm-gen cubins compiled with wrong arch.",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T07:48:33Z",
    "closed_at": "2025-09-24T11:13:36Z",
    "merged_at": "2025-09-24T11:13:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7953"
  },
  {
    "number": 7952,
    "title": "[TRTLLM-7078][chore] optimal kvcache transfer for VWSA",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T07:44:36Z",
    "closed_at": "2025-10-24T12:58:17Z",
    "merged_at": "2025-10-24T12:58:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7952"
  },
  {
    "number": 7951,
    "title": "[TRTLLM-7999][infra] Add B300/GB300 single gpu test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T07:32:00Z",
    "closed_at": "2025-09-26T01:59:12Z",
    "merged_at": "2025-09-26T01:59:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7951"
  },
  {
    "number": 7950,
    "title": "[None][ci] Waive llama4 unit test on DGX H200",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T07:25:04Z",
    "closed_at": "2025-10-08T02:24:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7950"
  },
  {
    "number": 7949,
    "title": "[None][infra] Test pipeline",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T06:46:27Z",
    "closed_at": "2025-09-25T06:22:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7949"
  },
  {
    "number": 7948,
    "title": "[None][ci] optimize test cases of dgx b200",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T06:40:07Z",
    "closed_at": "2025-09-24T07:39:46Z",
    "merged_at": "2025-09-24T07:39:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7948"
  },
  {
    "number": 7947,
    "title": "[https://nvbugs/5536131][fix] move llama3.3 dgx b200 test to pre-merge",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T05:41:17Z",
    "closed_at": "2025-09-24T09:12:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7947"
  },
  {
    "number": 7946,
    "title": "[None][infra] Skip failed test for nvbugs 5537738",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T05:39:58Z",
    "closed_at": "2025-09-24T06:48:51Z",
    "merged_at": "2025-09-24T06:48:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7946"
  },
  {
    "number": 7945,
    "title": "[None][chore] Recover cutlass-dsl pkg install and dsl op testing.",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T04:09:39Z",
    "closed_at": "2025-09-24T07:45:18Z",
    "merged_at": "2025-09-24T07:45:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7945"
  },
  {
    "number": 7944,
    "title": "[None][chore] add test_w4_1gpu[True-True-cutlass-fp8] & TestKimiK2::test_fp8_blocks…",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T03:49:19Z",
    "closed_at": "2025-09-24T10:25:18Z",
    "merged_at": "2025-09-24T10:25:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7944"
  },
  {
    "number": 7943,
    "title": "[https://nvbugs/5451205][feat] Add cuBLASLt NVFP4 GEMM backend support",
    "user": "Wong4j",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T03:37:13Z",
    "closed_at": "2025-10-23T07:55:11Z",
    "merged_at": "2025-10-23T07:55:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7943"
  },
  {
    "number": 7942,
    "title": "[None][chore] Bump version to 1.1.0",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T02:58:20Z",
    "closed_at": "2025-09-26T05:17:37Z",
    "merged_at": "2025-09-26T05:17:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7942"
  },
  {
    "number": 7941,
    "title": "[None][chore] Bump version to 1.2.0rc0",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T02:54:03Z",
    "closed_at": "2025-09-29T09:39:07Z",
    "merged_at": "2025-09-29T09:39:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7941"
  },
  {
    "number": 7940,
    "title": "[https://nvbugs/5532225] [fix] MoE use stream-dependent workspace",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-24T02:27:41Z",
    "closed_at": "2025-09-24T06:44:27Z",
    "merged_at": "2025-09-24T06:44:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7940"
  },
  {
    "number": 7939,
    "title": "[None][fix] Eagle: Attention DP",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T22:50:42Z",
    "closed_at": "2025-10-06T20:52:35Z",
    "merged_at": "2025-10-06T20:52:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7939"
  },
  {
    "number": 7937,
    "title": "[None][feat] GPT-OSS Sm120/Sm121 Support",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T20:23:21Z",
    "closed_at": "2025-10-06T20:59:07Z",
    "merged_at": "2025-10-06T20:59:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7937"
  },
  {
    "number": 7936,
    "title": "GPT-OSS Spark Support",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T19:34:06Z",
    "closed_at": "2025-09-23T20:23:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7936"
  },
  {
    "number": 7935,
    "title": "[None][chore] relax version constraints on fastapi",
    "user": "PeganovAnton",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T16:39:48Z",
    "closed_at": "2025-09-25T13:58:54Z",
    "merged_at": "2025-09-25T13:58:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7935"
  },
  {
    "number": 7934,
    "title": "[None][chore] Make sampler type beta.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T16:13:17Z",
    "closed_at": "2025-09-24T03:51:40Z",
    "merged_at": "2025-09-24T03:51:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7934"
  },
  {
    "number": 7933,
    "title": "[None][doc] fix invalid links in perf benchmarking.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T14:26:32Z",
    "closed_at": "2025-09-23T15:18:18Z",
    "merged_at": "2025-09-23T15:18:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7933"
  },
  {
    "number": 7932,
    "title": "[None][chore] upgrade pillow to the latest",
    "user": "PeganovAnton",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T13:57:25Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7932"
  },
  {
    "number": 7931,
    "title": "[https://nvbugs/5532248][fix] Fix fused_moe OOM",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T13:35:01Z",
    "closed_at": "2025-09-24T09:22:39Z",
    "merged_at": "2025-09-24T09:22:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7931"
  },
  {
    "number": 7930,
    "title": "[None][test] Waive another intermittent OOM test",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T13:28:44Z",
    "closed_at": "2025-09-23T14:34:09Z",
    "merged_at": "2025-09-23T14:34:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7930"
  },
  {
    "number": 7928,
    "title": "[None][chore] Mass integration of release/1.0 - 6th",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T10:05:55Z",
    "closed_at": "2025-09-25T13:02:36Z",
    "merged_at": "2025-09-25T13:02:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7928"
  },
  {
    "number": 7927,
    "title": "[None][feat] DeepEP LL fp8 dispatch/combine",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T09:38:10Z",
    "closed_at": "2025-09-25T01:20:25Z",
    "merged_at": "2025-09-25T01:20:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7927"
  },
  {
    "number": 7926,
    "title": "[TRTLLM-7964][infra] Set nixl to default cache transceiver backend",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T09:22:22Z",
    "closed_at": "2025-10-19T11:24:43Z",
    "merged_at": "2025-10-19T11:24:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7926"
  },
  {
    "number": 7925,
    "title": "[TRTLLM-5235][feat] Enable regex and EBNF grammar in trtllm-serve",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T09:07:44Z",
    "closed_at": "2025-09-24T10:30:23Z",
    "merged_at": "2025-09-24T10:30:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7925"
  },
  {
    "number": 7924,
    "title": "[TRTLLM-6541][test] Add NIM perf test cases",
    "user": "fredricz-20070104",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T08:40:56Z",
    "closed_at": "2025-09-25T05:15:26Z",
    "merged_at": "2025-09-25T05:15:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7924"
  },
  {
    "number": 7923,
    "title": "[None][feature] Add environment variable to adjust block pool allocation ration under kv cache manager",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T08:31:41Z",
    "closed_at": "2025-09-26T06:09:01Z",
    "merged_at": "2025-09-26T06:09:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7923"
  },
  {
    "number": 7922,
    "title": "[TLLM-6777][feature] Support SWA KV cache reuse OOW block detach",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T08:20:26Z",
    "closed_at": "2025-10-13T16:18:12Z",
    "merged_at": "2025-10-13T16:18:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7922"
  },
  {
    "number": 7921,
    "title": "[None][chore] update chunked prefill cases",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T07:46:08Z",
    "closed_at": "2025-09-24T07:14:50Z",
    "merged_at": "2025-09-24T07:14:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7921"
  },
  {
    "number": 7920,
    "title": "[None][fix] Re-add the import for allgather that was mistakenly removed.",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T03:54:20Z",
    "closed_at": "2025-09-23T10:09:48Z",
    "merged_at": "2025-09-23T10:09:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7920"
  },
  {
    "number": 7919,
    "title": "[None][fix] fix a bug with trtllm-gen kernels + attention sinks",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T03:52:39Z",
    "closed_at": "2025-09-23T07:32:05Z",
    "merged_at": "2025-09-23T07:32:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7919"
  },
  {
    "number": 7918,
    "title": "[TRTLLM-7758][feat] Optimize phi4-mm image modality inference",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T03:02:04Z",
    "closed_at": "2025-09-25T07:58:30Z",
    "merged_at": "2025-09-25T07:58:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7918"
  },
  {
    "number": 7917,
    "title": "[None] [feat] Update disagg gen-only benchmark.",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T01:44:12Z",
    "closed_at": "2025-09-28T01:56:56Z",
    "merged_at": "2025-09-28T01:56:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7917"
  },
  {
    "number": 7916,
    "title": "[TRTLLM-7775][feat] Integrate tinygemm2 for gpt-oss",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T01:05:19Z",
    "closed_at": "2025-10-02T17:47:04Z",
    "merged_at": "2025-10-02T17:47:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7916"
  },
  {
    "number": 7915,
    "title": "[None][bug] Add NCCL_GRAPH_REGISTER to test to avoid hang",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-23T00:51:57Z",
    "closed_at": "2025-10-28T04:02:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7915"
  },
  {
    "number": 7914,
    "title": "[#7654][feat] CMake option to link statically with TRT",
    "user": "WilliamTambellini",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T23:20:00Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7914"
  },
  {
    "number": 7913,
    "title": "[None][doc] Fix broken links in markdown files",
    "user": "karljang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T21:47:13Z",
    "closed_at": "2025-09-23T15:52:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7913"
  },
  {
    "number": 7912,
    "title": "[TRTLLM-6342][bug] Fix shape propagation after TP sharding",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T19:13:38Z",
    "closed_at": "2025-10-01T15:15:46Z",
    "merged_at": "2025-10-01T15:15:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7912"
  },
  {
    "number": 7911,
    "title": "[https://nvbugs/5525951][fix] Clarify that PP is not supported for GPTOSS",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T19:08:21Z",
    "closed_at": "2025-09-25T19:54:18Z",
    "merged_at": "2025-09-25T19:54:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7911"
  },
  {
    "number": 7910,
    "title": "[None][feat] Add NCCL device kernels for AR+RMS fusion",
    "user": "nv-lschneider",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T16:53:27Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7910"
  },
  {
    "number": 7909,
    "title": "[TRTLLM-8269][test] do not explicitly pass temperature=0 to select greedy sampling",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T16:07:32Z",
    "closed_at": "2025-09-29T13:52:18Z",
    "merged_at": "2025-09-29T13:52:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7909"
  },
  {
    "number": 7908,
    "title": "[TRTLLM-7728][perf] improve batched sampling perf for contiguous batches",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T11:53:29Z",
    "closed_at": "2025-09-29T12:32:50Z",
    "merged_at": "2025-09-29T12:32:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7908"
  },
  {
    "number": 7907,
    "title": "[https://nvbugs/5531963][fix] cherry pick #7725",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T11:23:05Z",
    "closed_at": "2025-09-22T13:55:05Z",
    "merged_at": "2025-09-22T13:55:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7907"
  },
  {
    "number": 7906,
    "title": "[https://nvbugs/5367180][fix] Fix xgrammar import before loading tensorrt_llm binary",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T10:41:28Z",
    "closed_at": "2025-09-23T07:29:58Z",
    "merged_at": "2025-09-23T07:29:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7906"
  },
  {
    "number": 7905,
    "title": "[None][infra] Skip failed test for nvbugs 5532023",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T10:19:03Z",
    "closed_at": "2025-09-22T10:49:45Z",
    "merged_at": "2025-09-22T10:49:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7905"
  },
  {
    "number": 7904,
    "title": "[https://nvbugs/5504086][fix] Fix MTP vanilla",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T08:51:45Z",
    "closed_at": "2025-09-23T00:38:28Z",
    "merged_at": "2025-09-23T00:38:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7904"
  },
  {
    "number": 7903,
    "title": "[https://nvbugs/5528405][fix] Set up draft_tokens before scheduling",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T08:19:08Z",
    "closed_at": "2025-09-24T01:56:18Z",
    "merged_at": "2025-09-24T01:56:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7903"
  },
  {
    "number": 7902,
    "title": "[None][chore] remove cubins for ci cases",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T08:09:42Z",
    "closed_at": "2025-09-24T06:56:32Z",
    "merged_at": "2025-09-24T06:56:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7902"
  },
  {
    "number": 7901,
    "title": "[None][infra] Waive a failed case on main",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T07:11:54Z",
    "closed_at": "2025-09-22T07:37:02Z",
    "merged_at": "2025-09-22T07:37:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7901"
  },
  {
    "number": 7900,
    "title": "[https://nvbugs/5351244][fix] CHERRY-PICK test_mpi_session (#7501)",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T07:06:46Z",
    "closed_at": "2025-09-22T12:03:30Z",
    "merged_at": "2025-09-22T12:03:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7900"
  },
  {
    "number": 7899,
    "title": "[None][test] rename llm_perf_full to llm_perf_core and add missing cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T06:18:03Z",
    "closed_at": "2025-09-23T06:04:34Z",
    "merged_at": "2025-09-23T06:04:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7899"
  },
  {
    "number": 7898,
    "title": "[None][chore] Cherry-pick from (#7598) Make low_precision_combine as a llm arg",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T06:12:49Z",
    "closed_at": "2025-09-29T02:32:34Z",
    "merged_at": "2025-09-29T02:32:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7898"
  },
  {
    "number": 7897,
    "title": "[None][fix] CHERRY-PICK trtllm-serve yaml loading (#7551)",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T06:02:15Z",
    "closed_at": "2025-09-23T06:56:53Z",
    "merged_at": "2025-09-23T06:56:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7897"
  },
  {
    "number": 7895,
    "title": "[https://nvbugs/5532023][fix] executor with-statement bug",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T05:40:39Z",
    "closed_at": "2025-09-23T09:05:40Z",
    "merged_at": "2025-09-23T09:05:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7895"
  },
  {
    "number": 7894,
    "title": "add initial v1.0 data without rtx 6000 blackwell se",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T04:46:00Z",
    "closed_at": "2025-09-22T04:49:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7894"
  },
  {
    "number": 7893,
    "title": "[TRTLLM-8209][feat] Support new structural tag API (upgrade XGrammar to 0.1.25)",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T04:45:26Z",
    "closed_at": "2025-09-23T01:10:09Z",
    "merged_at": "2025-09-23T01:10:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7893"
  },
  {
    "number": 7892,
    "title": "[None][feat] Support Qwen3 next",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T03:19:54Z",
    "closed_at": "2025-09-29T13:16:07Z",
    "merged_at": "2025-09-29T13:16:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7892"
  },
  {
    "number": 7891,
    "title": "[TRTLLM-6741][fix] Add heuristics for lm head tp size when `enable_lm_head_tp_in_adp=True`",
    "user": "Njuapp",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T02:50:57Z",
    "closed_at": "2025-09-30T01:24:36Z",
    "merged_at": "2025-09-30T01:24:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7891"
  },
  {
    "number": 7890,
    "title": "[None][chroe] Remove security scanning folder.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T02:20:02Z",
    "closed_at": "2025-09-22T03:00:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7890"
  },
  {
    "number": 7889,
    "title": "[None][debugging] NOT MERGE debug the CI",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-22T02:12:50Z",
    "closed_at": "2025-09-22T04:12:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7889"
  },
  {
    "number": 7888,
    "title": "[#7675][feat] CapturedGraph to support max_batch_size > max(cuda_graph_batch_sizes)",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-21T14:46:09Z",
    "closed_at": "2025-09-24T14:11:45Z",
    "merged_at": "2025-09-24T14:11:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7888"
  },
  {
    "number": 7887,
    "title": "[https://nvbugs/5477730][fix] Unwaive the test",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-21T12:42:12Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7887"
  },
  {
    "number": 7886,
    "title": "[None][chore] Remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-21T12:15:59Z",
    "closed_at": "2025-10-09T02:34:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7886"
  },
  {
    "number": 7885,
    "title": "[None][chore] Update trtllm-bench documentation on setting FP8 KV cache",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-20T03:24:00Z",
    "closed_at": "2025-09-25T01:28:53Z",
    "merged_at": "2025-09-25T01:28:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7885"
  },
  {
    "number": 7884,
    "title": "[None][doc] Update tech blog12",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-20T02:33:47Z",
    "closed_at": "2025-09-20T10:15:39Z",
    "merged_at": "2025-09-20T10:15:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7884"
  },
  {
    "number": 7883,
    "title": "[feat] Reasoning with guided decoder",
    "user": "Shang-Pin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T23:07:07Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7883"
  },
  {
    "number": 7882,
    "title": "[None][doc] Add an Exclamation Point to the README - release/1.0",
    "user": "zbpatel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T22:46:16Z",
    "closed_at": "2025-09-19T22:49:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7882"
  },
  {
    "number": 7881,
    "title": "[None][doc] run CI on release/1.0",
    "user": "zbpatel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T22:43:58Z",
    "closed_at": "2025-09-19T22:45:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7881"
  },
  {
    "number": 7880,
    "title": "[TRTLLM-7132][feat] Add Eagle 2-model GPTOSS to CI",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T21:36:46Z",
    "closed_at": "2025-10-24T21:40:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7880"
  },
  {
    "number": 7879,
    "title": "[https://nvbugs/5520490][fix] Fix intermittent test failures by avoiding external web data pulls",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T18:42:12Z",
    "closed_at": "2025-09-20T00:24:14Z",
    "merged_at": "2025-09-20T00:24:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7879"
  },
  {
    "number": 7877,
    "title": "[https://nvbugs/5477359][fix] Removing test waivers",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T16:02:35Z",
    "closed_at": "2025-09-22T15:59:13Z",
    "merged_at": "2025-09-22T15:59:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7877"
  },
  {
    "number": 7876,
    "title": "[None][test] Update llm_models_root to improve path handling on BareMetal environment",
    "user": "yufeiwu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T15:20:20Z",
    "closed_at": "2025-09-24T09:35:58Z",
    "merged_at": "2025-09-24T09:35:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7876"
  },
  {
    "number": 7875,
    "title": "[None][fix] Fix DeepGEMM commit",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T13:29:45Z",
    "closed_at": "2025-09-22T05:52:50Z",
    "merged_at": "2025-09-22T05:52:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7875"
  },
  {
    "number": 7874,
    "title": "[None][fix] Fix dummy load format for DeepSeek.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T13:24:14Z",
    "closed_at": "2025-09-24T15:03:17Z",
    "merged_at": "2025-09-24T15:03:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7874"
  },
  {
    "number": 7873,
    "title": "DO NOT REVIEW, TRY PIPELINE Revert \"[TRTLLM-5966][feat] Helix: make softmax stats pointer availab…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T12:25:04Z",
    "closed_at": "2025-10-27T09:15:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7873"
  },
  {
    "number": 7871,
    "title": "[None][fix] Disable torch.compile for CapturableGuidedDecoder",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T10:56:23Z",
    "closed_at": "2025-09-22T02:04:30Z",
    "merged_at": "2025-09-22T02:04:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7871"
  },
  {
    "number": 7870,
    "title": "[TRTLLM-8129][feat] Allreduce tuning and benchmark script revising",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T09:34:00Z",
    "closed_at": "2025-10-16T06:15:26Z",
    "merged_at": "2025-10-16T06:15:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7870"
  },
  {
    "number": 7869,
    "title": "[TRTLLM-7846][feat] Http disagg-cluster management implemention",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T09:12:32Z",
    "closed_at": "2025-10-09T01:44:02Z",
    "merged_at": "2025-10-09T01:44:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7869"
  },
  {
    "number": 7868,
    "title": "[None][chore] Update chunked prefill test case configs",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T09:06:34Z",
    "closed_at": "2025-09-29T02:37:34Z",
    "merged_at": "2025-09-29T02:37:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7868"
  },
  {
    "number": 7867,
    "title": "[None][doc] Facilitates the integration of the transfer agent",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T08:31:55Z",
    "closed_at": "2025-10-21T12:06:24Z",
    "merged_at": "2025-10-21T12:06:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7867"
  },
  {
    "number": 7866,
    "title": "[None][doc] add a guide for modifying APIs",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T08:14:37Z",
    "closed_at": "2025-09-22T05:33:47Z",
    "merged_at": "2025-09-22T05:33:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7866"
  },
  {
    "number": 7865,
    "title": "[None][chore] cleanup build script",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T08:04:10Z",
    "closed_at": "2025-09-24T13:15:02Z",
    "merged_at": "2025-09-24T13:15:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7865"
  },
  {
    "number": 7864,
    "title": "[None][doc] Tech blog: Combining Guided Decoding and Speculative Decoding: Making CPU and GPU Cooperate Seamlessly",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T07:11:24Z",
    "closed_at": "2025-09-19T10:38:12Z",
    "merged_at": "2025-09-19T10:38:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7864"
  },
  {
    "number": 7863,
    "title": "[None][doc] add stable label to all the un-labelled arguments in LLM class",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T06:31:26Z",
    "closed_at": "2025-09-22T05:47:36Z",
    "merged_at": "2025-09-22T05:47:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7863"
  },
  {
    "number": 7862,
    "title": "[None][test] Update nim function test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T06:19:45Z",
    "closed_at": "2025-10-13T09:07:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7862"
  },
  {
    "number": 7861,
    "title": "[None][fix] api stability bug in status label",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T06:05:28Z",
    "closed_at": "2025-09-22T06:17:27Z",
    "merged_at": "2025-09-22T06:17:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7861"
  },
  {
    "number": 7860,
    "title": "[None][chore] Update benchmark script",
    "user": "zerollzeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T06:04:12Z",
    "closed_at": "2025-09-23T10:15:43Z",
    "merged_at": "2025-09-23T10:15:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7860"
  },
  {
    "number": 7859,
    "title": "[None][infra] Waive failed tests in post-merge",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T05:25:12Z",
    "closed_at": "2025-09-19T06:16:12Z",
    "merged_at": "2025-09-19T06:16:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7859"
  },
  {
    "number": 7858,
    "title": "[https://nvbugs/5522847][fix] Disable GC on disagg server and client",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T05:23:57Z",
    "closed_at": "2025-09-23T01:16:56Z",
    "merged_at": "2025-09-23T01:16:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7858"
  },
  {
    "number": 7857,
    "title": "[https://nvbugs/5477404][chore] unwaive test_disaggregated_single_gpu.py::test_disaggregated_llama_context_capacity",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T04:49:34Z",
    "closed_at": "2025-09-24T02:31:35Z",
    "merged_at": "2025-09-24T02:31:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7857"
  },
  {
    "number": 7856,
    "title": "[https://nvbugs/5518713][fix] Trtllm-gen moe backend for blockwise fp8 ckpt (Qwen3-235B-A22B-FP8)",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T03:58:15Z",
    "closed_at": "2025-09-26T21:29:32Z",
    "merged_at": "2025-09-26T21:29:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7856"
  },
  {
    "number": 7855,
    "title": "[https://nvbugs/5525849][fix] Cherry-pick to fix mismatch of max seq len between kv cache manager and dummy requests",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T03:31:29Z",
    "closed_at": "2025-09-22T10:07:48Z",
    "merged_at": "2025-09-22T10:07:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7855"
  },
  {
    "number": 7854,
    "title": "[None][fix] cherrypick to main: Fix possible mpi broadcast and gather issue on large object",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T03:16:37Z",
    "closed_at": "2025-09-22T02:17:24Z",
    "merged_at": "2025-09-22T02:17:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7854"
  },
  {
    "number": 7853,
    "title": "[None][ci] Waive llama3 auto dtype test bug in https://nvbugs/5527956.",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T02:58:44Z",
    "closed_at": "2025-09-19T06:55:00Z",
    "merged_at": "2025-09-19T06:55:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7853"
  },
  {
    "number": 7852,
    "title": "[None][chore] polish error message in cute_dsl_utils.py",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T02:54:52Z",
    "closed_at": "2025-09-19T04:05:12Z",
    "merged_at": "2025-09-19T04:05:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7852"
  },
  {
    "number": 7851,
    "title": "[None][chroe] Rename TensorRT-LLM to TensorRT LLM for source code.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T02:09:35Z",
    "closed_at": "2025-09-22T17:05:47Z",
    "merged_at": "2025-09-22T17:05:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7851"
  },
  {
    "number": 7850,
    "title": "[None][doc] Rename TensorRT-LLM to TensorRT LLM for homepage and the …",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T02:01:50Z",
    "closed_at": "2025-09-19T14:05:42Z",
    "merged_at": "2025-09-19T14:05:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7850"
  },
  {
    "number": 7849,
    "title": "[https://nvbugs/5521799][fix] Trim incorrectly generated harmony messages",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-19T00:49:12Z",
    "closed_at": "2025-09-24T08:38:44Z",
    "merged_at": "2025-09-24T08:38:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7849"
  },
  {
    "number": 7848,
    "title": "[None][doc] Update Perf-Overview.md for release/1.0",
    "user": "zbpatel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T23:50:09Z",
    "closed_at": "2025-09-22T05:38:17Z",
    "merged_at": "2025-09-22T05:38:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7848"
  },
  {
    "number": 7846,
    "title": "[None][doc] Added line about partial reuse",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T19:51:07Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7846"
  },
  {
    "number": 7845,
    "title": "[https://nvbugs/5522462][fix] Fix FP8 scout illegal memory access",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T19:30:56Z",
    "closed_at": "2025-09-19T14:30:37Z",
    "merged_at": "2025-09-19T14:30:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7845"
  },
  {
    "number": 7844,
    "title": "[#4674][bugfix] AutoDeploy Fix memory leak in fuse_moe",
    "user": "galagam",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T16:52:55Z",
    "closed_at": "2025-09-29T08:01:07Z",
    "merged_at": "2025-09-29T08:01:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7844"
  },
  {
    "number": 7843,
    "title": "[AutoDeploy] Linear Attention Support",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T14:48:42Z",
    "closed_at": "2025-10-03T20:31:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7843"
  },
  {
    "number": 7841,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T14:11:30Z",
    "closed_at": "2025-09-19T09:59:48Z",
    "merged_at": "2025-09-19T09:59:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7841"
  },
  {
    "number": 7840,
    "title": "[TRTLLM-8188][chore] refactor GenerationExecutorWorker with WorkerBase for better code reusing",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T13:36:55Z",
    "closed_at": "2025-09-20T13:24:23Z",
    "merged_at": "2025-09-20T13:24:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7840"
  },
  {
    "number": 7838,
    "title": "[TRTLLM-7073][feat] Support torch compile for PP for Llama and DeepSe…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T10:37:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7838"
  },
  {
    "number": 7837,
    "title": "[None][doc] Replace the main in the examples' link with commit id.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T10:29:37Z",
    "closed_at": "2025-09-18T15:44:01Z",
    "merged_at": "2025-09-18T15:44:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7837"
  },
  {
    "number": 7836,
    "title": "[None][chore] remove generated fmha_cubin.h from source tree",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T08:46:45Z",
    "closed_at": "2025-09-18T12:10:04Z",
    "merged_at": "2025-09-18T12:10:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7836"
  },
  {
    "number": 7835,
    "title": "[None][doc] scaffolding tech blog part one",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T06:53:40Z",
    "closed_at": "2025-09-25T06:42:00Z",
    "merged_at": "2025-09-25T06:42:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7835"
  },
  {
    "number": 7833,
    "title": "[None][chore] remove cli cases for rtx6k",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T06:43:54Z",
    "closed_at": "2025-09-19T08:33:59Z",
    "merged_at": "2025-09-19T08:33:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7833"
  },
  {
    "number": 7832,
    "title": "[TRTLLM-6286] [feat] Update CUTLASS to 4.2 and enable SM103 group gemm",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T06:34:44Z",
    "closed_at": "2025-09-19T01:50:55Z",
    "merged_at": "2025-09-19T01:50:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7832"
  },
  {
    "number": 7831,
    "title": "[TRTLLM-8031][feat] Add chunked return_generation_logits logic",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T05:44:46Z",
    "closed_at": "2025-10-01T16:47:08Z",
    "merged_at": "2025-10-01T16:47:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7831"
  },
  {
    "number": 7830,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T05:00:09Z",
    "closed_at": "2025-09-18T05:14:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7830"
  },
  {
    "number": 7829,
    "title": "[None][fix] refine `backend` option handling for commands",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T03:48:11Z",
    "closed_at": "2025-09-24T02:54:33Z",
    "merged_at": "2025-09-24T02:54:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7829"
  },
  {
    "number": 7828,
    "title": "[https://nvbugs/5512556][unwaive] Unwaive DeepSeek PP tests",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T03:26:19Z",
    "closed_at": "2025-09-22T02:26:30Z",
    "merged_at": "2025-09-22T02:26:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7828"
  },
  {
    "number": 7827,
    "title": "Rpc.rebase",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T03:22:42Z",
    "closed_at": "2025-10-13T06:48:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7827"
  },
  {
    "number": 7826,
    "title": "[None][chore] revert pr6250",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T02:11:49Z",
    "closed_at": "2025-09-18T02:41:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7826"
  },
  {
    "number": 7824,
    "title": "[None][chore] Version bump for 1.1.0rc6",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-18T01:42:17Z",
    "closed_at": "2025-09-18T03:13:56Z",
    "merged_at": "2025-09-18T03:13:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7824"
  },
  {
    "number": 7822,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T23:09:35Z",
    "closed_at": "2025-09-18T07:50:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7822"
  },
  {
    "number": 7821,
    "title": "[OMNIML-2336][feat] Add NVFP4 x FP8 moe kernels",
    "user": "sychen52",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T22:02:09Z",
    "closed_at": "2025-09-24T19:14:36Z",
    "merged_at": "2025-09-24T19:14:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7821"
  },
  {
    "number": 7820,
    "title": "[None][infra] Fix for generate lockfile pipeline",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T21:55:23Z",
    "closed_at": "2025-10-16T21:17:18Z",
    "merged_at": "2025-10-16T21:17:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7820"
  },
  {
    "number": 7819,
    "title": "[https://nvbugs/5519530][fix] Fix gptoss 2-gpu test",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T21:50:49Z",
    "closed_at": "2025-09-18T08:01:54Z",
    "merged_at": "2025-09-18T08:01:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7819"
  },
  {
    "number": 7817,
    "title": "[https://nvbugs/5519462][fix] skip deepseek test on preHopper",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T19:06:26Z",
    "closed_at": "2025-09-18T12:01:06Z",
    "merged_at": "2025-09-18T12:01:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7817"
  },
  {
    "number": 7816,
    "title": "[None][infra] update ci allow list 2025/09/17",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T19:04:31Z",
    "closed_at": "2025-09-18T06:21:40Z",
    "merged_at": "2025-09-18T06:21:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7816"
  },
  {
    "number": 7815,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T18:16:43Z",
    "closed_at": "2025-09-18T03:06:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7815"
  },
  {
    "number": 7814,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T15:31:55Z",
    "closed_at": "2025-09-18T03:04:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7814"
  },
  {
    "number": 7813,
    "title": "[https://nvbugs/1234567][fix] Revert https://github.com/NVIDIA/TensorRT-LLM/pull/7768/files",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T14:55:14Z",
    "closed_at": "2025-09-17T19:34:05Z",
    "merged_at": "2025-09-17T19:34:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7813"
  },
  {
    "number": 7812,
    "title": "[None][infra] Waive failed tests on main 09/17",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T14:53:30Z",
    "closed_at": "2025-09-17T15:44:37Z",
    "merged_at": "2025-09-17T15:44:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7812"
  },
  {
    "number": 7811,
    "title": "[TRTLLM-8714][fix] update create_input_processor to handle custom checkpoint format",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T14:48:43Z",
    "closed_at": "2025-10-23T08:27:57Z",
    "merged_at": "2025-10-23T08:27:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7811"
  },
  {
    "number": 7810,
    "title": "Revert \"[https://nvbugs/5517023][fix] Pass allreduce strategy and force NCCL on pre-Blackwell arch\"",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T14:45:30Z",
    "closed_at": "2025-09-17T14:47:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7810"
  },
  {
    "number": 7809,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T14:45:05Z",
    "closed_at": "2025-09-18T03:05:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7809"
  },
  {
    "number": 7808,
    "title": "[https://nvbugs/5513423][fix] Correctly respect min_tokens in PyTorch Workflow",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T13:15:16Z",
    "closed_at": "2025-09-22T05:15:18Z",
    "merged_at": "2025-09-22T05:15:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7808"
  },
  {
    "number": 7807,
    "title": "[TRTLLM-7250][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T13:02:32Z",
    "closed_at": "2025-09-18T09:13:07Z",
    "merged_at": "2025-09-18T09:13:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7807"
  },
  {
    "number": 7806,
    "title": "[https://nvbugs/5519544][fix] fix invalid expression for disabling pa…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T12:02:09Z",
    "closed_at": "2025-09-18T04:54:52Z",
    "merged_at": "2025-09-18T04:54:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7806"
  },
  {
    "number": 7805,
    "title": "[https://nvbugs/5498478][fix] Fix eagle3 fp8 kv target model + bf16 draft model + chunked prefill",
    "user": "DylanChen-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T11:34:18Z",
    "closed_at": "2025-11-05T03:56:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7805"
  },
  {
    "number": 7804,
    "title": "[None][ci] test ci",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T11:01:20Z",
    "closed_at": "2025-09-22T04:19:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7804"
  },
  {
    "number": 7803,
    "title": "[https://nvbugs/5523080][fix] Correct the batch index in device tensors",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T09:58:00Z",
    "closed_at": "2025-09-18T05:45:37Z",
    "merged_at": "2025-09-18T05:45:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7803"
  },
  {
    "number": 7802,
    "title": "[None][ci] restore unwaive list",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T09:50:01Z",
    "closed_at": "2025-09-18T02:50:34Z",
    "merged_at": "2025-09-18T02:50:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7802"
  },
  {
    "number": 7801,
    "title": "[TRTLLM-7250][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T09:42:21Z",
    "closed_at": "2025-09-18T06:48:16Z",
    "merged_at": "2025-09-18T06:48:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7801"
  },
  {
    "number": 7800,
    "title": "[None][ci] set TORCHINDUCTOR_COMPILE_THREADS correctly",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T09:26:12Z",
    "closed_at": "2025-09-18T23:19:50Z",
    "merged_at": "2025-09-18T23:19:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7800"
  },
  {
    "number": 7799,
    "title": "[TRTLLM-6541][test] WIP: Do not merge - Add multi node supporting for perf",
    "user": "fredricz-20070104",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T08:54:03Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7799"
  },
  {
    "number": 7798,
    "title": "[TRTLLM-6549][fix] add kv cache time output back",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T08:50:29Z",
    "closed_at": "2025-09-23T18:12:43Z",
    "merged_at": "2025-09-23T18:12:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7798"
  },
  {
    "number": 7797,
    "title": "[https://nvbugs/5522332][fix] Pin numpy version for Gemma. (cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/7783)",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T08:40:41Z",
    "closed_at": "2025-09-19T10:50:40Z",
    "merged_at": "2025-09-19T10:50:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7797"
  },
  {
    "number": 7796,
    "title": "[https://nvbugs/5508536][fix] Revert #7041: Move stop_criteria to sample_async (#7041)",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T08:02:47Z",
    "closed_at": "2025-09-18T01:27:01Z",
    "merged_at": "2025-09-18T01:27:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7796"
  },
  {
    "number": 7795,
    "title": "[None][doc] fix section header of llm_kv_cache_offloading example",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T07:55:46Z",
    "closed_at": "2025-09-17T09:26:12Z",
    "merged_at": "2025-09-17T09:26:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7795"
  },
  {
    "number": 7794,
    "title": "[None][fix] trtllm-serve AD fixes + processing changes for mistral3.1",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T07:42:50Z",
    "closed_at": "2025-09-24T12:59:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7794"
  },
  {
    "number": 7793,
    "title": "[None][ci] group parallel test cases by file",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T07:41:05Z",
    "closed_at": "2025-09-17T09:25:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7793"
  },
  {
    "number": 7792,
    "title": "[None][ci] Test CI for multi-node",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T07:37:30Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7792"
  },
  {
    "number": 7791,
    "title": "[https://nvbugs/5516661][fix] Drop waive case 5516661",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T07:17:42Z",
    "closed_at": "2025-09-18T00:55:32Z",
    "merged_at": "2025-09-18T00:55:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7791"
  },
  {
    "number": 7790,
    "title": "[https://nvbugs/5522851][fix] Correct the logic to update kv_lens_cuda",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T06:42:06Z",
    "closed_at": "2025-09-19T00:11:29Z",
    "merged_at": "2025-09-19T00:11:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7790"
  },
  {
    "number": 7789,
    "title": "[None][feat] Support kv_cahce_reuse for HyperCLOVAX-Vision model",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T06:41:16Z",
    "closed_at": "2025-10-21T02:11:25Z",
    "merged_at": "2025-10-21T02:11:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7789"
  },
  {
    "number": 7788,
    "title": "[None][ci] waive test_llama_eagle3[True-FLASHINFER-False-False-False-False-True]",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T06:39:55Z",
    "closed_at": "2025-09-17T07:12:55Z",
    "merged_at": "2025-09-17T07:12:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7788"
  },
  {
    "number": 7787,
    "title": "[None][doc] Update docker cmd in quick start guide and trtllm-serve …",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T06:15:53Z",
    "closed_at": "2025-09-17T12:35:05Z",
    "merged_at": "2025-09-17T12:35:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7787"
  },
  {
    "number": 7786,
    "title": "[None][ci] skip test_fp4_linear_cute_dsl",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T06:15:34Z",
    "closed_at": "2025-09-17T06:18:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7786"
  },
  {
    "number": 7785,
    "title": "[TRTLLM-7183][test] Feature fix model issue for disagg serving",
    "user": "fredricz-20070104",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T05:49:19Z",
    "closed_at": "2025-09-19T02:12:55Z",
    "merged_at": "2025-09-19T02:12:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7785"
  },
  {
    "number": 7784,
    "title": "[None][fix] Fix CI issue for dsl pkg install",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T04:48:53Z",
    "closed_at": "2025-09-18T05:58:20Z",
    "merged_at": "2025-09-18T05:58:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7784"
  },
  {
    "number": 7783,
    "title": "[https://nvbugs/5522332][fix] Pin numpy version for Gemma.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T04:42:23Z",
    "closed_at": "2025-09-17T08:23:28Z",
    "merged_at": "2025-09-17T08:23:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7783"
  },
  {
    "number": 7782,
    "title": "[TRTLLM-7250][fix] waive block tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T03:27:09Z",
    "closed_at": "2025-09-17T07:31:03Z",
    "merged_at": "2025-09-17T07:31:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7782"
  },
  {
    "number": 7781,
    "title": "[None][ci] waive test_llm_gemma_1gpu_summary_vswa",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T02:05:41Z",
    "closed_at": "2025-09-17T02:48:31Z",
    "merged_at": "2025-09-17T02:48:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7781"
  },
  {
    "number": 7780,
    "title": "[None][fix] Revert \"Revert \"[None][feat] support attention dp for qwen3 dense model\"\"",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T01:58:50Z",
    "closed_at": "2025-09-18T12:11:06Z",
    "merged_at": "2025-09-18T12:11:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7780"
  },
  {
    "number": 7779,
    "title": "[TRTLLM-7070][feat] add gpt-oss chunked prefill tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-17T01:15:29Z",
    "closed_at": "2025-09-22T07:12:43Z",
    "merged_at": "2025-09-22T07:12:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7779"
  },
  {
    "number": 7778,
    "title": "[None][AutoDeploy] Add the triton ref for Bamba custom ops",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T23:54:26Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7778"
  },
  {
    "number": 7777,
    "title": "[None][feat] Support KVcache reuse/Chunked prefill for multimodal llama4",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T23:43:49Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7777"
  },
  {
    "number": 7776,
    "title": "[TRTLLM-7292][feat] Support multi-threaded tokenizers for trtllm-serve (cherry-pick)",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T23:10:49Z",
    "closed_at": "2025-09-23T16:39:47Z",
    "merged_at": "2025-09-23T16:39:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7776"
  },
  {
    "number": 7775,
    "title": "[None][waive] Waive tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T22:47:19Z",
    "closed_at": "2025-09-16T23:42:33Z",
    "merged_at": "2025-09-16T23:42:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7775"
  },
  {
    "number": 7774,
    "title": "[None][doc] Cherry-pick deployment guide update from 1.1.0rc2 branch to main branch",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T22:31:46Z",
    "closed_at": "2025-09-18T14:50:27Z",
    "merged_at": "2025-09-18T14:50:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7774"
  },
  {
    "number": 7773,
    "title": "[None][infra] Update CI allowlist 2025-09-16",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T19:58:08Z",
    "closed_at": "2025-09-16T21:55:30Z",
    "merged_at": "2025-09-16T21:55:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7773"
  },
  {
    "number": 7772,
    "title": "[TRTLLM-7398][doc] Add doc for KV cache salting support",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T17:46:30Z",
    "closed_at": "2025-09-16T21:49:15Z",
    "merged_at": "2025-09-16T21:49:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7772"
  },
  {
    "number": 7771,
    "title": "[https://nvbugs/5508890][fix] gen. result cleanup when using PostprocWorker",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T16:27:45Z",
    "closed_at": "2025-09-18T06:01:19Z",
    "merged_at": "2025-09-18T06:01:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7771"
  },
  {
    "number": 7770,
    "title": "[#5860][feat] Add ModelOPT INT4 awq fake quant support in AutoDeploy",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T16:24:17Z",
    "closed_at": "2025-10-01T20:13:45Z",
    "merged_at": "2025-10-01T20:13:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7770"
  },
  {
    "number": 7769,
    "title": "Revert \"Revert \"[None][feat] support attention dp for qwen3 dense model\"\"",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T13:56:19Z",
    "closed_at": "2025-09-17T01:59:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7769"
  },
  {
    "number": 7768,
    "title": "[https://nvbugs/5517023][fix] Pass allreduce strategy and force NCCL on pre-Blackwell arch",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T11:32:41Z",
    "closed_at": "2025-09-17T05:28:43Z",
    "merged_at": "2025-09-17T05:28:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7768"
  },
  {
    "number": 7767,
    "title": "[None][feat] add an example of KV cache host offloading",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T11:17:42Z",
    "closed_at": "2025-09-17T05:51:15Z",
    "merged_at": "2025-09-17T05:51:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7767"
  },
  {
    "number": 7766,
    "title": "[TRTLLM-7989][infra] Bundle UCX and NIXL libs in the TRTLLM python package",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T10:09:26Z",
    "closed_at": "2025-09-22T08:43:36Z",
    "merged_at": "2025-09-22T08:43:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7766"
  },
  {
    "number": 7765,
    "title": "Revert \"[None][feat] support attention dp for qwen3 dense model\"",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T09:32:49Z",
    "closed_at": "2025-09-16T11:09:04Z",
    "merged_at": "2025-09-16T11:09:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7765"
  },
  {
    "number": 7764,
    "title": "[TRTLLM-6898][feat] Add swapab, tileN64, cga sync support for cute dsl nvfp4 gemm",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T09:21:28Z",
    "closed_at": "2025-09-18T13:20:04Z",
    "merged_at": "2025-09-18T13:20:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7764"
  },
  {
    "number": 7763,
    "title": "[#6102][fix] support non-system python installation",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T09:18:15Z",
    "closed_at": "2025-09-26T02:16:15Z",
    "merged_at": "2025-09-26T02:16:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7763"
  },
  {
    "number": 7762,
    "title": "[https://nvbugs/5468897][fix] fix invalid expression for disabling pa…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T08:42:16Z",
    "closed_at": "2025-09-17T03:14:54Z",
    "merged_at": "2025-09-17T03:14:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7762"
  },
  {
    "number": 7761,
    "title": "[TRTLLM-8637][feat] Optimize the routing kernel for DeepseekV3 (MoE CUTLASS backend); Add support for 384 experts (MoE TRTLLM backend)",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T08:36:18Z",
    "closed_at": "2025-10-20T02:08:32Z",
    "merged_at": "2025-10-20T02:08:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7761"
  },
  {
    "number": 7760,
    "title": "[None][fix] Detach preprocessing from trtllm-bench multimodal",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T08:32:55Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7760"
  },
  {
    "number": 7759,
    "title": "[TRTLLM-8070][test] add generation logits case for llama3",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T08:19:39Z",
    "closed_at": "2025-09-18T05:33:17Z",
    "merged_at": "2025-09-18T05:33:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7759"
  },
  {
    "number": 7758,
    "title": "[https://nvbugs/5517260][fix] move scaffolding contrib module's import to subdirectory",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T08:13:01Z",
    "closed_at": "2025-09-17T03:36:33Z",
    "merged_at": "2025-09-17T03:36:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7758"
  },
  {
    "number": 7757,
    "title": "[TRTLLM-6286] [perf] Add NoSmem epilogue schedule and dynamic cluster shape for sm10x group gemm",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T07:58:52Z",
    "closed_at": "2025-09-21T03:38:17Z",
    "merged_at": "2025-09-21T03:38:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7757"
  },
  {
    "number": 7756,
    "title": "[None][feat] Return topk logprobs in torch backend",
    "user": "dcaox",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T07:05:01Z",
    "closed_at": "2025-09-24T07:30:40Z",
    "merged_at": "2025-09-24T07:30:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7756"
  },
  {
    "number": 7755,
    "title": "[None][fix] Fix and add test for TRTLLM MoE backend",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T06:27:31Z",
    "closed_at": "2025-09-23T03:26:25Z",
    "merged_at": "2025-09-23T03:26:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7755"
  },
  {
    "number": 7754,
    "title": "[None][doc] Fix the link in the doc",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T05:55:07Z",
    "closed_at": "2025-09-16T07:49:45Z",
    "merged_at": "2025-09-16T07:49:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7754"
  },
  {
    "number": 7753,
    "title": "[https://nvbugs/5519525][fix] fix doc invalid link for bug 5519525",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T05:54:59Z",
    "closed_at": "2025-09-16T08:27:05Z",
    "merged_at": "2025-09-16T08:27:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7753"
  },
  {
    "number": 7751,
    "title": "[None][doc] Enhance api reference doc by labeling stable APIs",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T02:40:26Z",
    "closed_at": "2025-09-17T02:20:26Z",
    "merged_at": "2025-09-17T02:20:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7751"
  },
  {
    "number": 7750,
    "title": "[None] [chore] cherry pick changes on slurm scripts from `release/1.1.0rc2`",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T02:24:38Z",
    "closed_at": "2025-09-16T08:07:13Z",
    "merged_at": "2025-09-16T08:07:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7750"
  },
  {
    "number": 7749,
    "title": "[TRTLLM-6898][feat] Add swapab, tileN64, cga sync support for cute dsl nvfp4 gemm",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T02:21:59Z",
    "closed_at": "2025-09-16T09:22:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7749"
  },
  {
    "number": 7747,
    "title": "[https://nvbugs/5481434][feat] cherry-pick fix to reuse pytorch memory segments occupied by cudagraph",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T01:21:00Z",
    "closed_at": "2025-09-19T02:25:21Z",
    "merged_at": "2025-09-19T02:25:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7747"
  },
  {
    "number": 7746,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-16T01:13:38Z",
    "closed_at": "2025-09-16T10:43:40Z",
    "merged_at": "2025-09-16T10:43:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7746"
  },
  {
    "number": 7745,
    "title": "[None][auto_deploy] Bamba",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T21:52:52Z",
    "closed_at": "2025-09-22T15:03:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7745"
  },
  {
    "number": 7744,
    "title": "[#7704][chore] Enable MathJax to fix formulas in documentation",
    "user": "karljang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T20:29:26Z",
    "closed_at": "2025-09-19T15:42:27Z",
    "merged_at": "2025-09-19T15:42:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7744"
  },
  {
    "number": 7743,
    "title": "[None][infra] AutoDeploy: codeowners for autodeploy unit tests",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T16:45:07Z",
    "closed_at": "2025-09-15T18:20:12Z",
    "merged_at": "2025-09-15T18:20:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7743"
  },
  {
    "number": 7742,
    "title": "[None][chore] AutoDeploy: clean up of model unit test configuration",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T16:40:24Z",
    "closed_at": "2025-09-17T02:42:02Z",
    "merged_at": "2025-09-17T02:42:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7742"
  },
  {
    "number": 7740,
    "title": "[https://nvbugs/5503529][fix] Change test_llmapi_example_multilora to get adapters path from cmd line to avoid downloading from HF",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T14:54:20Z",
    "closed_at": "2025-09-16T08:35:14Z",
    "merged_at": "2025-09-16T08:35:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7740"
  },
  {
    "number": 7739,
    "title": "[TRTLLM-6295][test] Exit as early as possible and propagate exit status correctly for multi-node testing",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T14:21:51Z",
    "closed_at": "2025-09-16T01:59:18Z",
    "merged_at": "2025-09-16T01:59:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7739"
  },
  {
    "number": 7738,
    "title": "[TRTLLM-4500][feat] Add serialization/deserialization options for AutoTuner profiling cache",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T14:08:16Z",
    "closed_at": "2025-09-28T23:40:51Z",
    "merged_at": "2025-09-28T23:40:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7738"
  },
  {
    "number": 7737,
    "title": "[https://nvbugs/5516666][fix] cherrypick fix to the CUDA graph warmup issue when using speculative decoding",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T14:04:07Z",
    "closed_at": "2025-09-16T22:24:20Z",
    "merged_at": "2025-09-16T22:24:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7737"
  },
  {
    "number": 7736,
    "title": "[None][chore] AutoDeploy: neat disablement of transforms in pipeline",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T14:01:05Z",
    "closed_at": "2025-09-16T15:31:49Z",
    "merged_at": "2025-09-16T15:31:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7736"
  },
  {
    "number": 7735,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T13:58:47Z",
    "closed_at": "2025-09-16T02:58:06Z",
    "merged_at": "2025-09-16T02:58:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7735"
  },
  {
    "number": 7734,
    "title": "[https://nvbugs/5485325][fix] Cherry-pick #7373: fix the CUDA graph warmup issue when using speculative decoding",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T13:58:23Z",
    "closed_at": "2025-09-17T05:57:39Z",
    "merged_at": "2025-09-17T05:57:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7734"
  },
  {
    "number": 7733,
    "title": "[None][ci] move qwen3 tests from GB200 to B200",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T12:52:49Z",
    "closed_at": "2025-09-16T00:12:29Z",
    "merged_at": "2025-09-16T00:12:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7733"
  },
  {
    "number": 7732,
    "title": "[None][fix] Add TP information in weight scale loading in WeightOnlyQuantLinearMethod",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T12:31:52Z",
    "closed_at": "2025-09-18T08:30:51Z",
    "merged_at": "2025-09-18T08:30:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7732"
  },
  {
    "number": 7731,
    "title": "[TRTLLM-7384][feat] enable rejection sampling for CDL",
    "user": "kris1025",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T12:26:25Z",
    "closed_at": "2025-10-12T12:38:48Z",
    "merged_at": "2025-10-12T12:38:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7731"
  },
  {
    "number": 7730,
    "title": "[None][feat] Use list instead of torch tensor for new tokens in update requests",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T12:21:41Z",
    "closed_at": "2025-09-23T14:40:08Z",
    "merged_at": "2025-09-23T14:40:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7730"
  },
  {
    "number": 7729,
    "title": "[None][doc] Clean the doc folder and move the outdated docs into lega…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T11:11:46Z",
    "closed_at": "2025-09-16T03:43:19Z",
    "merged_at": "2025-09-16T03:43:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7729"
  },
  {
    "number": 7728,
    "title": "[https://nvbugs/5517404][fix] Use the correct cuda graph for dynamic spec dec",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T10:42:42Z",
    "closed_at": "2025-09-21T00:20:48Z",
    "merged_at": "2025-09-21T00:20:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7728"
  },
  {
    "number": 7727,
    "title": "[TRTLLM-7008][fix] cherrypick to main Add automatic shared memory delete if already exist",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T10:37:50Z",
    "closed_at": "2025-09-21T18:01:52Z",
    "merged_at": "2025-09-21T18:01:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7727"
  },
  {
    "number": 7726,
    "title": "[#7692][fix] recognize RequestError as per-request error in background handler",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T10:29:33Z",
    "closed_at": "2025-09-24T03:11:17Z",
    "merged_at": "2025-09-24T03:11:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7726"
  },
  {
    "number": 7725,
    "title": "[None][chore] Fix error when running trtllm-bench without cuda graph.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T10:02:04Z",
    "closed_at": "2025-09-16T03:30:24Z",
    "merged_at": "2025-09-16T03:30:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7725"
  },
  {
    "number": 7724,
    "title": "[https://nvbugs/5355219][fix] Fix trtllm moe backend  test config and Qwen3 MoE multi node",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T09:56:01Z",
    "closed_at": "2025-09-16T02:33:36Z",
    "merged_at": "2025-09-16T02:33:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7724"
  },
  {
    "number": 7723,
    "title": "[TRTLLM-7918][feat] Support kvcache reuse and chunk prefill for phi4mm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T09:47:14Z",
    "closed_at": "2025-09-18T09:37:16Z",
    "merged_at": "2025-09-18T09:37:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7723"
  },
  {
    "number": 7722,
    "title": "[TRTLLM-7918][feat] Revert \"Support kvcache reuse for phi4mm (#7563)\"",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T09:12:05Z",
    "closed_at": "2025-09-15T09:19:44Z",
    "merged_at": "2025-09-15T09:19:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7722"
  },
  {
    "number": 7721,
    "title": "[None][fix] Update function name in phi4mm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T09:08:37Z",
    "closed_at": "2025-09-15T09:20:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7721"
  },
  {
    "number": 7720,
    "title": "[None][fix] waive hang tests on main",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T08:48:14Z",
    "closed_at": "2025-09-16T09:05:15Z",
    "merged_at": "2025-09-16T09:05:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7720"
  },
  {
    "number": 7719,
    "title": "[None][fix] get Local IP by connect remote",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T08:46:33Z",
    "closed_at": "2025-09-19T02:01:03Z",
    "merged_at": "2025-09-19T02:01:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7719"
  },
  {
    "number": 7718,
    "title": "[https://nvbugs/5427043][fix] cherrypick: request length exceeds max_num_tokens",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T08:08:07Z",
    "closed_at": "2025-09-22T10:37:48Z",
    "merged_at": "2025-09-22T10:37:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7718"
  },
  {
    "number": 7717,
    "title": "[https://nvbugs/5516710][fix] fix Llama 3.3 TP PP case",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T07:52:03Z",
    "closed_at": "2025-09-17T19:35:17Z",
    "merged_at": "2025-09-17T19:35:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7717"
  },
  {
    "number": 7716,
    "title": "[None][feat] Cherry-pick DeepGEMM related commits from release/1.1.0rc2",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T07:29:12Z",
    "closed_at": "2025-09-18T05:51:49Z",
    "merged_at": "2025-09-18T05:51:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7716"
  },
  {
    "number": 7715,
    "title": "[https://nvbugs/5441734][fix] replace flasky scaled_mm test with more stable config",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T07:24:54Z",
    "closed_at": "2025-09-16T01:55:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7715"
  },
  {
    "number": 7714,
    "title": "[https://nvbugs/5516665][fix] Fix CUTLASS moe fake impl errors",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T07:21:26Z",
    "closed_at": "2025-09-22T18:08:40Z",
    "merged_at": "2025-09-22T18:08:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7714"
  },
  {
    "number": 7713,
    "title": "[None][doc] Fix the link in the doc",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T07:10:28Z",
    "closed_at": "2025-09-16T01:50:26Z",
    "merged_at": "2025-09-16T01:50:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7713"
  },
  {
    "number": 7712,
    "title": "[TRTLLM-7831][feat] Cherry-pick from #7423 Support fp8 block wide ep cherry pick",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T06:55:04Z",
    "closed_at": "2025-09-23T00:41:38Z",
    "merged_at": "2025-09-23T00:41:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7712"
  },
  {
    "number": 7711,
    "title": "[https://nvbugs/5471106][fix] Remove the waivers",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T06:48:54Z",
    "closed_at": "2025-09-16T09:43:40Z",
    "merged_at": "2025-09-16T09:43:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7711"
  },
  {
    "number": 7710,
    "title": "[https://nvbugs/5512734][fix] Update kv cache config for maverick",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T05:46:19Z",
    "closed_at": "2025-09-15T14:53:30Z",
    "merged_at": "2025-09-15T14:53:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7710"
  },
  {
    "number": 7709,
    "title": "[None][ci] Test waives for the main branch 09/15",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T05:42:26Z",
    "closed_at": "2025-09-15T14:13:56Z",
    "merged_at": "2025-09-15T14:13:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7709"
  },
  {
    "number": 7708,
    "title": "[https://nvbugs/5488582][fix] Cherry-pick 7495: Avoid unexpected Triton recompilation in DG fused_moe",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T05:19:41Z",
    "closed_at": "2025-09-17T01:00:29Z",
    "merged_at": "2025-09-17T01:00:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7708"
  },
  {
    "number": 7705,
    "title": "[https://nvbugs/5512734][fix] update kv_cache_config for maverick OOM issue",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T04:33:41Z",
    "closed_at": "2025-09-15T05:51:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7705"
  },
  {
    "number": 7702,
    "title": "[https://nvbugs/5437405][fix] cherry-pick PR 7000 (qwen3 235b eagle3 ci)",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T02:37:25Z",
    "closed_at": "2025-09-15T08:03:37Z",
    "merged_at": "2025-09-15T08:03:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7702"
  },
  {
    "number": 7701,
    "title": "[WIP][https://nvbugs/5451205][feat] Add cuBLASLt NVFP4 GEMM backend support",
    "user": "Wong4j",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T01:40:03Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7701"
  },
  {
    "number": 7700,
    "title": "[None][ci] Test waives for the release/1.0 branch 09/15",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T00:57:09Z",
    "closed_at": "2025-09-15T01:24:05Z",
    "merged_at": "2025-09-15T01:24:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7700"
  },
  {
    "number": 7699,
    "title": "[None][chore] move some cases from post-merge to pre-merge to detect errors in early stage",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-14T13:39:34Z",
    "closed_at": "2025-09-15T07:37:58Z",
    "merged_at": "2025-09-15T07:37:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7699"
  },
  {
    "number": 7698,
    "title": "[None][ci] Test waives for the main branch 09/14",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-14T13:28:37Z",
    "closed_at": "2025-09-14T15:48:04Z",
    "merged_at": "2025-09-14T15:48:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7698"
  },
  {
    "number": 7697,
    "title": "[None][chore] Remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-14T12:16:10Z",
    "closed_at": "2025-09-17T07:14:09Z",
    "merged_at": "2025-09-17T07:14:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7697"
  },
  {
    "number": 7696,
    "title": "[None][doc] Add labels description note into llm api section",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-14T04:17:22Z",
    "closed_at": "2025-09-15T06:15:09Z",
    "merged_at": "2025-09-15T06:15:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7696"
  },
  {
    "number": 7690,
    "title": "[None][infra] Update images",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-11T14:36:01Z",
    "closed_at": "2025-09-11T14:41:39Z",
    "merged_at": "2025-09-11T14:41:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7690"
  },
  {
    "number": 7689,
    "title": "[None][ci] Some improvements for Slurm CI",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-11T14:33:53Z",
    "closed_at": "2025-09-14T08:56:32Z",
    "merged_at": "2025-09-14T08:56:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7689"
  },
  {
    "number": 7687,
    "title": "[None][chore] Remove unused get_quant_scales methods",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T21:19:43Z",
    "closed_at": "2025-09-16T19:56:12Z",
    "merged_at": "2025-09-16T19:56:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7687"
  },
  {
    "number": 7686,
    "title": "[https://nvbugs/5471108][chore] Unwaiving disagg acc test",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T20:53:45Z",
    "closed_at": "2025-09-19T12:56:09Z",
    "merged_at": "2025-09-19T12:56:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7686"
  },
  {
    "number": 7685,
    "title": "[None][fix] Enable kv cache reuse for VLMs with special tokens",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T20:49:47Z",
    "closed_at": "2025-09-15T18:01:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7685"
  },
  {
    "number": 7684,
    "title": "[None][ci] move some test cases from l40s to a30",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T20:03:02Z",
    "closed_at": "2025-09-10T23:22:35Z",
    "merged_at": "2025-09-10T23:22:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7684"
  },
  {
    "number": 7683,
    "title": "[None][chore] Add build system codeowners",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T18:32:02Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7683"
  },
  {
    "number": 7682,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T18:01:47Z",
    "closed_at": "2025-09-15T03:44:53Z",
    "merged_at": "2025-09-15T03:44:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7682"
  },
  {
    "number": 7681,
    "title": "[https://nvbugs/5398180][feat] Improve Llama4 performance for small max_seqlen cases",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T17:24:38Z",
    "closed_at": "2025-09-15T01:04:32Z",
    "merged_at": "2025-09-15T01:04:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7681"
  },
  {
    "number": 7679,
    "title": "[https://nvbugs/5513192][fix] Add the missing param for kv_cache_tran…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T16:05:49Z",
    "closed_at": "2025-09-11T11:00:16Z",
    "merged_at": "2025-09-11T11:00:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7679"
  },
  {
    "number": 7678,
    "title": "[None][test] add test for min_tokens",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T15:47:44Z",
    "closed_at": "2025-09-15T07:59:23Z",
    "merged_at": "2025-09-15T07:59:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7678"
  },
  {
    "number": 7676,
    "title": "[None][infra] Waive failed cases on main 0910",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T13:43:10Z",
    "closed_at": "2025-09-10T17:43:30Z",
    "merged_at": "2025-09-10T17:43:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7676"
  },
  {
    "number": 7674,
    "title": "[#7675][feat] CapturedGraph to support max_batch_size > max(cuda_graph_batch_sizes)",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T13:34:09Z",
    "closed_at": "2025-09-21T14:47:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7674"
  },
  {
    "number": 7673,
    "title": "[https://nvbugs/5436461][fix] Adjust free_gpu_memory_fraction of test_eagle3",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T10:07:14Z",
    "closed_at": "2025-09-10T15:16:26Z",
    "merged_at": "2025-09-10T15:16:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7673"
  },
  {
    "number": 7672,
    "title": "[https://nvbugs/5477359][fix] Nanobind: Allow none types for fields in result",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T09:57:49Z",
    "closed_at": "2025-09-10T13:13:46Z",
    "merged_at": "2025-09-10T13:13:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7672"
  },
  {
    "number": 7671,
    "title": "[https://nvbugs/5477730][fix] Fix the alltoall case when tp_size larg…",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T09:37:43Z",
    "closed_at": "2025-09-10T12:21:10Z",
    "merged_at": "2025-09-10T12:21:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7671"
  },
  {
    "number": 7670,
    "title": "[https://nvbugs/5509024][fix] Print full parsed outputs and update keywords for multimodal model",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T09:30:09Z",
    "closed_at": "2025-09-16T09:44:54Z",
    "merged_at": "2025-09-16T09:44:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7670"
  },
  {
    "number": 7669,
    "title": "[None][feat] Add a standalone buffer cache class and reuse buffers between cduagraph and no-graph flow",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T08:13:53Z",
    "closed_at": "2025-09-26T14:28:06Z",
    "merged_at": "2025-09-26T14:28:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7669"
  },
  {
    "number": 7668,
    "title": "[None][infra] Bump version to 1.1.0rc5",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T07:34:46Z",
    "closed_at": "2025-09-10T08:06:55Z",
    "merged_at": "2025-09-10T08:06:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7668"
  },
  {
    "number": 7666,
    "title": "[None][feat] Add NCCL symmetric to all reduce benchmark",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T06:47:01Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7666"
  },
  {
    "number": 7664,
    "title": "[https://nvbugs/5488212][waive] Waive failed tests for L20",
    "user": "nvamyt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T02:59:44Z",
    "closed_at": "2025-09-10T14:32:15Z",
    "merged_at": "2025-09-10T14:32:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7664"
  },
  {
    "number": 7663,
    "title": "[https://nvbugs/5474409][fix] Disable concurrent loading by default",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T02:20:47Z",
    "closed_at": "2025-09-10T16:11:18Z",
    "merged_at": "2025-09-10T16:11:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7663"
  },
  {
    "number": 7662,
    "title": "[TRTLLM-7399][test] Add DS-R1/Qwen3 test cases for RTX 6000",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-10T00:58:11Z",
    "closed_at": "2025-09-24T15:40:26Z",
    "merged_at": "2025-09-24T15:40:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7662"
  },
  {
    "number": 7660,
    "title": "[None][feat] gpt-oss sm120/1211",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T19:38:21Z",
    "closed_at": "2025-09-24T22:59:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7660"
  },
  {
    "number": 7659,
    "title": "[TRTLLM-8044][refactor] Rename data -> cache for cacheTransceiver",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T18:31:58Z",
    "closed_at": "2025-09-16T12:43:57Z",
    "merged_at": "2025-09-16T12:43:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7659"
  },
  {
    "number": 7658,
    "title": "[None] [chore] Add NIM release branch approval rule to CODEOWNERS",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T18:01:10Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7658"
  },
  {
    "number": 7657,
    "title": "test",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T17:57:03Z",
    "closed_at": "2025-09-11T08:37:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7657"
  },
  {
    "number": 7656,
    "title": "[https://nvbugs/5498165][fix] fix permission error for config file lock",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T17:21:31Z",
    "closed_at": "2025-09-11T02:36:51Z",
    "merged_at": "2025-09-11T02:36:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7656"
  },
  {
    "number": 7655,
    "title": "[None][fix] fix post-merge issue raised by #5488",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T16:20:10Z",
    "closed_at": "2025-09-10T01:26:27Z",
    "merged_at": "2025-09-10T01:26:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7655"
  },
  {
    "number": 7653,
    "title": "[None][feat] Spark single gpu dev branch for testing",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T15:31:05Z",
    "closed_at": "2025-09-30T02:20:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7653"
  },
  {
    "number": 7651,
    "title": "[TRTLLM-6668][feat] Enable overlap scheduler for two-model spec decoding",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T13:50:30Z",
    "closed_at": "2025-09-15T23:33:45Z",
    "merged_at": "2025-09-15T23:33:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7651"
  },
  {
    "number": 7650,
    "title": "[#2859][feat] Support for Qwen2.5-VL models for TRT flow",
    "user": "gergely-magyar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T12:42:58Z",
    "closed_at": "2025-09-10T06:35:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7650"
  },
  {
    "number": 7649,
    "title": "[#2859][feat] Added support for Qwen2.5-VL",
    "user": "gergely-magyar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T11:35:13Z",
    "closed_at": "2025-09-09T11:40:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7649"
  },
  {
    "number": 7648,
    "title": "[None][feat] Attention NVFP4 Output Support for torch compile",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T11:29:34Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7648"
  },
  {
    "number": 7647,
    "title": "[TRTLLM-7953][feat] Enable multi-threading uvicorn server for trtllm-serve",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T10:46:51Z",
    "closed_at": "2025-09-19T02:21:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7647"
  },
  {
    "number": 7646,
    "title": "[https://nvbugs/5503440][fix] Fix potential hang due to wrong type of ZMQ socket and protocol for worker_init_status_queue",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T08:58:43Z",
    "closed_at": "2025-09-19T10:13:34Z",
    "merged_at": "2025-09-19T10:13:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7646"
  },
  {
    "number": 7645,
    "title": "[https://nvbugs/5410687][test] Add deepseek r1-w4afp8 quickstart",
    "user": "fredricz-20070104",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T08:53:25Z",
    "closed_at": "2025-09-10T02:21:01Z",
    "merged_at": "2025-09-10T02:21:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7645"
  },
  {
    "number": 7644,
    "title": "[https://nvbugs/5501557][fix] Fix nemotron build error",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T08:41:47Z",
    "closed_at": "2025-09-10T02:15:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7644"
  },
  {
    "number": 7643,
    "title": "[https://nvbugs/5355128][fix] Add missing wgmma intrinsic for starcoder",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T08:25:11Z",
    "closed_at": "2025-09-23T02:38:58Z",
    "merged_at": "2025-09-23T02:38:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7643"
  },
  {
    "number": 7642,
    "title": "[None][infra] Remove WAR on feat branch",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T08:07:09Z",
    "closed_at": "2025-09-11T08:05:14Z",
    "merged_at": "2025-09-11T08:05:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7642"
  },
  {
    "number": 7641,
    "title": "[None][doc] Use hash id for external link",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T08:00:26Z",
    "closed_at": "2025-09-09T08:34:06Z",
    "merged_at": "2025-09-09T08:34:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7641"
  },
  {
    "number": 7640,
    "title": "[None][chore] Mass integration of release/1.0 - 5th",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T07:29:53Z",
    "closed_at": "2025-09-22T06:28:38Z",
    "merged_at": "2025-09-22T06:28:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7640"
  },
  {
    "number": 7639,
    "title": "[None][fix] add the missing import raised by #7607",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T07:10:07Z",
    "closed_at": "2025-09-09T07:42:42Z",
    "merged_at": "2025-09-09T07:42:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7639"
  },
  {
    "number": 7638,
    "title": "[TRTLLM-7070][feat] add gpt-oss serve benchmark tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T06:52:06Z",
    "closed_at": "2025-09-16T08:39:31Z",
    "merged_at": "2025-09-16T08:39:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7638"
  },
  {
    "number": 7637,
    "title": "[None][feat] Support cached tokens for Openai server",
    "user": "wjueyao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T06:10:07Z",
    "closed_at": "2025-10-16T12:51:38Z",
    "merged_at": "2025-10-16T12:51:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7637"
  },
  {
    "number": 7636,
    "title": "[https://nvbugs/5501557][fix] Fix out-of-bounds vector access for model with multiple layer types",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T05:53:50Z",
    "closed_at": "2025-09-11T03:34:25Z",
    "merged_at": "2025-09-11T03:34:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7636"
  },
  {
    "number": 7635,
    "title": "[#7308] [feat] AutoDeploy: graph-less transformers mode for HF",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T05:42:44Z",
    "closed_at": "2025-09-18T02:44:25Z",
    "merged_at": "2025-09-18T02:44:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7635"
  },
  {
    "number": 7634,
    "title": "[None][doc] Fix a invalid link and a typo.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T03:31:07Z",
    "closed_at": "2025-09-09T04:49:28Z",
    "merged_at": "2025-09-09T04:49:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7634"
  },
  {
    "number": 7633,
    "title": "[None][infra] Disable CU12 build to save build time (cost > 5 hours on SBSA)",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T03:30:41Z",
    "closed_at": "2025-09-09T03:38:34Z",
    "merged_at": "2025-09-09T03:38:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7633"
  },
  {
    "number": 7632,
    "title": "[TRTLLM-6898][feat] Add Cute DSL nvfp4 linear op",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T03:19:32Z",
    "closed_at": "2025-09-16T06:25:27Z",
    "merged_at": "2025-09-16T06:25:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7632"
  },
  {
    "number": 7630,
    "title": "[None][chore] Validate features combination",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T02:24:58Z",
    "closed_at": "2025-09-25T00:01:13Z",
    "merged_at": "2025-09-25T00:01:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7630"
  },
  {
    "number": 7629,
    "title": "[None][ci] add DGX_H100-2_GPUs-PyTorch-Others-1 pipeline",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-09T02:06:38Z",
    "closed_at": "2025-09-09T15:06:32Z",
    "merged_at": "2025-09-09T15:06:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7629"
  },
  {
    "number": 7628,
    "title": "[TRTLLM-7410][feat] Enable KV cache reuse and chunked prefill for mistral3.1",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T21:04:28Z",
    "closed_at": "2025-09-17T15:11:17Z",
    "merged_at": "2025-09-17T15:11:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7628"
  },
  {
    "number": 7627,
    "title": "[None][test] Skip eagle3 test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T20:43:53Z",
    "closed_at": "2025-09-08T21:23:54Z",
    "merged_at": "2025-09-08T21:23:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7627"
  },
  {
    "number": 7626,
    "title": "[None][chore] Add vLLM custom dataset format to benchmark_serving script",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T20:23:52Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7626"
  },
  {
    "number": 7624,
    "title": "[TRTLLM-7731][feat] KV cache transmission in disagg with CP on gen side",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T18:27:50Z",
    "closed_at": "2025-09-20T13:15:26Z",
    "merged_at": "2025-09-20T13:15:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7624"
  },
  {
    "number": 7623,
    "title": "[None][fix] Update deployment guide and cherry-pick CI test fix from main",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T18:14:13Z",
    "closed_at": "2025-09-09T01:53:48Z",
    "merged_at": "2025-09-09T01:53:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7623"
  },
  {
    "number": 7622,
    "title": "[TRTLLM-7100][infra] Enhance disagg gen-only benchmarking method",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T17:21:28Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7622"
  },
  {
    "number": 7621,
    "title": "[None][ci] debug thop/parallel tests",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T16:57:50Z",
    "closed_at": "2025-09-18T23:51:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7621"
  },
  {
    "number": 7620,
    "title": "triton rms norm with residual fusion",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T16:30:04Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7620"
  },
  {
    "number": 7619,
    "title": "[None] [refactor] Minor cleanup and improvements",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T12:21:27Z",
    "closed_at": "2025-10-03T09:40:06Z",
    "merged_at": "2025-10-03T09:40:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7619"
  },
  {
    "number": 7618,
    "title": "[None][feat] support attention dp for qwen3 dense model",
    "user": "Nekofish-L",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T12:01:59Z",
    "closed_at": "2025-09-16T01:33:22Z",
    "merged_at": "2025-09-16T01:33:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7618"
  },
  {
    "number": 7617,
    "title": "[None][doc] Fix a invalid link.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T12:00:07Z",
    "closed_at": "2025-09-08T12:33:36Z",
    "merged_at": "2025-09-08T12:33:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7617"
  },
  {
    "number": 7616,
    "title": "[https://nvbugs/5505402] [fix] Disable deep_gemm for Qwen3 QKNormRoPEAttention and Linear layers due to accuracy issues",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T11:44:39Z",
    "closed_at": "2025-09-10T17:30:49Z",
    "merged_at": "2025-09-10T17:30:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7616"
  },
  {
    "number": 7615,
    "title": "[https://nvbugs/5434424][fix] A quick fix for the wrong output issue of SM89 blocked scaling batched GEMM when the input tensor is non-contiguous.",
    "user": "StudyingShao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T11:28:43Z",
    "closed_at": "2025-09-09T12:58:15Z",
    "merged_at": "2025-09-09T12:58:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7615"
  },
  {
    "number": 7614,
    "title": "[None][fix] Ensure that the W4A8 custom input scale remains aligned across all ranks",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T10:53:49Z",
    "closed_at": "2025-09-16T03:04:27Z",
    "merged_at": "2025-09-16T03:04:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7614"
  },
  {
    "number": 7613,
    "title": "[None][feat] Optimize kv cache transfer TEP",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T10:31:09Z",
    "closed_at": "2025-09-26T03:20:04Z",
    "merged_at": "2025-09-26T03:20:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7613"
  },
  {
    "number": 7612,
    "title": "[None][feat] support gpt-oss with fp8 kv cache",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T10:18:23Z",
    "closed_at": "2025-09-14T18:17:37Z",
    "merged_at": "2025-09-14T18:17:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7612"
  },
  {
    "number": 7611,
    "title": "[None] [fix] fix torch sampler non-greedy sampling strategy not effected bug",
    "user": "kris1025",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T09:37:53Z",
    "closed_at": "2025-09-24T03:31:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7611"
  },
  {
    "number": 7610,
    "title": "[TRTLLM-6994][feat] FP8 Context MLA integration (Cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/6059 from release/1.1.0rc2)",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T08:56:36Z",
    "closed_at": "2025-09-19T01:40:49Z",
    "merged_at": "2025-09-19T01:40:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7610"
  },
  {
    "number": 7608,
    "title": "[https://nvbugs/5481087][fix] Skip w4a8 GPTOSS test on pre-Blackwell.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T07:58:44Z",
    "closed_at": "2025-09-09T02:17:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7608"
  },
  {
    "number": 7607,
    "title": "[None][chore] Mass integration of release/1.0 - 4th (release/1.0 doc change mainly)",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T07:48:27Z",
    "closed_at": "2025-09-09T04:16:03Z",
    "merged_at": "2025-09-09T04:16:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7607"
  },
  {
    "number": 7606,
    "title": "[https://nvbugs/5474169][fix] seq_len mismatch between kv cache manager and graph attn metadata",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T07:48:23Z",
    "closed_at": "2025-09-09T00:32:31Z",
    "merged_at": "2025-09-09T00:32:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7606"
  },
  {
    "number": 7605,
    "title": "[TRTLLM-7958][doc] add 1.0 release notes",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T07:27:09Z",
    "closed_at": "2025-09-17T06:38:19Z",
    "merged_at": "2025-09-17T06:38:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7605"
  },
  {
    "number": 7604,
    "title": "[https://nvbugs/5506683][fix] adjust the CI",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T06:56:05Z",
    "closed_at": "2025-09-08T07:41:41Z",
    "merged_at": "2025-09-08T07:41:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7604"
  },
  {
    "number": 7603,
    "title": "[https://nvbugs/5503423][waive] Waive Llama3.1-70B-FP8 test on RTX PRO 6000",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T06:13:16Z",
    "closed_at": "2025-09-09T01:29:08Z",
    "merged_at": "2025-09-09T01:29:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7603"
  },
  {
    "number": 7602,
    "title": "[None][chore] Enable multiple postprocess workers tests for chat completions api",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T05:58:37Z",
    "closed_at": "2025-09-15T04:16:45Z",
    "merged_at": "2025-09-15T04:16:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7602"
  },
  {
    "number": 7601,
    "title": "[None][infra] Add back rtx-pro-6000 stages since the node is available",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T05:26:44Z",
    "closed_at": "2025-09-08T09:45:11Z",
    "merged_at": "2025-09-08T09:45:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7601"
  },
  {
    "number": 7600,
    "title": "[None][feat] Cherry-pick Responses API and multiple postprocess workers support for chat harmony",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T03:38:40Z",
    "closed_at": "2025-09-09T11:28:30Z",
    "merged_at": "2025-09-09T11:28:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7600"
  },
  {
    "number": 7599,
    "title": "[None][chore] Remove executor config in create_py_executor",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T03:27:54Z",
    "closed_at": "2025-09-18T06:24:59Z",
    "merged_at": "2025-09-18T06:24:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7599"
  },
  {
    "number": 7598,
    "title": "[None][chore] Make use_low_precision_moe_combine as a llm arg",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T03:22:55Z",
    "closed_at": "2025-09-08T21:22:06Z",
    "merged_at": "2025-09-08T21:22:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7598"
  },
  {
    "number": 7597,
    "title": "[None][feat] Optimize MLA kernels with separate reduction kernels",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T03:18:54Z",
    "closed_at": "2025-09-09T08:58:44Z",
    "merged_at": "2025-09-09T08:58:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7597"
  },
  {
    "number": 7596,
    "title": "[https://nvbugs/5461761][fix]: Skip the problematic subtest",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T03:15:56Z",
    "closed_at": "2025-09-10T01:12:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7596"
  },
  {
    "number": 7595,
    "title": "Test new machine",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T02:37:47Z",
    "closed_at": "2025-09-09T08:52:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7595"
  },
  {
    "number": 7594,
    "title": "[None][fix] Fix mpi_broadcast for multi-node single GPU cases",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-08T00:16:01Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7594"
  },
  {
    "number": 7593,
    "title": "[None][ci] Block some nodes to avoid unstable network access",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-07T14:45:07Z",
    "closed_at": "2025-09-07T16:25:38Z",
    "merged_at": "2025-09-07T16:25:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7593"
  },
  {
    "number": 7592,
    "title": "[None][infra] Skip RTX Pro 6000 test stages due to HW are offline",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-07T12:32:40Z",
    "closed_at": "2025-09-07T13:49:07Z",
    "merged_at": "2025-09-07T13:49:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7592"
  },
  {
    "number": 7591,
    "title": "[None][chore] Remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-07T12:16:59Z",
    "closed_at": "2025-09-09T08:08:43Z",
    "merged_at": "2025-09-09T08:08:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7591"
  },
  {
    "number": 7590,
    "title": "[None][fix] Fix the incorrect code style in dataType.h",
    "user": "Fan-Yunfan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-07T11:42:51Z",
    "closed_at": "2025-09-10T07:43:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7590"
  },
  {
    "number": 7589,
    "title": "[None][fix] enable NvFP4/FP8 quantization for Nemotron-H architecture",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-07T10:16:09Z",
    "closed_at": "2025-09-09T08:42:22Z",
    "merged_at": "2025-09-09T08:42:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7589"
  },
  {
    "number": 7588,
    "title": "[#7288][feat] Added AutoDeploy backend support to test_perf.py",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-07T10:13:10Z",
    "closed_at": "2025-09-29T04:21:27Z",
    "merged_at": "2025-09-29T04:21:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7588"
  },
  {
    "number": 7587,
    "title": "[TRTLLM-7963][feat] Cold L2 cache when doing autotune benchmarking",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-06T11:47:11Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7587"
  },
  {
    "number": 7586,
    "title": "[None][feat] Achieve cold L2 for each kernel profing repeated in the …",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-06T11:05:27Z",
    "closed_at": "2025-09-06T11:47:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7586"
  },
  {
    "number": 7585,
    "title": "[None][ci] Waive qwen3 test for accuracy bug in https://nvbugs/5505402",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-06T06:17:42Z",
    "closed_at": "2025-09-06T13:29:16Z",
    "merged_at": "2025-09-06T13:29:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7585"
  },
  {
    "number": 7584,
    "title": "[None][ci] Revert \"[https://nvbugs/5461761][fix] Remove the waiver (#7476)\"",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-06T04:44:37Z",
    "closed_at": "2025-09-06T05:02:09Z",
    "merged_at": "2025-09-06T05:02:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7584"
  },
  {
    "number": 7583,
    "title": "[None][feat] Extend VLM factory and add Mistral3 factory",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-06T04:04:27Z",
    "closed_at": "2025-09-09T06:47:19Z",
    "merged_at": "2025-09-09T06:47:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7583"
  },
  {
    "number": 7582,
    "title": "[None][chore] Bump version to 1.1.0rc2.post2",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-06T03:09:38Z",
    "closed_at": "2025-09-07T15:09:48Z",
    "merged_at": "2025-09-07T15:09:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7582"
  },
  {
    "number": 7581,
    "title": "[TRTLLM-6994][feat] FP8 Context MLA integration.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-06T03:01:26Z",
    "closed_at": "2025-09-08T02:10:30Z",
    "merged_at": "2025-09-08T02:10:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7581"
  },
  {
    "number": 7580,
    "title": "[TRTLLM-7015] [feat] Enable `prompt_logprobs` in pytorch backend",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-06T02:44:10Z",
    "closed_at": "2025-09-24T01:48:10Z",
    "merged_at": "2025-09-24T01:48:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7580"
  },
  {
    "number": 7579,
    "title": "[TRTLLM-8533][chore] extract weights loading related logic to model loader",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-06T00:51:03Z",
    "closed_at": "2025-09-25T17:19:23Z",
    "merged_at": "2025-09-25T17:19:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7579"
  },
  {
    "number": 7577,
    "title": "[TRTLLM-7328][feat] E-PD Disagg Support via llmapi (3/N)",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T23:59:53Z",
    "closed_at": "2025-09-23T02:07:19Z",
    "merged_at": "2025-09-23T02:07:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7577"
  },
  {
    "number": 7576,
    "title": "Adding KvCacheConfig, SchedulerConfig, and DynamicBatchConfig",
    "user": "nv-kmcgill53",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T22:44:51Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7576"
  },
  {
    "number": 7575,
    "title": "[https://nvbugs/5429636][feat] Configurable kv_transfer_timeout_ms for failed transfer memory leaks",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T22:15:21Z",
    "closed_at": "2025-10-17T18:34:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7575"
  },
  {
    "number": 7574,
    "title": "Revert \"[https://nvbugs/5492124][fix] revert to use triton==3.3.1 (#7…",
    "user": "tijyojwad",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T19:53:09Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7574"
  },
  {
    "number": 7573,
    "title": "[https://nvbugs/5470782][chore] Remove the skip statement in 1.0 rele…",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T18:21:19Z",
    "closed_at": "2025-09-09T04:04:14Z",
    "merged_at": "2025-09-09T04:04:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7573"
  },
  {
    "number": 7572,
    "title": "[TRTLLM-8521][chore] remove circular dependency between model engine and cuda graph runner",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T17:58:31Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7572"
  },
  {
    "number": 7571,
    "title": "[TRTLLM-6741] [feat] enable LM tp for MTP, under attention dp case (cherry-pick #7128)",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T17:45:21Z",
    "closed_at": "2025-09-17T01:41:33Z",
    "merged_at": "2025-09-17T01:41:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7571"
  },
  {
    "number": 7570,
    "title": "[TRTLLM-4629] [feat] Step1: trtllm-gen kernels support sm103",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T16:37:28Z",
    "closed_at": "2025-09-07T02:04:11Z",
    "merged_at": "2025-09-07T02:04:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7570"
  },
  {
    "number": 7569,
    "title": "[None][ci] move some test cases of DGX H100 to post merge",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T16:37:01Z",
    "closed_at": "2025-09-06T05:03:39Z",
    "merged_at": "2025-09-06T05:03:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7569"
  },
  {
    "number": 7568,
    "title": "[TRTLLM-4629] [feat] Add support of CUDA13 and sm103 devices",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T16:16:56Z",
    "closed_at": "2025-09-16T01:56:19Z",
    "merged_at": "2025-09-16T01:56:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7568"
  },
  {
    "number": 7567,
    "title": "[None][ci] Improve SSH connection stability",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T15:26:38Z",
    "closed_at": "2025-09-06T09:08:19Z",
    "merged_at": "2025-09-06T09:08:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7567"
  },
  {
    "number": 7566,
    "title": "phi4 fp4",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T14:54:48Z",
    "closed_at": "2025-10-08T05:05:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7566"
  },
  {
    "number": 7565,
    "title": "[TRTLLM-6707][fix] nanobind fix for executor exit call",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T13:58:12Z",
    "closed_at": "2025-09-09T13:56:04Z",
    "merged_at": "2025-09-09T13:56:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7565"
  },
  {
    "number": 7564,
    "title": "[None][infra] Waive failed tests on main branch 0905",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T12:59:15Z",
    "closed_at": "2025-09-05T14:46:46Z",
    "merged_at": "2025-09-05T14:46:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7564"
  },
  {
    "number": 7563,
    "title": "[TRTLLM-7918][feat] Support kvcache reuse for phi4mm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T09:28:22Z",
    "closed_at": "2025-09-15T07:47:01Z",
    "merged_at": "2025-09-15T07:47:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7563"
  },
  {
    "number": 7562,
    "title": "[None][chore] Upgrade nvidia-modelopt to 0.37.0",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T09:11:06Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7562"
  },
  {
    "number": 7561,
    "title": "[None][test] Add accuracy benchmark in stress test",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T08:55:22Z",
    "closed_at": "2025-09-19T08:09:47Z",
    "merged_at": "2025-09-19T08:09:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7561"
  },
  {
    "number": 7560,
    "title": "[https://nvbugs/5416501][doc] add known issues to llmapi doc",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T08:16:42Z",
    "closed_at": "2025-09-08T08:42:55Z",
    "merged_at": "2025-09-08T08:42:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7560"
  },
  {
    "number": 7559,
    "title": "[TRTLLM-4629] [feat] Merge main to feat/b300_cu13",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T07:55:41Z",
    "closed_at": "2025-09-08T02:06:30Z",
    "merged_at": "2025-09-08T02:06:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7559"
  },
  {
    "number": 7558,
    "title": "Draft: Add support of CUDA13 and sm103 devices",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T07:31:08Z",
    "closed_at": "2025-09-07T02:32:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7558"
  },
  {
    "number": 7557,
    "title": "[None][ci] Increase the number of retries in docker image generation",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T06:38:09Z",
    "closed_at": "2025-09-05T06:47:14Z",
    "merged_at": "2025-09-05T06:47:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7557"
  },
  {
    "number": 7556,
    "title": "[https://nvbugs/5498967][fix] Downgrade NCCL",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T06:30:15Z",
    "closed_at": "2025-09-08T01:57:37Z",
    "merged_at": "2025-09-08T01:57:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7556"
  },
  {
    "number": 7555,
    "title": "[None][fix] trtllm-serve yaml loading",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T04:08:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7555"
  },
  {
    "number": 7554,
    "title": "[None][doc] Rename TensorRT-LLM to TensorRT LLM.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T03:19:35Z",
    "closed_at": "2025-09-05T08:54:57Z",
    "merged_at": "2025-09-05T08:54:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7554"
  },
  {
    "number": 7553,
    "title": "[None][fix] using arrival time in llmapi when creating LlmRequest in pytorch workflow",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T03:13:48Z",
    "closed_at": "2025-09-15T11:26:01Z",
    "merged_at": "2025-09-15T11:26:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7553"
  },
  {
    "number": 7552,
    "title": "[None][infra] update nspect version",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T02:59:29Z",
    "closed_at": "2025-09-05T06:59:23Z",
    "merged_at": "2025-09-05T06:59:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7552"
  },
  {
    "number": 7551,
    "title": "[None][fix] trtllm-serve yaml loading",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T02:52:34Z",
    "closed_at": "2025-09-06T08:19:29Z",
    "merged_at": "2025-09-06T08:19:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7551"
  },
  {
    "number": 7550,
    "title": "[None][fix] Debug with Blossom hook",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T02:15:37Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7550"
  },
  {
    "number": 7549,
    "title": "[None][doc] Update kvcache part",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T01:27:23Z",
    "closed_at": "2025-09-05T07:46:13Z",
    "merged_at": "2025-09-05T07:46:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7549"
  },
  {
    "number": 7548,
    "title": "[#7308][feat] AutoDeploy: basic transformers mode with model build + cached attention + multi-gpu support",
    "user": "h-guo18",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-05T01:21:07Z",
    "closed_at": "2025-09-09T05:49:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7548"
  },
  {
    "number": 7546,
    "title": "[None][feat] Eagle, use last hidden post norm",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T23:07:09Z",
    "closed_at": "2025-09-15T16:23:58Z",
    "merged_at": "2025-09-15T16:23:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7546"
  },
  {
    "number": 7545,
    "title": "[#5255][autodeploy] Update FuseAllreduceResidualRMSNorm to use pattern matcher utility; remove fuse_collective",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T23:02:13Z",
    "closed_at": "2025-10-05T08:15:47Z",
    "merged_at": "2025-10-05T08:15:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7545"
  },
  {
    "number": 7544,
    "title": "[None][feat] Update multimodal utility `get_num_tokens_per_image` for better generalization",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T22:39:29Z",
    "closed_at": "2025-09-08T11:42:47Z",
    "merged_at": "2025-09-08T11:42:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7544"
  },
  {
    "number": 7543,
    "title": "[https://nvbugs/5502352][fix] Fix 2-model CDL path",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T21:44:20Z",
    "closed_at": "2025-09-07T03:53:28Z",
    "merged_at": "2025-09-07T03:53:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7543"
  },
  {
    "number": 7542,
    "title": "[None][ci] remove unnecessary test_modeling_deepseek.py",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T20:57:02Z",
    "closed_at": "2025-09-05T03:05:27Z",
    "merged_at": "2025-09-05T03:05:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7542"
  },
  {
    "number": 7541,
    "title": "[https://nvbugs/5474409][fix] Potential fix for oom",
    "user": "arekay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T17:39:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7541"
  },
  {
    "number": 7539,
    "title": "[TRTLLM-6780][fix] Add multimodal data to dummy requests during memory profiling",
    "user": "johncalesp",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T16:15:14Z",
    "closed_at": "2025-10-16T15:49:23Z",
    "merged_at": "2025-10-16T15:49:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7539"
  },
  {
    "number": 7538,
    "title": "[None][fix] Fix a typo in the Slurm CI codes (#7485)",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T12:14:18Z",
    "closed_at": "2025-09-04T13:49:19Z",
    "merged_at": "2025-09-04T13:49:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7538"
  },
  {
    "number": 7537,
    "title": "[None][fix] Add synchronize for kvcache cleanup",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T09:52:59Z",
    "closed_at": "2025-09-18T03:27:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7537"
  },
  {
    "number": 7536,
    "title": "[None][feat] Optimize MLA kernels with separate reduction kernels",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T09:51:45Z",
    "closed_at": "2025-09-08T03:18:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7536"
  },
  {
    "number": 7535,
    "title": "Draft: [TRTLLM][feat] Set NIXL backend to default",
    "user": "BatshevaBlack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T09:15:58Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7535"
  },
  {
    "number": 7534,
    "title": "[None][fix] Update DG commit",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T09:13:41Z",
    "closed_at": "2025-09-04T13:08:37Z",
    "merged_at": "2025-09-04T13:08:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7534"
  },
  {
    "number": 7531,
    "title": "[debug] Fix trtllm serve kill",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T08:57:42Z",
    "closed_at": "2025-09-09T00:21:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7531"
  },
  {
    "number": 7530,
    "title": "[None][fix]UCX zmq ip support ipv6",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T08:45:25Z",
    "closed_at": "2025-09-10T02:24:42Z",
    "merged_at": "2025-09-10T02:24:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7530"
  },
  {
    "number": 7529,
    "title": "[None][fix] Make tile_tokens_dim calculation just in time before kernel launching.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T08:29:31Z",
    "closed_at": "2025-09-18T02:58:53Z",
    "merged_at": "2025-09-18T02:58:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7529"
  },
  {
    "number": 7527,
    "title": "[None][ci] - Avoid the Git-metadata error and skip VCS in build-info collection",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T07:53:32Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7527"
  },
  {
    "number": 7526,
    "title": "[None][chore] remove executor config in kv cache creator",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T07:38:34Z",
    "closed_at": "2025-09-10T13:14:45Z",
    "merged_at": "2025-09-10T13:14:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7526"
  },
  {
    "number": 7525,
    "title": "[None][chore] Bump version to 1.1.0rc4",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T07:36:50Z",
    "closed_at": "2025-09-04T08:30:48Z",
    "merged_at": "2025-09-04T08:30:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7525"
  },
  {
    "number": 7524,
    "title": "[None][chore] Fix kernel launch param and add TRTLLM MoE backend test",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T07:27:08Z",
    "closed_at": "2025-09-09T15:45:36Z",
    "merged_at": "2025-09-09T15:45:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7524"
  },
  {
    "number": 7523,
    "title": "[None][chore] Upgrade transformers to 4.56.0",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T07:23:57Z",
    "closed_at": "2025-09-22T14:20:17Z",
    "merged_at": "2025-09-22T14:20:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7523"
  },
  {
    "number": 7522,
    "title": "[None][fix] convert_load_format for int and str enum",
    "user": "weedge",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T07:10:46Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7522"
  },
  {
    "number": 7521,
    "title": "[TRTLLM-6308][feat] Support Aggregate mode for phi4-mm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T06:26:45Z",
    "closed_at": "2025-09-04T12:16:10Z",
    "merged_at": "2025-09-04T12:16:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7521"
  },
  {
    "number": 7520,
    "title": "[TRTLLM-7349][feat] Adding new orchestrator type --  ray",
    "user": "joyang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T06:15:29Z",
    "closed_at": "2025-10-04T00:12:25Z",
    "merged_at": "2025-10-04T00:12:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7520"
  },
  {
    "number": 7519,
    "title": "[None][chore] Mass integration of release/1.0 - 3rd",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T06:08:24Z",
    "closed_at": "2025-09-08T06:03:05Z",
    "merged_at": "2025-09-08T06:03:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7519"
  },
  {
    "number": 7518,
    "title": "[TRTLLM-7292][feat] Support manual gc in trtllm-serve",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T06:05:45Z",
    "closed_at": "2025-09-29T16:56:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7518"
  },
  {
    "number": 7517,
    "title": "[TRTLLM-6496][feat] Add LoRa Torch tests for the latest NIM model list - TEMP DRAFT to run tests on 8xH200",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T04:39:54Z",
    "closed_at": "2025-09-07T05:50:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7517"
  },
  {
    "number": 7516,
    "title": "[None][chore] remove executor config in instantiate sampler",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T04:14:34Z",
    "closed_at": "2025-09-08T16:02:40Z",
    "merged_at": "2025-09-08T16:02:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7516"
  },
  {
    "number": 7515,
    "title": "[TRTLLM-7292][feat] Support multi-threaded tokenizers for trtllm-serve",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T03:57:58Z",
    "closed_at": "2025-09-05T22:10:23Z",
    "merged_at": "2025-09-05T22:10:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7515"
  },
  {
    "number": 7514,
    "title": "[TRTLLM-6589][feat] Support CUDA graph for DeepEP",
    "user": "yifeizhang-c",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T03:48:55Z",
    "closed_at": "2025-10-02T17:13:24Z",
    "merged_at": "2025-10-02T17:13:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7514"
  },
  {
    "number": 7513,
    "title": "[None][chore] Add env var to disable harmony",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-04T02:13:40Z",
    "closed_at": "2025-09-04T02:22:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7513"
  },
  {
    "number": 7512,
    "title": "[https://nvbugs/5474409][fix] Potential fix for oom",
    "user": "arekay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T23:30:16Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7512"
  },
  {
    "number": 7511,
    "title": "[None][fix] Assign [] to req.py_draft_tokens instead of None when spec decode is off",
    "user": "zheyuf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T22:43:57Z",
    "closed_at": "2025-09-23T13:54:18Z",
    "merged_at": "2025-09-23T13:54:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7511"
  },
  {
    "number": 7510,
    "title": "[TRTLLM-6371][feat] Restructure C++ KVCacheManager to better handle limited attention layers",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T17:12:20Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7510"
  },
  {
    "number": 7509,
    "title": "[https://nvbugs/5496960][fix] Fix Gemma model forward.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T13:22:50Z",
    "closed_at": "2025-09-04T11:09:43Z",
    "merged_at": "2025-09-04T11:09:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7509"
  },
  {
    "number": 7508,
    "title": "[TRTLLM-7779][feat] Support multiple postprocess workers for chat completions API",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T12:28:30Z",
    "closed_at": "2025-09-08T03:11:36Z",
    "merged_at": "2025-09-08T03:11:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7508"
  },
  {
    "number": 7507,
    "title": "[None][fix] Fix possible mpi broadcast and gather issue on large object",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T11:49:48Z",
    "closed_at": "2025-09-03T16:12:29Z",
    "merged_at": "2025-09-03T16:12:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7507"
  },
  {
    "number": 7505,
    "title": "[https://nvbugs/5494698][fix] skip gemma3 27b on blackwell",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T09:42:46Z",
    "closed_at": "2025-09-10T13:09:28Z",
    "merged_at": "2025-09-10T13:09:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7505"
  },
  {
    "number": 7504,
    "title": "[None][ci] restructure llmapi related test lists",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T09:36:56Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7504"
  },
  {
    "number": 7503,
    "title": "[TRTLLM-7871][infra] Extend test_perf.py to add disagg-serving perf tests.",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T08:52:37Z",
    "closed_at": "2025-09-10T09:35:52Z",
    "merged_at": "2025-09-10T09:35:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7503"
  },
  {
    "number": 7502,
    "title": "[None][fix] fix hunyuan_moe init bug",
    "user": "sorenwu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T08:40:09Z",
    "closed_at": "2025-09-04T07:06:00Z",
    "merged_at": "2025-09-04T07:06:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7502"
  },
  {
    "number": 7501,
    "title": "[https://nvbugs/5351244][fix] test_mpi_session",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T07:49:03Z",
    "closed_at": "2025-09-04T02:10:43Z",
    "merged_at": "2025-09-04T02:10:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7501"
  },
  {
    "number": 7500,
    "title": "[TRTLLM-7854][infra] Add gemm test before pytest stage to check machine status",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T07:21:05Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7500"
  },
  {
    "number": 7499,
    "title": "[TRTLLM-7318][feat] MnnvlThroughput AlltoAll implementation.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T07:19:41Z",
    "closed_at": "2025-10-27T17:23:07Z",
    "merged_at": "2025-10-27T17:23:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7499"
  },
  {
    "number": 7498,
    "title": "[https://nvbugs/5351244][fix] test_mpi_session",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T07:01:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7498"
  },
  {
    "number": 7497,
    "title": "[TRTLLM-7351][infra] Add isolate marker for L0",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T07:00:07Z",
    "closed_at": "2025-10-14T23:58:15Z",
    "merged_at": "2025-10-14T23:58:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7497"
  },
  {
    "number": 7496,
    "title": "[None][feat] add model seed-oss",
    "user": "Nekofish-L",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T06:31:36Z",
    "closed_at": "2025-09-24T10:57:12Z",
    "merged_at": "2025-09-24T10:57:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7496"
  },
  {
    "number": 7495,
    "title": "[https://nvbugs/5488582][fix] Avoid unexpected Triton recompilation in DG fused_moe.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T06:24:55Z",
    "closed_at": "2025-09-04T15:37:09Z",
    "merged_at": "2025-09-04T15:37:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7495"
  },
  {
    "number": 7493,
    "title": "[None][chore] add TorchLlmArgs to the connector api",
    "user": "richardhuo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T05:25:02Z",
    "closed_at": "2025-09-09T13:05:59Z",
    "merged_at": "2025-09-09T13:05:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7493"
  },
  {
    "number": 7492,
    "title": "[TRTLLM-7876][test] Test trtllm-serve with --extra_llm_api_options",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T04:26:31Z",
    "closed_at": "2025-09-04T02:34:38Z",
    "merged_at": "2025-09-04T02:34:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7492"
  },
  {
    "number": 7491,
    "title": "[None][fix] Update DG side branch name",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T02:24:13Z",
    "closed_at": "2025-09-04T04:10:50Z",
    "merged_at": "2025-09-04T04:10:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7491"
  },
  {
    "number": 7490,
    "title": "[#3325][feat] Add MCTS and TOT tree-based inference controllers to Scaffolding",
    "user": "therealnaveenkamal",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-03T01:57:34Z",
    "closed_at": "2025-09-05T02:46:50Z",
    "merged_at": "2025-09-05T02:46:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7490"
  },
  {
    "number": 7489,
    "title": "[None][ci] set TORCHINDUCTOR_COMPILE_THREADS for thop/parallel tests",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T22:43:15Z",
    "closed_at": "2025-09-04T13:04:52Z",
    "merged_at": "2025-09-04T13:04:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7489"
  },
  {
    "number": 7487,
    "title": "[None][fix] Cherry-Pick MNNVLAllreduce Fixes into release/1.1.0rc2 branch",
    "user": "timlee0212",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T16:49:08Z",
    "closed_at": "2025-09-05T04:08:37Z",
    "merged_at": "2025-09-05T04:08:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7487"
  },
  {
    "number": 7486,
    "title": "[https://nvbugs/5488141][fix] Unwaive llama3 test_eagle3",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T16:09:09Z",
    "closed_at": "2025-09-03T06:10:40Z",
    "merged_at": "2025-09-03T06:10:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7486"
  },
  {
    "number": 7485,
    "title": "[None][fix] Fix a typo in the Slurm CI codes",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T16:02:59Z",
    "closed_at": "2025-09-04T05:56:28Z",
    "merged_at": "2025-09-04T05:56:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7485"
  },
  {
    "number": 7483,
    "title": "[None][chore] Add note about trtllm-serve to the devel container",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T13:21:52Z",
    "closed_at": "2025-09-02T18:27:57Z",
    "merged_at": "2025-09-02T18:27:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7483"
  },
  {
    "number": 7482,
    "title": "[None][infra] Waive failed tests on main branch 0902",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T13:21:02Z",
    "closed_at": "2025-09-02T14:16:50Z",
    "merged_at": "2025-09-02T14:16:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7482"
  },
  {
    "number": 7481,
    "title": "[TRTLLM-7027][feat] Fuse d2t to logitsBitmaskKernel and fix a race condition in one-model spec",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T12:26:48Z",
    "closed_at": "2025-09-04T15:30:14Z",
    "merged_at": "2025-09-04T15:30:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7481"
  },
  {
    "number": 7480,
    "title": "[draft][don't review now] Add CuTe DSL nvfp4 linear",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T10:43:09Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7480"
  },
  {
    "number": 7479,
    "title": "[None][ci] Cherry-pick some improvements for Slurm CI setup from main branch",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T10:04:49Z",
    "closed_at": "2025-09-03T22:42:28Z",
    "merged_at": "2025-09-03T22:42:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7479"
  },
  {
    "number": 7478,
    "title": "[https://nvbugs/5441729][test] Fix test_modeling_llama_min_latency.py failures",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T08:46:34Z",
    "closed_at": "2025-10-13T07:35:03Z",
    "merged_at": "2025-10-13T07:35:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7478"
  },
  {
    "number": 7477,
    "title": "[TRTLLM-7192][feat] optimize MLA chunked prefill && support fp8 mla chunked prefill",
    "user": "jmydurant",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T08:35:27Z",
    "closed_at": "2025-09-15T13:43:49Z",
    "merged_at": "2025-09-15T13:43:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7477"
  },
  {
    "number": 7476,
    "title": "[https://nvbugs/5461761][fix] Remove the waiver",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T08:34:07Z",
    "closed_at": "2025-09-05T07:29:55Z",
    "merged_at": "2025-09-05T07:29:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7476"
  },
  {
    "number": 7475,
    "title": "feat: draft for deep conf in Scaffolding",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T08:15:07Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7475"
  },
  {
    "number": 7474,
    "title": "[None][chore] Remove temp attention window concept in KV cache manager",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T08:00:47Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7474"
  },
  {
    "number": 7473,
    "title": "[None][infra] wip unwaive the failed test",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T07:53:54Z",
    "closed_at": "2025-09-03T00:26:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7473"
  },
  {
    "number": 7472,
    "title": "[TRTLLM-7187][fix] Build wheel with NIXL",
    "user": "BatshevaBlack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T07:41:16Z",
    "closed_at": "2025-09-07T23:05:37Z",
    "merged_at": "2025-09-07T23:05:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7472"
  },
  {
    "number": 7471,
    "title": "[None][infra] waive test case failed on post-merge",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T07:32:09Z",
    "closed_at": "2025-09-02T10:20:08Z",
    "merged_at": "2025-09-02T10:20:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7471"
  },
  {
    "number": 7470,
    "title": "[TRTLLM-7182][test] add multi-nodes test for disagg-serving",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T07:15:13Z",
    "closed_at": "2025-09-24T00:31:57Z",
    "merged_at": "2025-09-24T00:31:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7470"
  },
  {
    "number": 7469,
    "title": "[None][chore] Remove onboard block switch for KV cache manager",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T07:14:21Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7469"
  },
  {
    "number": 7468,
    "title": "[None][test] update nim and full test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T06:49:43Z",
    "closed_at": "2025-09-04T13:06:01Z",
    "merged_at": "2025-09-04T13:06:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7468"
  },
  {
    "number": 7467,
    "title": "[None][fix] Add try-catch in stream generator",
    "user": "zhanghaotong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T06:24:46Z",
    "closed_at": "2025-09-08T20:09:26Z",
    "merged_at": "2025-09-08T20:09:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7467"
  },
  {
    "number": 7466,
    "title": "[None] [test] Add MNNVL AlltoAll tests to pre-merge",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T06:09:15Z",
    "closed_at": "2025-09-30T03:12:25Z",
    "merged_at": "2025-09-30T03:12:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7466"
  },
  {
    "number": 7465,
    "title": "[None] [test] Add MNNVL AlltoAll tests to pre-merge",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T06:05:35Z",
    "closed_at": "2025-09-05T17:19:09Z",
    "merged_at": "2025-09-05T17:19:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7465"
  },
  {
    "number": 7464,
    "title": "[None][ci] move test_moe.py to from parallel to serial",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T05:47:12Z",
    "closed_at": "2025-09-02T22:39:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7464"
  },
  {
    "number": 7463,
    "title": "[None][chore] Remove executor_config in create_py_executor_instance",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T05:38:36Z",
    "closed_at": "2025-09-05T12:56:03Z",
    "merged_at": "2025-09-05T12:56:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7463"
  },
  {
    "number": 7462,
    "title": "[https://nvbugs/5472947][fix] wait on isend handles before reusing buffers",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T05:31:31Z",
    "closed_at": "2025-09-03T07:50:03Z",
    "merged_at": "2025-09-03T07:50:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7462"
  },
  {
    "number": 7461,
    "title": "[TRTLLM-6742][feat] L2 Prefetch",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T04:51:43Z",
    "closed_at": "2025-10-24T10:07:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7461"
  },
  {
    "number": 7460,
    "title": "[None][infra] Test crun output",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T03:20:44Z",
    "closed_at": "2025-09-02T09:01:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7460"
  },
  {
    "number": 7458,
    "title": "[None][chore] Remove two unused parameters in create_py_executor",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T01:38:12Z",
    "closed_at": "2025-09-03T23:31:31Z",
    "merged_at": "2025-09-03T23:31:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7458"
  },
  {
    "number": 7457,
    "title": "[https://nvbugs/5481434][feat] Reuse pytorch memory segments occupied by cudagraph pool",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T01:20:21Z",
    "closed_at": "2025-09-04T04:08:40Z",
    "merged_at": "2025-09-04T04:08:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7457"
  },
  {
    "number": 7456,
    "title": "[None][fix] Don't review, test CI",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-02T00:50:36Z",
    "closed_at": "2025-09-09T00:21:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7456"
  },
  {
    "number": 7455,
    "title": "[https://nvbugs/5448767][fix] sync termination of requests across PP ranks",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T23:51:53Z",
    "closed_at": "2025-09-07T12:45:50Z",
    "merged_at": "2025-09-07T12:45:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7455"
  },
  {
    "number": 7454,
    "title": "[https://nvbugs/5476580][fix] unwaive test_nvfp4_4gpus",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T23:46:08Z",
    "closed_at": "2025-09-02T08:17:14Z",
    "merged_at": "2025-09-02T08:17:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7454"
  },
  {
    "number": 7453,
    "title": "[None] [fix] Minor fixes to slurm and benchmark scripts",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T22:47:13Z",
    "closed_at": "2025-09-02T05:57:03Z",
    "merged_at": "2025-09-02T05:57:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7453"
  },
  {
    "number": 7452,
    "title": "[#5860][autodeploy] GPT-OSS MXFP4 support V2: patch before export",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T20:17:56Z",
    "closed_at": "2025-09-23T00:09:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7452"
  },
  {
    "number": 7451,
    "title": "[#5860][autodeploy] GPT-OSS MXFP4 support",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T19:46:49Z",
    "closed_at": "2025-09-26T22:36:06Z",
    "merged_at": "2025-09-26T22:36:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7451"
  },
  {
    "number": 7450,
    "title": "[None][fix] Fix incorrect logger reference in FusedMoE quantization logic",
    "user": "chiwanpark",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T19:00:59Z",
    "closed_at": "2025-10-22T08:46:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7450"
  },
  {
    "number": 7449,
    "title": "[https://nvbugs/5454559][fix] handle bias term in fuse_gate_mlp",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T17:27:05Z",
    "closed_at": "2025-09-09T08:26:17Z",
    "merged_at": "2025-09-09T08:26:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7449"
  },
  {
    "number": 7448,
    "title": "[None][infra] Waive failed tests on release branch 0901",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T14:42:05Z",
    "closed_at": "2025-09-01T15:24:52Z",
    "merged_at": "2025-09-01T15:24:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7448"
  },
  {
    "number": 7447,
    "title": "[None][infra] Waive failed case on main 0901",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T14:32:35Z",
    "closed_at": "2025-09-01T15:27:24Z",
    "merged_at": "2025-09-01T15:27:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7447"
  },
  {
    "number": 7446,
    "title": "[TRTLLM-7187][fix] Add nixl to PATH",
    "user": "BatshevaBlack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T13:41:30Z",
    "closed_at": "2025-09-02T07:40:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7446"
  },
  {
    "number": 7445,
    "title": "[TRTLLM-7187][fix] Add nixl to PATH",
    "user": "BatshevaBlack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T13:25:05Z",
    "closed_at": "2025-09-01T13:39:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7445"
  },
  {
    "number": 7444,
    "title": "[https://nvbugs/5478458][fix] unittest/_torch/sampler/test_trtllm_sampler.py::test_trtllm_sampler",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T13:08:22Z",
    "closed_at": "2025-09-02T06:46:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7444"
  },
  {
    "number": 7443,
    "title": "[None][feat] Support EPLB in Qwen3 MoE",
    "user": "lucifer1004",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T11:48:13Z",
    "closed_at": "2025-09-19T08:45:35Z",
    "merged_at": "2025-09-19T08:45:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7443"
  },
  {
    "number": 7442,
    "title": "[https://nvbugs/5485102][fix] Correctly set stride for piecewise outp…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T10:43:41Z",
    "closed_at": "2025-09-04T02:48:15Z",
    "merged_at": "2025-09-04T02:48:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7442"
  },
  {
    "number": 7441,
    "title": "[https://nvbugs/5483615][fix] Remove unnecessary assertion to let mai…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T10:29:36Z",
    "closed_at": "2025-09-05T02:56:21Z",
    "merged_at": "2025-09-05T02:56:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7441"
  },
  {
    "number": 7439,
    "title": "[TRTLLM-6481][fix] Migrate KV cache cleanup to main and update max seq len",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T09:32:26Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7439"
  },
  {
    "number": 7437,
    "title": "[https://nvbugs/5485886][fix] Fix resource free of Eagle3ResourceManager",
    "user": "kris1025",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T08:52:28Z",
    "closed_at": "2025-09-04T09:38:14Z",
    "merged_at": "2025-09-04T09:38:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7437"
  },
  {
    "number": 7436,
    "title": "[https://nvbugs/5480415][fix] Fix phi4mm multi-gpu test",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T08:39:52Z",
    "closed_at": "2025-09-02T01:43:49Z",
    "merged_at": "2025-09-02T01:43:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7436"
  },
  {
    "number": 7435,
    "title": "[https://nvbugs/5492485][fix] Use offline dataset from llm-models instead.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T08:38:10Z",
    "closed_at": "2025-09-04T16:58:16Z",
    "merged_at": "2025-09-04T16:58:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7435"
  },
  {
    "number": 7433,
    "title": "[TRTLLM-7008][fix] cherrypick fix to 1.0 Add automatic shared memory delete if already exist",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T08:16:06Z",
    "closed_at": "2025-09-02T03:23:53Z",
    "merged_at": "2025-09-02T03:23:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7433"
  },
  {
    "number": 7432,
    "title": "[https://nvbugs/5455140][fix] unwaive release/1.0 DS R1 test cases with bug already fixed",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T08:07:30Z",
    "closed_at": "2025-09-09T01:48:59Z",
    "merged_at": "2025-09-09T01:48:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7432"
  },
  {
    "number": 7431,
    "title": "[https://nvbugs/5492124][fix] revert to use triton==3.3.1",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T08:06:08Z",
    "closed_at": "2025-09-01T16:41:02Z",
    "merged_at": "2025-09-01T16:41:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7431"
  },
  {
    "number": 7430,
    "title": "[TRTLLM-6199][infra] Update for using open driver from BSL",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T08:01:22Z",
    "closed_at": "2025-09-04T03:47:40Z",
    "merged_at": "2025-09-04T03:47:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7430"
  },
  {
    "number": 7429,
    "title": "[https://nvbugs/5445466][fix] unwaive DS R1 test cases with bug already fixed",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T07:47:28Z",
    "closed_at": "2025-09-09T09:25:50Z",
    "merged_at": "2025-09-09T09:25:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7429"
  },
  {
    "number": 7427,
    "title": "[https://nvbugs/5461761][fix] Remove the waiver",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T06:40:47Z",
    "closed_at": "2025-09-04T03:34:26Z",
    "merged_at": "2025-09-04T03:34:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7427"
  },
  {
    "number": 7426,
    "title": "[https://nvbugs/5461761][Fix] Remove the waiver",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T06:36:19Z",
    "closed_at": "2025-09-01T06:37:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7426"
  },
  {
    "number": 7425,
    "title": "[None][fix] Fix nanobind failure",
    "user": "Tom-Zheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T06:35:38Z",
    "closed_at": "2025-09-01T21:26:40Z",
    "merged_at": "2025-09-01T21:26:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7425"
  },
  {
    "number": 7424,
    "title": "[TRTLLM-7427][feat] Refactor lora config to support customized ckpt converter",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T06:14:19Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7424"
  },
  {
    "number": 7423,
    "title": "[TRTLLM-7831][feat] Support block wise FP8 in wide ep",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T06:11:50Z",
    "closed_at": "2025-09-08T07:31:25Z",
    "merged_at": "2025-09-08T07:31:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7423"
  },
  {
    "number": 7422,
    "title": "[TRTLLM-6747][feat] Merge add sparse exp and shared exp into local re…",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T06:07:30Z",
    "closed_at": "2025-09-01T06:15:05Z",
    "merged_at": "2025-09-01T06:15:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7422"
  },
  {
    "number": 7421,
    "title": "[None][test] auto reuse torch empty cache on qa test",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T05:55:48Z",
    "closed_at": "2025-09-02T08:44:48Z",
    "merged_at": "2025-09-02T08:44:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7421"
  },
  {
    "number": 7420,
    "title": "[None][fix] Cherry-pick 6850: Complete the last missing allreduce op in Llama3/4.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T05:33:25Z",
    "closed_at": "2025-09-04T19:12:28Z",
    "merged_at": "2025-09-04T19:12:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7420"
  },
  {
    "number": 7419,
    "title": "[None][perf] Fix the tactic sorting in TrtllmGenBatchedGemmRunner::getValidConfigIndices",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T05:25:09Z",
    "closed_at": "2025-09-25T08:27:57Z",
    "merged_at": "2025-09-25T08:27:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7419"
  },
  {
    "number": 7417,
    "title": "[None][ci] Correct docker args for GPU devices and remove some stale CI codes",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T05:07:18Z",
    "closed_at": "2025-09-02T08:06:52Z",
    "merged_at": "2025-09-02T08:06:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7417"
  },
  {
    "number": 7416,
    "title": "[None][opt] Add batch waiting when scheduling",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T04:35:53Z",
    "closed_at": "2025-09-23T02:27:37Z",
    "merged_at": "2025-09-23T02:27:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7416"
  },
  {
    "number": 7415,
    "title": "[None][chore] Use llm args for py_executor",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T03:54:21Z",
    "closed_at": "2025-09-02T04:37:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7415"
  },
  {
    "number": 7414,
    "title": "[None][infra] Test release ci",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T03:13:03Z",
    "closed_at": "2025-09-01T05:45:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7414"
  },
  {
    "number": 7413,
    "title": "[TRTLLM-6643][feat] Add DeepSeek-v3-0324 e2e torch test",
    "user": "aalanwyr",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T03:01:06Z",
    "closed_at": "2025-09-02T09:21:28Z",
    "merged_at": "2025-09-02T09:21:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7413"
  },
  {
    "number": 7412,
    "title": "[None][feat] Disagg cache transmission with CP",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-01T02:57:12Z",
    "closed_at": "2025-09-17T00:47:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7412"
  },
  {
    "number": 7410,
    "title": "[None][doc] fix example in docstring",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-31T23:30:50Z",
    "closed_at": "2025-09-02T08:59:49Z",
    "merged_at": "2025-09-02T08:59:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7410"
  },
  {
    "number": 7409,
    "title": "[None] [fix] Fix nsys in slurm scripts",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-31T22:06:48Z",
    "closed_at": "2025-09-01T07:03:33Z",
    "merged_at": "2025-09-01T07:03:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7409"
  },
  {
    "number": 7408,
    "title": "[None][chore] Remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-31T12:16:13Z",
    "closed_at": "2025-09-05T06:11:17Z",
    "merged_at": "2025-09-05T06:11:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7408"
  },
  {
    "number": 7407,
    "title": "[None][ci] Some improvements for Slurm CI setup",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-31T08:03:44Z",
    "closed_at": "2025-09-01T02:57:36Z",
    "merged_at": "2025-09-01T02:57:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7407"
  },
  {
    "number": 7397,
    "title": "[https://nvbugs/5458798][fix] Disabled test_trtllm_bench_backend_comparison due to timeout",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-31T06:39:34Z",
    "closed_at": "2025-09-02T18:21:42Z",
    "merged_at": "2025-09-02T18:21:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7397"
  },
  {
    "number": 7396,
    "title": "[None][chore] bump version to 1.1.0rc2.post1",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-31T06:27:12Z",
    "closed_at": "2025-08-31T15:06:55Z",
    "merged_at": "2025-08-31T15:06:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7396"
  },
  {
    "number": 7394,
    "title": "[None][chore] Bump version to 1.1.0rc3",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-31T00:51:55Z",
    "closed_at": "2025-08-31T02:20:39Z",
    "merged_at": "2025-08-31T02:20:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7394"
  },
  {
    "number": 7393,
    "title": "[TRTLLM-6342][feat] Support for partial sharding from factory",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-30T19:08:02Z",
    "closed_at": "2025-09-19T16:07:42Z",
    "merged_at": "2025-09-19T16:07:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7393"
  },
  {
    "number": 7392,
    "title": "[https://nvbugs/5445466][fix] unwaive DS R1 test cases with bug already fixed",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-30T16:00:33Z",
    "closed_at": "2025-09-10T06:54:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7392"
  },
  {
    "number": 7391,
    "title": "[https://nvbugs/5474169][fix]Adjust max seq len for kvcache for memory estimation",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-30T14:59:07Z",
    "closed_at": "2025-09-01T06:40:58Z",
    "merged_at": "2025-09-01T06:40:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7391"
  },
  {
    "number": 7390,
    "title": "[https://nvbugs/5481434][feat] Reuse pytorch memory segments occupied by cudagraph pool",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-30T14:47:53Z",
    "closed_at": "2025-09-01T13:47:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7390"
  },
  {
    "number": 7389,
    "title": "[TRTLLM-6342][fix] Fixed triggering BMM sharding",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-30T07:42:51Z",
    "closed_at": "2025-09-04T06:01:27Z",
    "merged_at": "2025-09-04T06:01:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7389"
  },
  {
    "number": 7388,
    "title": "[None][infra] Try to fix docker container failed to be killed issue",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-30T05:33:03Z",
    "closed_at": "2025-09-08T18:28:02Z",
    "merged_at": "2025-09-08T18:28:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7388"
  },
  {
    "number": 7387,
    "title": "[https://nvbugs/5489015][fix] Support communicator split in MNNVL allreduce and fix the binding issues.",
    "user": "timlee0212",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-30T05:32:19Z",
    "closed_at": "2025-09-16T23:43:20Z",
    "merged_at": "2025-09-16T23:43:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7387"
  },
  {
    "number": 7386,
    "title": "[None][infra] Disable GB200-PyTorch-1 due to OOM issue",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-30T03:25:45Z",
    "closed_at": "2025-09-01T05:56:31Z",
    "merged_at": "2025-09-01T05:56:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7386"
  },
  {
    "number": 7385,
    "title": "[None][infra] Adjust labeling llm prompt for bug issues",
    "user": "karljang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-30T00:18:42Z",
    "closed_at": "2025-09-10T21:10:32Z",
    "merged_at": "2025-09-10T21:10:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7385"
  },
  {
    "number": 7384,
    "title": "[None][feat] Add FP16/BF16 UB allreduce support to TRT path",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T23:02:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7384"
  },
  {
    "number": 7383,
    "title": "[None][feat] Use a shell context to install dependancies",
    "user": "v-shobhit",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T22:09:40Z",
    "closed_at": "2025-09-10T16:57:37Z",
    "merged_at": "2025-09-10T16:57:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7383"
  },
  {
    "number": 7382,
    "title": "[TRTLLM-6081][Doc] Address review concerns",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T19:39:51Z",
    "closed_at": "2025-09-04T16:49:06Z",
    "merged_at": "2025-09-04T16:49:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7382"
  },
  {
    "number": 7381,
    "title": "Respond to 2nd review",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T19:24:15Z",
    "closed_at": "2025-08-29T19:26:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7381"
  },
  {
    "number": 7380,
    "title": "[None][doc] Exposing the ADP balance strategy tech blog",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T16:47:48Z",
    "closed_at": "2025-08-29T17:19:14Z",
    "merged_at": "2025-08-29T17:19:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7380"
  },
  {
    "number": 7379,
    "title": "[https://nvbugs/5445466][fix] Eliminate race when loading HF dynamic modules (#7268)",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T16:16:04Z",
    "closed_at": "2025-08-30T09:44:25Z",
    "merged_at": "2025-08-30T09:44:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7379"
  },
  {
    "number": 7378,
    "title": "[None][Doc] Polish parallelism doc and add wide ep sectionwq.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T15:58:53Z",
    "closed_at": "2025-09-01T16:42:09Z",
    "merged_at": "2025-09-01T16:42:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7378"
  },
  {
    "number": 7377,
    "title": "[TRTLLM-7008][fix] Add automatic shared memory delete if already exist",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T14:03:21Z",
    "closed_at": "2025-09-03T16:44:07Z",
    "merged_at": "2025-09-03T16:44:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7377"
  },
  {
    "number": 7376,
    "title": "[None][feat] wide_ep support block-wise FP8 on blackwell",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T12:25:12Z",
    "closed_at": "2025-09-01T06:11:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7376"
  },
  {
    "number": 7375,
    "title": "[None] [fix] store blog 10 media via lfs",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T12:24:52Z",
    "closed_at": "2025-08-30T02:17:53Z",
    "merged_at": "2025-08-30T02:17:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7375"
  },
  {
    "number": 7373,
    "title": "[https://nvbugs/5485325][fix] Add a postprocess to the model engine to fix the CUDA graph warmup issue when using speculative decoding",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T10:26:08Z",
    "closed_at": "2025-09-05T04:04:42Z",
    "merged_at": "2025-09-05T04:04:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7373"
  },
  {
    "number": 7372,
    "title": "[None][chore] rm executor config in kv cache connector",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T10:14:43Z",
    "closed_at": "2025-09-03T00:13:13Z",
    "merged_at": "2025-09-03T00:13:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7372"
  },
  {
    "number": 7371,
    "title": "[TRTLLM-7261][feat] Support phi-4 model in pytorch backend",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T10:14:14Z",
    "closed_at": "2025-09-03T02:27:42Z",
    "merged_at": "2025-09-03T02:27:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7371"
  },
  {
    "number": 7370,
    "title": "[None][infra] Waive failed tests on main branch 08/29",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T09:46:19Z",
    "closed_at": "2025-08-29T14:28:21Z",
    "merged_at": "2025-08-29T14:28:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7370"
  },
  {
    "number": 7369,
    "title": "[TRTLLM-6747][feat] Merge add sparse exp and shared exp into local reduction",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T09:32:46Z",
    "closed_at": "2025-09-01T01:20:01Z",
    "merged_at": "2025-09-01T01:20:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7369"
  },
  {
    "number": 7367,
    "title": "[None][fix] fix doc formula",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T08:19:53Z",
    "closed_at": "2025-08-29T08:48:10Z",
    "merged_at": "2025-08-29T08:48:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7367"
  },
  {
    "number": 7366,
    "title": "[https://nvbugs/5485593][fix] improve accuracy/test_disaggregated_serving.py",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T07:45:29Z",
    "closed_at": "2025-09-03T13:38:54Z",
    "merged_at": "2025-09-03T13:38:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7366"
  },
  {
    "number": 7365,
    "title": "[TRTLLM-7279][test] add accuracy test for deepseek-r1 with chunked_prefill",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T07:29:39Z",
    "closed_at": "2025-09-15T05:38:52Z",
    "merged_at": "2025-09-15T05:38:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7365"
  },
  {
    "number": 7363,
    "title": "[TRTLLM-7330][feat] Eagle3 cuda graph support for the first draft model inference",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T07:12:20Z",
    "closed_at": "2025-09-26T03:28:06Z",
    "merged_at": "2025-09-26T03:28:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7363"
  },
  {
    "number": 7362,
    "title": "[None][doc] update architecture overview doc",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T06:46:20Z",
    "closed_at": "2025-08-29T08:00:04Z",
    "merged_at": "2025-08-29T08:00:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7362"
  },
  {
    "number": 7361,
    "title": "[TRTLLM-6642][feat] add gptoss 20g tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T05:53:49Z",
    "closed_at": "2025-09-05T06:20:29Z",
    "merged_at": "2025-09-05T06:20:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7361"
  },
  {
    "number": 7360,
    "title": "[TRTLLM-7410][feat] Support hashing and KV cache reuse for videos",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T05:07:18Z",
    "closed_at": "2025-09-04T18:39:23Z",
    "merged_at": "2025-09-04T18:39:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7360"
  },
  {
    "number": 7359,
    "title": "[None][doc] Update trtllm-eval doc",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T04:37:38Z",
    "closed_at": "2025-08-29T05:17:08Z",
    "merged_at": "2025-08-29T05:17:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7359"
  },
  {
    "number": 7358,
    "title": "[None] [doc] Update DeepSeek example doc",
    "user": "jiahanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T03:46:08Z",
    "closed_at": "2025-09-01T18:43:59Z",
    "merged_at": "2025-09-01T18:43:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7358"
  },
  {
    "number": 7356,
    "title": "[None][fix] Revert TP Sharding read from the model config (#6972)",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T03:19:34Z",
    "closed_at": "2025-09-03T00:10:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7356"
  },
  {
    "number": 7355,
    "title": "[None][feat] Support DeepGEMM swap-AB on sm100",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T03:05:29Z",
    "closed_at": "2025-09-01T12:30:55Z",
    "merged_at": "2025-09-01T12:30:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7355"
  },
  {
    "number": 7354,
    "title": "[https://nvbugs/5448767][fix] disable kv cache reuse for disagg pp>1 tests",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-29T02:02:43Z",
    "closed_at": "2025-08-29T07:33:16Z",
    "merged_at": "2025-08-29T07:33:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7354"
  },
  {
    "number": 7353,
    "title": "Perf/gpt oss eagle",
    "user": "ameynaik-hub",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T23:16:57Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7353"
  },
  {
    "number": 7352,
    "title": "[None][chore] Fix formatting error in Gemma3 readme",
    "user": "karljang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T22:29:48Z",
    "closed_at": "2025-09-02T17:15:38Z",
    "merged_at": "2025-09-02T17:15:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7352"
  },
  {
    "number": 7351,
    "title": "[None] [chore] Update .coderabbit.yaml review configuration",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T22:21:32Z",
    "closed_at": "2025-08-29T04:10:32Z",
    "merged_at": "2025-08-29T04:10:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7351"
  },
  {
    "number": 7349,
    "title": "[TRTLLM-5059][feat] Enable KV-cache reuse and add E2E tests for llava-next",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T17:23:15Z",
    "closed_at": "2025-09-09T18:51:37Z",
    "merged_at": "2025-09-09T18:51:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7349"
  },
  {
    "number": 7348,
    "title": "[None][fix] Fix KV cache recompute in draft_target spec decode",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T17:04:05Z",
    "closed_at": "2025-09-03T19:04:15Z",
    "merged_at": "2025-09-03T19:04:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7348"
  },
  {
    "number": 7347,
    "title": "[None][doc] Update doc for multimodal",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T16:48:21Z",
    "closed_at": "2025-09-16T19:53:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7347"
  },
  {
    "number": 7346,
    "title": "[None][infra] Waive failed tests on main branch 08/26",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T14:18:57Z",
    "closed_at": "2025-08-28T16:24:08Z",
    "merged_at": "2025-08-28T16:24:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7346"
  },
  {
    "number": 7345,
    "title": "[https://nvbugs/5481385][fix] Fix max_seq_len in cuda graph warmup and intermediate_size in fused_moe_deepgemm",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T12:37:48Z",
    "closed_at": "2025-08-29T09:00:43Z",
    "merged_at": "2025-08-29T09:00:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7345"
  },
  {
    "number": 7344,
    "title": "draft: enable LM tp for MTP, under attention dp case",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T12:19:15Z",
    "closed_at": "2025-09-04T17:22:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7344"
  },
  {
    "number": 7343,
    "title": "[None][chore] Add env var to disable harmony",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T11:47:29Z",
    "closed_at": "2025-09-04T02:22:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7343"
  },
  {
    "number": 7342,
    "title": "[TRTLLM-7250][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T09:56:10Z",
    "closed_at": "2025-08-30T04:49:14Z",
    "merged_at": "2025-08-30T04:49:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7342"
  },
  {
    "number": 7341,
    "title": "[TRTLLM-7208][feat] Implement basic functionalities for Responses API",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T09:51:00Z",
    "closed_at": "2025-09-02T11:08:23Z",
    "merged_at": "2025-09-02T11:08:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7341"
  },
  {
    "number": 7340,
    "title": "[https://nvbugs/5480289][fix] release slot manager in mtp MTPHiddenStatesManager",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T09:45:53Z",
    "closed_at": "2025-09-03T02:37:51Z",
    "merged_at": "2025-09-03T02:37:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7340"
  },
  {
    "number": 7338,
    "title": "[https://nvbugs/5470769][fix] fix disagg-serving accuracy test case",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T09:39:36Z",
    "closed_at": "2025-09-04T01:11:01Z",
    "merged_at": "2025-09-04T01:11:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7338"
  },
  {
    "number": 7336,
    "title": "Feat/add lora cuda graph support",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T09:18:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7336"
  },
  {
    "number": 7335,
    "title": "Feat/support lora cuda graph",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T08:43:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7335"
  },
  {
    "number": 7334,
    "title": "[https://nvbugs/5485430][fix] Copy the nanobind file when using precompiled package",
    "user": "jiaganc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T08:31:35Z",
    "closed_at": "2025-09-02T05:49:31Z",
    "merged_at": "2025-09-02T05:49:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7334"
  },
  {
    "number": 7333,
    "title": "[None][ci] skip TestGPTOSS",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T08:06:02Z",
    "closed_at": "2025-08-28T09:01:49Z",
    "merged_at": "2025-08-28T09:01:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7333"
  },
  {
    "number": 7332,
    "title": "[https://nvbugs/5481087][fix] fix bug of ci when we use mocker",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T08:05:58Z",
    "closed_at": "2025-09-01T08:22:45Z",
    "merged_at": "2025-09-01T08:22:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7332"
  },
  {
    "number": 7331,
    "title": "[https://nvbugs/5477730][fix] Fix the alltoall case when tp_size larger than ep_size",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T07:59:55Z",
    "closed_at": "2025-09-04T12:10:04Z",
    "merged_at": "2025-09-04T12:10:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7331"
  },
  {
    "number": 7330,
    "title": "[none][doc] update feature combination matrix for 1.0",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T06:45:27Z",
    "closed_at": "2025-08-28T07:08:27Z",
    "merged_at": "2025-08-28T07:08:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7330"
  },
  {
    "number": 7329,
    "title": "[https://nvbugs/5453949][chore] unwaive test_nixl_backend accuracy tests",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T06:32:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7329"
  },
  {
    "number": 7328,
    "title": "[None][test] add gpt oss model for trtllm perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T06:21:59Z",
    "closed_at": "2025-09-17T07:23:21Z",
    "merged_at": "2025-09-17T07:23:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7328"
  },
  {
    "number": 7326,
    "title": "[TRTLLM-7280][test] Add beam search CudaGraph + Overlap Scheduler tests",
    "user": "fredricz-20070104",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T02:58:32Z",
    "closed_at": "2025-08-29T06:16:23Z",
    "merged_at": "2025-08-29T06:16:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7326"
  },
  {
    "number": 7325,
    "title": "[None][fix] Disable mandatory PR checklist enforcement",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T02:39:57Z",
    "closed_at": "2025-08-28T03:06:57Z",
    "merged_at": "2025-08-28T03:06:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7325"
  },
  {
    "number": 7324,
    "title": "[None] [refactor] [Breaking change] Restore trtllm-bench chunking behavior to be backend-dependent",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T02:24:42Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7324"
  },
  {
    "number": 7323,
    "title": "[https://nvbugs/5481080][fix] Fix GPTOSS W4A16 reference",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T01:42:03Z",
    "closed_at": "2025-09-08T20:59:28Z",
    "merged_at": "2025-09-08T20:59:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7323"
  },
  {
    "number": 7321,
    "title": "[None][ci] fix test list name",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T00:53:32Z",
    "closed_at": "2025-08-28T02:33:22Z",
    "merged_at": "2025-08-28T02:33:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7321"
  },
  {
    "number": 7320,
    "title": "[#7208][fix] Fix config type of MedusaConfig",
    "user": "karljang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T23:04:56Z",
    "closed_at": "2025-09-10T06:25:17Z",
    "merged_at": "2025-09-10T06:25:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7320"
  },
  {
    "number": 7318,
    "title": "[https://nvbugs/5470782][fix] Add specific test names for test_deepseek.py",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T20:27:22Z",
    "closed_at": "2025-09-02T17:31:41Z",
    "merged_at": "2025-09-02T17:31:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7318"
  },
  {
    "number": 7317,
    "title": "[fix] update flashinfer version to align with vllm 0.10.0",
    "user": "tijyojwad",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T19:57:02Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7317"
  },
  {
    "number": 7316,
    "title": "[None][fix] Fixed regex in test_trtllm_bench_backend_comparison",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T19:53:37Z",
    "closed_at": "2025-09-01T09:02:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7316"
  },
  {
    "number": 7313,
    "title": "[#7222][autodeploy] Separate run_shape_prop as another graph utility",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T19:20:00Z",
    "closed_at": "2025-09-03T23:32:50Z",
    "merged_at": "2025-09-03T23:32:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7313"
  },
  {
    "number": 7309,
    "title": "User/yuanjingx/slurm test tmp",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T19:07:53Z",
    "closed_at": "2025-09-08T18:34:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7309"
  },
  {
    "number": 7305,
    "title": "[https://nvbugs/5401936][fix] Unwaive Gemma3 modeling tests",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T18:00:15Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7305"
  },
  {
    "number": 7304,
    "title": "[None] [feat] Use numa to bind CPU",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T16:25:16Z",
    "closed_at": "2025-08-28T10:27:11Z",
    "merged_at": "2025-08-28T10:27:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7304"
  },
  {
    "number": 7303,
    "title": "[TRTLLM-6639][test] Add lora adapter test cases with llmapi",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T15:17:38Z",
    "closed_at": "2025-09-10T05:19:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7303"
  },
  {
    "number": 7302,
    "title": "[None][test] Update case that not support passing quantization fp8 for pytorch backend",
    "user": "nvamyt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T15:10:55Z",
    "closed_at": "2025-09-01T04:59:22Z",
    "merged_at": "2025-09-01T04:59:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7302"
  },
  {
    "number": 7301,
    "title": "Fix loading Qwen VL fp4 ckpt",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T15:00:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7301"
  },
  {
    "number": 7300,
    "title": "[None][infra] Waive failed tests on main 08/27",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T13:31:35Z",
    "closed_at": "2025-08-27T13:53:34Z",
    "merged_at": "2025-08-27T13:53:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7300"
  },
  {
    "number": 7298,
    "title": "[https://nvbugs/5467548][fix] DeepSeek illegal memory access.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T10:52:04Z",
    "closed_at": "2025-08-29T04:19:03Z",
    "merged_at": "2025-08-29T04:19:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7298"
  },
  {
    "number": 7297,
    "title": "[TRTLLM-7457][ci] Update unittest parallel config",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T10:30:31Z",
    "closed_at": "2025-08-29T01:28:05Z",
    "merged_at": "2025-08-29T01:28:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7297"
  },
  {
    "number": 7296,
    "title": "[TRTLLM-5670][feat] Support top log probabilities",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T10:16:45Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7296"
  },
  {
    "number": 7295,
    "title": "[https://nvbugs/5485325][fix] Fix illegal memory access when max_nun_token equals to max_seq_len when MTP>=1",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T10:13:23Z",
    "closed_at": "2025-10-14T03:43:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7295"
  },
  {
    "number": 7294,
    "title": "[TRTLLM-7728][feat] batched sampling by strategy (supersedes enable_mixed_sampler, cf. TRTLLM-7156)",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T09:46:38Z",
    "closed_at": "2025-09-23T23:05:05Z",
    "merged_at": "2025-09-23T23:05:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7294"
  },
  {
    "number": 7293,
    "title": "[https://nvbugs/5453727][fix] unwaive qwen3 CI tests",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T09:43:03Z",
    "closed_at": "2025-08-27T14:59:00Z",
    "merged_at": "2025-08-27T14:59:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7293"
  },
  {
    "number": 7292,
    "title": "[TRTLLM-7250][fix] waive failed cases",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T09:27:29Z",
    "closed_at": "2025-08-27T10:04:46Z",
    "merged_at": "2025-08-27T10:04:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7292"
  },
  {
    "number": 7291,
    "title": "[None][ci] parallelize unit tests of auto deploy in B200",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T09:19:46Z",
    "closed_at": "2025-08-27T14:32:11Z",
    "merged_at": "2025-08-27T14:32:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7291"
  },
  {
    "number": 7290,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T09:18:56Z",
    "closed_at": "2025-08-27T09:24:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7290"
  },
  {
    "number": 7289,
    "title": "[TRTLLM-8000][infra] Catch error in merge waive list stage",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T08:49:01Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7289"
  },
  {
    "number": 7287,
    "title": "[None][opt] Add batch waiting when scheduling",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T08:21:44Z",
    "closed_at": "2025-09-05T01:35:19Z",
    "merged_at": "2025-09-05T01:35:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7287"
  },
  {
    "number": 7286,
    "title": "[https://nvbugs/5378031] [feat] W4A8 AWQ MoE supports Per Expert Pre-quant Scale Factor for PyT backend",
    "user": "yumin066",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T07:53:24Z",
    "closed_at": "2025-10-16T03:07:48Z",
    "merged_at": "2025-10-16T03:07:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7286"
  },
  {
    "number": 7285,
    "title": "[None][perf] Autotune TRT-LLM Gen MoE when using CUDA graphs",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T07:44:56Z",
    "closed_at": "2025-09-03T02:09:00Z",
    "merged_at": "2025-09-03T02:09:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7285"
  },
  {
    "number": 7284,
    "title": "[TRTLLM-6646][test] NIM migration to TRT-LLM LLMAPI : Add QWQ-32b torch test",
    "user": "aalanwyr",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T07:33:53Z",
    "closed_at": "2025-08-29T03:09:11Z",
    "merged_at": "2025-08-29T03:09:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7284"
  },
  {
    "number": 7283,
    "title": "[TRTLLM-7412][feat] Turn off spec decode when the rolling average acceptance length drops below threshold.",
    "user": "zheyuf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T06:57:41Z",
    "closed_at": "2025-10-13T22:51:15Z",
    "merged_at": "2025-10-13T22:51:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7283"
  },
  {
    "number": 7282,
    "title": "[None][ci] remove test_llm_api_autodeploy from B200 test db",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T06:48:05Z",
    "closed_at": "2025-08-27T07:12:30Z",
    "merged_at": "2025-08-27T07:12:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7282"
  },
  {
    "number": 7280,
    "title": "[TRTLLM-7440][fix] Split `fused_input_embed` to separate out host sync",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T05:45:03Z",
    "closed_at": "2025-09-07T03:11:40Z",
    "merged_at": "2025-09-07T03:11:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7280"
  },
  {
    "number": 7279,
    "title": "[None][fix] Update maxnt of llama_v3.2_1b bench",
    "user": "nvamyt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T04:47:29Z",
    "closed_at": "2025-08-27T08:56:29Z",
    "merged_at": "2025-08-27T08:56:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7279"
  },
  {
    "number": 7278,
    "title": "[None][infra] pipeline test",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T04:44:12Z",
    "closed_at": "2025-10-16T18:13:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7278"
  },
  {
    "number": 7277,
    "title": "[TRTLLM-7408][feat] Wrap MOE with custom op.",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T02:42:50Z",
    "closed_at": "2025-09-09T16:18:56Z",
    "merged_at": "2025-09-09T16:18:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7277"
  },
  {
    "number": 7276,
    "title": "[None][doc] add runtime optimizations for overview doc",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T02:16:40Z",
    "closed_at": "2025-08-29T00:11:23Z",
    "merged_at": "2025-08-29T00:11:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7276"
  },
  {
    "number": 7275,
    "title": "[https://nvbugs/5480415][fix] Fix phi4mm multi-gpu test",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T02:10:56Z",
    "closed_at": "2025-08-28T02:24:20Z",
    "merged_at": "2025-08-28T02:24:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7275"
  },
  {
    "number": 7274,
    "title": "[https://nvbugs/5430124][ci] Unwaive Mistral 3.1 Small tests",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T00:49:10Z",
    "closed_at": "2025-08-28T04:03:32Z",
    "merged_at": "2025-08-28T04:03:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7274"
  },
  {
    "number": 7273,
    "title": "[TRTLLM-7442][model] Remove unnecessary D2H copies",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-27T00:33:42Z",
    "closed_at": "2025-09-04T03:14:21Z",
    "merged_at": "2025-09-04T03:14:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7273"
  },
  {
    "number": 7272,
    "title": "[https://nvbugs/5474453][fix] fix path to tested model",
    "user": "nzmora-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T22:19:35Z",
    "closed_at": "2025-08-28T12:01:49Z",
    "merged_at": "2025-08-28T12:01:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7272"
  },
  {
    "number": 7271,
    "title": "[https://nvbugs/5480550][fix] Increase timeout for Gemma3 27B test",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T22:15:20Z",
    "closed_at": "2025-08-27T15:45:06Z",
    "merged_at": "2025-08-27T15:45:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7271"
  },
  {
    "number": 7269,
    "title": "[None][chore] Replace --eos_id with --ignore_eos in trtllm-bench",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T21:01:33Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7269"
  },
  {
    "number": 7268,
    "title": "[https://nvbugs/5445466][fix] Eliminate race when loading HF dynamic modules",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T20:21:53Z",
    "closed_at": "2025-08-29T04:36:30Z",
    "merged_at": "2025-08-29T04:36:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7268"
  },
  {
    "number": 7267,
    "title": "[https://nvbugs/5478151][fix] Add missing spec for Llama-3.3 70B",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T19:34:46Z",
    "closed_at": "2025-08-27T01:56:58Z",
    "merged_at": "2025-08-27T01:56:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7267"
  },
  {
    "number": 7266,
    "title": "[None][update] Update disagg code owners",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T17:50:21Z",
    "closed_at": "2025-08-26T18:36:29Z",
    "merged_at": "2025-08-26T18:36:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7266"
  },
  {
    "number": 7265,
    "title": "[https://nvbugs/5430125][ci] Unwaive test case for mistral 3.1 small",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T17:34:34Z",
    "closed_at": "2025-08-26T21:32:03Z",
    "merged_at": "2025-08-26T21:32:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7265"
  },
  {
    "number": 7262,
    "title": "[None][fix] Fix possible hang issue in WideEP and move some tests to pre-merge",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T13:47:53Z",
    "closed_at": "2025-08-27T05:39:24Z",
    "merged_at": "2025-08-27T05:39:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7262"
  },
  {
    "number": 7261,
    "title": "[TRTLLM-7207][feat] Chat completions API for gpt-oss",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T12:34:10Z",
    "closed_at": "2025-08-28T02:22:07Z",
    "merged_at": "2025-08-28T02:22:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7261"
  },
  {
    "number": 7260,
    "title": "[None][fix] Updated blog9_Deploying_GPT_OSS_on_TRTLLM",
    "user": "Maurits-de-Groot",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T11:48:15Z",
    "closed_at": "2025-08-26T15:26:20Z",
    "merged_at": "2025-08-26T15:26:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7260"
  },
  {
    "number": 7258,
    "title": "[None][infra] Waive failed cases for release/1.0 08/26",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T10:52:26Z",
    "closed_at": "2025-08-26T11:50:28Z",
    "merged_at": "2025-08-26T11:50:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7258"
  },
  {
    "number": 7257,
    "title": "[None][ci] move qwen3 tests from b200 to gb200",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T10:45:52Z",
    "closed_at": "2025-08-26T15:50:53Z",
    "merged_at": "2025-08-26T15:50:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7257"
  },
  {
    "number": 7256,
    "title": "[https://nvbugs/5480415][fix] Fix phi4mm multi-gpu test",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T10:01:05Z",
    "closed_at": "2025-09-03T05:42:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7256"
  },
  {
    "number": 7255,
    "title": "[None] [feat] Python SLURM launcher ",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T09:38:27Z",
    "closed_at": "2025-10-27T15:59:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7255"
  },
  {
    "number": 7254,
    "title": "[TRTLLM-7457][ci] Update & cleanup unittest parallel config",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T09:12:16Z",
    "closed_at": "2025-08-27T04:45:59Z",
    "merged_at": "2025-08-27T04:45:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7254"
  },
  {
    "number": 7252,
    "title": "[None][docs] refine docs for accuracy evaluation of gpt-oss models",
    "user": "binghanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T08:16:34Z",
    "closed_at": "2025-09-08T01:56:23Z",
    "merged_at": "2025-09-08T01:56:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7252"
  },
  {
    "number": 7251,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T08:01:28Z",
    "closed_at": "2025-08-26T09:13:44Z",
    "merged_at": "2025-08-26T09:13:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7251"
  },
  {
    "number": 7250,
    "title": "[TRTLLM-7385][feat] Optimize Qwen2/2.5-VL performance",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T07:40:28Z",
    "closed_at": "2025-09-22T10:40:03Z",
    "merged_at": "2025-09-22T10:40:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7250"
  },
  {
    "number": 7248,
    "title": "[https://nvbugs/5412456][fix] Remove from waives.txt",
    "user": "zhou-yuxin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T07:25:42Z",
    "closed_at": "2025-08-27T02:05:53Z",
    "merged_at": "2025-08-27T02:05:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7248"
  },
  {
    "number": 7245,
    "title": "[https://nvbugs/5451426][fix] Avoid torch compile on full eagle3 worker",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T07:04:44Z",
    "closed_at": "2025-08-27T01:59:07Z",
    "merged_at": "2025-08-27T01:59:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7245"
  },
  {
    "number": 7244,
    "title": "[None][debug] Update for debugging node 0082",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T06:56:09Z",
    "closed_at": "2025-09-04T09:06:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7244"
  },
  {
    "number": 7243,
    "title": "[https://nvbugs/5453806][unwaive] Unwaive fp8 kvcache attention test",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T05:01:35Z",
    "closed_at": "2025-09-05T16:13:58Z",
    "merged_at": "2025-09-05T16:13:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7243"
  },
  {
    "number": 7242,
    "title": "[https://nvbugs/5453992][unwaive] Unwaive llama quickstart test",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T04:48:09Z",
    "closed_at": "2025-09-02T12:28:33Z",
    "merged_at": "2025-09-02T12:28:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7242"
  },
  {
    "number": 7241,
    "title": "[None][doc] Display tech blog for nvidia.github.io domain.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T04:33:07Z",
    "closed_at": "2025-08-26T07:36:29Z",
    "merged_at": "2025-08-26T07:36:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7241"
  },
  {
    "number": 7240,
    "title": "feat: RayExecutor",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T04:23:30Z",
    "closed_at": "2025-08-26T22:00:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7240"
  },
  {
    "number": 7239,
    "title": "[None][chore] Use llm args in create_py_executor",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T03:29:13Z",
    "closed_at": "2025-09-01T23:27:56Z",
    "merged_at": "2025-09-01T23:27:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7239"
  },
  {
    "number": 7238,
    "title": "[None][fix] Remove and fuse some element-wise ops in the ds-r1-fp8 model",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T03:13:50Z",
    "closed_at": "2025-08-27T02:35:39Z",
    "merged_at": "2025-08-27T02:35:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7238"
  },
  {
    "number": 7237,
    "title": "[https://nvbugs/5467062][fix] fix triton version for NIM LLM team",
    "user": "tijyojwad",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T02:54:32Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7237"
  },
  {
    "number": 7236,
    "title": "[None][chore] share input_ids buffers among different cuda graphs",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T02:14:58Z",
    "closed_at": "2025-09-06T21:49:42Z",
    "merged_at": "2025-09-06T21:49:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7236"
  },
  {
    "number": 7235,
    "title": "[TRTLLM-7009][fix] Support cuda graph padding for qwen-vl models",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T01:22:44Z",
    "closed_at": "2025-08-26T01:47:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7235"
  },
  {
    "number": 7234,
    "title": "[None][feat] MultiLayer Eagle",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-26T00:33:17Z",
    "closed_at": "2025-09-04T14:49:14Z",
    "merged_at": "2025-09-04T14:49:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7234"
  },
  {
    "number": 7233,
    "title": "[None][doc] Update autodeploy README.md, deprecate lm_eval in examples folder",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T23:50:28Z",
    "closed_at": "2025-08-26T17:47:57Z",
    "merged_at": "2025-08-26T17:47:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7233"
  },
  {
    "number": 7232,
    "title": "[None][feat] Add logging for OAI disagg server",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T23:12:29Z",
    "closed_at": "2025-08-27T04:02:03Z",
    "merged_at": "2025-08-27T04:02:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7232"
  },
  {
    "number": 7231,
    "title": "[https://nvbugs/5463720][fix] tp-split the inferred `mlp_hidden_size` for nemotron-nas",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T22:49:16Z",
    "closed_at": "2025-08-27T12:04:42Z",
    "merged_at": "2025-08-27T12:04:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7231"
  },
  {
    "number": 7230,
    "title": "[None][fix] Fix data type of KV Cache percentage in bench.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T22:45:56Z",
    "closed_at": "2025-08-26T16:28:10Z",
    "merged_at": "2025-08-26T16:28:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7230"
  },
  {
    "number": 7229,
    "title": "[None][chore] Update CI allowlist 2025-08-25",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T22:03:38Z",
    "closed_at": "2025-08-26T05:53:48Z",
    "merged_at": "2025-08-26T05:53:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7229"
  },
  {
    "number": 7228,
    "title": "[None][feat] KV Cache Connector API",
    "user": "richardhuo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T20:41:46Z",
    "closed_at": "2025-08-29T03:09:28Z",
    "merged_at": "2025-08-29T03:09:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7228"
  },
  {
    "number": 7227,
    "title": "[#5861][autodeploy] Refactor: Quantization Transforms with Inheritance",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T20:09:51Z",
    "closed_at": "2025-09-10T05:00:06Z",
    "merged_at": "2025-09-10T05:00:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7227"
  },
  {
    "number": 7226,
    "title": "[None][chore] Update CI allowlist 2025-08-25",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T18:25:57Z",
    "closed_at": "2025-08-25T23:10:18Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7226"
  },
  {
    "number": 7225,
    "title": "[None][feat] Skip prefetching consolidated safetensors when appropriate",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T17:47:42Z",
    "closed_at": "2025-08-26T16:40:17Z",
    "merged_at": "2025-08-26T16:40:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7225"
  },
  {
    "number": 7224,
    "title": "[None][feat] Update TargetInfo to accommodate CP in disagg",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T17:42:58Z",
    "closed_at": "2025-08-29T19:56:21Z",
    "merged_at": "2025-08-29T19:56:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7224"
  },
  {
    "number": 7223,
    "title": "[None][fix][AutoDeploy] canonicalize_graph before shape prop for consistent state_dict",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T17:38:05Z",
    "closed_at": "2025-08-25T20:59:58Z",
    "merged_at": "2025-08-25T20:59:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7223"
  },
  {
    "number": 7221,
    "title": "[#6120][feat] AutoDeploy: flexible args for sequence interface + AD multi-modal input processor + llama4 VLM example",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T16:29:25Z",
    "closed_at": "2025-09-06T02:10:49Z",
    "merged_at": "2025-09-06T02:10:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7221"
  },
  {
    "number": 7220,
    "title": "[https://nvbugs/5473781][fix] Fix llama 4 FP8 for PP>1",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T15:54:15Z",
    "closed_at": "2025-09-24T16:16:27Z",
    "merged_at": "2025-09-24T16:16:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7220"
  },
  {
    "number": 7219,
    "title": "[TRTLLM-6142][feat] Reland: set torch recompile_limit based on cuda_graph_batch_sizes and refactored",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T14:57:21Z",
    "closed_at": "2025-09-08T12:45:59Z",
    "merged_at": "2025-09-08T12:45:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7219"
  },
  {
    "number": 7218,
    "title": "[#7217][AutoDeploy]: fix EP sharding",
    "user": "nzmora-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T14:24:24Z",
    "closed_at": "2025-08-25T17:43:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7218"
  },
  {
    "number": 7216,
    "title": "[None][fix] fix get_iteration_stats IndexError",
    "user": "macrocell",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T13:50:11Z",
    "closed_at": "2025-09-24T14:43:03Z",
    "merged_at": "2025-09-24T14:43:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7216"
  },
  {
    "number": 7215,
    "title": "[https://nvbugs/5477332][fix] Relax atol in test_mamba2_chunk_scan_combined_prefill_chunking",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T13:33:56Z",
    "closed_at": "2025-08-26T07:48:58Z",
    "merged_at": "2025-08-26T07:48:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7215"
  },
  {
    "number": 7214,
    "title": "[None][fix] mxfp4 padding bug for TRT-LLM and CUTLASS MoE backends",
    "user": "nekorobov",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T13:01:20Z",
    "closed_at": "2025-08-28T17:08:06Z",
    "merged_at": "2025-08-28T17:08:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7214"
  },
  {
    "number": 7213,
    "title": "[None][doc] add adp balance blog",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T11:08:50Z",
    "closed_at": "2025-08-28T15:19:35Z",
    "merged_at": "2025-08-28T15:19:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7213"
  },
  {
    "number": 7212,
    "title": "[https://nvbugs/5451028][fix] Lower BS for NemotronUltra tests",
    "user": "Naveassaf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T11:00:57Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7212"
  },
  {
    "number": 7211,
    "title": "[None][test] add kv cache size in bench metric and fix failed cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T10:04:21Z",
    "closed_at": "2025-08-26T02:09:23Z",
    "merged_at": "2025-08-26T02:09:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7211"
  },
  {
    "number": 7210,
    "title": "[None][fix] fix log_once usage",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T10:03:26Z",
    "closed_at": "2025-08-26T11:13:04Z",
    "merged_at": "2025-08-26T11:13:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7210"
  },
  {
    "number": 7209,
    "title": "Don't review - Testing: Support fp8 block wide ep",
    "user": "xxi-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T09:45:48Z",
    "closed_at": "2025-08-29T06:07:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7209"
  },
  {
    "number": 7207,
    "title": "[TRTLLM-6577][feat] Support nano_v2_vlm in pytorch backend",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T08:28:09Z",
    "closed_at": "2025-09-18T08:26:20Z",
    "merged_at": "2025-09-18T08:26:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7207"
  },
  {
    "number": 7206,
    "title": "[TRTLLM-4517] [feat] Additional model outputs",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T08:25:50Z",
    "closed_at": "2025-10-13T13:33:18Z",
    "merged_at": "2025-10-13T13:33:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7206"
  },
  {
    "number": 7205,
    "title": "[https://nvbugs/5452463][doc] update disagg doc about UCX_MAX_RNDV_RAILS",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T08:06:55Z",
    "closed_at": "2025-08-26T02:42:42Z",
    "merged_at": "2025-08-26T02:42:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7205"
  },
  {
    "number": 7204,
    "title": "Add blockscale fp8 gemm to qwen2.5vl vlm part",
    "user": "tiffany940107",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T07:59:34Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7204"
  },
  {
    "number": 7203,
    "title": "[TRTLLM-7346][fix] Improve performance of PyTorchModelEngine._get_lora_params_from_requests",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T07:47:53Z",
    "closed_at": "2025-08-28T08:06:32Z",
    "merged_at": "2025-08-28T08:06:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7203"
  },
  {
    "number": 7202,
    "title": "[None][chore] [WIP] Create PyExecutor from TorchLlmArgs Part 2",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T07:09:56Z",
    "closed_at": "2025-08-26T11:47:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7202"
  },
  {
    "number": 7201,
    "title": "[None][infra] Waive failed tests on main branch",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T06:11:37Z",
    "closed_at": "2025-08-25T13:04:38Z",
    "merged_at": "2025-08-25T13:04:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7201"
  },
  {
    "number": 7200,
    "title": "[None][test] Update qwen3 timeout to 60 minutes",
    "user": "nvamyt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T05:35:46Z",
    "closed_at": "2025-08-26T06:18:42Z",
    "merged_at": "2025-08-26T06:18:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7200"
  },
  {
    "number": 7199,
    "title": "[None][chore] Some improvements for CI stability",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T05:12:44Z",
    "closed_at": "2025-08-28T20:19:20Z",
    "merged_at": "2025-08-28T20:19:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7199"
  },
  {
    "number": 7198,
    "title": "[None][infra] Using local variables in rerun function",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T03:29:18Z",
    "closed_at": "2025-09-02T05:55:26Z",
    "merged_at": "2025-09-02T05:55:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7198"
  },
  {
    "number": 7197,
    "title": "[None][doc] add LoRA to feature combination matrix",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T03:24:46Z",
    "closed_at": "2025-08-25T09:05:20Z",
    "merged_at": "2025-08-25T09:05:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7197"
  },
  {
    "number": 7195,
    "title": "[TRTLLM-6794][feat] enable rejection sampler for ngram",
    "user": "kris1025",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T03:14:16Z",
    "closed_at": "2025-08-29T03:21:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7195"
  },
  {
    "number": 7194,
    "title": "[None][ci] waive test_mamba2_chunk_scan_combined_prefill_chunking[seqlens1-8]",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T03:08:56Z",
    "closed_at": "2025-08-25T03:53:00Z",
    "merged_at": "2025-08-25T03:52:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7194"
  },
  {
    "number": 7193,
    "title": "[None][fix] update skip case",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T03:04:53Z",
    "closed_at": "2025-08-26T04:31:49Z",
    "merged_at": "2025-08-26T04:31:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7193"
  },
  {
    "number": 7192,
    "title": "[None][chore] Update pre-merge test to add DeepSeek/LLaMA and gpt-oss",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T02:59:12Z",
    "closed_at": "2025-08-29T09:03:47Z",
    "merged_at": "2025-08-29T09:03:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7192"
  },
  {
    "number": 7191,
    "title": "[TRTLLM-7030][fix] BREAKING CHANGE: Mismatch between docs and actual commands",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T02:15:33Z",
    "closed_at": "2025-08-25T12:21:43Z",
    "merged_at": "2025-08-25T12:21:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7191"
  },
  {
    "number": 7190,
    "title": "[None][doc] fix tensorrt legacy quickstart page",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T00:31:28Z",
    "closed_at": "2025-08-25T11:30:51Z",
    "merged_at": "2025-08-25T11:30:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7190"
  },
  {
    "number": 7189,
    "title": "[https://nvbugs/5458798][fix] AD perf test outliers handling, tightened threshold, re-enabled in CI, fixed mem threshold",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-24T12:38:07Z",
    "closed_at": "2025-08-27T14:57:46Z",
    "merged_at": "2025-08-27T14:57:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7189"
  },
  {
    "number": 7188,
    "title": "[None][chore] Remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-24T12:19:08Z",
    "closed_at": "2025-08-27T10:06:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7188"
  },
  {
    "number": 7185,
    "title": "[None][fix] Waive test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-23T19:56:35Z",
    "closed_at": "2025-08-24T02:45:11Z",
    "merged_at": "2025-08-24T02:45:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7185"
  },
  {
    "number": 7184,
    "title": "[None][feat] Add GptOssParser for GPT-OSS model reasoning",
    "user": "mmangkad",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-23T15:11:47Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7184"
  },
  {
    "number": 7183,
    "title": "[None][opt] Balance the request based on number of tokens in AttentionDP",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-23T14:15:07Z",
    "closed_at": "2025-08-27T03:16:12Z",
    "merged_at": "2025-08-27T03:16:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7183"
  },
  {
    "number": 7182,
    "title": "[None][fix] Switch llm api quickstart example location per workflow.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-23T10:48:54Z",
    "closed_at": "2025-08-25T02:17:20Z",
    "merged_at": "2025-08-25T02:17:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7182"
  },
  {
    "number": 7180,
    "title": "[TRTLLM-7321][doc] Refine GPT-OSS doc",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T23:56:07Z",
    "closed_at": "2025-08-24T12:53:53Z",
    "merged_at": "2025-08-24T12:53:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7180"
  },
  {
    "number": 7179,
    "title": "[None][chore] Enable auto deploy accuracy test in CI",
    "user": "ajrasane",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T22:27:41Z",
    "closed_at": "2025-08-24T15:42:30Z",
    "merged_at": "2025-08-24T15:42:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7179"
  },
  {
    "number": 7178,
    "title": "[#6529][feat] CMake option to link statically with cublas/curand",
    "user": "WilliamTambellini",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T20:39:35Z",
    "closed_at": "2025-09-09T06:26:45Z",
    "merged_at": "2025-09-09T06:26:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7178"
  },
  {
    "number": 7177,
    "title": "[None][fix] Cherry-pick #7102 and #7230 to release/1.0",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T18:42:49Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7177"
  },
  {
    "number": 7176,
    "title": "[TRTLLM-6055][infra] Slurm Test refactor",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T17:07:11Z",
    "closed_at": "2025-10-20T16:46:44Z",
    "merged_at": "2025-10-20T16:46:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7176"
  },
  {
    "number": 7175,
    "title": "[None][fix] Remove the wheel from intermediate docker storage",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T16:31:49Z",
    "closed_at": "2025-08-27T15:32:17Z",
    "merged_at": "2025-08-27T15:32:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7175"
  },
  {
    "number": 7174,
    "title": "[https://nvbugs/5409416][fix] test_openai_multi_chat_example",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T15:24:35Z",
    "closed_at": "2025-08-26T09:33:58Z",
    "merged_at": "2025-08-26T09:33:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7174"
  },
  {
    "number": 7173,
    "title": "[https://nvbugs/5467062][fix] pass logitsPostProcessorBatched by reference",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T14:54:22Z",
    "closed_at": "2025-08-25T21:59:06Z",
    "merged_at": "2025-08-25T21:59:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7173"
  },
  {
    "number": 7172,
    "title": "[https://nvbugs/5467062][fix] pass logitsPostProcessorBatched by reference",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T14:51:25Z",
    "closed_at": "2025-08-22T23:32:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7172"
  },
  {
    "number": 7171,
    "title": "[None][chore] Mass integration of release/1.0 - 2nd",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T14:29:23Z",
    "closed_at": "2025-09-01T03:02:32Z",
    "merged_at": "2025-09-01T03:02:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7171"
  },
  {
    "number": 7170,
    "title": "[https://nvbugs/5461712] [fix] Disable deep_gemm for Qwen3 due to accuracy issues",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T13:08:41Z",
    "closed_at": "2025-08-23T09:26:13Z",
    "merged_at": "2025-08-23T09:26:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7170"
  },
  {
    "number": 7169,
    "title": "[None][ci] move more test cases of B200 to post merge",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T11:57:51Z",
    "closed_at": "2025-08-26T10:38:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7169"
  },
  {
    "number": 7168,
    "title": "[https://nvbugs/5467232][fix] Fix load_torch_hf_lora to override lora_config.trtllm_modules_to_hf_modules with default only when it has no value",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T10:27:16Z",
    "closed_at": "2025-08-25T07:37:58Z",
    "merged_at": "2025-08-25T07:37:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7168"
  },
  {
    "number": 7167,
    "title": "[None][chore] Bump version to 1.1.0rc2",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T10:13:22Z",
    "closed_at": "2025-08-22T14:02:29Z",
    "merged_at": "2025-08-22T14:02:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7167"
  },
  {
    "number": 7165,
    "title": "[None][ci] move all B200 TensorRT test cases to post merge",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T09:56:57Z",
    "closed_at": "2025-08-22T10:47:24Z",
    "merged_at": "2025-08-22T10:47:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7165"
  },
  {
    "number": 7162,
    "title": "[None][feat] Enable xqa jit path for previously precompiled case",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T09:24:55Z",
    "closed_at": "2025-09-26T08:47:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7162"
  },
  {
    "number": 7161,
    "title": "[TRTLLM-6393][feat] add static tree sampling and verification",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T09:06:04Z",
    "closed_at": "2025-09-26T17:16:17Z",
    "merged_at": "2025-09-26T17:16:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7161"
  },
  {
    "number": 7160,
    "title": "[None][test] add kv cache size in bench metric and fix failed cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T08:45:08Z",
    "closed_at": "2025-08-26T02:10:02Z",
    "merged_at": "2025-08-26T02:10:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7160"
  },
  {
    "number": 7159,
    "title": "[https://nvbugs/5473789][bug] install cuda-toolkit to fix sanity check",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T08:41:11Z",
    "closed_at": "2025-08-26T10:51:22Z",
    "merged_at": "2025-08-26T10:51:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7159"
  },
  {
    "number": 7158,
    "title": "[None][feat] add gpt-osss tests to sanity list",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T08:27:27Z",
    "closed_at": "2025-08-25T02:22:07Z",
    "merged_at": "2025-08-25T02:22:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7158"
  },
  {
    "number": 7157,
    "title": "[https://nvbugs/5474037][fix] Fix building tritonbuild/tritonrelease images",
    "user": "dbari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T07:58:09Z",
    "closed_at": "2025-08-22T14:47:38Z",
    "merged_at": "2025-08-22T14:47:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7157"
  },
  {
    "number": 7156,
    "title": "[https://nvbugs/5455140][chore] Add some CUDA graph memory logs to help locate an occasional OOM issue.",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T07:55:45Z",
    "closed_at": "2025-08-27T08:01:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7156"
  },
  {
    "number": 7155,
    "title": "[TRTLLM-6876][feat] Add low precision all2all for mnnvl",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T07:49:27Z",
    "closed_at": "2025-08-28T10:26:16Z",
    "merged_at": "2025-08-28T10:26:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7155"
  },
  {
    "number": 7154,
    "title": "[None][chore] Refactored the handle logits pp communication",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T07:47:02Z",
    "closed_at": "2025-08-25T20:14:08Z",
    "merged_at": "2025-08-25T20:14:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7154"
  },
  {
    "number": 7153,
    "title": "[None][debug]Update for debugging node 081",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T07:44:09Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7153"
  },
  {
    "number": 7152,
    "title": "[https://nvbugs/5453709][fix] Remove transformers version limit in Qwen2VL",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T07:35:59Z",
    "closed_at": "2025-09-09T02:38:20Z",
    "merged_at": "2025-09-09T02:38:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7152"
  },
  {
    "number": 7151,
    "title": "[https://nvbugs/5453727][fix] Fix bug of how GPT-OSS setup the parameters in CI",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T07:32:40Z",
    "closed_at": "2025-08-27T07:26:11Z",
    "merged_at": "2025-08-27T07:26:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7151"
  },
  {
    "number": 7150,
    "title": "[None][fix] Fix MoE load balancer config loading",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T07:26:06Z",
    "closed_at": "2025-08-25T05:42:55Z",
    "merged_at": "2025-08-25T05:42:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7150"
  },
  {
    "number": 7149,
    "title": "[TRTLLM-6825][fix] Update lora for phi4-mm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T06:13:12Z",
    "closed_at": "2025-08-23T12:57:00Z",
    "merged_at": "2025-08-23T12:57:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7149"
  },
  {
    "number": 7148,
    "title": "[None][fix] Fix spec_metadata calls in modeling_gpt_oss for spec-dec …",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T05:05:40Z",
    "closed_at": "2025-08-22T16:09:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7148"
  },
  {
    "number": 7147,
    "title": "[https://nvbugs/5473023] fix: pass logitsPostProcessorBatched by reference",
    "user": "tijyojwad",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T04:52:56Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7147"
  },
  {
    "number": 7144,
    "title": "[https://nvbugs/5449032][fix] Add more llm-args to llm_mgmn_trtllm_bench.sh",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-22T02:26:11Z",
    "closed_at": "2025-08-22T04:24:35Z",
    "merged_at": "2025-08-22T04:24:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7144"
  },
  {
    "number": 7143,
    "title": "[TRTLLM-7321][doc] Add GPT-OSS Deployment Guide into official doc site",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T23:14:38Z",
    "closed_at": "2025-08-22T08:17:01Z",
    "merged_at": "2025-08-22T08:17:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7143"
  },
  {
    "number": 7141,
    "title": "[#7136][feat] trtllm-serve + autodeploy integration",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T21:46:37Z",
    "closed_at": "2025-08-22T15:30:53Z",
    "merged_at": "2025-08-22T15:30:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7141"
  },
  {
    "number": 7140,
    "title": "[None][doc] add GPT OSS Eagle3 blog",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T21:16:34Z",
    "closed_at": "2025-09-03T16:28:01Z",
    "merged_at": "2025-09-03T16:28:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7140"
  },
  {
    "number": 7139,
    "title": "[https://nvbugs/5470840][fix] Disaggregated unit test MPI Init handling",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T20:13:04Z",
    "closed_at": "2025-08-25T23:48:39Z",
    "merged_at": "2025-08-25T23:48:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7139"
  },
  {
    "number": 7137,
    "title": "[None][docs] Update Dynasor paper info",
    "user": "AndyDai-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T19:36:00Z",
    "closed_at": "2025-08-30T01:47:47Z",
    "merged_at": "2025-08-30T01:47:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7137"
  },
  {
    "number": 7135,
    "title": "[None][docs] update stale link for AutoDeploy",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T17:09:14Z",
    "closed_at": "2025-08-22T01:41:44Z",
    "merged_at": "2025-08-22T01:41:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7135"
  },
  {
    "number": 7134,
    "title": "[None][refactor] Move draft token padding out of Drafter",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T17:06:09Z",
    "closed_at": "2025-08-27T09:07:51Z",
    "merged_at": "2025-08-27T09:07:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7134"
  },
  {
    "number": 7133,
    "title": "[None][fix] Fix the incorrect header file import in dataType.h",
    "user": "Fan-Yunfan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T16:04:09Z",
    "closed_at": "2025-09-11T00:59:04Z",
    "merged_at": "2025-09-11T00:59:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7133"
  },
  {
    "number": 7132,
    "title": "[https://nvbugs/5467232][fix] Fix load_torch_hf_lora to override lora_config.trtllm_modules_to_hf_modules with default only when it has no value",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T15:46:19Z",
    "closed_at": "2025-08-24T12:00:24Z",
    "merged_at": "2025-08-24T12:00:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7132"
  },
  {
    "number": 7130,
    "title": "[None][infra] Skip failed tests for release branch 08/21",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T12:49:22Z",
    "closed_at": "2025-08-21T16:09:51Z",
    "merged_at": "2025-08-21T16:09:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7130"
  },
  {
    "number": 7129,
    "title": "[None][infra] Waive failed case for main branch 08/21",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T12:35:03Z",
    "closed_at": "2025-08-21T16:08:55Z",
    "merged_at": "2025-08-21T16:08:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7129"
  },
  {
    "number": 7128,
    "title": "[TRTLLM-6741] [feat] enable LM tp for MTP, under attention dp case",
    "user": "Njuapp",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T11:32:25Z",
    "closed_at": "2025-09-17T01:42:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7128"
  },
  {
    "number": 7127,
    "title": "[https://nvbugs/5448426][fix] Fix illegal memory access in cuda graph",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T10:31:28Z",
    "closed_at": "2025-08-25T02:04:34Z",
    "merged_at": "2025-08-25T02:04:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7127"
  },
  {
    "number": 7126,
    "title": "[None][perf] Accelerate global scale calculations for deepEP fp4 combine",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T10:14:33Z",
    "closed_at": "2025-08-26T16:13:13Z",
    "merged_at": "2025-08-26T16:13:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7126"
  },
  {
    "number": 7125,
    "title": "[https://nvbugs/5460335][fix] Support phi4 fp8 in trt-backend",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T10:12:15Z",
    "closed_at": "2025-08-27T05:45:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7125"
  },
  {
    "number": 7124,
    "title": "[None][test] add deepseek r1/v3 model with chunked prefill cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T10:01:35Z",
    "closed_at": "2025-09-19T03:12:54Z",
    "merged_at": "2025-09-19T03:12:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7124"
  },
  {
    "number": 7123,
    "title": "[None][fix] DeepSeek-R1 W4A8 weight loading issue; fixes regression from #6200",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T09:59:36Z",
    "closed_at": "2025-09-06T16:04:57Z",
    "merged_at": "2025-09-06T16:04:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7123"
  },
  {
    "number": 7122,
    "title": "[https://nvbugs/5467981][fix] Fix Qwen2.5-VL fails with cuda graph padding",
    "user": "DylanChen-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T09:51:12Z",
    "closed_at": "2025-09-15T07:02:34Z",
    "merged_at": "2025-09-15T07:02:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7122"
  },
  {
    "number": 7121,
    "title": "[None] [chore] Make disagg example compatible with recommended usage",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T09:49:43Z",
    "closed_at": "2025-08-27T15:57:47Z",
    "merged_at": "2025-08-27T15:57:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7121"
  },
  {
    "number": 7120,
    "title": "[None][fix] Read eos_token_id from generation_config for kimi_k2",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T09:18:15Z",
    "closed_at": "2025-09-23T02:47:03Z",
    "merged_at": "2025-09-23T02:47:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7120"
  },
  {
    "number": 7119,
    "title": "[None][chore] remove CLI support for mamba cache dtype setting",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T09:08:11Z",
    "closed_at": "2025-08-25T12:08:52Z",
    "merged_at": "2025-08-25T12:08:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7119"
  },
  {
    "number": 7118,
    "title": "[None][fix] Fix mm_placholder_counts extraction issue.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T07:52:30Z",
    "closed_at": "2025-08-22T04:28:30Z",
    "merged_at": "2025-08-22T04:28:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7118"
  },
  {
    "number": 7117,
    "title": "[TRTLLM-7361][feat] KV cache transfer for uneven pp",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T07:32:13Z",
    "closed_at": "2025-09-08T17:37:46Z",
    "merged_at": "2025-09-08T17:37:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7117"
  },
  {
    "number": 7116,
    "title": "[None][feat] Hopper Fp8 context mla",
    "user": "zhou-yuxin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T07:25:38Z",
    "closed_at": "2025-08-26T09:10:21Z",
    "merged_at": "2025-08-26T09:10:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7116"
  },
  {
    "number": 7114,
    "title": "[None][fix] Fix a numerical stability issue for XQA with spec dec",
    "user": "lowsfer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T06:56:24Z",
    "closed_at": "2025-09-04T00:40:06Z",
    "merged_at": "2025-09-04T00:40:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7114"
  },
  {
    "number": 7113,
    "title": "[None][feat] Apply AutoTuner to fp8_block_scale_deep_gemm to trigger JIT ahead of time.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T06:52:19Z",
    "closed_at": "2025-08-25T02:48:32Z",
    "merged_at": "2025-08-25T02:48:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7113"
  },
  {
    "number": 7112,
    "title": "[None][feat] Make the should_use_spec_decode logic a bit smarter",
    "user": "zheyuf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T06:47:04Z",
    "closed_at": "2025-09-10T04:54:00Z",
    "merged_at": "2025-09-10T04:53:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7112"
  },
  {
    "number": 7111,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T06:41:03Z",
    "closed_at": "2025-08-25T01:24:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7111"
  },
  {
    "number": 7110,
    "title": "[https://nvbugs/5467062][fix] pass logitsPostProcessorBatched by reference",
    "user": "milesial",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T06:14:59Z",
    "closed_at": "2025-08-22T08:42:06Z",
    "merged_at": "2025-08-22T08:42:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7110"
  },
  {
    "number": 7109,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T05:38:08Z",
    "closed_at": "2025-08-22T02:39:25Z",
    "merged_at": "2025-08-22T02:39:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7109"
  },
  {
    "number": 7108,
    "title": "[TRTLLM-7245][feat] add test_multi_nodes_eval tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T05:02:30Z",
    "closed_at": "2025-08-22T09:17:27Z",
    "merged_at": "2025-08-22T09:17:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7108"
  },
  {
    "number": 7107,
    "title": "[None][feat] add Hopper FP8 context MLA",
    "user": "zhou-yuxin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T04:59:34Z",
    "closed_at": "2025-08-21T07:08:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7107"
  },
  {
    "number": 7106,
    "title": "[TRTLLM-7398][feat] Support KV cache salting for secure KV cache reuse",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T04:33:00Z",
    "closed_at": "2025-09-06T21:58:32Z",
    "merged_at": "2025-09-06T21:58:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7106"
  },
  {
    "number": 7105,
    "title": "[None][chore] Create PyExecutor from TorchLlmArgs Part 1",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T03:19:23Z",
    "closed_at": "2025-08-26T02:42:02Z",
    "merged_at": "2025-08-26T02:42:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7105"
  },
  {
    "number": 7104,
    "title": "[None][perf] Disable Swap AB when num tokens exceeds N dimension",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-21T01:58:49Z",
    "closed_at": "2025-08-29T01:29:56Z",
    "merged_at": "2025-08-29T01:29:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7104"
  },
  {
    "number": 7102,
    "title": "[None][fix] Correct KV cache percentage report out.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T22:25:49Z",
    "closed_at": "2025-08-22T17:28:57Z",
    "merged_at": "2025-08-22T17:28:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7102"
  },
  {
    "number": 7101,
    "title": "[None][doc] Update gpt-oss deployment guide to latest release image",
    "user": "farshadghodsian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T22:02:32Z",
    "closed_at": "2025-08-21T06:33:08Z",
    "merged_at": "2025-08-21T06:33:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7101"
  },
  {
    "number": 7100,
    "title": "[TRTLLM-7353][feat] Implement capturable drafting loops for speculation",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T19:58:11Z",
    "closed_at": "2025-09-01T18:37:44Z",
    "merged_at": "2025-09-01T18:37:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7100"
  },
  {
    "number": 7099,
    "title": "[https://nvbugs/5434320][bug] Fix disagg pp bug",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T19:11:57Z",
    "closed_at": "2025-08-27T17:38:01Z",
    "merged_at": "2025-08-27T17:38:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7099"
  },
  {
    "number": 7098,
    "title": "[None][fix] fixing the math on asymmetric tp+pp tests",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T18:14:38Z",
    "closed_at": "2025-09-07T18:27:46Z",
    "merged_at": "2025-09-07T18:27:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7098"
  },
  {
    "number": 7097,
    "title": "[None][chore] cherry-pick 6940",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T15:40:13Z",
    "closed_at": "2025-08-25T02:28:46Z",
    "merged_at": "2025-08-25T02:28:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7097"
  },
  {
    "number": 7096,
    "title": "[None][infra] Add build configs for tritonrelease",
    "user": "dbari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T15:11:00Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7096"
  },
  {
    "number": 7095,
    "title": "[None][feat] Allow building release images from downloaded wheel",
    "user": "dbari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T14:33:39Z",
    "closed_at": "2025-08-25T14:22:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7095"
  },
  {
    "number": 7093,
    "title": "[None][ci] move some tests of b200 to post merge",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T09:54:36Z",
    "closed_at": "2025-08-20T23:43:41Z",
    "merged_at": "2025-08-20T23:43:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7093"
  },
  {
    "number": 7092,
    "title": "[None][infra] Waive failed tests on main branch 8/20",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T09:41:58Z",
    "closed_at": "2025-08-20T10:33:46Z",
    "merged_at": "2025-08-20T10:33:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7092"
  },
  {
    "number": 7090,
    "title": "[#6901][fix] Breaking Change: disable nvtx annotation by default.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T08:54:06Z",
    "closed_at": "2025-09-30T08:00:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7090"
  },
  {
    "number": 7089,
    "title": "[TRTLLM-6960][fix] replace flasky scaled_mm test with more stable config",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T08:45:12Z",
    "closed_at": "2025-08-27T03:58:34Z",
    "merged_at": "2025-08-27T03:58:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7089"
  },
  {
    "number": 7088,
    "title": "[TRTLLM-7458][infra] Retry test stage",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T08:36:57Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7088"
  },
  {
    "number": 7087,
    "title": "[None][fix] Use safeInitRowMax instead of fp32_lowest to avoid NaN",
    "user": "lowsfer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T08:36:09Z",
    "closed_at": "2025-08-21T05:12:22Z",
    "merged_at": "2025-08-21T05:12:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7087"
  },
  {
    "number": 7086,
    "title": "[TRTLLM-5950][infra] Removing remaining turtle keywords from the code base",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T08:19:02Z",
    "closed_at": "2025-09-07T06:26:19Z",
    "merged_at": "2025-09-07T06:26:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7086"
  },
  {
    "number": 7084,
    "title": "[None][chore] waive failed cases on H100",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T07:45:42Z",
    "closed_at": "2025-08-21T03:15:23Z",
    "merged_at": "2025-08-21T03:15:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7084"
  },
  {
    "number": 7083,
    "title": "[TRTLLM-7363][test] Add 8-GPU test cases for RTX6000",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T07:37:44Z",
    "closed_at": "2025-09-03T12:36:52Z",
    "merged_at": "2025-09-03T12:36:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7083"
  },
  {
    "number": 7081,
    "title": "[None][feat] Add Tencent HunYuanDenseV1 model support",
    "user": "sorenwu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T07:19:03Z",
    "closed_at": "2025-09-23T01:27:29Z",
    "merged_at": "2025-09-23T01:27:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7081"
  },
  {
    "number": 7079,
    "title": "[TRTLLM-7335][infra] use JobBuilder to trigger downstream job",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T07:16:52Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7079"
  },
  {
    "number": 7077,
    "title": "[None][fix] update accelerate dependency to 1.7+ for AutoDeploy",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T07:00:32Z",
    "closed_at": "2025-08-21T02:52:37Z",
    "merged_at": "2025-08-21T02:52:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7077"
  },
  {
    "number": 7076,
    "title": "[https://nvbugs/5443039][fix] Fix AutoDeploy pattern matcher for torch 2.8",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T06:59:11Z",
    "closed_at": "2025-08-21T05:14:52Z",
    "merged_at": "2025-08-21T05:14:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7076"
  },
  {
    "number": 7075,
    "title": "[https://nvbugs/5440241][fix] Fix 70B GSM8K Accuracy drop",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T06:47:47Z",
    "closed_at": "2025-08-20T22:11:00Z",
    "merged_at": "2025-08-20T22:11:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7075"
  },
  {
    "number": 7074,
    "title": "[None][infra] Split DGX_B200 stage into multiple parts and pre-/post-merge",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T05:59:49Z",
    "closed_at": "2025-08-25T01:09:04Z",
    "merged_at": "2025-08-25T01:09:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7074"
  },
  {
    "number": 7073,
    "title": "[None][infra] Prepare for single GPU GB200 test pipeline",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T05:42:55Z",
    "closed_at": "2025-08-24T13:46:40Z",
    "merged_at": "2025-08-24T13:46:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7073"
  },
  {
    "number": 7072,
    "title": "[None][fix] Fix W4A8 MoE kernel issue",
    "user": "yuhyao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T04:57:27Z",
    "closed_at": "2025-08-20T10:52:47Z",
    "merged_at": "2025-08-20T10:52:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7072"
  },
  {
    "number": 7071,
    "title": "[None][test] Add MNNVL to unittest and speed up weight creation",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T04:14:47Z",
    "closed_at": "2025-09-02T06:10:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7071"
  },
  {
    "number": 7070,
    "title": "[None][fix] fix scaffolding dynasor test",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T03:17:07Z",
    "closed_at": "2025-08-20T07:20:47Z",
    "merged_at": "2025-08-20T07:20:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7070"
  },
  {
    "number": 7069,
    "title": "model generate wrong text like � when utf8 decode error in some languages like chinese",
    "user": "flycloud-hz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T03:12:36Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7069"
  },
  {
    "number": 7068,
    "title": "[https://nvbugs/5392414] [fix] For release 1.0 cherry pick. Add customized default routing method",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T03:01:01Z",
    "closed_at": "2025-08-21T12:06:50Z",
    "merged_at": "2025-08-21T12:06:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7068"
  },
  {
    "number": 7067,
    "title": "[None][test] add l20 specific qa test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T02:58:37Z",
    "closed_at": "2025-08-25T04:44:08Z",
    "merged_at": "2025-08-25T04:44:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7067"
  },
  {
    "number": 7066,
    "title": "[None][infra] Test machine",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T02:48:04Z",
    "closed_at": "2025-09-09T02:28:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7066"
  },
  {
    "number": 7065,
    "title": "[None][fix] Fix Spec-Dec XQA kernel multiblock mode NaN issue",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T00:42:03Z",
    "closed_at": "2025-08-29T17:26:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7065"
  },
  {
    "number": 7064,
    "title": "[None][mock] [do not review] pr to check if pr-checklist action is working",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-20T00:06:20Z",
    "closed_at": "2025-08-27T18:36:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7064"
  },
  {
    "number": 7063,
    "title": "[None][chore] No-op changes to support context parallelism in disaggregated serving later",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T23:40:16Z",
    "closed_at": "2025-08-21T15:21:28Z",
    "merged_at": "2025-08-21T15:21:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7063"
  },
  {
    "number": 7062,
    "title": "[None][chore] Add kv cache retention config in pytorch runtime",
    "user": "xuanzic",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T22:31:15Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7062"
  },
  {
    "number": 7061,
    "title": "[None][mock] [do not review] pr to check if pr-checklist action is working",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T22:14:14Z",
    "closed_at": "2025-08-20T00:05:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7061"
  },
  {
    "number": 7060,
    "title": "[https://nvbugs/5369366] [fix] Report failing requests",
    "user": "arekay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T19:26:08Z",
    "closed_at": "2025-09-04T19:56:23Z",
    "merged_at": "2025-09-04T19:56:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7060"
  },
  {
    "number": 7059,
    "title": "[None][infra] \"[TRTLLM-6960][fix] enable scaled_mm tests (#6936)\"",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T19:10:31Z",
    "closed_at": "2025-08-20T05:45:10Z",
    "merged_at": "2025-08-20T05:45:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7059"
  },
  {
    "number": 7058,
    "title": "[None][chore] Remove test_fp4_quantize_gemm_torch_profiling",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T18:48:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7058"
  },
  {
    "number": 7057,
    "title": "[#4403][refactor] Move fusion, kvcache, and compile to modular inference optimizer",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T18:09:20Z",
    "closed_at": "2025-08-21T17:30:36Z",
    "merged_at": "2025-08-21T17:30:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7057"
  },
  {
    "number": 7056,
    "title": "[None][doc] update v1.0 doc for trtllm-serve",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T17:43:06Z",
    "closed_at": "2025-08-20T17:57:21Z",
    "merged_at": "2025-08-20T17:57:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7056"
  },
  {
    "number": 7055,
    "title": "Draft: [None][doc] add configs for perf benchmarking",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T16:56:23Z",
    "closed_at": "2025-08-26T04:25:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7055"
  },
  {
    "number": 7054,
    "title": "[None][autodeploy] Add group attention pattern that supports attention masks",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T16:06:14Z",
    "closed_at": "2025-08-19T22:57:10Z",
    "merged_at": "2025-08-19T22:57:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7054"
  },
  {
    "number": 7053,
    "title": "[TRTLLM-7263][fix] Prevent recreation of cublas handles in lora_grouped_gemm every call",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T14:45:02Z",
    "closed_at": "2025-08-20T10:41:20Z",
    "merged_at": "2025-08-20T10:41:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7053"
  },
  {
    "number": 7048,
    "title": "[None][feat]optimize trtllm-gen 2CTA MLA",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T14:28:05Z",
    "closed_at": "2025-09-04T09:59:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7048"
  },
  {
    "number": 7044,
    "title": "[None][chore] Remove duplicate test waives",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T12:54:08Z",
    "closed_at": "2025-08-19T13:04:31Z",
    "merged_at": "2025-08-19T13:04:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7044"
  },
  {
    "number": 7043,
    "title": "[https://nvbugs/5451296][bug] Cherry-pick #7017 from release/1.0 branch",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T12:46:29Z",
    "closed_at": "2025-08-19T15:25:05Z",
    "merged_at": "2025-08-19T15:25:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7043"
  },
  {
    "number": 7042,
    "title": "[TRTLLM-6604][fix] Remove LoRA NemotronNAS Test Waiver",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T12:42:08Z",
    "closed_at": "2025-08-20T11:54:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7042"
  },
  {
    "number": 7041,
    "title": "[TRTLLM-7153] [feat] Move stop_criteria to sample_async",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T12:29:32Z",
    "closed_at": "2025-09-07T14:36:49Z",
    "merged_at": "2025-09-07T14:36:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7041"
  },
  {
    "number": 7037,
    "title": "[None][infra] Waive failed tests on main 08/19",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T11:37:02Z",
    "closed_at": "2025-08-19T13:31:07Z",
    "merged_at": "2025-08-19T13:31:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7037"
  },
  {
    "number": 7036,
    "title": "[None][infra] Waive failed tests  for release branch 08/19",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T11:36:30Z",
    "closed_at": "2025-08-19T12:42:47Z",
    "merged_at": "2025-08-19T12:42:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7036"
  },
  {
    "number": 7035,
    "title": "[TRTLLM-7348] [feat] Enable Cross-Attention to use XQA kernels for Whisper",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T10:19:15Z",
    "closed_at": "2025-08-20T14:11:26Z",
    "merged_at": "2025-08-20T14:11:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7035"
  },
  {
    "number": 7034,
    "title": "[https://nvbugs/5445466][fix] unwaive DSR1-fp4 latency and latency_trtllmgen",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T10:02:17Z",
    "closed_at": "2025-09-01T02:34:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7034"
  },
  {
    "number": 7033,
    "title": "[TRTLLM-7346][fix] Improve performance of PyTorchModelEngine._get_lora_params_from_requests",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T09:59:15Z",
    "closed_at": "2025-08-25T07:37:41Z",
    "merged_at": "2025-08-25T07:37:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7033"
  },
  {
    "number": 7031,
    "title": "[https://nvbugs/5451342][fix] Use runtime max_batch_size when cuda_graph_config.max_batch_size is not provided in trtllm-bench",
    "user": "jiaganc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T08:57:28Z",
    "closed_at": "2025-08-26T12:10:35Z",
    "merged_at": "2025-08-26T12:10:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7031"
  },
  {
    "number": 7030,
    "title": "[None][fix] fix llmapi import error",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T08:31:23Z",
    "closed_at": "2025-08-20T02:58:14Z",
    "merged_at": "2025-08-20T02:58:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7030"
  },
  {
    "number": 7029,
    "title": "[None] [doc] Add more documents for large scale EP",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T08:11:10Z",
    "closed_at": "2025-08-19T11:04:40Z",
    "merged_at": "2025-08-19T11:04:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7029"
  },
  {
    "number": 7028,
    "title": "[https://nvbugs/5457504][fix] fix kv cache event test in disaggregated worker tests",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T08:10:01Z",
    "closed_at": "2025-08-26T06:25:11Z",
    "merged_at": "2025-08-26T06:25:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7028"
  },
  {
    "number": 7027,
    "title": "[https://nvbugs/5461980][draft] torch.cuda.event.synchronize() hangs",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T07:59:40Z",
    "closed_at": "2025-09-02T06:52:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7027"
  },
  {
    "number": 7026,
    "title": "[None][chore] Only check the bindings lib for current build",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T07:05:14Z",
    "closed_at": "2025-08-20T18:17:17Z",
    "merged_at": "2025-08-20T18:17:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7026"
  },
  {
    "number": 7025,
    "title": "[TRTLLM-7096][infra] Testing cache transmission functionality in Python",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T07:03:10Z",
    "closed_at": "2025-08-25T01:47:40Z",
    "merged_at": "2025-08-25T01:47:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7025"
  },
  {
    "number": 7024,
    "title": "[None][infra] Upgrade UCX to v1.19.x and NIXL to 0.5.0",
    "user": "BatshevaBlack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T07:02:41Z",
    "closed_at": "2025-08-21T02:49:55Z",
    "merged_at": "2025-08-21T02:49:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7024"
  },
  {
    "number": 7023,
    "title": "[None][infra] Upgrade UCX to v1.19.x and NIXL to 0.5.0",
    "user": "BatshevaBlack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T07:02:09Z",
    "closed_at": "2025-08-19T07:35:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7023"
  },
  {
    "number": 7022,
    "title": "[https://nvbugs/5455140][fix] unwaive DSR1-fp4 throughput_tp8",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T06:34:26Z",
    "closed_at": "2025-08-19T12:48:05Z",
    "merged_at": "2025-08-19T12:48:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7022"
  },
  {
    "number": 7021,
    "title": "[None][chore] Wrap the swiglu into custom op to avoid redundant device copy.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T05:57:05Z",
    "closed_at": "2025-08-27T05:02:10Z",
    "merged_at": "2025-08-27T05:02:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7021"
  },
  {
    "number": 7020,
    "title": "[None] [feat] nsys profile output kernel classifier",
    "user": "gracehonv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T05:00:05Z",
    "closed_at": "2025-08-23T04:57:37Z",
    "merged_at": "2025-08-23T04:57:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7020"
  },
  {
    "number": 7019,
    "title": "[https://nvbugs/5451296][fix] zmq nonblock bug with retry",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T02:55:43Z",
    "closed_at": "2025-08-21T00:34:47Z",
    "merged_at": "2025-08-21T00:34:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7019"
  },
  {
    "number": 7018,
    "title": "[None][fix] Fix multimodal mrope propagation on generation phase",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T00:53:47Z",
    "closed_at": "2025-09-18T05:34:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7018"
  },
  {
    "number": 7017,
    "title": "[https://nvbugs/5451296][bug] Fix a thread leak in test_llm_args.py",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T00:47:07Z",
    "closed_at": "2025-08-19T11:19:54Z",
    "merged_at": "2025-08-19T11:19:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7017"
  },
  {
    "number": 7016,
    "title": "[https://nvbugs/5463720][fix]: Naively handle per-layer MLP dimensions for Nemotron 49B",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-19T00:01:08Z",
    "closed_at": "2025-08-30T00:25:18Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7016"
  },
  {
    "number": 7015,
    "title": "[None][chore] Update namelist in blossom-ci",
    "user": "karljang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T23:52:50Z",
    "closed_at": "2025-08-20T07:29:02Z",
    "merged_at": "2025-08-20T07:29:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7015"
  },
  {
    "number": 7014,
    "title": "[https://nvbugs/5464088] [fix] Guard against fp8 activations in lora forward; update perf test config",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T23:19:39Z",
    "closed_at": "2025-08-21T12:28:55Z",
    "merged_at": "2025-08-21T12:28:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7014"
  },
  {
    "number": 7013,
    "title": "[None][feat] Skip prefetching consolidated safetensors when appropriate",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T22:53:21Z",
    "closed_at": "2025-08-26T03:56:21Z",
    "merged_at": "2025-08-26T03:56:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7013"
  },
  {
    "number": 7012,
    "title": "[None][feat] Save state first pass",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T20:16:43Z",
    "closed_at": "2025-10-01T22:40:55Z",
    "merged_at": "2025-10-01T22:40:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7012"
  },
  {
    "number": 7011,
    "title": "[https://nvbugs/5454875][ci] Unwaive Mistral Small 3.1 test",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T19:56:15Z",
    "closed_at": "2025-08-19T04:32:15Z",
    "merged_at": "2025-08-19T04:32:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7011"
  },
  {
    "number": 7010,
    "title": "[https://nvbugs/5448442][fix] Skip trtllm moe backend for sm120",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T19:37:28Z",
    "closed_at": "2025-08-21T17:34:07Z",
    "merged_at": "2025-08-21T17:34:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7010"
  },
  {
    "number": 7009,
    "title": "test package removal in container",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T19:36:32Z",
    "closed_at": "2025-08-20T18:32:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7009"
  },
  {
    "number": 7008,
    "title": "[https://nvbugs/5462007][ci] Unwaive Mistral Small 3.1 FP8 test",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T18:33:55Z",
    "closed_at": "2025-08-18T23:50:04Z",
    "merged_at": "2025-08-18T23:50:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7008"
  },
  {
    "number": 7007,
    "title": "[None][autodeploy] Doc: fix link path in trtllm bench doc",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T17:55:45Z",
    "closed_at": "2025-08-19T00:43:29Z",
    "merged_at": "2025-08-19T00:43:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7007"
  },
  {
    "number": 7005,
    "title": "[None][infra] Cherry-pick #6836 from main branch and improve SSH connection (#6971)",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T17:23:28Z",
    "closed_at": "2025-08-18T17:35:30Z",
    "merged_at": "2025-08-18T17:35:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7005"
  },
  {
    "number": 7004,
    "title": "[https://nvbugs/5444937][chore] Fixing KV events tests",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T15:25:24Z",
    "closed_at": "2025-08-19T15:18:04Z",
    "merged_at": "2025-08-19T15:18:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7004"
  },
  {
    "number": 7003,
    "title": "[None][fix] Fix build of tritonbuild/tritonrelease image",
    "user": "dbari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T15:10:54Z",
    "closed_at": "2025-08-20T14:57:39Z",
    "merged_at": "2025-08-20T14:57:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7003"
  },
  {
    "number": 7002,
    "title": "Draft: [AD] Enabled test_trtllm_bench_backend_comparison on L0 of B200 and A30",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T14:07:44Z",
    "closed_at": "2025-09-30T14:58:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7002"
  },
  {
    "number": 7001,
    "title": "[TRTLLM-6746][feat] Enable two-model spec dec for MTP Eagle",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T13:38:25Z",
    "closed_at": "2025-09-18T16:05:37Z",
    "merged_at": "2025-09-18T16:05:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7001"
  },
  {
    "number": 7000,
    "title": "[https://nvbugs/5437405][fix] qwen3 235b eagle3 ci",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T13:28:02Z",
    "closed_at": "2025-08-21T05:17:32Z",
    "merged_at": "2025-08-21T05:17:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7000"
  },
  {
    "number": 6999,
    "title": "[None][chore] Remove duplicate test waives",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T13:27:36Z",
    "closed_at": "2025-08-18T14:04:44Z",
    "merged_at": "2025-08-18T14:04:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6999"
  },
  {
    "number": 6998,
    "title": "[None][chore] Remove duplicate test waives",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T13:05:21Z",
    "closed_at": "2025-08-18T13:15:49Z",
    "merged_at": "2025-08-18T13:15:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6998"
  },
  {
    "number": 6997,
    "title": "[https://nvbugs/5458798][fix] Relaxed test threshold, added documentation",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T12:58:56Z",
    "closed_at": "2025-08-19T07:24:04Z",
    "merged_at": "2025-08-19T07:24:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6997"
  },
  {
    "number": 6996,
    "title": "[https://nvbugs/5458874][fix] Fix Nemotron-H flaky CUDA graph / overlap scheduler test",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T12:30:21Z",
    "closed_at": "2025-08-19T12:45:06Z",
    "merged_at": "2025-08-19T12:45:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6996"
  },
  {
    "number": 6995,
    "title": "[None] [chore] Update wide-ep genonly scripts",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T12:13:04Z",
    "closed_at": "2025-08-19T11:44:08Z",
    "merged_at": "2025-08-19T11:44:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6995"
  },
  {
    "number": 6993,
    "title": "[None][infra] Waive failed tests for release branch 0818",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T11:16:55Z",
    "closed_at": "2025-08-18T12:31:50Z",
    "merged_at": "2025-08-18T12:31:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6993"
  },
  {
    "number": 6992,
    "title": "[None][infra] Waive failed tests on main 0818",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T10:55:54Z",
    "closed_at": "2025-08-18T12:34:52Z",
    "merged_at": "2025-08-18T12:34:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6992"
  },
  {
    "number": 6991,
    "title": "[https://nvbugs/5457489][fix] unwaive some tests",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T09:34:03Z",
    "closed_at": "2025-08-21T00:49:58Z",
    "merged_at": "2025-08-21T00:49:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6991"
  },
  {
    "number": 6990,
    "title": "[https://nvbugs/5450074][fix] Reduce the device memory requirements for testing",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T09:24:07Z",
    "closed_at": "2025-08-22T09:33:31Z",
    "merged_at": "2025-08-22T09:33:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6990"
  },
  {
    "number": 6989,
    "title": "[TRTLLM-7205][feat] add llama4 tp4 tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T09:20:55Z",
    "closed_at": "2025-08-20T05:22:06Z",
    "merged_at": "2025-08-20T05:22:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6989"
  },
  {
    "number": 6987,
    "title": "[https://nvbugs/5451028][fix] Constrain NemotronSuper test parameters…",
    "user": "Naveassaf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T09:11:36Z",
    "closed_at": "2025-08-19T07:19:51Z",
    "merged_at": "2025-08-19T07:19:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6987"
  },
  {
    "number": 6986,
    "title": "[TRTLLM-6139][infra] Add b300 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T09:09:07Z",
    "closed_at": "2025-09-25T06:23:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6986"
  },
  {
    "number": 6985,
    "title": "[TRTLLM-6549][feat] add perf metrics endpoint to openai server and openai disagg server",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T09:04:09Z",
    "closed_at": "2025-08-26T07:34:44Z",
    "merged_at": "2025-08-26T07:34:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6985"
  },
  {
    "number": 6984,
    "title": "[https://nvbugs/5453827][fix] Fix RPATH of th_common shared library to find pip-installed NCCL",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T08:59:10Z",
    "closed_at": "2025-08-21T09:58:30Z",
    "merged_at": "2025-08-21T09:58:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6984"
  },
  {
    "number": 6983,
    "title": "[None] [fix] Fix the macro name",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T06:37:51Z",
    "closed_at": "2025-08-18T07:08:32Z",
    "merged_at": "2025-08-18T07:08:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6983"
  },
  {
    "number": 6982,
    "title": "[https://nvbugs/5442827][fix] Unwaive DSR1-fp4 throughput_tp8 test",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T05:44:32Z",
    "closed_at": "2025-08-19T07:11:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6982"
  },
  {
    "number": 6981,
    "title": "[TRTLLM-7030][fix] uppercase def value in pd-config",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T05:42:45Z",
    "closed_at": "2025-08-18T06:33:23Z",
    "merged_at": "2025-08-18T06:33:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6981"
  },
  {
    "number": 6980,
    "title": "[None][fix] Refactor triton paddings",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T05:30:31Z",
    "closed_at": "2025-10-15T19:59:01Z",
    "merged_at": "2025-10-15T19:59:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6980"
  },
  {
    "number": 6978,
    "title": "[https://nvbugs/5427801][fix] Torch compile support for Llama4 and Ea…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T03:46:11Z",
    "closed_at": "2025-08-20T07:06:57Z",
    "merged_at": "2025-08-20T07:06:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6978"
  },
  {
    "number": 6976,
    "title": "[https://nvbugs/5448767][fix] fix mpi4py deadlocks in pp event-loop",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T03:27:16Z",
    "closed_at": "2025-08-27T06:01:48Z",
    "merged_at": "2025-08-27T06:01:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6976"
  },
  {
    "number": 6975,
    "title": "[https://nvbugs/5394392][fix] Enlarge scheduler capacity under disagg bs == 1",
    "user": "yifeizhang-c",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T03:21:31Z",
    "closed_at": "2025-08-20T08:32:28Z",
    "merged_at": "2025-08-20T08:32:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6975"
  },
  {
    "number": 6974,
    "title": "[None][fix] Fix bug of prompt and output token length of the RandomDataset under --random-token-ids option",
    "user": "samuellees",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T02:01:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6974"
  },
  {
    "number": 6973,
    "title": "[TRTLLM-6743][feat] Optimize and refactor alltoall in WideEP",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T01:42:42Z",
    "closed_at": "2025-08-24T12:15:30Z",
    "merged_at": "2025-08-24T12:15:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6973"
  },
  {
    "number": 6972,
    "title": "[TRTLLM-6342][feat] TP Sharding read from the model config",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T01:38:24Z",
    "closed_at": "2025-08-25T22:41:28Z",
    "merged_at": "2025-08-25T22:41:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6972"
  },
  {
    "number": 6971,
    "title": "[None][infra] Cherry-pick #6836 from main branch and improve SSH connection",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-18T01:06:55Z",
    "closed_at": "2025-08-18T17:11:11Z",
    "merged_at": "2025-08-18T17:11:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6971"
  },
  {
    "number": 6970,
    "title": "[https://nvbugs/5451028][fix] Constrain NemotronSuper test parameters to prevent OOMs",
    "user": "Naveassaf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-17T13:53:31Z",
    "closed_at": "2025-08-17T17:38:37Z",
    "merged_at": "2025-08-17T17:38:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6970"
  },
  {
    "number": 6969,
    "title": "[None][chore] Remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-17T12:16:02Z",
    "closed_at": "2025-08-19T08:01:33Z",
    "merged_at": "2025-08-19T08:01:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6969"
  },
  {
    "number": 6968,
    "title": "[TRTLLM-7263][fix] Prevent recreation of cublas handles in lora_grouped_gemm every call",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-17T11:22:49Z",
    "closed_at": "2025-08-19T12:39:57Z",
    "merged_at": "2025-08-19T12:39:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6968"
  },
  {
    "number": 6967,
    "title": "[https://nvbugs/5440241][fix] Fix 70B GSM8K Accuracy drop",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-17T10:51:45Z",
    "closed_at": "2025-08-25T14:09:30Z",
    "merged_at": "2025-08-25T14:09:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6967"
  },
  {
    "number": 6966,
    "title": "[None][infra] Upgrade UCX to v1.19.x and NIXL to 0.5.0",
    "user": "BatshevaBlack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-17T08:38:17Z",
    "closed_at": "2025-08-21T04:40:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6966"
  },
  {
    "number": 6965,
    "title": "[https://nvbugs/5390853][fix] Fix _test_openai_lora.py - disable cuda graph",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-17T08:36:41Z",
    "closed_at": "2025-08-17T13:56:17Z",
    "merged_at": "2025-08-17T13:56:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6965"
  },
  {
    "number": 6964,
    "title": "[None][chore] Mass integration of release/1.0 keep commit history linear",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-17T06:22:37Z",
    "closed_at": "2025-08-18T02:45:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6964"
  },
  {
    "number": 6963,
    "title": "[None][infra] Fix workflow for stale issues and PRs",
    "user": "karljang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-16T22:54:08Z",
    "closed_at": "2025-08-18T16:51:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6963"
  },
  {
    "number": 6961,
    "title": "Fix/llm shutdown",
    "user": "Tushar-ml",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-16T10:04:48Z",
    "closed_at": "2025-08-16T10:05:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6961"
  },
  {
    "number": 6960,
    "title": "Update pull_request_template.md",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-16T02:49:19Z",
    "closed_at": "2025-08-19T22:11:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6960"
  },
  {
    "number": 6959,
    "title": "[None] infra: add PR template checklist and action to require it",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-16T02:28:55Z",
    "closed_at": "2025-08-27T18:54:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6959"
  },
  {
    "number": 6957,
    "title": "[None][fix] Fix llama4 multimodal by skipping request validation",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T20:58:30Z",
    "closed_at": "2025-08-21T01:58:54Z",
    "merged_at": "2025-08-21T01:58:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6957"
  },
  {
    "number": 6956,
    "title": "[https://nvbugs/5453667] [fix] reverting a breaking change: make trtllm-bench `enable_chunked_context` defaults backend-dependent",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T17:50:09Z",
    "closed_at": "2025-08-16T04:29:03Z",
    "merged_at": "2025-08-16T04:29:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6956"
  },
  {
    "number": 6955,
    "title": "[https://nvbugs/5412562][feat] Allocate MoE workspace only when necessary (release/1.0 retargeted)",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T16:28:19Z",
    "closed_at": "2025-08-18T00:50:36Z",
    "merged_at": "2025-08-18T00:50:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6955"
  },
  {
    "number": 6954,
    "title": "[None][doc] Update gpt oss doc",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T16:11:41Z",
    "closed_at": "2025-08-18T05:27:30Z",
    "merged_at": "2025-08-18T05:27:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6954"
  },
  {
    "number": 6953,
    "title": "[None][chore] Bump version to 1.1.0rc1",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T16:04:51Z",
    "closed_at": "2025-08-16T02:32:47Z",
    "merged_at": "2025-08-16T02:32:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6953"
  },
  {
    "number": 6952,
    "title": "[https://nvbugs/5401114][fix] Unwaive Gemma3 tests",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T16:00:16Z",
    "closed_at": "2025-08-15T23:35:02Z",
    "merged_at": "2025-08-15T23:35:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6952"
  },
  {
    "number": 6951,
    "title": "[None][infra] Waive failed cases in main branch",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T15:04:34Z",
    "closed_at": "2025-08-17T11:27:59Z",
    "merged_at": "2025-08-17T11:27:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6951"
  },
  {
    "number": 6950,
    "title": "[https://nvbugs/5405041][fix] Update wide ep doc",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T14:32:29Z",
    "closed_at": "2025-08-17T02:09:01Z",
    "merged_at": "2025-08-17T02:09:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6950"
  },
  {
    "number": 6949,
    "title": "[https://nvbugs/5405041][fix] Update wide ep doc",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T14:27:05Z",
    "closed_at": "2025-08-15T14:28:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6949"
  },
  {
    "number": 6948,
    "title": "[TRTLLM-7028][feat] Enable guided decoding with speculative decoding (part 2: one-model engine)",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T13:56:06Z",
    "closed_at": "2025-09-03T22:16:11Z",
    "merged_at": "2025-09-03T22:16:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6948"
  },
  {
    "number": 6947,
    "title": "[https://nvbugs/5453709][fix] Make GptOssConfig as optional importing",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T11:23:40Z",
    "closed_at": "2025-08-19T04:27:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6947"
  },
  {
    "number": 6946,
    "title": "[https://nvbugs/5394685][fix] proper fix for the accuracy issue in 2CTA MLA kernels (release 1.0)",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T10:04:46Z",
    "closed_at": "2025-08-19T07:10:30Z",
    "merged_at": "2025-08-19T07:10:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6946"
  },
  {
    "number": 6945,
    "title": "[None][infra] update feature_combination_matrix of disaggregated and Eagle3",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T09:50:16Z",
    "closed_at": "2025-08-18T01:18:17Z",
    "merged_at": "2025-08-18T01:18:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6945"
  },
  {
    "number": 6944,
    "title": "[None][chore] unwaive test_disaggregated_genbs1",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T09:13:54Z",
    "closed_at": "2025-08-20T01:57:36Z",
    "merged_at": "2025-08-20T01:57:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6944"
  },
  {
    "number": 6943,
    "title": "[None][ci] unwaive test_ptp_star_attention_example",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T09:11:40Z",
    "closed_at": "2025-08-15T09:33:25Z",
    "merged_at": "2025-08-15T09:33:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6943"
  },
  {
    "number": 6942,
    "title": "[TRTLLM-7255][feat] Add iteration log parser script for benchmark log",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T08:21:10Z",
    "closed_at": "2025-10-20T05:34:53Z",
    "merged_at": "2025-10-20T05:34:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6942"
  },
  {
    "number": 6941,
    "title": "[https://nvbugs/5394685][fix] proper fix for the accuracy issue in 2CTA MLA kernels",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T08:16:34Z",
    "closed_at": "2025-08-15T15:29:36Z",
    "merged_at": "2025-08-15T15:29:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6941"
  },
  {
    "number": 6940,
    "title": "[https://nvbugs/5448437][fix] fix some nixl tests",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T08:06:50Z",
    "closed_at": "2025-08-20T06:19:49Z",
    "merged_at": "2025-08-20T06:19:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6940"
  },
  {
    "number": 6939,
    "title": "[TRTLLM-6541][test] Add NIM Related Cases [StarCoder2_7B] and [Codestral_22B_V01]",
    "user": "fredricz-20070104",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T07:49:32Z",
    "closed_at": "2025-08-19T04:13:04Z",
    "merged_at": "2025-08-19T04:13:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6939"
  },
  {
    "number": 6938,
    "title": "[https://nvbugs/5371480][fix] Enable test_phi3_small_8k",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T07:45:50Z",
    "closed_at": "2025-08-19T01:42:35Z",
    "merged_at": "2025-08-19T01:42:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6938"
  },
  {
    "number": 6937,
    "title": "[https://nvbugs/5449218][fix] Fix KvCacheConfig error in test_perf",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T07:15:48Z",
    "closed_at": "2025-08-18T07:58:54Z",
    "merged_at": "2025-08-18T07:58:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6937"
  },
  {
    "number": 6936,
    "title": "[TRTLLM-6960][fix] enable scaled_mm tests",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T07:10:41Z",
    "closed_at": "2025-08-19T02:18:04Z",
    "merged_at": "2025-08-19T02:18:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6936"
  },
  {
    "number": 6935,
    "title": "[None][fix] Add missing cassert include to fix assert undefined compilation error",
    "user": "Zhao-Dongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T06:42:39Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6935"
  },
  {
    "number": 6934,
    "title": "[None][fix] Skip Topk if 0",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T06:03:50Z",
    "closed_at": "2025-08-16T06:17:36Z",
    "merged_at": "2025-08-16T06:17:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6934"
  },
  {
    "number": 6933,
    "title": "[https://nvbugs/5405041][fix] Update wide-ep doc",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T05:59:22Z",
    "closed_at": "2025-08-15T09:32:33Z",
    "merged_at": "2025-08-15T09:32:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6933"
  },
  {
    "number": 6932,
    "title": "[None][chore] Update transformers to 4.53.2",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T05:47:35Z",
    "closed_at": "2025-08-19T04:18:46Z",
    "merged_at": "2025-08-19T04:18:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6932"
  },
  {
    "number": 6931,
    "title": "[None][feat] Add Request specific exception",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T05:10:28Z",
    "closed_at": "2025-09-04T22:43:43Z",
    "merged_at": "2025-09-04T22:43:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6931"
  },
  {
    "number": 6930,
    "title": "[None][fix] Remove PP check in Disagg",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T04:57:32Z",
    "closed_at": "2025-08-19T07:57:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6930"
  },
  {
    "number": 6929,
    "title": "[None][doc] Modify the description for mla chunked context",
    "user": "jmydurant",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T04:33:01Z",
    "closed_at": "2025-08-15T04:52:26Z",
    "merged_at": "2025-08-15T04:52:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6929"
  },
  {
    "number": 6927,
    "title": "[TRTLLM-6835][fix] Fix potential hang caused by python multiprocessing when prefetching weights",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T03:47:34Z",
    "closed_at": "2025-08-18T02:20:10Z",
    "merged_at": "2025-08-18T02:20:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6927"
  },
  {
    "number": 6926,
    "title": "[None][feat] [WIP]Add the LLM Args validation",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T03:26:10Z",
    "closed_at": "2025-08-22T02:43:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6926"
  },
  {
    "number": 6925,
    "title": "[None][fix] Fix the code conflicts",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T03:15:10Z",
    "closed_at": "2025-08-15T03:15:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6925"
  },
  {
    "number": 6924,
    "title": "[TRTLLM-4501][feat] Add input tensor pre-hook function API for the tuning process.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T03:11:27Z",
    "closed_at": "2025-10-15T13:18:11Z",
    "merged_at": "2025-10-15T13:18:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6924"
  },
  {
    "number": 6923,
    "title": "[None][opt] Add batch wait timeout in fetching requests",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T02:57:38Z",
    "closed_at": "2025-08-19T07:50:08Z",
    "merged_at": "2025-08-19T07:50:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6923"
  },
  {
    "number": 6922,
    "title": "[None][fix] Fix assertion errors of quantization when using online EPLB",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T02:44:43Z",
    "closed_at": "2025-08-19T18:28:36Z",
    "merged_at": "2025-08-19T18:28:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6922"
  },
  {
    "number": 6921,
    "title": "[None][fix] Update tests to use standardized uppercase backend identifiers",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-15T02:20:53Z",
    "closed_at": "2025-08-15T03:14:16Z",
    "merged_at": "2025-08-15T03:14:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6921"
  },
  {
    "number": 6918,
    "title": "[None] [infra] stricter coderabbit pr title generation instructions",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T23:45:23Z",
    "closed_at": "2025-08-19T02:11:37Z",
    "merged_at": "2025-08-19T02:11:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6918"
  },
  {
    "number": 6917,
    "title": "[Fix][Disaggregated]: Fix precision issue due to KV layout mismatch for split/concat kernels",
    "user": "ZhangGe6",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T23:36:20Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6917"
  },
  {
    "number": 6916,
    "title": "[None][chore] Waive E2E GB200 tests for Gemma3 27B",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T22:53:31Z",
    "closed_at": "2025-08-19T09:19:35Z",
    "merged_at": "2025-08-19T09:19:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6916"
  },
  {
    "number": 6915,
    "title": "[None][perf] Make finalize fusion part of the tactic selection logic",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T22:43:29Z",
    "closed_at": "2025-08-21T21:08:04Z",
    "merged_at": "2025-08-21T21:08:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6915"
  },
  {
    "number": 6914,
    "title": "[None][chore] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T21:43:18Z",
    "closed_at": "2025-08-15T06:00:17Z",
    "merged_at": "2025-08-15T06:00:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6914"
  },
  {
    "number": 6913,
    "title": "[https://nvbugs/5449155][fix] Fix DeepSeek R1 weight loading for TP16",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T21:22:17Z",
    "closed_at": "2025-08-19T02:25:44Z",
    "merged_at": "2025-08-19T02:25:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6913"
  },
  {
    "number": 6911,
    "title": "[https://nvbugs/5455836][fix] Fix llama 4 FP4",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T20:03:29Z",
    "closed_at": "2025-08-15T14:09:09Z",
    "merged_at": "2025-08-15T14:09:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6911"
  },
  {
    "number": 6910,
    "title": "[TRTLLM-6371][chore] Only onboard blocks that will be accessed",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T18:32:46Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6910"
  },
  {
    "number": 6909,
    "title": "[https://nvbugs/5448525][fix] Mistral Small 3.1 accuracy tests",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T18:27:10Z",
    "closed_at": "2025-08-18T03:17:37Z",
    "merged_at": "2025-08-18T03:17:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6909"
  },
  {
    "number": 6908,
    "title": "[None][doc] Update gpt-oss doc on MoE support matrix",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T18:25:16Z",
    "closed_at": "2025-08-15T00:50:32Z",
    "merged_at": "2025-08-15T00:50:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6908"
  },
  {
    "number": 6907,
    "title": "[None][fix] Revert phi4-mm aggregate mode",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T17:23:05Z",
    "closed_at": "2025-08-14T19:45:46Z",
    "merged_at": "2025-08-14T19:45:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6907"
  },
  {
    "number": 6906,
    "title": "[None] [feat] Support accurate device iter time",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T15:41:18Z",
    "closed_at": "2025-08-18T05:47:15Z",
    "merged_at": "2025-08-18T05:47:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6906"
  },
  {
    "number": 6905,
    "title": "[None][infra] update CODEOWNERS for release",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T15:34:07Z",
    "closed_at": "2025-08-15T16:34:30Z",
    "merged_at": "2025-08-15T16:34:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6905"
  },
  {
    "number": 6904,
    "title": "[TRTLLM-5966][feat] Helix: add custom position ids to MLA kernels",
    "user": "MatthiasKohl",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T14:34:12Z",
    "closed_at": "2025-09-19T12:55:33Z",
    "merged_at": "2025-09-19T12:55:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6904"
  },
  {
    "number": 6902,
    "title": "[None][infra] Waive failed cases on main 08/14",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T13:16:29Z",
    "closed_at": "2025-08-14T15:59:44Z",
    "merged_at": "2025-08-14T15:59:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6902"
  },
  {
    "number": 6900,
    "title": "[None][infra] Upgrade UCX to v1.19.x and NIXL to 0.5.0",
    "user": "BatshevaBlack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T09:18:55Z",
    "closed_at": "2025-08-17T08:33:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6900"
  },
  {
    "number": 6899,
    "title": "[None][doc] add status labels to LLM class's api reference",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T08:08:58Z",
    "closed_at": "2025-08-20T01:50:05Z",
    "merged_at": "2025-08-20T01:50:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6899"
  },
  {
    "number": 6898,
    "title": "[https://nvbugs/5451434][fix] Fix triton docker build",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T07:47:25Z",
    "closed_at": "2025-08-15T06:08:40Z",
    "merged_at": "2025-08-15T06:08:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6898"
  },
  {
    "number": 6897,
    "title": "[None][chore] add a EditorConfig config",
    "user": "zhenhuaw-me",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T07:39:38Z",
    "closed_at": "2025-08-15T07:54:38Z",
    "merged_at": "2025-08-15T07:54:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6897"
  },
  {
    "number": 6896,
    "title": "[https://nvbugs/5394685][fix] using static scheduler 2CTA MLA as WAR for an accuracy issue",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T07:38:21Z",
    "closed_at": "2025-08-15T00:51:04Z",
    "merged_at": "2025-08-15T00:51:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6896"
  },
  {
    "number": 6895,
    "title": "[https://nvbugs/5433545][fix] TestPhi4MiniInstruct::test_auto_dtype - Use max_seq_len=4096 to fallback to the short RoPE factor",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T07:28:28Z",
    "closed_at": "2025-08-22T17:28:10Z",
    "merged_at": "2025-08-22T17:28:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6895"
  },
  {
    "number": 6894,
    "title": "[TRTLLM-7101][infra] Reuse passed tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T07:05:00Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6894"
  },
  {
    "number": 6893,
    "title": "[https://nvbugs/5374016][fix] improve error message",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T07:01:21Z",
    "closed_at": "2025-08-19T02:29:08Z",
    "merged_at": "2025-08-19T02:29:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6893"
  },
  {
    "number": 6891,
    "title": "[None][fix] update skip config",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T06:36:39Z",
    "closed_at": "2025-08-18T05:50:47Z",
    "merged_at": "2025-08-18T05:50:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6891"
  },
  {
    "number": 6890,
    "title": "[WIP][chore] : CUDA13 build",
    "user": "borisfom",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T06:35:57Z",
    "closed_at": "2025-08-20T20:11:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6890"
  },
  {
    "number": 6889,
    "title": "[https://nvbugs/5383702][fix] test_llm_api_pytorch.py::TestLlama3_1_8BInstruct::test_fp8_4gpus",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T06:35:21Z",
    "closed_at": "2025-08-21T00:56:43Z",
    "merged_at": "2025-08-21T00:56:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6889"
  },
  {
    "number": 6888,
    "title": "[https://nvbugs/5448579][fix] EXAONE-4.0 accuracy test bugfix",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T06:17:39Z",
    "closed_at": "2025-08-19T07:29:33Z",
    "merged_at": "2025-08-19T07:29:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6888"
  },
  {
    "number": 6887,
    "title": "[None][notmerge] CI test",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T06:17:19Z",
    "closed_at": "2025-08-14T06:24:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6887"
  },
  {
    "number": 6886,
    "title": "[https://nvbugs/5445466][fix] Bypass MLP TP split for MNNVL in DeepSeek V3 to avoid hanging.",
    "user": "timlee0212",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T06:08:59Z",
    "closed_at": "2025-08-28T22:17:48Z",
    "merged_at": "2025-08-28T22:17:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6886"
  },
  {
    "number": 6885,
    "title": "[None][chore] Update transformers to 4.53.2",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T06:02:11Z",
    "closed_at": "2025-08-15T05:47:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6885"
  },
  {
    "number": 6884,
    "title": "[TRTLLM-7048][feat] add benchmark TRT flow test for MIG",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T05:52:04Z",
    "closed_at": "2025-08-15T06:01:05Z",
    "merged_at": "2025-08-15T06:01:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6884"
  },
  {
    "number": 6883,
    "title": "[None][doc] update moe support matrix for DS R1",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T04:34:06Z",
    "closed_at": "2025-08-14T05:55:11Z",
    "merged_at": "2025-08-14T05:55:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6883"
  },
  {
    "number": 6882,
    "title": "[https://nvbugs/5450262][fix] Fix unsupported alltoall use case",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T03:55:46Z",
    "closed_at": "2025-08-14T21:46:54Z",
    "merged_at": "2025-08-14T21:46:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6882"
  },
  {
    "number": 6881,
    "title": "[https://nvbugs/5451373][fix] : Fix the accuracy issue when using FP8 context MLA",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T03:12:16Z",
    "closed_at": "2025-08-15T08:53:57Z",
    "merged_at": "2025-08-15T08:53:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6881"
  },
  {
    "number": 6880,
    "title": "[None][chore] Add docs for Gemma3 VLMs",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T01:10:47Z",
    "closed_at": "2025-08-15T01:23:33Z",
    "merged_at": "2025-08-15T01:23:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6880"
  },
  {
    "number": 6879,
    "title": "[None][chore] fix markdown format for the deployment guide",
    "user": "zhenhuaw-me",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-14T01:03:05Z",
    "closed_at": "2025-08-14T02:19:12Z",
    "merged_at": "2025-08-14T02:19:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6879"
  },
  {
    "number": 6878,
    "title": "[https://nvbugs/5440521][bug] Fix sequence slot allocation for attention DP",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T23:35:04Z",
    "closed_at": "2025-08-19T06:38:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6878"
  },
  {
    "number": 6877,
    "title": "[https://nvbugs/5453667] [fix] add --disable_chunked_context to perf tests (trt backend only)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T22:50:08Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6877"
  },
  {
    "number": 6876,
    "title": "[#5861][feat] Implement Annotation-Based Quantization Transformation Pipeline",
    "user": "therealnaveenkamal",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T22:11:08Z",
    "closed_at": "2025-08-16T00:01:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6876"
  },
  {
    "number": 6875,
    "title": "[None][chore] update disagg readme and scripts for pipeline parallelism",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T22:03:48Z",
    "closed_at": "2025-08-27T04:53:57Z",
    "merged_at": "2025-08-27T04:53:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6875"
  },
  {
    "number": 6874,
    "title": "[#6798][fix] fix compilation error in ub_allocator in single device build",
    "user": "WilliamTambellini",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T21:23:42Z",
    "closed_at": "2025-09-09T11:13:53Z",
    "merged_at": "2025-09-09T11:13:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6874"
  },
  {
    "number": 6873,
    "title": "[https://nvbugs/5455651][fix] Make ngram use XQA attention on Blackwell",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T20:51:16Z",
    "closed_at": "2025-08-14T22:36:19Z",
    "merged_at": "2025-08-14T22:36:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6873"
  },
  {
    "number": 6872,
    "title": "[https://nvbugs/5441714][chore] remove skip on disagg n-gram test",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T19:25:52Z",
    "closed_at": "2025-08-14T22:45:01Z",
    "merged_at": "2025-08-14T22:45:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6872"
  },
  {
    "number": 6871,
    "title": "[None][infra] bump version to 1.0.1",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T19:24:26Z",
    "closed_at": "2025-08-22T16:58:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6871"
  },
  {
    "number": 6870,
    "title": "[https://nvbugs/5401114][fix] Unwaive Gemma3 tests",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T19:08:13Z",
    "closed_at": "2025-08-14T00:05:36Z",
    "merged_at": "2025-08-14T00:05:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6870"
  },
  {
    "number": 6869,
    "title": "[None][refactor] Simplify decoder state initialization for speculative decoding",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T15:48:56Z",
    "closed_at": "2025-08-22T16:44:17Z",
    "merged_at": "2025-08-22T16:44:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6869"
  },
  {
    "number": 6868,
    "title": "[TRTLLM-6481][fix] Fix deepseek r1 accuracy issue",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T14:56:04Z",
    "closed_at": "2025-08-15T07:56:36Z",
    "merged_at": "2025-08-15T07:56:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6868"
  },
  {
    "number": 6867,
    "title": "[TRTLLM-7155][feat] Unify sampler handle logits implementation.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T12:14:49Z",
    "closed_at": "2025-08-22T06:09:30Z",
    "merged_at": "2025-08-22T06:09:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6867"
  },
  {
    "number": 6866,
    "title": "[None][feat] Support running heterogeneous model execution for Nemotron-H",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T11:35:04Z",
    "closed_at": "2025-08-13T16:51:20Z",
    "merged_at": "2025-08-13T16:51:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6866"
  },
  {
    "number": 6865,
    "title": "[TRTLLM-5966][feat] Helix: make softmax stats pointer available to attention gen",
    "user": "MatthiasKohl",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T11:26:25Z",
    "closed_at": "2025-09-17T21:01:25Z",
    "merged_at": "2025-09-17T21:01:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6865"
  },
  {
    "number": 6864,
    "title": "[None][chore] Mass integration of release/1.0",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T10:44:05Z",
    "closed_at": "2025-08-22T01:25:15Z",
    "merged_at": "2025-08-22T01:25:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6864"
  },
  {
    "number": 6863,
    "title": "[None][infra] Waive failed cases on main",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T10:39:59Z",
    "closed_at": "2025-08-13T13:50:14Z",
    "merged_at": "2025-08-13T13:50:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6863"
  },
  {
    "number": 6862,
    "title": "[None][fix] max_num_sequences argument in nanobind",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T10:16:10Z",
    "closed_at": "2025-08-13T23:16:18Z",
    "merged_at": "2025-08-13T23:16:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6862"
  },
  {
    "number": 6861,
    "title": "Draft:[TRTLLM-7078][chore] optimal kvcache transfer for VWSA",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T09:11:45Z",
    "closed_at": "2025-09-24T07:43:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6861"
  },
  {
    "number": 6860,
    "title": "[https://nvbugs/5445466][fix] fix deepseek r1 hang by not enabling mnnvl by default",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T09:02:56Z",
    "closed_at": "2025-08-14T14:36:56Z",
    "merged_at": "2025-08-14T14:36:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6860"
  },
  {
    "number": 6859,
    "title": "[None][infra] Add retry 3 times if ssh cluster failed",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T08:49:09Z",
    "closed_at": "2025-08-26T09:11:50Z",
    "merged_at": "2025-08-26T09:11:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6859"
  },
  {
    "number": 6858,
    "title": "[https://nvbugs/5427801][fix] Torch compile support for Llama4 and Ea…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T08:29:38Z",
    "closed_at": "2025-08-15T15:14:21Z",
    "merged_at": "2025-08-15T15:14:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6858"
  },
  {
    "number": 6857,
    "title": "Support W4A8 method of AngleSlim tool",
    "user": "bppan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T08:18:25Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6857"
  },
  {
    "number": 6856,
    "title": "[TRTLLM-7169][infra] Fix Slurm multi-node test showing \"Submit Test Results\" in the test name",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T07:56:21Z",
    "closed_at": "2025-09-12T10:46:20Z",
    "merged_at": "2025-09-12T10:46:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6856"
  },
  {
    "number": 6855,
    "title": "[TRTLLM-7158][feat] Introduce sampler options in trtllm bench",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T07:46:17Z",
    "closed_at": "2025-08-18T22:10:06Z",
    "merged_at": "2025-08-18T22:10:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6855"
  },
  {
    "number": 6854,
    "title": "[TRTLLM-7094][feat] Gpt-oss reasoning content parsing in trtllm-serve",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T07:20:54Z",
    "closed_at": "2025-08-26T14:07:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6854"
  },
  {
    "number": 6853,
    "title": "[https://nvbugs/5412885][doc] Add the workaround doc for H200 OOM",
    "user": "zhenhuaw-me",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T06:47:48Z",
    "closed_at": "2025-08-13T11:51:38Z",
    "merged_at": "2025-08-13T11:51:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6853"
  },
  {
    "number": 6852,
    "title": "Draft/add lora analysis scripts",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T06:45:27Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6852"
  },
  {
    "number": 6851,
    "title": "[TRTLLM-7093][fix] the perf regression to cvt_fp4 kernels",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T04:21:31Z",
    "closed_at": "2025-08-13T11:13:40Z",
    "merged_at": "2025-08-13T11:13:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6851"
  },
  {
    "number": 6850,
    "title": "[None][fix] Complete the last missing allreduce op in Llama3/4.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T02:45:01Z",
    "closed_at": "2025-08-15T01:07:10Z",
    "merged_at": "2025-08-15T01:07:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6850"
  },
  {
    "number": 6849,
    "title": "[None][chore] clean codes",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T02:34:28Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6849"
  },
  {
    "number": 6848,
    "title": "[https://nvbugs/5437405][fix] Unwaive passed test",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T02:12:28Z",
    "closed_at": "2025-08-19T08:33:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6848"
  },
  {
    "number": 6847,
    "title": "[https://nvbugs/5375646][fix] update waives.txt for nvbug 5375646",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T01:45:03Z",
    "closed_at": "2025-08-18T03:22:02Z",
    "merged_at": "2025-08-18T03:22:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6847"
  },
  {
    "number": 6846,
    "title": "[None][refactor] refactor the CUDA graph runner to manage all CUDA graphs",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T01:13:55Z",
    "closed_at": "2025-08-25T12:52:05Z",
    "merged_at": "2025-08-25T12:52:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6846"
  },
  {
    "number": 6845,
    "title": "[None][fix] fix logprob in trtllm-serve",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-13T00:06:55Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6845"
  },
  {
    "number": 6844,
    "title": "[None][feat] Refactor llama4 for multimodal encoder IFB",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T23:43:24Z",
    "closed_at": "2025-08-28T20:22:20Z",
    "merged_at": "2025-08-28T20:22:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6844"
  },
  {
    "number": 6843,
    "title": "[TRTLLM-6903][feat] Support chunked prefill for multimodal models",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T23:21:34Z",
    "closed_at": "2025-09-15T03:10:10Z",
    "merged_at": "2025-09-15T03:10:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6843"
  },
  {
    "number": 6842,
    "title": "[None][fix]Make logprobs faster by not copying over everything",
    "user": "Pernekhan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T22:37:15Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6842"
  },
  {
    "number": 6841,
    "title": "[None][fix] Fix batching bug in Mistral3 model",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T22:28:46Z",
    "closed_at": "2025-08-14T06:15:44Z",
    "merged_at": "2025-08-14T06:15:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6841"
  },
  {
    "number": 6840,
    "title": "[None][chore] Add tests for non-existent and completed request cancellation",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T22:23:10Z",
    "closed_at": "2025-08-14T22:57:02Z",
    "merged_at": "2025-08-14T22:57:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6840"
  },
  {
    "number": 6838,
    "title": "[https://nvbugs/5375594][fix] fix oom issue on structural_tag test case",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T17:15:20Z",
    "closed_at": "2025-08-13T14:09:35Z",
    "merged_at": "2025-08-13T14:09:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6838"
  },
  {
    "number": 6837,
    "title": "[https://nvbugs/5452167][fix] Fix ngram padding issue",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T16:08:02Z",
    "closed_at": "2025-08-13T03:23:16Z",
    "merged_at": "2025-08-13T03:23:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6837"
  },
  {
    "number": 6836,
    "title": "[TRTLLM-7141][infra] Use repo mirrors to avoid intermittent network failures",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T15:45:15Z",
    "closed_at": "2025-08-15T03:16:08Z",
    "merged_at": "2025-08-15T03:16:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6836"
  },
  {
    "number": 6834,
    "title": "[https://nvbugs/5394685][fix] the bug with spec-decoding + SWA && an accuracy issue related to 2CTA MLA",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T15:28:00Z",
    "closed_at": "2025-08-13T20:55:57Z",
    "merged_at": "2025-08-13T20:55:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6834"
  },
  {
    "number": 6833,
    "title": "[https://nvbugs/5410399][chore] Unwaive mtp llmapi test",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T15:02:37Z",
    "closed_at": "2025-08-13T21:38:45Z",
    "merged_at": "2025-08-13T21:38:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6833"
  },
  {
    "number": 6832,
    "title": "[None][fix] fix Llama3 eagle3 test case OOM",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T14:54:52Z",
    "closed_at": "2025-08-13T06:21:05Z",
    "merged_at": "2025-08-13T06:21:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6832"
  },
  {
    "number": 6831,
    "title": "[TRTLLM-7157][feat] BREAKING CHANGE Introduce sampler_type, detect sampler according to options",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T14:52:03Z",
    "closed_at": "2025-08-16T04:27:24Z",
    "merged_at": "2025-08-16T04:27:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6831"
  },
  {
    "number": 6830,
    "title": "[#6186][feat] Introduce QKNormRoPEAttention module",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T14:02:59Z",
    "closed_at": "2025-09-05T12:04:41Z",
    "merged_at": "2025-09-05T12:04:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6830"
  },
  {
    "number": 6829,
    "title": "[https://nvbugs/5437106][fix] Add L4 Scout benchmarking WAR option in deploy guide",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T13:12:34Z",
    "closed_at": "2025-08-15T00:53:13Z",
    "merged_at": "2025-08-15T00:53:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6829"
  },
  {
    "number": 6828,
    "title": "[TRTLLM-6771][feat] Support MMMU for multimodal models",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T13:06:49Z",
    "closed_at": "2025-08-21T00:54:13Z",
    "merged_at": "2025-08-21T00:54:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6828"
  },
  {
    "number": 6826,
    "title": "[None][fix] fix CUDA graph config for test_llm_api_pytorch.py.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T10:33:03Z",
    "closed_at": "2025-08-13T02:24:15Z",
    "merged_at": "2025-08-13T02:24:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6826"
  },
  {
    "number": 6825,
    "title": "[None][fix] Fix python-only build that uses TRTLLM_USE_PRECOMPILED",
    "user": "jiaganc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T10:06:24Z",
    "closed_at": "2025-08-14T15:26:36Z",
    "merged_at": "2025-08-14T15:26:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6825"
  },
  {
    "number": 6824,
    "title": "[https://nvbugs/5448754][fix] Download HF model for all nodes.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T09:46:10Z",
    "closed_at": "2025-09-01T08:00:44Z",
    "merged_at": "2025-09-01T08:00:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6824"
  },
  {
    "number": 6823,
    "title": "[None][fix] Clean up linking to CUDA stub libraries in build_wheel.py",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T09:45:50Z",
    "closed_at": "2025-08-18T15:20:51Z",
    "merged_at": "2025-08-18T15:20:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6823"
  },
  {
    "number": 6822,
    "title": "[None][feat] DeepEP LL combine FP4",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T09:43:51Z",
    "closed_at": "2025-08-13T08:20:21Z",
    "merged_at": "2025-08-13T08:20:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6822"
  },
  {
    "number": 6821,
    "title": "[https://nvbugs/5427043][fix] request length exceeds max_num_tokens",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T09:38:13Z",
    "closed_at": "2025-08-14T05:31:13Z",
    "merged_at": "2025-08-14T05:31:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6821"
  },
  {
    "number": 6820,
    "title": "[TRTLLM-6308][feat] Support Aggregate mode for phi4-mm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T09:17:25Z",
    "closed_at": "2025-08-14T04:45:23Z",
    "merged_at": "2025-08-14T04:45:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6820"
  },
  {
    "number": 6819,
    "title": "Support hunyuan moe v2",
    "user": "sorenwu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T08:33:12Z",
    "closed_at": "2025-08-12T08:33:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6819"
  },
  {
    "number": 6818,
    "title": "[https://nvbugs/5392414] [fix] Add customized default routing method",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T08:23:50Z",
    "closed_at": "2025-08-21T08:58:41Z",
    "merged_at": "2025-08-21T08:58:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6818"
  },
  {
    "number": 6817,
    "title": "[TRTLLM-6825][fix] Update lora for phi4-mm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T08:21:39Z",
    "closed_at": "2025-08-22T02:00:05Z",
    "merged_at": "2025-08-22T02:00:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6817"
  },
  {
    "number": 6816,
    "title": "[TRTLLM-5966][feat] Helix: extend mapping to support different CP types",
    "user": "MatthiasKohl",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T07:21:17Z",
    "closed_at": "2025-08-14T16:00:03Z",
    "merged_at": "2025-08-14T16:00:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6816"
  },
  {
    "number": 6815,
    "title": "[TRTLLM-5966][feat] Helix: add alltoall op",
    "user": "MatthiasKohl",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T06:56:45Z",
    "closed_at": "2025-09-25T14:18:30Z",
    "merged_at": "2025-09-25T14:18:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6815"
  },
  {
    "number": 6814,
    "title": "[None][fix] fix ci",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T06:40:28Z",
    "closed_at": "2025-08-12T09:21:50Z",
    "merged_at": "2025-08-12T09:21:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6814"
  },
  {
    "number": 6813,
    "title": "[https://nvbugs/5451373][waive]: Waive fp8 kvcache deepseek-lite test…",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T06:39:39Z",
    "closed_at": "2025-08-14T08:30:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6813"
  },
  {
    "number": 6812,
    "title": "[None][chore] waive GB300 known issues",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T06:32:22Z",
    "closed_at": "2025-08-13T05:13:37Z",
    "merged_at": "2025-08-13T05:13:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6812"
  },
  {
    "number": 6811,
    "title": "[None][fix] Pre-allocate workspaces for DeepGEMM MoE to avoid frequent cudaFree/cudaMalloc",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T05:11:34Z",
    "closed_at": "2025-08-13T02:27:58Z",
    "merged_at": "2025-08-13T02:27:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6811"
  },
  {
    "number": 6810,
    "title": "[None][fix] debug CI",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T04:44:03Z",
    "closed_at": "2025-08-12T06:37:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6810"
  },
  {
    "number": 6809,
    "title": "[OMNIML-2336][feat] Add NVFP4 x FP8",
    "user": "sychen52",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T03:18:46Z",
    "closed_at": "2025-09-04T16:03:38Z",
    "merged_at": "2025-09-04T16:03:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6809"
  },
  {
    "number": 6808,
    "title": "[https://nvbugs/5450855][fix] Cherry pick #6700 and #6702 from main",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T02:23:05Z",
    "closed_at": "2025-08-12T10:11:47Z",
    "merged_at": "2025-08-12T10:11:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6808"
  },
  {
    "number": 6807,
    "title": "[None][fix] Fix free memory fraction calculation in resource manager",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T01:51:44Z",
    "closed_at": "2025-08-20T08:13:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6807"
  },
  {
    "number": 6806,
    "title": "[TRTLLM-6496][feat] Add LoRa Torch tests for the latest NIM model list",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-12T00:38:14Z",
    "closed_at": "2025-10-03T19:10:49Z",
    "merged_at": "2025-10-03T19:10:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6806"
  },
  {
    "number": 6803,
    "title": "[#6530][fix] Fix script when using calibration tensors from modelopt",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T21:18:04Z",
    "closed_at": "2025-08-13T03:41:11Z",
    "merged_at": "2025-08-13T03:41:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6803"
  },
  {
    "number": 6801,
    "title": "[https://nvbugs/5429636][fix] fix KVC mem leakage by adding kv_transfer_timeout_ms",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T19:49:42Z",
    "closed_at": "2025-09-05T22:17:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6801"
  },
  {
    "number": 6800,
    "title": "[None][fix] Correct reporting of torch_dtype for ModelConfig class.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T19:41:33Z",
    "closed_at": "2025-08-15T02:46:20Z",
    "merged_at": "2025-08-15T02:46:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6800"
  },
  {
    "number": 6799,
    "title": "[https://nvbugs/5445774][fix] Unwaive Gemma3 27B fp8 test",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T18:03:56Z",
    "closed_at": "2025-08-12T15:54:15Z",
    "merged_at": "2025-08-12T15:54:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6799"
  },
  {
    "number": 6797,
    "title": "[None][fix] Fix perfect router.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T14:04:54Z",
    "closed_at": "2025-08-15T03:09:09Z",
    "merged_at": "2025-08-15T03:09:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6797"
  },
  {
    "number": 6796,
    "title": "[None] [chore] Mamba cache in separate file",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T12:49:59Z",
    "closed_at": "2025-08-15T10:42:51Z",
    "merged_at": "2025-08-15T10:42:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6796"
  },
  {
    "number": 6795,
    "title": "[https://nvbugs/5444095][infra] waive test_ptp_quickstart_multimodal llava test",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T10:49:55Z",
    "closed_at": "2025-08-11T15:58:38Z",
    "merged_at": "2025-08-11T15:58:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6795"
  },
  {
    "number": 6794,
    "title": "[None] [feat] Enable run_post_quant_allgather for MoE TRTLLM backend",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T10:49:48Z",
    "closed_at": "2025-09-23T00:24:21Z",
    "merged_at": "2025-09-23T00:24:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6794"
  },
  {
    "number": 6793,
    "title": "[https://nvbugs/5383702][fix] error propagation in GenerationExecutor",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T10:19:52Z",
    "closed_at": "2025-08-12T04:28:06Z",
    "merged_at": "2025-08-12T04:28:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6793"
  },
  {
    "number": 6792,
    "title": "[TRTLLM-5093][infra] Write env variables to a file",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T09:58:38Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6792"
  },
  {
    "number": 6791,
    "title": "[None][infra] Unwaive an updated case to test",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T09:38:38Z",
    "closed_at": "2025-08-11T10:47:33Z",
    "merged_at": "2025-08-11T10:47:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6791"
  },
  {
    "number": 6790,
    "title": "[None][fix] Add attention DP padding back on SM120",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T09:35:36Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6790"
  },
  {
    "number": 6789,
    "title": "[TRTLLM-7008][fix] fix wideEP weights loading and args",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T09:08:50Z",
    "closed_at": "2025-08-12T23:14:21Z",
    "merged_at": "2025-08-12T23:14:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6789"
  },
  {
    "number": 6788,
    "title": "[TRTLLM-7008][fix] fix wideEP weights loading",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T08:37:46Z",
    "closed_at": "2025-08-11T08:41:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6788"
  },
  {
    "number": 6786,
    "title": "[TRTLLM-6683][feat] Support LoRA reload CPU cache evicted adapter",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T08:31:14Z",
    "closed_at": "2025-08-11T18:31:40Z",
    "merged_at": "2025-08-11T18:31:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6786"
  },
  {
    "number": 6785,
    "title": "[None][feat] Support Yarn on Qwen3",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T08:27:08Z",
    "closed_at": "2025-08-16T23:21:29Z",
    "merged_at": "2025-08-16T23:21:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6785"
  },
  {
    "number": 6784,
    "title": "[None][fix] Add FP4 all2all unitest and fix a bug for module WideEPMoE",
    "user": "StudyingShao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T07:20:57Z",
    "closed_at": "2025-08-14T05:35:37Z",
    "merged_at": "2025-08-14T05:35:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6784"
  },
  {
    "number": 6783,
    "title": "[None][doc] Update metric args",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T07:08:20Z",
    "closed_at": "2025-09-10T12:49:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6783"
  },
  {
    "number": 6782,
    "title": "[None][infra] Waive failed tests on release branch 0811",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T06:50:26Z",
    "closed_at": "2025-08-11T07:14:48Z",
    "merged_at": "2025-08-11T07:14:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6782"
  },
  {
    "number": 6781,
    "title": "[TRTQA-2920][chore] improve hang tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T06:43:06Z",
    "closed_at": "2025-08-12T10:26:51Z",
    "merged_at": "2025-08-12T10:26:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6781"
  },
  {
    "number": 6780,
    "title": "[TRTLLM-7030][fix] BREAKING CHANGE: Mismatch between docs and actual commands",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T06:20:21Z",
    "closed_at": "2025-08-25T12:22:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6780"
  },
  {
    "number": 6779,
    "title": "[None][infra] DO NOT REVIEW, Just Test",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T06:19:23Z",
    "closed_at": "2025-08-25T02:29:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6779"
  },
  {
    "number": 6778,
    "title": "[None][infra] Waive failed tests on main 0811",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T05:55:56Z",
    "closed_at": "2025-08-11T07:16:26Z",
    "merged_at": "2025-08-11T07:16:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6778"
  },
  {
    "number": 6777,
    "title": "[TRTLLM-5633][infra] Force set changed file diff to empty string for post-merge CI",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T05:55:09Z",
    "closed_at": "2025-08-11T06:38:06Z",
    "merged_at": "2025-08-11T06:38:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6777"
  },
  {
    "number": 6776,
    "title": "[https://nvbugs/5412456][fix] Fix an illegal instruction was encountered",
    "user": "zhou-yuxin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T05:21:38Z",
    "closed_at": "2025-08-13T07:45:59Z",
    "merged_at": "2025-08-13T07:45:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6776"
  },
  {
    "number": 6775,
    "title": "[None][feat] Enable gpt oss on DGX H100.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T03:33:50Z",
    "closed_at": "2025-09-23T16:35:19Z",
    "merged_at": "2025-09-23T16:35:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6775"
  },
  {
    "number": 6774,
    "title": "[TRTLLM-6854][feat] Enable guided decoding with CUDA graph padding and draft model chunked prefill",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T03:09:25Z",
    "closed_at": "2025-08-12T01:30:06Z",
    "merged_at": "2025-08-12T01:30:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6774"
  },
  {
    "number": 6773,
    "title": "[None][chore] remove out-of-date comment in star attention test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T02:19:00Z",
    "closed_at": "2025-08-11T03:35:39Z",
    "merged_at": "2025-08-11T03:35:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6773"
  },
  {
    "number": 6772,
    "title": "[None][chore] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T02:05:17Z",
    "closed_at": "2025-08-11T06:39:58Z",
    "merged_at": "2025-08-11T06:39:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6772"
  },
  {
    "number": 6771,
    "title": "Draft: test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T01:55:36Z",
    "closed_at": "2025-08-11T01:56:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6771"
  },
  {
    "number": 6770,
    "title": "refactor cuda graph runner",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-11T00:52:02Z",
    "closed_at": "2025-08-11T00:52:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6770"
  },
  {
    "number": 6769,
    "title": "Draft: test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-10T12:15:36Z",
    "closed_at": "2025-08-11T01:29:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6769"
  },
  {
    "number": 6768,
    "title": "[TRTLLM-6341][feature] Support SWA KV cache",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-10T06:58:01Z",
    "closed_at": "2025-09-24T06:28:24Z",
    "merged_at": "2025-09-24T06:28:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6768"
  },
  {
    "number": 6767,
    "title": "[TRTLLM-6341][chore] Preliminary refactors on the kv cache manager before supporting swa kv cache reuse",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-10T06:51:00Z",
    "closed_at": "2025-08-20T05:57:57Z",
    "merged_at": "2025-08-20T05:57:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6767"
  },
  {
    "number": 6766,
    "title": "[TRTLLM-7030][fix] Refactor the example doc of dist-serving",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-09T09:42:22Z",
    "closed_at": "2025-08-13T09:39:28Z",
    "merged_at": "2025-08-13T09:39:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6766"
  },
  {
    "number": 6765,
    "title": "[TRTLLM-5252][fix] Propagate mapping to intermediate layers (#6611)",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-09T05:36:53Z",
    "closed_at": "2025-08-11T17:13:10Z",
    "merged_at": "2025-08-11T17:13:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6765"
  },
  {
    "number": 6764,
    "title": "[None][test] Add accuracy evaluation for AutoDeploy",
    "user": "ajrasane",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-09T01:51:46Z",
    "closed_at": "2025-08-15T17:46:09Z",
    "merged_at": "2025-08-15T17:46:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6764"
  },
  {
    "number": 6763,
    "title": "[None][chore] Find LLM_ROOT and LLM_BACKEND_ROOT dynamically",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T21:49:21Z",
    "closed_at": "2025-08-11T22:18:19Z",
    "merged_at": "2025-08-11T22:18:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6763"
  },
  {
    "number": 6762,
    "title": "[TRTLLM-7025] [infra] Reorganize CODEOWNERS to rectify `examples` mapping",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T19:39:41Z",
    "closed_at": "2025-08-08T22:33:09Z",
    "merged_at": "2025-08-08T22:33:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6762"
  },
  {
    "number": 6761,
    "title": "[None][fix] Accommodate Phi3/4 to work with ModelOpt's FP8 ckpts in Torch",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T17:39:21Z",
    "closed_at": "2025-08-19T16:22:46Z",
    "merged_at": "2025-08-19T16:22:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6761"
  },
  {
    "number": 6760,
    "title": "[#4403][autodeploy] Refactor: Move more transformations to new inf optimizer, Add quantization_source to factory interface",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T16:25:29Z",
    "closed_at": "2025-08-12T05:02:47Z",
    "merged_at": "2025-08-12T05:02:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6760"
  },
  {
    "number": 6759,
    "title": "[#4403][refactor] Move more transformations to new inf optimizer, Add quantization_source to factory interface",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T15:58:47Z",
    "closed_at": "2025-08-08T16:24:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6759"
  },
  {
    "number": 6758,
    "title": "[None][fix] Fix: Using RAII to automatically manage the allocation and release of va_list for potential resource leak",
    "user": "Fan-Yunfan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T15:41:59Z",
    "closed_at": "2025-08-16T07:19:19Z",
    "merged_at": "2025-08-16T07:19:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6758"
  },
  {
    "number": 6756,
    "title": "[None][feat] Add single block version renormalized routing kernel",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T14:59:28Z",
    "closed_at": "2025-08-17T05:47:13Z",
    "merged_at": "2025-08-17T05:47:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6756"
  },
  {
    "number": 6754,
    "title": "[None] [ci] Reorganize CMake and Python integration test infrastructure for C++ tests",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T14:44:28Z",
    "closed_at": "2025-08-24T18:53:17Z",
    "merged_at": "2025-08-24T18:53:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6754"
  },
  {
    "number": 6753,
    "title": "[https://nvbugs/5444937][fix] Fixing kv_cache_event unit test",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T13:18:54Z",
    "closed_at": "2025-08-10T23:45:38Z",
    "merged_at": "2025-08-10T23:45:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6753"
  },
  {
    "number": 6752,
    "title": "[None][chore] Guided decoding benchmarking",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T10:54:49Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6752"
  },
  {
    "number": 6751,
    "title": "[None][infra] Waive test main 0808",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T10:11:25Z",
    "closed_at": "2025-08-10T03:54:08Z",
    "merged_at": "2025-08-10T03:54:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6751"
  },
  {
    "number": 6750,
    "title": "[TRTLLM-6633][feat] Padding for piecewise cudagraph",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T09:55:13Z",
    "closed_at": "2025-08-26T22:31:33Z",
    "merged_at": "2025-08-26T22:31:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6750"
  },
  {
    "number": 6749,
    "title": "[TRTLLM-6975][test] Add multi-turn test cases for VLM models",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T09:29:21Z",
    "closed_at": "2025-08-13T05:10:13Z",
    "merged_at": "2025-08-13T05:10:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6749"
  },
  {
    "number": 6748,
    "title": "[None][chore] always try-catch when clear build folder in build_wheel.py",
    "user": "zhenhuaw-me",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T09:24:24Z",
    "closed_at": "2025-08-11T12:02:17Z",
    "merged_at": "2025-08-11T12:02:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6748"
  },
  {
    "number": 6747,
    "title": "[TRTLLM-6768][infra] Fix params for not updating github status",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T09:07:29Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6747"
  },
  {
    "number": 6746,
    "title": "[None][fix] acceptance rate calculation fix in benchmark_serving",
    "user": "zerollzeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T08:58:55Z",
    "closed_at": "2025-08-19T09:29:37Z",
    "merged_at": "2025-08-19T09:29:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6746"
  },
  {
    "number": 6745,
    "title": "[TRTLLM-6906][chore] Using pybind to bind functions in thop/attentionOp",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T07:10:59Z",
    "closed_at": "2025-08-12T08:45:16Z",
    "merged_at": "2025-08-12T08:45:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6745"
  },
  {
    "number": 6744,
    "title": "[https://nvbugs/5444624][fix] Fix LLM_ROOT in triton_backend build.sh",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T06:55:30Z",
    "closed_at": "2025-08-11T02:45:51Z",
    "merged_at": "2025-08-11T02:45:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6744"
  },
  {
    "number": 6743,
    "title": "[TRTLLM-7326][feat] Add standalone multimodal encoder",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T06:54:56Z",
    "closed_at": "2025-08-20T04:42:51Z",
    "merged_at": "2025-08-20T04:42:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6743"
  },
  {
    "number": 6742,
    "title": "[TRTLLM-6791][infra] Add check for uploading stage name and avoid overriding test result tar file",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T06:49:22Z",
    "closed_at": "2025-09-12T17:15:34Z",
    "merged_at": "2025-09-12T17:15:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6742"
  },
  {
    "number": 6741,
    "title": "[TRTLLM-7014][chore] Add accuracy test for ctx and gen workers with different models",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T06:47:18Z",
    "closed_at": "2025-08-19T01:58:23Z",
    "merged_at": "2025-08-19T01:58:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6741"
  },
  {
    "number": 6740,
    "title": "[None][doc] add blackwell information into support matrix",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T06:44:09Z",
    "closed_at": "2025-09-01T18:04:45Z",
    "merged_at": "2025-09-01T18:04:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6740"
  },
  {
    "number": 6739,
    "title": "[None][doc] Add doc for multimodal feature support matrix (#6619)",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T06:27:57Z",
    "closed_at": "2025-08-08T07:03:14Z",
    "merged_at": "2025-08-08T07:03:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6739"
  },
  {
    "number": 6738,
    "title": "[None][test] Add perf-sweep scripts",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T06:25:22Z",
    "closed_at": "2025-08-14T06:04:48Z",
    "merged_at": "2025-08-14T06:04:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6738"
  },
  {
    "number": 6737,
    "title": "[https://nvbugs/5431127][fix] Run test_disaggregated_deepseek_v3_lite_fp8_nixl[DeepSeek-V3-Lite-fp8] only on hopper",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T06:24:24Z",
    "closed_at": "2025-08-11T05:29:11Z",
    "merged_at": "2025-08-11T05:29:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6737"
  },
  {
    "number": 6736,
    "title": "[https://nvbugs/5431127][fix] Run test_disaggregated_deepseek_v3_lite_fp8_nixl[DeepSeek-V3-Lite-fp8] only on hopper",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T06:19:05Z",
    "closed_at": "2025-08-11T02:05:10Z",
    "merged_at": "2025-08-11T02:05:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6736"
  },
  {
    "number": 6735,
    "title": "[TRTLLM-6675][infra] Cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/6623",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T06:10:04Z",
    "closed_at": "2025-08-14T04:36:39Z",
    "merged_at": "2025-08-14T04:36:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6735"
  },
  {
    "number": 6734,
    "title": "[None][test] fix yml condition error under qa folder",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T05:54:04Z",
    "closed_at": "2025-08-08T05:59:02Z",
    "merged_at": "2025-08-08T05:59:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6734"
  },
  {
    "number": 6733,
    "title": "[None][test] fix yml condition error under qa folder",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T05:50:40Z",
    "closed_at": "2025-08-08T05:59:09Z",
    "merged_at": "2025-08-08T05:59:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6733"
  },
  {
    "number": 6732,
    "title": "[None][feat] Add gpt-oss GSM8K test.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T05:20:12Z",
    "closed_at": "2025-08-11T02:45:43Z",
    "merged_at": "2025-08-11T02:45:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6732"
  },
  {
    "number": 6731,
    "title": "[TRTLLM-5252][feat] Add fp8 support for Mistral Small 3.1",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T04:03:57Z",
    "closed_at": "2025-08-14T01:25:56Z",
    "merged_at": "2025-08-14T01:25:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6731"
  },
  {
    "number": 6730,
    "title": "[None][fix] fix same pp disagg",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T03:54:07Z",
    "closed_at": "2025-08-11T02:45:15Z",
    "merged_at": "2025-08-11T02:45:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6730"
  },
  {
    "number": 6729,
    "title": "[TRTLLM-6893][infra] Disable the x86 / SBSA build stage when run BuildDockerImage",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T03:46:51Z",
    "closed_at": "2025-09-04T11:20:15Z",
    "merged_at": "2025-09-04T11:20:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6729"
  },
  {
    "number": 6728,
    "title": "[TRTLLM-7319][perf] Fuse slicing into MoE.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T03:40:24Z",
    "closed_at": "2025-08-25T20:52:30Z",
    "merged_at": "2025-08-25T20:52:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6728"
  },
  {
    "number": 6727,
    "title": "[TRTLLM-4721][test] Add qa test for llm-api",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T03:16:17Z",
    "closed_at": "2025-08-11T00:03:16Z",
    "merged_at": "2025-08-11T00:03:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6727"
  },
  {
    "number": 6726,
    "title": "[None][feat] Update TRTLLM MoE cubins",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T03:14:50Z",
    "closed_at": "2025-10-08T04:48:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6726"
  },
  {
    "number": 6725,
    "title": "[None][infra] Setup the code review rule on the release branch",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-08T02:49:48Z",
    "closed_at": "2025-08-14T04:08:08Z",
    "merged_at": "2025-08-14T04:08:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6725"
  },
  {
    "number": 6724,
    "title": "[None][doc] add legacy section for tensorrt engine",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T23:48:15Z",
    "closed_at": "2025-08-15T03:08:38Z",
    "merged_at": "2025-08-15T03:08:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6724"
  },
  {
    "number": 6723,
    "title": "[None][fix] Fix the issue of responsibility boundary between the assert and tllmException files",
    "user": "Fan-Yunfan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T23:39:05Z",
    "closed_at": "2025-08-15T02:34:50Z",
    "merged_at": "2025-08-15T02:34:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6723"
  },
  {
    "number": 6722,
    "title": "[None][fix] Make TP working for Triton MOE (in additional to EP we are using)",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T21:04:00Z",
    "closed_at": "2025-08-15T20:58:42Z",
    "merged_at": "2025-08-15T20:58:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6722"
  },
  {
    "number": 6721,
    "title": "[None][fix] WAR GPT OSS on H20 with Triton MOE",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T21:00:38Z",
    "closed_at": "2025-08-08T23:47:10Z",
    "merged_at": "2025-08-08T23:47:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6721"
  },
  {
    "number": 6720,
    "title": "[None][fix] Refactoring to avoid circular import when importing torch models",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T20:47:43Z",
    "closed_at": "2025-08-11T22:00:42Z",
    "merged_at": "2025-08-11T22:00:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6720"
  },
  {
    "number": 6719,
    "title": "[TRTQA-2920][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T20:17:31Z",
    "closed_at": "2025-08-08T02:54:13Z",
    "merged_at": "2025-08-08T02:54:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6719"
  },
  {
    "number": 6718,
    "title": "[None][feat] Optimize CUDA graph memory usage for spec decode cases",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T18:20:43Z",
    "closed_at": "2025-08-08T17:56:54Z",
    "merged_at": "2025-08-08T17:56:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6718"
  },
  {
    "number": 6716,
    "title": "[None][chore] Remove py_executor from disagg gh team",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T16:54:53Z",
    "closed_at": "2025-08-07T17:29:01Z",
    "merged_at": "2025-08-07T17:29:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6716"
  },
  {
    "number": 6715,
    "title": "[None][infra] Avoid intermittent access broken to nvcr.io",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T16:48:21Z",
    "closed_at": "2025-08-12T03:49:00Z",
    "merged_at": "2025-08-12T03:49:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6715"
  },
  {
    "number": 6714,
    "title": "[https://nvbugs/5394409][feat] Support Mistral Small 3.1 multimodal in Triton Backend",
    "user": "dbari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T15:25:28Z",
    "closed_at": "2025-08-21T16:08:38Z",
    "merged_at": "2025-08-21T16:08:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6714"
  },
  {
    "number": 6712,
    "title": "[https://nvbugs/5344910][fix] Corrected memory position when setting buffers to 0 in standalone_stable_radix_topk_",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T13:36:59Z",
    "closed_at": "2025-08-08T13:26:00Z",
    "merged_at": "2025-08-08T13:26:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6712"
  },
  {
    "number": 6711,
    "title": "[None][infra] Fix guardwords",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T12:39:33Z",
    "closed_at": "2025-08-07T13:06:47Z",
    "merged_at": "2025-08-07T13:06:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6711"
  },
  {
    "number": 6710,
    "title": "[TRTLLM-6991][chore] add DeepSeek-R1 FP8 accuracy tests on Blackwell",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T11:58:09Z",
    "closed_at": "2025-08-19T04:03:03Z",
    "merged_at": "2025-08-19T04:03:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6710"
  },
  {
    "number": 6709,
    "title": "[None][fix]revert kvcache transfer",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T11:51:14Z",
    "closed_at": "2025-08-08T11:18:53Z",
    "merged_at": "2025-08-08T11:18:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6709"
  },
  {
    "number": 6708,
    "title": "[https://nvbugs/5410687][fix] Hopper w4a8 groupwise MoE interleave",
    "user": "symphonylyh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T11:44:07Z",
    "closed_at": "2025-08-07T22:30:16Z",
    "merged_at": "2025-08-07T22:30:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6708"
  },
  {
    "number": 6707,
    "title": "[None][fix]revert dp_tp optimal kvcache transfer #6657",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T11:40:37Z",
    "closed_at": "2025-08-07T11:44:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6707"
  },
  {
    "number": 6704,
    "title": "[TRTLLM-6854][feat] Enable guided decoding with disagg serving",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T10:49:31Z",
    "closed_at": "2025-08-08T04:10:37Z",
    "merged_at": "2025-08-08T04:10:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6704"
  },
  {
    "number": 6703,
    "title": "[None][package] Pin cuda-python version to >=12,<13",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T10:27:37Z",
    "closed_at": "2025-08-07T12:40:24Z",
    "merged_at": "2025-08-07T12:40:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6703"
  },
  {
    "number": 6702,
    "title": "[None][package] Pin cuda-python version to >=12,<13",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T10:23:36Z",
    "closed_at": "2025-08-07T14:01:04Z",
    "merged_at": "2025-08-07T14:01:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6702"
  },
  {
    "number": 6701,
    "title": "[https://nvbugs/5441438][fix] Set correct draft length for the cuda graph dummy request",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T09:44:12Z",
    "closed_at": "2025-08-12T01:28:48Z",
    "merged_at": "2025-08-12T01:28:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6701"
  },
  {
    "number": 6700,
    "title": "[None][fix] Migrate to new cuda binding package name",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T09:38:11Z",
    "closed_at": "2025-08-07T20:29:55Z",
    "merged_at": "2025-08-07T20:29:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6700"
  },
  {
    "number": 6699,
    "title": "[https://nvbugs/5429689][fix] Fix mllama model structure update with transformers issue",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T09:23:05Z",
    "closed_at": "2025-08-11T02:48:35Z",
    "merged_at": "2025-08-11T02:48:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6699"
  },
  {
    "number": 6698,
    "title": "[TRTLLM-6853][feat] refactor deepseekv3 model",
    "user": "kris1025",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T09:08:07Z",
    "closed_at": "2025-08-14T15:03:18Z",
    "merged_at": "2025-08-14T15:03:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6698"
  },
  {
    "number": 6696,
    "title": "[TRTLLM-5930][doc] 1.0 Documentation.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T08:38:54Z",
    "closed_at": "2025-09-04T09:29:43Z",
    "merged_at": "2025-09-04T09:29:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6696"
  },
  {
    "number": 6695,
    "title": "[None][doc] update feature_combination_matrix of SWA",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T08:35:23Z",
    "closed_at": "2025-08-08T01:27:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6695"
  },
  {
    "number": 6694,
    "title": "[https://nvbugs/5431127][fix] Run test_disaggregated_deepseek_v3_lite_fp8_nixl only on hopper",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T08:32:37Z",
    "closed_at": "2025-08-08T05:58:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6694"
  },
  {
    "number": 6693,
    "title": "[https://nvbugs/5442608][fix] Update CUDA graph config for get_model_yaml_config.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T08:21:39Z",
    "closed_at": "2025-08-10T05:48:56Z",
    "merged_at": "2025-08-10T05:48:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6693"
  },
  {
    "number": 6692,
    "title": "[None][doc] update feature_combination_matrix of SWA",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T08:21:33Z",
    "closed_at": "2025-08-07T08:23:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6692"
  },
  {
    "number": 6691,
    "title": "[None][doc] update feature_combination_matrix doc",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T08:03:12Z",
    "closed_at": "2025-08-26T00:25:32Z",
    "merged_at": "2025-08-26T00:25:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6691"
  },
  {
    "number": 6690,
    "title": "[https://nvbugs/5437106][fix] Fix llama4 scout TRTLLM attn_backend",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T07:55:41Z",
    "closed_at": "2025-08-08T09:48:23Z",
    "merged_at": "2025-08-08T09:48:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6690"
  },
  {
    "number": 6689,
    "title": "Draft: LoRA CUDA graph support",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T07:33:29Z",
    "closed_at": "2025-08-28T08:30:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6689"
  },
  {
    "number": 6688,
    "title": "[None][test] cherry-pick: correct test-db context for perf yaml file and add mistral cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T06:58:57Z",
    "closed_at": "2025-08-07T10:16:42Z",
    "merged_at": "2025-08-07T10:16:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6688"
  },
  {
    "number": 6687,
    "title": "[TRTLLM-5574][test] Add NIM required VLM models multi-gpu test",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T06:58:11Z",
    "closed_at": "2025-08-08T01:58:59Z",
    "merged_at": "2025-08-08T01:58:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6687"
  },
  {
    "number": 6686,
    "title": "[None][test] correct test-db context for perf yaml file",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T05:21:37Z",
    "closed_at": "2025-08-07T06:47:11Z",
    "merged_at": "2025-08-07T06:47:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6686"
  },
  {
    "number": 6685,
    "title": "[TRTLLM-5252][test] add for mistral_small_3.1_24b perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T03:01:45Z",
    "closed_at": "2025-08-08T02:57:04Z",
    "merged_at": "2025-08-08T02:57:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6685"
  },
  {
    "number": 6684,
    "title": "[TRTLLM-6541][test] Add NIM Related Cases Part 1",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T02:48:47Z",
    "closed_at": "2025-08-19T05:59:14Z",
    "merged_at": "2025-08-19T05:59:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6684"
  },
  {
    "number": 6683,
    "title": "[TRTLLM-5532][feat] store the block of context request into kv cache",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T02:43:31Z",
    "closed_at": "2025-08-11T08:14:53Z",
    "merged_at": "2025-08-11T08:14:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6683"
  },
  {
    "number": 6682,
    "title": "[None][test] Add Mistral Small 3.1 24B accuracy test to QA test list",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T02:41:27Z",
    "closed_at": "2025-08-07T07:24:36Z",
    "merged_at": "2025-08-07T07:24:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6682"
  },
  {
    "number": 6681,
    "title": "[None][doc]: remove the outdated features which marked as Experimental (#5995)",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T02:26:28Z",
    "closed_at": "2025-08-07T08:07:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6681"
  },
  {
    "number": 6679,
    "title": "[None][fix] Fix const modifier inconsistency in log function declaration/implementation",
    "user": "Fan-Yunfan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-07T01:29:59Z",
    "closed_at": "2025-08-21T03:08:11Z",
    "merged_at": "2025-08-21T03:08:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6679"
  },
  {
    "number": 6678,
    "title": "[TRTLLM-6656][chore] Validate FP8 support for Gemma3",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T21:17:54Z",
    "closed_at": "2025-08-07T17:14:04Z",
    "merged_at": "2025-08-07T17:14:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6678"
  },
  {
    "number": 6677,
    "title": "[None][infra] test b200 reservation",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T21:13:33Z",
    "closed_at": "2025-08-22T16:58:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6677"
  },
  {
    "number": 6676,
    "title": "[None][feat] Clean up ngram auto mode, add max_concurrency to configs",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T20:53:00Z",
    "closed_at": "2025-08-07T16:51:48Z",
    "merged_at": "2025-08-07T16:51:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6676"
  },
  {
    "number": 6675,
    "title": "[None][autodeploy] Waive sdpa attention matcher test",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T19:52:06Z",
    "closed_at": "2025-08-06T22:08:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6675"
  },
  {
    "number": 6674,
    "title": "[None][feat] adding support for disaggregated multi-instance tests",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T19:06:12Z",
    "closed_at": "2025-08-11T20:00:58Z",
    "merged_at": "2025-08-11T20:00:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6674"
  },
  {
    "number": 6673,
    "title": "[https://nvbugs/5385987][fix] Fix Qwen2 quantization issue by pinning transformers version",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T18:34:24Z",
    "closed_at": "2025-08-12T00:16:49Z",
    "merged_at": "2025-08-12T00:16:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6673"
  },
  {
    "number": 6672,
    "title": "Initial sampler topKtopP",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T18:12:00Z",
    "closed_at": "2025-08-08T20:15:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6672"
  },
  {
    "number": 6671,
    "title": "[None][fix] Fix get_model_config due to HF upgrade for GPT OSS",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T17:46:50Z",
    "closed_at": "2025-08-06T17:54:53Z",
    "merged_at": "2025-08-06T17:54:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6671"
  },
  {
    "number": 6670,
    "title": "[None][doc] Added GPT-OSS deployment guide and updated readme",
    "user": "farshadghodsian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T15:48:57Z",
    "closed_at": "2025-08-20T17:09:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6670"
  },
  {
    "number": 6669,
    "title": "[None][doc] Add deployment guide section to the official doc website",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T15:37:29Z",
    "closed_at": "2025-08-07T14:30:48Z",
    "merged_at": "2025-08-07T14:30:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6669"
  },
  {
    "number": 6668,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T15:32:01Z",
    "closed_at": "2025-08-07T03:07:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6668"
  },
  {
    "number": 6667,
    "title": "[None][doc] Add K2 tool calling examples",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T13:23:20Z",
    "closed_at": "2025-08-11T08:25:41Z",
    "merged_at": "2025-08-11T08:25:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6667"
  },
  {
    "number": 6666,
    "title": "[https://nvbugs/5433581][fix] Revert deep_gemm installation workaround for SBSA",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T10:27:53Z",
    "closed_at": "2025-08-06T10:50:54Z",
    "merged_at": "2025-08-06T10:50:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6666"
  },
  {
    "number": 6665,
    "title": "[TRTLLM-6650][fix] Enhance CUDA graph + Beam search to correctly handle padding",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T10:13:07Z",
    "closed_at": "2025-08-08T12:00:33Z",
    "merged_at": "2025-08-08T12:00:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6665"
  },
  {
    "number": 6664,
    "title": "[None][chore] update readme for perf release test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T09:38:19Z",
    "closed_at": "2025-08-07T00:00:45Z",
    "merged_at": "2025-08-07T00:00:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6664"
  },
  {
    "number": 6663,
    "title": "[None][fix] Explicitly add tiktoken as required by kimi k2",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T09:14:24Z",
    "closed_at": "2025-08-07T01:47:45Z",
    "merged_at": "2025-08-07T01:47:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6663"
  },
  {
    "number": 6662,
    "title": "[None][test] remove trt backend cases in release perf test and move NIM cases to llm_perf_nim.yml",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T09:10:55Z",
    "closed_at": "2025-08-07T00:02:13Z",
    "merged_at": "2025-08-07T00:02:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6662"
  },
  {
    "number": 6661,
    "title": "[None][infra] update feature_combination_matrix of disaggregated and chunked prefill",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T08:54:05Z",
    "closed_at": "2025-08-20T05:14:35Z",
    "merged_at": "2025-08-20T05:14:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6661"
  },
  {
    "number": 6660,
    "title": "[https://nvbugs/5409414][fix] fix Not registered specs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T08:30:39Z",
    "closed_at": "2025-08-07T07:55:53Z",
    "merged_at": "2025-08-07T07:55:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6660"
  },
  {
    "number": 6659,
    "title": "[TRTLLM-6892][infra] Run guardwords scan first in Release Check stage",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T08:07:21Z",
    "closed_at": "2025-08-07T03:00:16Z",
    "merged_at": "2025-08-07T03:00:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6659"
  },
  {
    "number": 6658,
    "title": "[https://nvbugs/5375966][chore] Unwaive test_disaggregated_deepseek_v3_lite_fp8_attention_dp_one",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T08:00:41Z",
    "closed_at": "2025-08-07T02:25:17Z",
    "merged_at": "2025-08-07T02:25:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6658"
  },
  {
    "number": 6657,
    "title": "[None][chore] optimize kv cache transfer for context TEP and  gen DEP",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T06:47:50Z",
    "closed_at": "2025-08-07T03:36:06Z",
    "merged_at": "2025-08-07T03:36:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6657"
  },
  {
    "number": 6656,
    "title": "[https://nvbugs/5438869][fix] Set nvfp4 expert w1 w3 weight scale to the same value if they're not",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T06:13:01Z",
    "closed_at": "2025-08-12T12:47:10Z",
    "merged_at": "2025-08-12T12:47:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6656"
  },
  {
    "number": 6655,
    "title": "[None][feat] Add support for Hopper MLA chunked prefill",
    "user": "jmydurant",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T05:23:22Z",
    "closed_at": "2025-08-14T02:39:26Z",
    "merged_at": "2025-08-14T02:39:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6655"
  },
  {
    "number": 6654,
    "title": "[None][refactor] Combine resmooth_to_fp8_e8m0 and transform_sf_into_required_layout",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T04:53:38Z",
    "closed_at": "2025-08-08T09:11:41Z",
    "merged_at": "2025-08-08T09:11:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6654"
  },
  {
    "number": 6653,
    "title": "[None][fix] Remove lock related typo in py_executor",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T03:40:59Z",
    "closed_at": "2025-08-08T09:48:58Z",
    "merged_at": "2025-08-08T09:48:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6653"
  },
  {
    "number": 6652,
    "title": "[None][chore] Bump version to 1.0.0",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T03:39:15Z",
    "closed_at": "2025-08-07T06:15:34Z",
    "merged_at": "2025-08-07T06:15:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6652"
  },
  {
    "number": 6651,
    "title": "[None][chore] Bump version to 1.1.0rc0",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T03:33:48Z",
    "closed_at": "2025-08-07T05:39:50Z",
    "merged_at": "2025-08-07T05:39:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6651"
  },
  {
    "number": 6650,
    "title": "[TRTLLM-6764][test] add new feature cases in cluster(B200/GB200) and sanity test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T02:46:00Z",
    "closed_at": "2025-08-06T05:45:14Z",
    "merged_at": "2025-08-06T05:45:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6650"
  },
  {
    "number": 6649,
    "title": "[None][doc] Unify the tech blogs naming.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T02:14:58Z",
    "closed_at": "2025-08-06T05:45:40Z",
    "merged_at": "2025-08-06T05:45:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6649"
  },
  {
    "number": 6648,
    "title": "[https://nvbugs/5430124][fix] Mistral mixture_text_image test case fix",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T02:02:25Z",
    "closed_at": "2025-08-06T13:58:58Z",
    "merged_at": "2025-08-06T13:58:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6648"
  },
  {
    "number": 6647,
    "title": "[None][doc] Exposing the GPT OSS model support blog",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-06T01:56:52Z",
    "closed_at": "2025-08-06T03:50:34Z",
    "merged_at": "2025-08-06T03:50:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6647"
  },
  {
    "number": 6646,
    "title": "[None][fix] Fix 6522 mpi.pkl5.intracomm.Request has wait not Wait",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T22:55:15Z",
    "closed_at": "2025-08-06T06:18:09Z",
    "merged_at": "2025-08-06T06:18:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6646"
  },
  {
    "number": 6645,
    "title": "[None] [feat] Add model gpt-oss",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T21:11:39Z",
    "closed_at": "2025-08-07T07:04:19Z",
    "merged_at": "2025-08-07T07:04:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6645"
  },
  {
    "number": 6644,
    "title": "[https://nvbugs/5328160][fix] Unwaive disaggregated serving tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T20:44:20Z",
    "closed_at": "2025-08-06T13:08:29Z",
    "merged_at": "2025-08-06T13:08:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6644"
  },
  {
    "number": 6643,
    "title": "[None][enhance] Add use chat template for random dataset.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T20:01:26Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6643"
  },
  {
    "number": 6642,
    "title": "Fa3 attention",
    "user": "zhumakhan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T19:33:24Z",
    "closed_at": "2025-08-05T19:33:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6642"
  },
  {
    "number": 6641,
    "title": "[None][feat] Add GPT OSS support for AutoDeploy",
    "user": "nvchenghaoz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T18:26:37Z",
    "closed_at": "2025-08-12T18:03:23Z",
    "merged_at": "2025-08-12T18:03:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6641"
  },
  {
    "number": 6640,
    "title": "[None][doc] Add llama4 hybrid guide",
    "user": "jiahanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T17:45:12Z",
    "closed_at": "2025-08-06T05:25:38Z",
    "merged_at": "2025-08-06T05:25:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6640"
  },
  {
    "number": 6639,
    "title": "[None][feat] Update Transformers",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T17:19:58Z",
    "closed_at": "2025-08-05T17:26:31Z",
    "merged_at": "2025-08-05T17:26:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6639"
  },
  {
    "number": 6638,
    "title": "Draft: Split atten_metadata",
    "user": "nvkgoyal",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T16:51:24Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6638"
  },
  {
    "number": 6637,
    "title": "[None][doc] Adding GPT-OSS Deployment Guide documentation",
    "user": "farshadghodsian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T16:35:06Z",
    "closed_at": "2025-08-05T17:19:48Z",
    "merged_at": "2025-08-05T17:19:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6637"
  },
  {
    "number": 6636,
    "title": "[None][doc] Update Enc_Dec README.md",
    "user": "esnvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T15:28:51Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6636"
  },
  {
    "number": 6635,
    "title": "[None][ci] move unittests to sub-directories",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T15:02:06Z",
    "closed_at": "2025-08-20T09:42:23Z",
    "merged_at": "2025-08-20T09:42:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6635"
  },
  {
    "number": 6634,
    "title": "[#5048][enhance] AutoDeploy: Optimize prepare_inputs",
    "user": "galagam",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T12:35:41Z",
    "closed_at": "2025-08-10T10:55:05Z",
    "merged_at": "2025-08-10T10:55:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6634"
  },
  {
    "number": 6633,
    "title": "[None][feat] move kv cache measure into transfer session",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T10:49:05Z",
    "closed_at": "2025-08-08T09:49:23Z",
    "merged_at": "2025-08-08T09:49:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6633"
  },
  {
    "number": 6632,
    "title": "[None][test] align kv_frac in perf test with perflab and add more cases for 4 gpus GB200",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T10:34:15Z",
    "closed_at": "2025-08-06T06:25:58Z",
    "merged_at": "2025-08-06T06:25:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6632"
  },
  {
    "number": 6631,
    "title": "[https://nvbugs/5436461][infra] Adjust free_gpu_memory_fraction of test_eagle3 to prevent OOM on CI",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T10:29:07Z",
    "closed_at": "2025-08-08T07:30:47Z",
    "merged_at": "2025-08-08T07:30:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6631"
  },
  {
    "number": 6630,
    "title": "[https://nvbugs/5433581][infra] Temporarily disable Docker Image use wheel from build stage",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T10:19:43Z",
    "closed_at": "2025-08-05T13:33:11Z",
    "merged_at": "2025-08-05T13:33:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6630"
  },
  {
    "number": 6629,
    "title": "[TRTLLM-5863][feat] Support MoE INT8 Weight-Only-Quantization in PyTorch Workflow",
    "user": "Yuening-wa",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T09:54:19Z",
    "closed_at": "2025-08-15T21:15:50Z",
    "merged_at": "2025-08-15T21:15:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6629"
  },
  {
    "number": 6628,
    "title": "[TRTLLM-6637][feat] Resolve KV cache divergence issue",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T09:43:06Z",
    "closed_at": "2025-08-09T15:15:04Z",
    "merged_at": "2025-08-09T15:15:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6628"
  },
  {
    "number": 6626,
    "title": "[None][fix] Fix unnecessary GPU synchronization in torch sampler caused by incorrect tensor reference",
    "user": "zhanghaotong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T09:26:22Z",
    "closed_at": "2025-08-08T03:44:47Z",
    "merged_at": "2025-08-08T03:44:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6626"
  },
  {
    "number": 6625,
    "title": "[#6187][feat] add LayerNorm module",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T09:04:09Z",
    "closed_at": "2025-08-12T19:43:31Z",
    "merged_at": "2025-08-12T19:43:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6625"
  },
  {
    "number": 6624,
    "title": "[None][perf] Improve the performance of online EPLB on Hopper by better overlapping",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T08:34:41Z",
    "closed_at": "2025-08-12T01:25:13Z",
    "merged_at": "2025-08-12T01:25:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6624"
  },
  {
    "number": 6623,
    "title": "[TRTLLM-6675][infra] Nixl test completion",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T08:23:45Z",
    "closed_at": "2025-08-08T02:15:55Z",
    "merged_at": "2025-08-08T02:15:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6623"
  },
  {
    "number": 6622,
    "title": "[TRTLLM-6772][feat] Multimodal benchmark_serving support",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T08:18:58Z",
    "closed_at": "2025-08-13T02:34:03Z",
    "merged_at": "2025-08-13T02:34:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6622"
  },
  {
    "number": 6621,
    "title": "doc: cherry-pick 6012 changes",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T07:18:16Z",
    "closed_at": "2025-08-05T07:19:47Z",
    "merged_at": "2025-08-05T07:19:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6621"
  },
  {
    "number": 6620,
    "title": "doc: update index.rst for 1.0 doc",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T06:46:54Z",
    "closed_at": "2025-08-05T06:57:15Z",
    "merged_at": "2025-08-05T06:57:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6620"
  },
  {
    "number": 6619,
    "title": "[None][doc] Add doc for multimodal feature support matrix",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T05:47:48Z",
    "closed_at": "2025-08-08T06:20:29Z",
    "merged_at": "2025-08-08T06:20:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6619"
  },
  {
    "number": 6618,
    "title": "[TRTLLM-6080]doc: update doc per TW comments",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T05:39:21Z",
    "closed_at": "2025-08-05T06:56:05Z",
    "merged_at": "2025-08-05T06:56:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6618"
  },
  {
    "number": 6617,
    "title": "[https://nvbugs/5436461][infra] Skip test_eagle3 test with device memory check",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T05:06:59Z",
    "closed_at": "2025-08-05T06:36:03Z",
    "merged_at": "2025-08-05T06:36:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6617"
  },
  {
    "number": 6616,
    "title": "[TRTLLM-6898][feat] make fused_moe_cute_dsl work on blackwell",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T04:40:54Z",
    "closed_at": "2025-08-08T07:03:49Z",
    "merged_at": "2025-08-08T07:03:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6616"
  },
  {
    "number": 6615,
    "title": "[None][refactor] Refactor Torch Compile Backend, MoeLoadBalancer and warmup Logic",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T03:30:39Z",
    "closed_at": "2025-08-19T01:58:45Z",
    "merged_at": "2025-08-19T01:58:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6615"
  },
  {
    "number": 6614,
    "title": "[None][fix] Adjust default moe_max_num_tokens to fix OOM.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T02:52:07Z",
    "closed_at": "2025-08-13T02:28:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6614"
  },
  {
    "number": 6612,
    "title": "[None][doc] Fix blog4 typo",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-05T01:47:05Z",
    "closed_at": "2025-08-05T02:20:37Z",
    "merged_at": "2025-08-05T02:20:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6612"
  },
  {
    "number": 6611,
    "title": "[TRTLLM-5252][fix] Propagate mapping to intermediate layers",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T22:21:08Z",
    "closed_at": "2025-08-08T05:50:37Z",
    "merged_at": "2025-08-08T05:50:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6611"
  },
  {
    "number": 6610,
    "title": "[None][doc] Created Deployment Guide for SGLang DeepSeek-R1 FP8 and NVFP4",
    "user": "jamieliNVIDIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T18:09:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6610"
  },
  {
    "number": 6609,
    "title": "Update CMakeLists.txt extend find_library names",
    "user": "mc-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T18:00:22Z",
    "closed_at": "2025-09-05T22:09:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6609"
  },
  {
    "number": 6608,
    "title": "[None][feat] Enable nanobind as the default binding library",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T17:37:55Z",
    "closed_at": "2025-08-22T07:48:41Z",
    "merged_at": "2025-08-22T07:48:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6608"
  },
  {
    "number": 6607,
    "title": "[https://nvbugs/5433581][infra] Update install docs and CI script for SBSA deep_gemm workaround",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T17:01:27Z",
    "closed_at": "2025-08-05T03:36:39Z",
    "merged_at": "2025-08-05T03:36:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6607"
  },
  {
    "number": 6605,
    "title": "[TRTLLM-5633][infra] Change the TOT repo to default-llm-repo for merge waive list",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T14:12:25Z",
    "closed_at": "2025-08-06T10:19:04Z",
    "merged_at": "2025-08-06T10:19:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6605"
  },
  {
    "number": 6604,
    "title": "[None][chore] Enhance trtllm-serve example test",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T12:51:02Z",
    "closed_at": "2025-08-06T12:30:35Z",
    "merged_at": "2025-08-06T12:30:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6604"
  },
  {
    "number": 6603,
    "title": "[TRTLLM-6092][doc] Add LoRA feature usage doc",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T12:10:50Z",
    "closed_at": "2025-08-07T09:24:13Z",
    "merged_at": "2025-08-07T09:24:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6603"
  },
  {
    "number": 6602,
    "title": "[None][infra] Waive failed case in post-merge on main",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T10:42:42Z",
    "closed_at": "2025-08-04T11:39:44Z",
    "merged_at": "2025-08-04T11:39:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6602"
  },
  {
    "number": 6600,
    "title": "[TRTQA-2920][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T09:14:52Z",
    "closed_at": "2025-08-05T10:12:55Z",
    "merged_at": "2025-08-05T10:12:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6600"
  },
  {
    "number": 6598,
    "title": "[TRTLLM-6364] [fix] Update PR title regex to allow optional spaces between ticket and type",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T09:01:17Z",
    "closed_at": "2025-08-04T10:34:25Z",
    "merged_at": "2025-08-04T10:34:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6598"
  },
  {
    "number": 6597,
    "title": "[None][chore] Bump version to 1.0.0rc6",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T08:16:22Z",
    "closed_at": "2025-08-04T08:39:15Z",
    "merged_at": "2025-08-04T08:39:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6597"
  },
  {
    "number": 6596,
    "title": "[None][test] update invalid test name",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T07:12:17Z",
    "closed_at": "2025-08-04T12:01:06Z",
    "merged_at": "2025-08-04T12:01:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6596"
  },
  {
    "number": 6594,
    "title": "[None][infra] Split GB200 stages for each test",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T06:17:02Z",
    "closed_at": "2025-08-05T11:10:01Z",
    "merged_at": "2025-08-05T11:10:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6594"
  },
  {
    "number": 6593,
    "title": "[TRTLLM-6864][feat] add CONTEXT_ONLY benchmark flag in disagg-server",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T05:33:00Z",
    "closed_at": "2025-08-06T02:23:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6593"
  },
  {
    "number": 6592,
    "title": "[TRTLLM-6823][doc] Add checkpoint refactor docs",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T05:32:44Z",
    "closed_at": "2025-08-10T23:47:40Z",
    "merged_at": "2025-08-10T23:47:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6592"
  },
  {
    "number": 6591,
    "title": "[None][chore] Update Gemma3 closeness check to mitigate flakiness",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T05:19:50Z",
    "closed_at": "2025-08-04T14:10:58Z",
    "merged_at": "2025-08-04T14:10:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6591"
  },
  {
    "number": 6590,
    "title": "[None][chore] add missing tests to test list",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T05:14:51Z",
    "closed_at": "2025-08-06T14:12:27Z",
    "merged_at": "2025-08-06T14:12:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6590"
  },
  {
    "number": 6589,
    "title": "[None][fix] fix kimi k2 serving and add test for Kimi-K2",
    "user": "pengbowang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T04:56:18Z",
    "closed_at": "2025-08-05T10:05:33Z",
    "merged_at": "2025-08-05T10:05:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6589"
  },
  {
    "number": 6588,
    "title": "[https://nvbugs/5433581][fix] DeepGEMM installation on SBSA",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T04:17:11Z",
    "closed_at": "2025-08-06T08:44:21Z",
    "merged_at": "2025-08-06T08:44:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6588"
  },
  {
    "number": 6587,
    "title": "[None][feat] Support CancelRequest for Disaggregated Serving",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T03:58:44Z",
    "closed_at": "2025-10-09T18:44:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6587"
  },
  {
    "number": 6586,
    "title": "[TRTLLM-6992][feat] Add runtime swap AB for SM100 FP8 blockwise GEMM",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T03:46:22Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6586"
  },
  {
    "number": 6585,
    "title": "[https://nvbugs/5430932][infra] update namelist",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T03:42:34Z",
    "closed_at": "2025-08-04T03:51:08Z",
    "merged_at": "2025-08-04T03:51:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6585"
  },
  {
    "number": 6584,
    "title": "[https://nvbugs/5409420][fix] Fix test_ptp_star_attention_example",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T03:33:59Z",
    "closed_at": "2025-08-11T02:14:20Z",
    "merged_at": "2025-08-11T02:14:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6584"
  },
  {
    "number": 6583,
    "title": "[None][doc] Align example disagg config cache_transceiver_config values",
    "user": "yifeizhang-c",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T03:27:19Z",
    "closed_at": "2025-08-04T08:09:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6583"
  },
  {
    "number": 6582,
    "title": "[https://nvbugs/5355007][fix] Set `enable_chunked_context` as True by default in trtllm bench",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T03:14:41Z",
    "closed_at": "2025-08-05T18:11:36Z",
    "merged_at": "2025-08-05T18:11:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6582"
  },
  {
    "number": 6581,
    "title": "[TRTQA-2920][fix] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T02:01:08Z",
    "closed_at": "2025-08-05T02:41:24Z",
    "merged_at": "2025-08-05T02:41:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6581"
  },
  {
    "number": 6580,
    "title": "doc: Add link to the examples for deploying Dynamo with TRT-LLM on K8s",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-04T00:19:49Z",
    "closed_at": "2025-08-05T22:09:59Z",
    "merged_at": "2025-08-05T22:09:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6580"
  },
  {
    "number": 6579,
    "title": "[TRTLLM-6859][doc] Add DeepSeek R1 deployment guide.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-03T16:10:27Z",
    "closed_at": "2025-08-06T14:13:54Z",
    "merged_at": "2025-08-06T14:13:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6579"
  },
  {
    "number": 6578,
    "title": "Temp",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-03T15:35:12Z",
    "closed_at": "2025-08-05T21:06:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6578"
  },
  {
    "number": 6576,
    "title": "[None][fix] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-03T12:15:50Z",
    "closed_at": "2025-08-04T05:52:11Z",
    "merged_at": "2025-08-04T05:52:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6576"
  },
  {
    "number": 6575,
    "title": "doc: [TRTLLM-6089] Add long sequence document for Feature section",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-03T11:29:03Z",
    "closed_at": "2025-08-06T05:29:54Z",
    "merged_at": "2025-08-06T05:29:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6575"
  },
  {
    "number": 6574,
    "title": "[TRTLLM-6174][feat] Enable FP32 mamba ssm cache",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-03T08:34:04Z",
    "closed_at": "2025-08-10T20:27:52Z",
    "merged_at": "2025-08-10T20:27:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6574"
  },
  {
    "number": 6573,
    "title": "[None][fix] xqa precision for fp16/bf16 kv cache",
    "user": "Bruce-Lee-LY",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-03T03:49:42Z",
    "closed_at": "2025-08-04T06:34:20Z",
    "merged_at": "2025-08-04T06:34:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6573"
  },
  {
    "number": 6572,
    "title": "[None][feat] Switch to internal version of MMProjector in Gemma3",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-03T02:47:49Z",
    "closed_at": "2025-08-06T01:48:23Z",
    "merged_at": "2025-08-06T01:48:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6572"
  },
  {
    "number": 6571,
    "title": "[TRTLLM-5563][infra] Move test_rerun.py to script folder",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-02T09:49:53Z",
    "closed_at": "2025-08-04T05:26:05Z",
    "merged_at": "2025-08-04T05:26:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6571"
  },
  {
    "number": 6570,
    "title": "[None][fix] Fix attention dp log",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-02T06:32:16Z",
    "closed_at": "2025-08-12T08:53:10Z",
    "merged_at": "2025-08-12T08:53:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6570"
  },
  {
    "number": 6568,
    "title": "[None][feat] Add GPTQ Int8 Options for Qwen TRT path",
    "user": "JyChang012",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T23:49:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6568"
  },
  {
    "number": 6567,
    "title": "[TRTLLM-6090] doc: Add multimodal part to the feature section",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T22:08:21Z",
    "closed_at": "2025-08-05T07:20:45Z",
    "merged_at": "2025-08-05T07:20:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6567"
  },
  {
    "number": 6566,
    "title": "Dependencies upgrade",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T19:42:55Z",
    "closed_at": "2025-08-01T19:44:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6566"
  },
  {
    "number": 6565,
    "title": "[None][doc] Add new doc",
    "user": "jamieliNVIDIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T18:46:02Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6565"
  },
  {
    "number": 6564,
    "title": "[TRTLLM-5500][infra] Update CODEOWNERS with new ownership rules for additional paths",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T17:34:16Z",
    "closed_at": "2025-08-05T19:54:24Z",
    "merged_at": "2025-08-05T19:54:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6564"
  },
  {
    "number": 6563,
    "title": "[TRTLLM-6881][feat] Include attention dp rank info with KV cache events",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T17:30:46Z",
    "closed_at": "2025-08-07T12:17:07Z",
    "merged_at": "2025-08-07T12:17:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6563"
  },
  {
    "number": 6562,
    "title": "[TRTLLM-6069]doc: add trtllm-serve usage for cli reference section",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T16:31:44Z",
    "closed_at": "2025-09-17T03:32:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6562"
  },
  {
    "number": 6561,
    "title": "[None][benchmark] measure the first two tokens interval",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T15:59:06Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6561"
  },
  {
    "number": 6560,
    "title": "[None][chore] Add unit test for Gemma3 lora",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T15:42:52Z",
    "closed_at": "2025-08-04T08:56:57Z",
    "merged_at": "2025-08-04T08:56:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6560"
  },
  {
    "number": 6559,
    "title": "[None][refactor] Simplify decoder state initialization",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T15:22:27Z",
    "closed_at": "2025-08-12T19:44:41Z",
    "merged_at": "2025-08-12T19:44:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6559"
  },
  {
    "number": 6558,
    "title": "[None][Infra] - Skip failed tests in post-merge",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T12:37:52Z",
    "closed_at": "2025-08-01T14:21:23Z",
    "merged_at": "2025-08-01T14:21:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6558"
  },
  {
    "number": 6557,
    "title": "[https://nvbugs/5410391][bug] Support to share device buffers in attention meta",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T12:16:39Z",
    "closed_at": "2025-08-22T05:19:27Z",
    "merged_at": "2025-08-22T05:19:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6557"
  },
  {
    "number": 6556,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T10:03:03Z",
    "closed_at": "2025-08-01T12:08:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6556"
  },
  {
    "number": 6555,
    "title": "[TRTLLM-6893][infra] fix Build Docker Image tag issue",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T09:57:03Z",
    "closed_at": "2025-08-05T08:33:37Z",
    "merged_at": "2025-08-05T08:33:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6555"
  },
  {
    "number": 6554,
    "title": "[https://nvbugs/5252313][fix] Fix torch compile + MTP",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T09:30:22Z",
    "closed_at": "2025-08-05T14:31:30Z",
    "merged_at": "2025-08-05T14:31:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6554"
  },
  {
    "number": 6553,
    "title": "[None][doc] Exposing the latest tech blogs in README.md",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T08:52:36Z",
    "closed_at": "2025-08-01T09:41:53Z",
    "merged_at": "2025-08-01T09:41:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6553"
  },
  {
    "number": 6552,
    "title": "[None][fix] update nemotron nas tests free_gpu_memory_fraction=0.8",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T08:52:20Z",
    "closed_at": "2025-08-01T10:27:23Z",
    "merged_at": "2025-08-01T10:27:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6552"
  },
  {
    "number": 6550,
    "title": "[None][doc] Create deployment guide for Llama4 Scout FP8 and NVFP4",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T07:59:25Z",
    "closed_at": "2025-08-06T14:15:24Z",
    "merged_at": "2025-08-06T14:15:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6550"
  },
  {
    "number": 6549,
    "title": "[None][infra] Pin the version for triton to 3.3.1 (#6508) (#6519)",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T07:56:38Z",
    "closed_at": "2025-08-01T11:33:24Z",
    "merged_at": "2025-08-01T11:33:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6549"
  },
  {
    "number": 6548,
    "title": "[None][feat] improve dataloading for benchmark_dataset by using batch…",
    "user": "zerollzeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T07:52:25Z",
    "closed_at": "2025-08-11T01:50:41Z",
    "merged_at": "2025-08-11T01:50:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6548"
  },
  {
    "number": 6547,
    "title": "[None][doc] blog: Scaling Expert Parallelism in TensorRT-LLM (Part 2: Performance Status and Optimization)",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T07:32:08Z",
    "closed_at": "2025-08-01T08:46:15Z",
    "merged_at": "2025-08-01T08:46:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6547"
  },
  {
    "number": 6546,
    "title": "feat: Support custom repo_dir for SLURM script ",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T07:16:05Z",
    "closed_at": "2025-08-13T02:06:59Z",
    "merged_at": "2025-08-13T02:06:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6546"
  },
  {
    "number": 6545,
    "title": "[TRTLLM-4501][feat] AutoTuner tuning config refactor and valid tactic generalization.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T06:44:43Z",
    "closed_at": "2025-08-13T08:25:22Z",
    "merged_at": "2025-08-13T08:25:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6545"
  },
  {
    "number": 6544,
    "title": "[None][chore] Mass integration of release/0.21 (part5)",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T06:44:26Z",
    "closed_at": "2025-08-04T03:19:59Z",
    "merged_at": "2025-08-04T03:19:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6544"
  },
  {
    "number": 6543,
    "title": "[None][doc] Create deployment guide for Llama 3.3 70B FP8 and NVFP4",
    "user": "jamieliNVIDIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T05:53:24Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6543"
  },
  {
    "number": 6542,
    "title": "[None][feat] Add test for speculative rejection sampler (2-model)",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T05:35:47Z",
    "closed_at": "2025-08-14T02:09:36Z",
    "merged_at": "2025-08-14T02:09:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6542"
  },
  {
    "number": 6541,
    "title": "[None][fix] Fix NCCL Ops when using MoE chunking",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T05:22:27Z",
    "closed_at": "2025-08-05T07:50:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6541"
  },
  {
    "number": 6540,
    "title": "[TRTLLM-6263][feat] Enable fp8 SwiGLU to minimize host overhead",
    "user": "JunyiXu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T05:04:40Z",
    "closed_at": "2025-08-06T02:42:19Z",
    "merged_at": "2025-08-06T02:42:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6540"
  },
  {
    "number": 6538,
    "title": "[None][feat] Use Separate QKV Input Layout for Context MLA",
    "user": "zhhuang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T04:42:05Z",
    "closed_at": "2025-08-19T14:04:48Z",
    "merged_at": "2025-08-19T14:04:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6538"
  },
  {
    "number": 6537,
    "title": "[https://nvbugs/5394392][fix] Enlarge scheduler capacity under disagg bs == 1",
    "user": "yifeizhang-c",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T03:41:33Z",
    "closed_at": "2025-08-15T16:52:06Z",
    "merged_at": "2025-08-15T16:52:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6537"
  },
  {
    "number": 6536,
    "title": "[TRTLLM-6856][feat] add disaggregated serving tests to QA list",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T03:17:26Z",
    "closed_at": "2025-08-05T02:47:53Z",
    "merged_at": "2025-08-05T02:47:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6536"
  },
  {
    "number": 6535,
    "title": "[None][doc] add introduction doc on qa test",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T03:14:07Z",
    "closed_at": "2025-08-05T09:02:18Z",
    "merged_at": "2025-08-05T09:02:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6535"
  },
  {
    "number": 6534,
    "title": "doc: move the parallelism.md to features folder.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T03:03:59Z",
    "closed_at": "2025-08-01T03:10:26Z",
    "merged_at": "2025-08-01T03:10:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6534"
  },
  {
    "number": 6533,
    "title": "test: move ministral_8b_fp8 to fp8_specific gpu list(exclude Ampere)",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T02:50:29Z",
    "closed_at": "2025-08-04T05:22:39Z",
    "merged_at": "2025-08-04T05:22:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6533"
  },
  {
    "number": 6532,
    "title": "TRTLLM-6080 doc: Add Paged Attention, IFB, and Request Scheduling page",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-01T00:08:26Z",
    "closed_at": "2025-08-05T05:12:33Z",
    "merged_at": "2025-08-05T05:12:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6532"
  },
  {
    "number": 6531,
    "title": "[https://nvbugs/5423962][fix] Address broken links",
    "user": "chenopis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T20:50:17Z",
    "closed_at": "2025-08-07T20:00:05Z",
    "merged_at": "2025-08-07T20:00:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6531"
  },
  {
    "number": 6528,
    "title": "[None][doc] Move AutoDeploy README.md to torch docs",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T18:49:29Z",
    "closed_at": "2025-08-08T23:11:45Z",
    "merged_at": "2025-08-08T23:11:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6528"
  },
  {
    "number": 6527,
    "title": "[TRTLLM-6064] doc: add installation section",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T16:29:10Z",
    "closed_at": "2025-08-01T02:01:39Z",
    "merged_at": "2025-08-01T02:01:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6527"
  },
  {
    "number": 6526,
    "title": "[None][fix] Serialize the window_size in the kv event",
    "user": "richardhuo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T16:16:57Z",
    "closed_at": "2025-08-01T22:25:19Z",
    "merged_at": "2025-08-01T22:25:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6526"
  },
  {
    "number": 6525,
    "title": "doc: remove useless developer guide folder",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T15:45:39Z",
    "closed_at": "2025-07-31T15:50:11Z",
    "merged_at": "2025-07-31T15:50:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6525"
  },
  {
    "number": 6524,
    "title": "[None][refactor] Simplify finish reasons handling in DecoderState",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T15:43:33Z",
    "closed_at": "2025-08-02T05:17:44Z",
    "merged_at": "2025-08-02T05:17:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6524"
  },
  {
    "number": 6523,
    "title": "fix: correct scaling factor calculation in tests/unittest/trt/functional/test_fp4_gemm.py",
    "user": "muse-coder",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T15:20:42Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6523"
  },
  {
    "number": 6522,
    "title": "[TRTLLM-6826][feat] Allow sending more than 2GiB through MPI by using mpi4py.util.pkl5",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T15:19:54Z",
    "closed_at": "2025-08-05T08:28:27Z",
    "merged_at": "2025-08-05T08:28:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6522"
  },
  {
    "number": 6521,
    "title": "[None][fix] Remove expand  configuration from mamba2 mixer",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T15:10:41Z",
    "closed_at": "2025-08-05T08:18:26Z",
    "merged_at": "2025-08-05T08:18:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6521"
  },
  {
    "number": 6520,
    "title": "[AutoDeploy] merge feat/ad-2025-07-22",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T15:00:42Z",
    "closed_at": "2025-08-01T15:51:08Z",
    "merged_at": "2025-08-01T15:51:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6520"
  },
  {
    "number": 6519,
    "title": "[None][infra] Pin the version for triton to 3.3.1 (#6508)",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T14:54:57Z",
    "closed_at": "2025-08-01T07:04:38Z",
    "merged_at": "2025-08-01T07:04:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6519"
  },
  {
    "number": 6518,
    "title": "DRAFT: Prepare inputs optimizations",
    "user": "galagam",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T12:24:02Z",
    "closed_at": "2025-07-31T12:29:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6518"
  },
  {
    "number": 6515,
    "title": "[TRTLLM-4279] fix: Add a protection test for checking trtllm custom ops",
    "user": "yali-arch",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T11:42:58Z",
    "closed_at": "2025-08-01T07:59:09Z",
    "merged_at": "2025-08-01T07:59:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6515"
  },
  {
    "number": 6514,
    "title": "[TRTLLM-6898] [feat] Add CuTe DSL fp8 gemm, bmm and group gemm ops on Blackwell architecuture",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T10:35:25Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6514"
  },
  {
    "number": 6512,
    "title": "refactor: Return the first token ahead of time",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T09:17:35Z",
    "closed_at": "2025-08-07T17:40:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6512"
  },
  {
    "number": 6511,
    "title": "chore: Make example SLURM scripts more parameterized",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T09:14:38Z",
    "closed_at": "2025-08-01T04:53:16Z",
    "merged_at": "2025-08-01T04:53:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6511"
  },
  {
    "number": 6510,
    "title": "[TRTLLM-6683][feat] Support LoRA reload CPU cache evicted adapter",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T08:48:22Z",
    "closed_at": "2025-08-07T06:05:37Z",
    "merged_at": "2025-08-07T06:05:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6510"
  },
  {
    "number": 6509,
    "title": "[TRTLLM-6096]doc: update quick links section",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T08:32:44Z",
    "closed_at": "2025-07-31T11:41:41Z",
    "merged_at": "2025-07-31T11:41:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6509"
  },
  {
    "number": 6508,
    "title": "[None][infra] Pin the version for triton to 3.3.1",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T08:30:43Z",
    "closed_at": "2025-07-31T11:25:16Z",
    "merged_at": "2025-07-31T11:25:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6508"
  },
  {
    "number": 6506,
    "title": "[None][fix] Upgrade dependencies version to avoid security vulnerability",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T07:09:27Z",
    "closed_at": "2025-08-06T21:21:04Z",
    "merged_at": "2025-08-06T21:21:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6506"
  },
  {
    "number": 6505,
    "title": "Fix merge waive list 2",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T06:43:00Z",
    "closed_at": "2025-07-31T07:03:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6505"
  },
  {
    "number": 6504,
    "title": "[Infra][TRTLLM-5633] - Fix merge waive list",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T06:02:22Z",
    "closed_at": "2025-07-31T06:57:51Z",
    "merged_at": "2025-07-31T06:57:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6504"
  },
  {
    "number": 6502,
    "title": "[https://nvbugs/5412562][feat] Allocate MoE workspace only when necessary",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T04:54:22Z",
    "closed_at": "2025-08-15T16:29:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6502"
  },
  {
    "number": 6501,
    "title": "[https://nvbugs/5415862][fix] Update cublas as 12.9.1 and cuda memory alignment as 256",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T03:35:27Z",
    "closed_at": "2025-08-15T03:10:59Z",
    "merged_at": "2025-08-15T03:10:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6501"
  },
  {
    "number": 6500,
    "title": "Feat/mtp opt 2 lamport allgather",
    "user": "ameynaik-hub",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T02:27:03Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6500"
  },
  {
    "number": 6499,
    "title": "fix: Fix poor generation with FP8 Gemma3 1B checkpoint",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T02:23:26Z",
    "closed_at": "2025-08-01T00:18:24Z",
    "merged_at": "2025-08-01T00:18:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6499"
  },
  {
    "number": 6498,
    "title": "[fix] Fix DeepSeek w4a8 weight loading",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T02:02:24Z",
    "closed_at": "2025-08-04T02:12:06Z",
    "merged_at": "2025-08-04T02:12:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6498"
  },
  {
    "number": 6497,
    "title": "[None][fix] Refactoring input prep to allow out-of-tree models",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-31T00:16:57Z",
    "closed_at": "2025-08-13T00:29:11Z",
    "merged_at": "2025-08-13T00:29:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6497"
  },
  {
    "number": 6496,
    "title": "[None][feat] Add support for fused gate_up_proj scales for FP8 blockwise",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T23:17:02Z",
    "closed_at": "2025-08-05T18:22:33Z",
    "merged_at": "2025-08-05T18:22:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6496"
  },
  {
    "number": 6495,
    "title": "[None][doc] Update Documentation link to point to docs instead of docs source code",
    "user": "asrivas",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T23:05:35Z",
    "closed_at": "2025-09-17T20:39:18Z",
    "merged_at": "2025-09-17T20:39:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6495"
  },
  {
    "number": 6494,
    "title": "[TRTLLM-6812][feat] Add standardized GitHub issue templates and disable blank issues",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T21:23:20Z",
    "closed_at": "2025-08-11T17:08:49Z",
    "merged_at": "2025-08-11T17:08:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6494"
  },
  {
    "number": 6493,
    "title": "[TRTLLM-6420][feat] add support for Eclairv2 model - cherry-pick changes and minor fix",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T21:20:58Z",
    "closed_at": "2025-08-09T01:40:49Z",
    "merged_at": "2025-08-09T01:40:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6493"
  },
  {
    "number": 6491,
    "title": "[nvbug/5374773] chore: Update nanobind with fail_fast_on_attention_window_too_large changes",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T17:33:06Z",
    "closed_at": "2025-07-31T19:25:30Z",
    "merged_at": "2025-07-31T19:25:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6491"
  },
  {
    "number": 6490,
    "title": "[infra] Remove auto_assign_reviewers option from .coderabbit.yaml",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T17:32:26Z",
    "closed_at": "2025-07-31T06:07:21Z",
    "merged_at": "2025-07-31T06:07:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6490"
  },
  {
    "number": 6489,
    "title": "[None][fix] disagg ctx pp4 + gen pp4 integ test",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T17:07:25Z",
    "closed_at": "2025-08-07T15:18:02Z",
    "merged_at": "2025-08-07T15:18:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6489"
  },
  {
    "number": 6488,
    "title": "[None][feat] KV Cache Connector API",
    "user": "jthomson04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T16:04:18Z",
    "closed_at": "2025-09-03T17:58:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6488"
  },
  {
    "number": 6487,
    "title": "[None][test] Test trtllm-bench AD vs, PT BEs on H100 single gpu",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T15:19:20Z",
    "closed_at": "2025-08-11T05:33:14Z",
    "merged_at": "2025-08-11T05:33:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6487"
  },
  {
    "number": 6486,
    "title": "Deepseek R1 FP8 Support on Blackwell",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T13:58:50Z",
    "closed_at": "2025-08-01T02:26:29Z",
    "merged_at": "2025-08-01T02:26:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6486"
  },
  {
    "number": 6485,
    "title": "[https://nvbugs/5404046][fix] Fix Nemotron-H flaky CUDA graph / overlap scheduler test",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T13:45:10Z",
    "closed_at": "2025-07-31T18:35:11Z",
    "merged_at": "2025-07-31T18:35:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6485"
  },
  {
    "number": 6484,
    "title": "refactor: Remove unused buffers and bindings from sampler",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T10:49:33Z",
    "closed_at": "2025-08-01T04:43:04Z",
    "merged_at": "2025-08-01T04:43:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6484"
  },
  {
    "number": 6483,
    "title": "[None][infra] Enable test of chunked prefill with logit post processor",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T10:28:38Z",
    "closed_at": "2025-08-04T05:46:07Z",
    "merged_at": "2025-08-04T05:46:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6483"
  },
  {
    "number": 6482,
    "title": "[None][chore] Disable add special tokens for Llama3.3 70B",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T10:24:46Z",
    "closed_at": "2025-08-01T09:03:27Z",
    "merged_at": "2025-08-01T09:03:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6482"
  },
  {
    "number": 6481,
    "title": "fix: remove duplicate layer multiplication in KV cache size calculation",
    "user": "jaedeok-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T09:53:54Z",
    "closed_at": "2025-08-01T02:34:34Z",
    "merged_at": "2025-08-01T02:34:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6481"
  },
  {
    "number": 6480,
    "title": "doc: add bielik on support-matrix.md",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T09:16:19Z",
    "closed_at": "2025-07-31T04:48:54Z",
    "merged_at": "2025-07-31T04:48:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6480"
  },
  {
    "number": 6479,
    "title": "test: add accuracy reference",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T08:55:04Z",
    "closed_at": "2025-07-31T02:27:29Z",
    "merged_at": "2025-07-31T02:27:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6479"
  },
  {
    "number": 6478,
    "title": "[None][feat] Refactor Llava-Next",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T08:43:16Z",
    "closed_at": "2025-08-06T00:53:54Z",
    "merged_at": "2025-08-06T00:53:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6478"
  },
  {
    "number": 6477,
    "title": "[doc][ci][Qwen3][nvbugs 5374145] Add Qwen3 235B eagle3 CI",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T08:16:49Z",
    "closed_at": "2025-07-31T01:37:23Z",
    "merged_at": "2025-07-31T01:37:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6477"
  },
  {
    "number": 6476,
    "title": "[TRTLLM-6685][feat] Add speculative metrics for trt llm bench",
    "user": "kris1025",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T06:55:14Z",
    "closed_at": "2025-08-04T22:22:57Z",
    "merged_at": "2025-08-04T22:22:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6476"
  },
  {
    "number": 6475,
    "title": "[TRTLLM-6222][feat] NVFP4 Quantization for DeepSeek-R1 MTP and Attention ",
    "user": "binghanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T06:54:26Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6475"
  },
  {
    "number": 6474,
    "title": "test: modify max_lora_rank of phi4_multimodal to 320",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T06:49:58Z",
    "closed_at": "2025-08-04T02:20:23Z",
    "merged_at": "2025-08-04T02:20:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6474"
  },
  {
    "number": 6472,
    "title": "[None][fix] fix: resolve GPU memory imbalance in concurrent weight loading",
    "user": "Nekofish-L",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T06:32:12Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6472"
  },
  {
    "number": 6471,
    "title": "fix: Fix missing key",
    "user": "zerollzeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T06:02:44Z",
    "closed_at": "2025-08-01T06:25:59Z",
    "merged_at": "2025-08-01T06:25:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6471"
  },
  {
    "number": 6470,
    "title": "[None][feat] Add Qwen3 MoE support to TensorRT backend",
    "user": "gkswns0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T05:36:08Z",
    "closed_at": "2025-08-06T09:02:36Z",
    "merged_at": "2025-08-06T09:02:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6470"
  },
  {
    "number": 6469,
    "title": "TEST CI DEBUG WORKFLOW",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T03:38:01Z",
    "closed_at": "2025-08-01T11:36:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6469"
  },
  {
    "number": 6468,
    "title": "support 1.0 parallelism doc",
    "user": "Kefeng-Duan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T03:34:06Z",
    "closed_at": "2025-08-01T02:12:50Z",
    "merged_at": "2025-08-01T02:12:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6468"
  },
  {
    "number": 6467,
    "title": "[https://nvbugs/5419066][fix] Use trt flow LLM",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T03:20:05Z",
    "closed_at": "2025-08-01T07:33:08Z",
    "merged_at": "2025-08-01T07:33:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6467"
  },
  {
    "number": 6466,
    "title": "test: Add time logging for lora tests",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T02:47:08Z",
    "closed_at": "2025-07-30T21:02:43Z",
    "merged_at": "2025-07-30T21:02:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6466"
  },
  {
    "number": 6465,
    "title": "[None][infra] update namelist",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T02:42:44Z",
    "closed_at": "2025-08-04T03:32:34Z",
    "merged_at": "2025-08-04T03:32:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6465"
  },
  {
    "number": 6464,
    "title": "[TRTLLM-6761][refactor] Replace LogitBiasLogitsProcessor with embedding bias tensor system",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-30T01:09:18Z",
    "closed_at": "2025-08-05T14:14:25Z",
    "merged_at": "2025-08-05T14:14:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6464"
  },
  {
    "number": 6463,
    "title": "[fix] Move kv_cache_free_gpu_mem_fraction arg to benchmark command in tests",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T23:57:22Z",
    "closed_at": "2025-07-30T00:53:44Z",
    "merged_at": "2025-07-30T00:53:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6463"
  },
  {
    "number": 6462,
    "title": "Revert \"test:[nvbug 5415268] add kv_cache_free_gpu_mem_fraction param and llama4 rcca cases\"",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T23:52:12Z",
    "closed_at": "2025-07-30T02:43:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6462"
  },
  {
    "number": 6461,
    "title": "Unwaive Gemma2 LoRA test on H100",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T22:14:42Z",
    "closed_at": "2025-07-30T16:56:12Z",
    "merged_at": "2025-07-30T16:56:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6461"
  },
  {
    "number": 6460,
    "title": "[nvbug/5409417] Unwaive llava test case",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T21:30:38Z",
    "closed_at": "2025-07-30T18:38:48Z",
    "merged_at": "2025-07-30T18:38:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6460"
  },
  {
    "number": 6459,
    "title": "Add GLM4 MoE support with PyTorch backend",
    "user": "pst2154",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T19:38:32Z",
    "closed_at": "2025-07-29T19:38:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6459"
  },
  {
    "number": 6458,
    "title": "[None][fix] Update to pull LLM from a central location.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T19:26:23Z",
    "closed_at": "2025-08-25T20:07:30Z",
    "merged_at": "2025-08-25T20:07:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6458"
  },
  {
    "number": 6457,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T16:49:01Z",
    "closed_at": "2025-07-30T02:36:58Z",
    "merged_at": "2025-07-30T02:36:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6457"
  },
  {
    "number": 6456,
    "title": "added AD perf and mem test",
    "user": "MrGeva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T16:13:38Z",
    "closed_at": "2025-07-31T11:53:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6456"
  },
  {
    "number": 6455,
    "title": "[Perf]: Add residual, norm for nemotron_nas models",
    "user": "NVShreyas",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T16:05:22Z",
    "closed_at": "2025-07-30T16:10:38Z",
    "merged_at": "2025-07-30T16:10:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6455"
  },
  {
    "number": 6453,
    "title": "[TRTLLM-6611][feat] Add warnings and stricter validation to LoraManager adapter loading",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T15:31:22Z",
    "closed_at": "2025-08-01T02:22:52Z",
    "merged_at": "2025-08-01T02:22:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6453"
  },
  {
    "number": 6452,
    "title": "[TRTLLM-6637][feat] Move KV cache preparation into ModelEngine",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T14:20:28Z",
    "closed_at": "2025-08-01T09:06:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6452"
  },
  {
    "number": 6451,
    "title": "imp(torchsampler):support sample params temperature/topp/topk",
    "user": "xq25478",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T13:37:53Z",
    "closed_at": "2025-08-06T06:46:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6451"
  },
  {
    "number": 6450,
    "title": "imp(torchsampler):support openai stop in text level",
    "user": "xq25478",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T13:34:45Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6450"
  },
  {
    "number": 6449,
    "title": "imp(TorchSampler):support torchsampler stop",
    "user": "xq25478",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T13:32:39Z",
    "closed_at": "2025-07-29T13:33:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6449"
  },
  {
    "number": 6448,
    "title": "Attention attention plugin",
    "user": "zhumakhan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T11:22:35Z",
    "closed_at": "2025-07-29T11:24:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6448"
  },
  {
    "number": 6447,
    "title": "[nvbug 5380101][fix] Fix nemotronNAS loading for TP>1",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T10:59:16Z",
    "closed_at": "2025-07-30T11:22:33Z",
    "merged_at": "2025-07-30T11:22:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6447"
  },
  {
    "number": 6446,
    "title": "add propagation of trust_remote_code to OpenAIServer",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T10:49:30Z",
    "closed_at": "2025-07-30T19:25:41Z",
    "merged_at": "2025-07-30T19:25:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6446"
  },
  {
    "number": 6445,
    "title": "chore: clean code of PyExecutor",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T09:56:28Z",
    "closed_at": "2025-07-30T06:11:44Z",
    "merged_at": "2025-07-30T06:11:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6445"
  },
  {
    "number": 6444,
    "title": "chore: remove unused kv_cache_dtype in api reference",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T09:31:48Z",
    "closed_at": "2025-07-29T18:57:21Z",
    "merged_at": "2025-07-29T18:57:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6444"
  },
  {
    "number": 6443,
    "title": "[None][chore] Add readme for perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T09:30:40Z",
    "closed_at": "2025-08-05T06:07:42Z",
    "merged_at": "2025-08-05T06:07:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6443"
  },
  {
    "number": 6442,
    "title": "doc: [TRTLLM-6093] Add Attention Variants introduction for Feature section",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T08:53:01Z",
    "closed_at": "2025-08-01T07:00:01Z",
    "merged_at": "2025-08-01T07:00:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6442"
  },
  {
    "number": 6441,
    "title": "[doc] update the doc of feature combination matrix",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T08:52:01Z",
    "closed_at": "2025-07-30T10:48:49Z",
    "merged_at": "2025-07-30T10:48:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6441"
  },
  {
    "number": 6440,
    "title": "[TRTLLM-5312][infra] Add triton trigger rules",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T08:30:25Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6440"
  },
  {
    "number": 6439,
    "title": "[nvbug/5410296][fix] Fix OOM in Llama 4 disagg-serve tests",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T07:34:51Z",
    "closed_at": "2025-07-30T16:41:37Z",
    "merged_at": "2025-07-30T16:41:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6439"
  },
  {
    "number": 6438,
    "title": "doc: update release notes",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T07:24:08Z",
    "closed_at": "2025-07-29T08:22:29Z",
    "merged_at": "2025-07-29T08:22:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6438"
  },
  {
    "number": 6437,
    "title": "fix: fix illeagel memory access",
    "user": "dongjiyingdjy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T06:37:32Z",
    "closed_at": "2025-07-31T02:01:34Z",
    "merged_at": "2025-07-31T02:01:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6437"
  },
  {
    "number": 6436,
    "title": "[TRTLLM-6473][test] add speculative decoding and ep load balance cases into QA test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T06:33:27Z",
    "closed_at": "2025-08-04T02:11:26Z",
    "merged_at": "2025-08-04T02:11:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6436"
  },
  {
    "number": 6435,
    "title": "[fix] Switch placement of image placeholder for mistral 3.1",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T06:06:12Z",
    "closed_at": "2025-07-30T06:10:36Z",
    "merged_at": "2025-07-30T06:10:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6435"
  },
  {
    "number": 6434,
    "title": "infra: Update flashinfer issue fix code",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T05:34:44Z",
    "closed_at": "2025-07-29T06:59:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6434"
  },
  {
    "number": 6433,
    "title": "[doc] Add FlashInfer installation doc in Ubuntu startup",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T05:26:38Z",
    "closed_at": "2025-08-05T08:39:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6433"
  },
  {
    "number": 6432,
    "title": "test:[nvbug 5415268] add kv_cache_free_gpu_mem_fraction param and llama4 rcca cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T05:19:06Z",
    "closed_at": "2025-08-04T09:00:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6432"
  },
  {
    "number": 6431,
    "title": "doc: update multimodal models on support-matrix.md",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T04:55:48Z",
    "closed_at": "2025-07-31T00:50:18Z",
    "merged_at": "2025-07-31T00:50:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6431"
  },
  {
    "number": 6430,
    "title": "test:[nvbug 5415268] add kv_cache_free_gpu_mem_fraction param and llama4 rcca cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T03:11:59Z",
    "closed_at": "2025-07-29T05:52:45Z",
    "merged_at": "2025-07-29T05:52:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6430"
  },
  {
    "number": 6429,
    "title": "[fix] Fix wide EP when using DeepEP with online EPLB",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T03:07:27Z",
    "closed_at": "2025-07-30T04:13:18Z",
    "merged_at": "2025-07-30T04:13:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6429"
  },
  {
    "number": 6428,
    "title": "doc: remove backend parameter for trtllm-bench when backend is set to…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T03:01:59Z",
    "closed_at": "2025-07-29T15:01:21Z",
    "merged_at": "2025-07-29T15:01:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6428"
  },
  {
    "number": 6427,
    "title": "[nvbugs/5414909] fix: Qwen2-VL keyword on L20",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-29T02:03:26Z",
    "closed_at": "2025-07-30T09:29:56Z",
    "merged_at": "2025-07-30T09:29:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6427"
  },
  {
    "number": 6426,
    "title": "[#6425][fix] address CUDA stream sync issue in ModelRunnerCPP",
    "user": "xsxszab",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T22:20:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6426"
  },
  {
    "number": 6423,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T19:26:43Z",
    "closed_at": "2025-07-29T09:00:31Z",
    "merged_at": "2025-07-29T09:00:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6423"
  },
  {
    "number": 6421,
    "title": "[https://nvbugs/5418673][fix] Fix llama4 test",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T17:22:03Z",
    "closed_at": "2025-08-11T17:16:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6421"
  },
  {
    "number": 6420,
    "title": "Fix e2e test failure for RTX6000 Pro ",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T17:06:20Z",
    "closed_at": "2025-07-31T03:32:44Z",
    "merged_at": "2025-07-31T03:32:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6420"
  },
  {
    "number": 6419,
    "title": "chore: update trtllm-serve usage doc by removing backend parameter when it use torch as backend.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T16:47:17Z",
    "closed_at": "2025-07-30T15:11:06Z",
    "merged_at": "2025-07-30T15:11:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6419"
  },
  {
    "number": 6418,
    "title": "chore: add trtllm-serve json schema example into doc.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T16:23:14Z",
    "closed_at": "2025-07-30T08:33:09Z",
    "merged_at": "2025-07-30T08:33:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6418"
  },
  {
    "number": 6417,
    "title": "[https://nvbugs/5419069][fix] Fix the mismatched layer name components.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T15:46:04Z",
    "closed_at": "2025-08-01T08:32:40Z",
    "merged_at": "2025-08-01T08:32:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6417"
  },
  {
    "number": 6416,
    "title": "[infra] Remove auto_apply_labels option from .coderabbit.yaml reviews section",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T15:29:16Z",
    "closed_at": "2025-07-28T21:16:45Z",
    "merged_at": "2025-07-28T21:16:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6416"
  },
  {
    "number": 6415,
    "title": "NOT MEANT FOR REVIEW YET [TRTLLM-6121] TRTLLM Sampler PP support",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T15:26:23Z",
    "closed_at": "2025-08-08T22:08:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6415"
  },
  {
    "number": 6414,
    "title": "[Experimental] feat: Enable guided decoding with speculative decoding (part 2: one-model engine) ",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T14:44:35Z",
    "closed_at": "2025-08-15T13:56:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6414"
  },
  {
    "number": 6413,
    "title": "[5385981] fix: Update the usage of VisionAttention init API.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T14:32:06Z",
    "closed_at": "2025-07-29T08:41:48Z",
    "merged_at": "2025-07-29T08:41:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6413"
  },
  {
    "number": 6412,
    "title": "fix: Unwaive triton cpp test [nvbug 5401088]",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T14:30:37Z",
    "closed_at": "2025-07-30T15:25:18Z",
    "merged_at": "2025-07-30T15:25:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6412"
  },
  {
    "number": 6409,
    "title": "[None][fix] fix the building of trtllm with command `TRTLLM_USE_PRECOMPILED=1 pip install -e .`",
    "user": "foreverlms",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T13:54:53Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6409"
  },
  {
    "number": 6408,
    "title": "feat: Support structural tag in C++ runtime and upgrade xgrammar to 0.1.21",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T13:22:56Z",
    "closed_at": "2025-07-31T01:53:53Z",
    "merged_at": "2025-07-31T01:53:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6408"
  },
  {
    "number": 6407,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T12:43:17Z",
    "closed_at": "2025-07-28T12:51:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6407"
  },
  {
    "number": 6404,
    "title": "doc: update architecture overview",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T10:41:16Z",
    "closed_at": "2025-07-31T08:38:18Z",
    "merged_at": "2025-07-31T08:38:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6404"
  },
  {
    "number": 6403,
    "title": "use cudaSetDevice to create context ,fix nvbug 5394497",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T10:27:50Z",
    "closed_at": "2025-08-03T17:32:56Z",
    "merged_at": "2025-08-03T17:32:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6403"
  },
  {
    "number": 6402,
    "title": "[https://nvbugs/5381276][fix] fix warning for fused_a_gemm",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T09:43:44Z",
    "closed_at": "2025-08-01T13:37:21Z",
    "merged_at": "2025-08-01T13:37:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6402"
  },
  {
    "number": 6401,
    "title": "[https://nvbugspro.nvidia.com/bug/5415268] fix illegal smem access with chunked attention",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T09:35:18Z",
    "closed_at": "2025-07-30T03:33:22Z",
    "merged_at": "2025-07-30T03:33:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6401"
  },
  {
    "number": 6400,
    "title": "chore: delete useless gitkeep files.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T09:00:17Z",
    "closed_at": "2025-07-28T15:38:31Z",
    "merged_at": "2025-07-28T15:38:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6400"
  },
  {
    "number": 6399,
    "title": "TRTLLM-6077 doc: Add perf analysis and benchmark guide",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T08:57:24Z",
    "closed_at": "2025-08-04T01:51:05Z",
    "merged_at": "2025-08-04T01:51:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6399"
  },
  {
    "number": 6398,
    "title": "[TRTLLM-6759] Disagg-serving + PP tests on top of #6369",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T08:52:31Z",
    "closed_at": "2025-07-29T02:14:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6398"
  },
  {
    "number": 6397,
    "title": "chore: add EXAONE4 accuracy test",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T08:28:18Z",
    "closed_at": "2025-08-04T02:14:16Z",
    "merged_at": "2025-08-04T02:14:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6397"
  },
  {
    "number": 6396,
    "title": "[bug fix] fix warning involved by ring attn",
    "user": "forrestl111",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T08:02:06Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6396"
  },
  {
    "number": 6395,
    "title": "[Infra] - Dependencies upgrade test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T07:11:22Z",
    "closed_at": "2025-07-28T09:46:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6395"
  },
  {
    "number": 6394,
    "title": "test: waive failed cases",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T06:40:29Z",
    "closed_at": "2025-07-28T10:31:44Z",
    "merged_at": "2025-07-28T10:31:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6394"
  },
  {
    "number": 6393,
    "title": "Rename layer to comply with deepseek",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T06:14:41Z",
    "closed_at": "2025-07-30T02:00:48Z",
    "merged_at": "2025-07-30T02:00:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6393"
  },
  {
    "number": 6392,
    "title": "tests: Add llama4 functional cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T06:00:42Z",
    "closed_at": "2025-07-29T07:49:43Z",
    "merged_at": "2025-07-29T07:49:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6392"
  },
  {
    "number": 6391,
    "title": "[None][chore] add online help to build_wheel.py and fix a doc link",
    "user": "zhenhuaw-me",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T05:53:47Z",
    "closed_at": "2025-08-04T05:14:55Z",
    "merged_at": "2025-08-04T05:14:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6391"
  },
  {
    "number": 6390,
    "title": "tests: add TestNemotronH cuda graph tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T04:53:27Z",
    "closed_at": "2025-07-30T08:45:58Z",
    "merged_at": "2025-07-30T08:45:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6390"
  },
  {
    "number": 6387,
    "title": "chore: fix a typo in waives.txt",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T03:29:41Z",
    "closed_at": "2025-07-28T05:47:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6387"
  },
  {
    "number": 6386,
    "title": "[None][infra] Enable accuracy test for eagle3 and chunked prefill",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T03:12:57Z",
    "closed_at": "2025-08-04T05:45:25Z",
    "merged_at": "2025-08-04T05:45:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6386"
  },
  {
    "number": 6384,
    "title": "[Infa] - waive failed cases and fix a typo",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T02:42:00Z",
    "closed_at": "2025-07-28T06:06:57Z",
    "merged_at": "2025-07-28T06:06:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6384"
  },
  {
    "number": 6383,
    "title": "[TRTLLM-6731][infra] Upgrade NIXL to 0.4.1",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-28T01:21:45Z",
    "closed_at": "2025-08-08T05:58:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6383"
  },
  {
    "number": 6382,
    "title": "[None][fix] Fix the incorrect alignment check for MXFP8xMXFP4 MOE",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-27T21:36:14Z",
    "closed_at": "2025-08-10T21:40:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6382"
  },
  {
    "number": 6381,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-27T12:15:33Z",
    "closed_at": "2025-07-29T09:01:23Z",
    "merged_at": "2025-07-29T09:01:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6381"
  },
  {
    "number": 6380,
    "title": "Bugfix/fix nemotron nas lora support",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-27T11:31:14Z",
    "closed_at": "2025-07-31T17:39:35Z",
    "merged_at": "2025-07-31T17:39:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6380"
  },
  {
    "number": 6379,
    "title": "[TRTLLM-6674][feat] (Breaking Change) Hopper SWA non-cyclic kernels + KV reuse + Spec Dec",
    "user": "symphonylyh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-26T20:00:16Z",
    "closed_at": "2025-08-05T07:47:41Z",
    "merged_at": "2025-08-05T07:47:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6379"
  },
  {
    "number": 6377,
    "title": "ci: skip GB200-4_GPUs-PyTorch-1 test stage",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-26T06:06:31Z",
    "closed_at": "2025-07-28T02:16:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6377"
  },
  {
    "number": 6376,
    "title": "Publish N-Gram tech blog in README.",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-26T03:07:38Z",
    "closed_at": "2025-08-18T16:28:18Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6376"
  },
  {
    "number": 6373,
    "title": "[infra] Add an auto-labeling github action to TRTLLM",
    "user": "poweiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T21:08:40Z",
    "closed_at": "2025-07-28T19:25:51Z",
    "merged_at": "2025-07-28T19:25:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6373"
  },
  {
    "number": 6372,
    "title": "Downgrade CUBLAS to 12.9.0.13-1",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T19:35:55Z",
    "closed_at": "2025-07-29T19:53:48Z",
    "merged_at": "2025-07-29T19:53:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6372"
  },
  {
    "number": 6371,
    "title": "[TRTLLM-6657][feat] Add LoRA support for Gemma3",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T18:28:13Z",
    "closed_at": "2025-08-01T13:19:54Z",
    "merged_at": "2025-08-01T13:19:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6371"
  },
  {
    "number": 6370,
    "title": "[None][infra]Update slurm config keys",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T17:53:12Z",
    "closed_at": "2025-07-28T18:56:37Z",
    "merged_at": "2025-07-28T18:56:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6370"
  },
  {
    "number": 6369,
    "title": "feat: Add support for disaggregation with pp with pytorch backend",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T14:11:21Z",
    "closed_at": "2025-07-30T13:42:13Z",
    "merged_at": "2025-07-30T13:42:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6369"
  },
  {
    "number": 6368,
    "title": "chore: Improve the AutoTuner log information.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T14:11:06Z",
    "closed_at": "2025-08-01T01:19:52Z",
    "merged_at": "2025-08-01T01:19:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6368"
  },
  {
    "number": 6367,
    "title": "chore: disallow arbitrary arguments in llm_args.xxxConfigs",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T11:44:17Z",
    "closed_at": "2025-07-29T20:16:52Z",
    "merged_at": "2025-07-29T20:16:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6367"
  },
  {
    "number": 6365,
    "title": "chore: add _prepare_and_schedule_batch function in PyExecutor",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T10:02:38Z",
    "closed_at": "2025-07-28T09:50:27Z",
    "merged_at": "2025-07-28T09:50:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6365"
  },
  {
    "number": 6363,
    "title": "[TRTLLM-6392][feat] Support turning on/off spec decoding dynamically",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T09:43:55Z",
    "closed_at": "2025-07-31T19:31:39Z",
    "merged_at": "2025-07-31T19:31:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6363"
  },
  {
    "number": 6362,
    "title": "[None][feat] Add C++ RequestSpecificException",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T09:01:43Z",
    "closed_at": "2025-08-15T05:05:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6362"
  },
  {
    "number": 6359,
    "title": "[nvbug/5320234] fix: test_trtllm_bench_llmapi_launch",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T08:27:41Z",
    "closed_at": "2025-07-28T01:01:10Z",
    "merged_at": "2025-07-28T01:01:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6359"
  },
  {
    "number": 6358,
    "title": "WAR: Remove CUDA sources and keys to avoid conflicts",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T08:12:44Z",
    "closed_at": "2025-07-25T09:42:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6358"
  },
  {
    "number": 6357,
    "title": "[TRTLLM-6624][feat] skip post blackwell",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T07:53:30Z",
    "closed_at": "2025-08-01T17:10:14Z",
    "merged_at": "2025-08-01T17:10:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6357"
  },
  {
    "number": 6356,
    "title": "doc: Add README for wide EP ",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T07:31:16Z",
    "closed_at": "2025-07-29T04:36:12Z",
    "merged_at": "2025-07-29T04:36:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6356"
  },
  {
    "number": 6355,
    "title": "[https://nvbugs/5340941][https://nvbugs/5375785] - fix: Wrap attentio…",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T07:30:53Z",
    "closed_at": "2025-08-01T11:38:06Z",
    "merged_at": "2025-08-01T11:38:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6355"
  },
  {
    "number": 6354,
    "title": "fix: Fix max attn window in TRTLLM Sampler.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T07:08:13Z",
    "closed_at": "2025-08-11T08:23:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6354"
  },
  {
    "number": 6353,
    "title": "feat: TRTLLM-6450 update long rope for phi3.5/phi4-mini/phi4-mm ",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T06:53:16Z",
    "closed_at": "2025-07-30T16:20:17Z",
    "merged_at": "2025-07-30T16:20:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6353"
  },
  {
    "number": 6352,
    "title": "[test] Unwaive mistral3.1 small E2E test",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T05:50:58Z",
    "closed_at": "2025-07-28T18:37:42Z",
    "merged_at": "2025-07-28T18:37:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6352"
  },
  {
    "number": 6351,
    "title": "chore: remove unused code in PyExecutor",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T03:46:43Z",
    "closed_at": "2025-07-29T08:24:27Z",
    "merged_at": "2025-07-29T08:24:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6351"
  },
  {
    "number": 6350,
    "title": "[None][chore] unify the backend strings and warn the default backend change",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T03:20:52Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6350"
  },
  {
    "number": 6349,
    "title": "Add disable_optimistic_tuning flag and update gb_per_token calculation title",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T02:25:42Z",
    "closed_at": "2025-08-27T18:43:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6349"
  },
  {
    "number": 6348,
    "title": "[TRTLLM-6106][feat] Add support for KVCache transfer from KVCache reuse path",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T02:23:30Z",
    "closed_at": "2025-09-27T23:29:30Z",
    "merged_at": "2025-09-27T23:29:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6348"
  },
  {
    "number": 6347,
    "title": "[None][feat] Add long data collection dataset support",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T02:16:31Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6347"
  },
  {
    "number": 6346,
    "title": "[None][fix] Fix KV event consumption",
    "user": "jthomson04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T01:13:37Z",
    "closed_at": "2025-10-18T22:41:26Z",
    "merged_at": "2025-10-18T22:41:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6346"
  },
  {
    "number": 6345,
    "title": "fix: support mixture of text & multimodal prompts",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T00:40:51Z",
    "closed_at": "2025-07-30T00:52:31Z",
    "merged_at": "2025-07-30T00:52:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6345"
  },
  {
    "number": 6344,
    "title": "[FIX] fix bugs caused by None attention_bias during Qwen3 model convert engine",
    "user": "Fan-Yunfan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-25T00:15:29Z",
    "closed_at": "2025-07-29T23:13:44Z",
    "merged_at": "2025-07-29T23:13:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6344"
  },
  {
    "number": 6343,
    "title": "[fix] Fixes to parameter usage and low latency configuration.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T23:12:54Z",
    "closed_at": "2025-07-29T05:36:14Z",
    "merged_at": "2025-07-29T05:36:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6343"
  },
  {
    "number": 6341,
    "title": "Unwaive Granite LoRA test on L40S",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T20:41:27Z",
    "closed_at": "2025-07-31T00:27:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6341"
  },
  {
    "number": 6340,
    "title": "[fix] README link directs to intended doc",
    "user": "lianakoleva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T18:57:23Z",
    "closed_at": "2025-07-26T15:27:10Z",
    "merged_at": "2025-07-26T15:27:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6340"
  },
  {
    "number": 6339,
    "title": "[nvbugs/5404000] fix: waive request_perf_metrics_draft test on pre-Hopper GPUs",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T18:56:54Z",
    "closed_at": "2025-07-28T19:36:44Z",
    "merged_at": "2025-07-28T19:36:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6339"
  },
  {
    "number": 6338,
    "title": "[fix] Add trust_remote_code option to prepare_dataset.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T18:56:26Z",
    "closed_at": "2025-07-28T21:49:45Z",
    "merged_at": "2025-07-28T21:49:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6338"
  },
  {
    "number": 6337,
    "title": "[https://nvbugs/5410279][test] resubmit timeout refactor",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T18:24:13Z",
    "closed_at": "2025-08-05T08:39:25Z",
    "merged_at": "2025-08-05T08:39:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6337"
  },
  {
    "number": 6334,
    "title": "[TRTLLM-4921][feat] Enable chunked prefill for Nemotron-H",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T14:46:42Z",
    "closed_at": "2025-08-22T16:15:21Z",
    "merged_at": "2025-08-22T16:15:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6334"
  },
  {
    "number": 6333,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T14:30:23Z",
    "closed_at": "2025-07-25T07:18:07Z",
    "merged_at": "2025-07-25T07:18:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6333"
  },
  {
    "number": 6332,
    "title": "feat: Adding stream_interval option to trtllm-bench throughput command.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T14:22:34Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6332"
  },
  {
    "number": 6331,
    "title": "[Infra] - Wiave failed tests in post-merge",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T12:27:16Z",
    "closed_at": "2025-07-24T13:18:07Z",
    "merged_at": "2025-07-24T13:18:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6331"
  },
  {
    "number": 6330,
    "title": "Revert 6312",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T11:54:45Z",
    "closed_at": "2025-08-11T08:42:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6330"
  },
  {
    "number": 6328,
    "title": "Remove swizzle for wide ep",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T09:48:35Z",
    "closed_at": "2025-07-30T05:59:58Z",
    "merged_at": "2025-07-30T05:59:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6328"
  },
  {
    "number": 6327,
    "title": "Update fmhaRunner.cpp to fix guardwords scan error",
    "user": "zhou-yuxin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T09:15:23Z",
    "closed_at": "2025-07-24T10:32:36Z",
    "merged_at": "2025-07-24T10:32:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6327"
  },
  {
    "number": 6326,
    "title": "fix: integration tests with nanobind",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T08:59:45Z",
    "closed_at": "2025-07-25T01:23:20Z",
    "merged_at": "2025-07-25T01:23:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6326"
  },
  {
    "number": 6325,
    "title": "chore: remove draft_model_engine from init parameter list of PyExecutor",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T06:13:28Z",
    "closed_at": "2025-07-30T07:31:50Z",
    "merged_at": "2025-07-30T07:31:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6325"
  },
  {
    "number": 6324,
    "title": "doc: update release notes",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T06:03:40Z",
    "closed_at": "2025-07-28T08:05:50Z",
    "merged_at": "2025-07-28T08:05:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6324"
  },
  {
    "number": 6323,
    "title": "[TRTLLM-7030][fix] BREAKING CHANGE: Mismatch between docs and actual commands",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T05:59:32Z",
    "closed_at": "2025-08-14T07:48:58Z",
    "merged_at": "2025-08-14T07:48:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6323"
  },
  {
    "number": 6322,
    "title": "[test] Add accuracy regression test for Mistral3.1",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T05:54:03Z",
    "closed_at": "2025-07-28T16:41:44Z",
    "merged_at": "2025-07-28T16:41:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6322"
  },
  {
    "number": 6321,
    "title": "feat: Support JSON Schema in OpenAI-Compatible API",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T05:43:33Z",
    "closed_at": "2025-07-25T16:55:57Z",
    "merged_at": "2025-07-25T16:55:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6321"
  },
  {
    "number": 6320,
    "title": "feat: Add non UB AR + Residual + Norm + Quant fusion",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T04:09:04Z",
    "closed_at": "2025-07-24T09:51:44Z",
    "merged_at": "2025-07-24T09:51:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6320"
  },
  {
    "number": 6319,
    "title": "[None][fix] Remove lock related typo in py_executor",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T04:00:49Z",
    "closed_at": "2025-08-06T03:17:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6319"
  },
  {
    "number": 6318,
    "title": "Move Fuse RMSNorm to new Inf Optimizer",
    "user": "h-guo18",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T03:45:07Z",
    "closed_at": "2025-07-24T03:46:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6318"
  },
  {
    "number": 6317,
    "title": "doc: fix invalid links related with llm api example",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T03:35:55Z",
    "closed_at": "2025-07-24T04:46:51Z",
    "merged_at": "2025-07-24T04:46:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6317"
  },
  {
    "number": 6316,
    "title": "fix nvbugs/5385981",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T03:18:50Z",
    "closed_at": "2025-07-28T09:00:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6316"
  },
  {
    "number": 6315,
    "title": "perf: customize cublastLt algo for Llamba 3.3 70B TP4",
    "user": "zhenhuaw-me",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T03:09:33Z",
    "closed_at": "2025-07-24T15:01:16Z",
    "merged_at": "2025-07-24T15:01:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6315"
  },
  {
    "number": 6314,
    "title": "[None][infra] Enable accuracy test for mtp and chunked prefill",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T01:32:38Z",
    "closed_at": "2025-08-18T23:42:52Z",
    "merged_at": "2025-08-18T23:42:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6314"
  },
  {
    "number": 6313,
    "title": "[fix] Update get_trtllm_bench_build_command to handle batch size and tokens",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T01:21:33Z",
    "closed_at": "2025-08-01T04:08:09Z",
    "merged_at": "2025-08-01T04:08:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6313"
  },
  {
    "number": 6312,
    "title": "Waive tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-24T00:51:31Z",
    "closed_at": "2025-07-24T01:15:07Z",
    "merged_at": "2025-07-24T01:15:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6312"
  },
  {
    "number": 6311,
    "title": "[doc] Add NGram tech blog",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T23:59:29Z",
    "closed_at": "2025-07-25T17:26:33Z",
    "merged_at": "2025-07-25T17:26:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6311"
  },
  {
    "number": 6310,
    "title": "[fix] Update to remove popping of KV cache and other args.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T23:19:10Z",
    "closed_at": "2025-07-24T19:54:34Z",
    "merged_at": "2025-07-24T19:54:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6310"
  },
  {
    "number": 6309,
    "title": "Revert \"tests: add timeout_manager to tensorrt flow test cases (#5942)\"",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T20:41:23Z",
    "closed_at": "2025-07-24T03:58:10Z",
    "merged_at": "2025-07-24T03:58:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6309"
  },
  {
    "number": 6308,
    "title": "[None][fix] switch to bash to interpret -e flag",
    "user": "lianakoleva",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T19:49:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6308"
  },
  {
    "number": 6307,
    "title": "[hotfix] Update get_pytest_timeout to prioritize marker retrieval with fallback logic",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T19:47:59Z",
    "closed_at": "2025-08-27T18:53:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6307"
  },
  {
    "number": 6306,
    "title": "[https://nvbugs/5398180][feat] Improve Llama4 performance for small max_seqlen cases",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T18:45:05Z",
    "closed_at": "2025-08-09T06:58:32Z",
    "merged_at": "2025-08-09T06:58:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6306"
  },
  {
    "number": 6305,
    "title": "[None][chore] upgrade mpi4py to 4.0.0",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T18:22:05Z",
    "closed_at": "2025-08-06T20:16:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6305"
  },
  {
    "number": 6304,
    "title": "Increase PyTorch test command timeouts from 90s to 120s in config files",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T18:14:38Z",
    "closed_at": "2025-08-27T18:52:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6304"
  },
  {
    "number": 6303,
    "title": "feat: Add Phi-4-Mini-Instruct in Pytorch backend for LLM API accuracy tests",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T17:40:59Z",
    "closed_at": "2025-07-28T21:02:14Z",
    "merged_at": "2025-07-28T21:02:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6303"
  },
  {
    "number": 6302,
    "title": "[AutoDeploy] disable flaky MoE nvfp4 test",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T16:30:40Z",
    "closed_at": "2025-07-23T17:13:01Z",
    "merged_at": "2025-07-23T17:13:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6302"
  },
  {
    "number": 6301,
    "title": "[fix][nvbugs/5390810] Improve the check for disaggregated serving test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T15:59:58Z",
    "closed_at": "2025-07-25T19:47:01Z",
    "merged_at": "2025-07-25T19:47:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6301"
  },
  {
    "number": 6300,
    "title": "[TRTLLM-6409][feat] Enable guided decoding with speculative decoding (part 1: two-model engine)",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T12:48:15Z",
    "closed_at": "2025-08-07T09:53:49Z",
    "merged_at": "2025-08-07T09:53:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6300"
  },
  {
    "number": 6299,
    "title": "[Infra] - Skip failed cases",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T11:35:47Z",
    "closed_at": "2025-07-23T13:26:32Z",
    "merged_at": "2025-07-23T13:26:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6299"
  },
  {
    "number": 6297,
    "title": "Optimize: moe pre all2all overlap",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T11:18:39Z",
    "closed_at": "2025-10-20T02:26:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6297"
  },
  {
    "number": 6296,
    "title": "DeepEP LL dispatch FP4",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T10:13:21Z",
    "closed_at": "2025-07-28T03:25:42Z",
    "merged_at": "2025-07-28T03:25:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6296"
  },
  {
    "number": 6293,
    "title": "test: skip llama3.3 70b test on cg4",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T09:13:46Z",
    "closed_at": "2025-07-25T02:29:57Z",
    "merged_at": "2025-07-25T02:29:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6293"
  },
  {
    "number": 6292,
    "title": "[nvbug/5409414, 5355707] tests: adjust batchsize and decoding name",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T09:09:49Z",
    "closed_at": "2025-07-28T07:33:31Z",
    "merged_at": "2025-07-28T07:33:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6292"
  },
  {
    "number": 6291,
    "title": "[TRTLLM-4279] test: Add a protection test for checking trtllm custom ops",
    "user": "yali-arch",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T09:04:00Z",
    "closed_at": "2025-07-31T11:39:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6291"
  },
  {
    "number": 6290,
    "title": "fix: Disable unifiedGEMM for lora kernel inference",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T08:53:57Z",
    "closed_at": "2025-08-09T06:44:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6290"
  },
  {
    "number": 6289,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T08:53:05Z",
    "closed_at": "2025-07-23T09:04:21Z",
    "merged_at": "2025-07-23T09:04:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6289"
  },
  {
    "number": 6288,
    "title": "[fix] Fix perf regression caused by MoE autotuner when using DeepEPLowLatency",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T08:49:42Z",
    "closed_at": "2025-07-28T05:37:11Z",
    "merged_at": "2025-07-28T05:37:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6288"
  },
  {
    "number": 6287,
    "title": "tests: only get timeout value from pytest marker",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T08:41:06Z",
    "closed_at": "2025-07-24T12:51:03Z",
    "merged_at": "2025-07-24T12:51:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6287"
  },
  {
    "number": 6286,
    "title": "feat: keep using previous CUBLAS version while upgrade DLFW to 25.06",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T08:37:49Z",
    "closed_at": "2025-07-28T06:23:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6286"
  },
  {
    "number": 6285,
    "title": "[https://nvbugs/5340941] - fix: Correct custom ops used by Qwen3 Moe …",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T08:35:29Z",
    "closed_at": "2025-07-25T06:49:46Z",
    "merged_at": "2025-07-25T06:49:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6285"
  },
  {
    "number": 6284,
    "title": "[Fix] the bug in the trtllm-gen heurisitcf for MLA kernels.",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T07:17:36Z",
    "closed_at": "2025-07-24T15:40:27Z",
    "merged_at": "2025-07-24T15:40:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6284"
  },
  {
    "number": 6283,
    "title": "test: organize perf cases and add missing perflab cases in qa test list",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T07:16:09Z",
    "closed_at": "2025-07-28T10:33:32Z",
    "merged_at": "2025-07-28T10:33:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6283"
  },
  {
    "number": 6282,
    "title": "[fix] Fix missing fields in xqa kernel cache key",
    "user": "lowsfer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T07:01:30Z",
    "closed_at": "2025-08-01T02:41:26Z",
    "merged_at": "2025-08-01T02:41:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6282"
  },
  {
    "number": 6280,
    "title": "chore: remove unused variables in pyexecutor",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T06:34:46Z",
    "closed_at": "2025-07-24T05:16:15Z",
    "merged_at": "2025-07-24T05:16:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6280"
  },
  {
    "number": 6279,
    "title": "fix precompiled multi_query_token kernel not having is_fp8_out hash key",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T06:29:00Z",
    "closed_at": "2025-07-26T00:45:53Z",
    "merged_at": "2025-07-26T00:45:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6279"
  },
  {
    "number": 6278,
    "title": "[TRTLLM-6364][infra] Validate for PR titles to ensure they follow the required format",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T04:14:17Z",
    "closed_at": "2025-08-01T03:26:05Z",
    "merged_at": "2025-08-01T03:26:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6278"
  },
  {
    "number": 6277,
    "title": "[Infra] - Increase unittest execution time since some test exceeds 1600",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T03:46:13Z",
    "closed_at": "2025-07-24T02:02:28Z",
    "merged_at": "2025-07-24T02:02:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6277"
  },
  {
    "number": 6276,
    "title": "Add NeMo LoRA checkpoint loading, validation, and related utilities title",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T03:36:59Z",
    "closed_at": "2025-07-29T15:48:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6276"
  },
  {
    "number": 6275,
    "title": "Add support for HuggingFace and NeMo LoRA adapters with KV head configs",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T03:35:25Z",
    "closed_at": "2025-07-23T05:11:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6275"
  },
  {
    "number": 6274,
    "title": "Add per-layer and uniform KV attention, improve LoRA checkpoint handling and tests",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T03:27:45Z",
    "closed_at": "2025-07-23T03:34:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6274"
  },
  {
    "number": 6273,
    "title": "fix: Tuning method for VSWA models in `trtllm-bench`",
    "user": "jaedeok-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-23T02:16:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6273"
  },
  {
    "number": 6272,
    "title": "[None][feat] Add PyTorch Runtime Support for MoE Weight Prefetching",
    "user": "nvxuanyuc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T23:02:58Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6272"
  },
  {
    "number": 6271,
    "title": "Update allreduce benchmark for torch",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T22:30:06Z",
    "closed_at": "2025-08-06T06:25:23Z",
    "merged_at": "2025-08-06T06:25:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6271"
  },
  {
    "number": 6270,
    "title": "[doc] Update perf_overview.md for release 0.21",
    "user": "zbpatel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T21:58:06Z",
    "closed_at": "2025-07-31T04:13:38Z",
    "merged_at": "2025-07-31T04:13:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6270"
  },
  {
    "number": 6268,
    "title": "[feat] Add support to load fp8 Meta Llama4 weights",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T20:58:16Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6268"
  },
  {
    "number": 6267,
    "title": "[fix] Cherry pick \"[TRTLLM-6262] Fix Llama4 Scout FP4 crash issue\"",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T20:56:40Z",
    "closed_at": "2025-07-23T02:00:38Z",
    "merged_at": "2025-07-23T02:00:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6267"
  },
  {
    "number": 6266,
    "title": "[nvbugs/5401156][fix] Avoid import all models when import trtllm._common",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T20:24:43Z",
    "closed_at": "2025-07-28T03:29:22Z",
    "merged_at": "2025-07-28T03:29:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6266"
  },
  {
    "number": 6265,
    "title": "fix: Fixing kv_cache_events unit tests [nvbug5362412]",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T18:39:29Z",
    "closed_at": "2025-07-25T12:55:44Z",
    "merged_at": "2025-07-25T12:55:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6265"
  },
  {
    "number": 6264,
    "title": "[None][infra] add eagle3 one model accuracy tests",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T17:27:06Z",
    "closed_at": "2025-08-02T23:07:47Z",
    "merged_at": "2025-08-02T23:07:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6264"
  },
  {
    "number": 6263,
    "title": "[TRTLLM-6654][feat] Add support for external multimodal embeddings",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T16:13:44Z",
    "closed_at": "2025-07-30T14:00:16Z",
    "merged_at": "2025-07-30T14:00:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6263"
  },
  {
    "number": 6262,
    "title": "[https://nvbugs/5387771] fix deadlocks due to insufficient numSemaphores",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T15:33:11Z",
    "closed_at": "2025-07-23T03:20:55Z",
    "merged_at": "2025-07-23T03:20:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6262"
  },
  {
    "number": 6259,
    "title": "[None][fix] Revert commit 48ddc3d & add test for disagg server with different max_num_tokens",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T14:18:24Z",
    "closed_at": "2025-08-04T07:09:51Z",
    "merged_at": "2025-08-04T07:09:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6259"
  },
  {
    "number": 6258,
    "title": "[https://nvbugs/5402719][fix]: Add cuda graph dummy requests to the spec_resource_manager",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T12:55:58Z",
    "closed_at": "2025-07-27T00:32:39Z",
    "merged_at": "2025-07-27T00:32:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6258"
  },
  {
    "number": 6257,
    "title": "[TRTLLM-4279] test: Add a protection test for checking trtllm custom ops",
    "user": "yali-arch",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T12:52:37Z",
    "closed_at": "2025-07-23T09:00:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6257"
  },
  {
    "number": 6256,
    "title": "[TRTLLM-5453][infra] Check all steps for test name and also check the test in waives.txt also exists in l0 or qa test list.",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T11:32:23Z",
    "closed_at": "2025-10-30T08:56:05Z",
    "merged_at": "2025-10-30T08:56:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6256"
  },
  {
    "number": 6255,
    "title": "Add register_fake for finegrained_mixed_dtype_gemm torch_op",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T09:10:40Z",
    "closed_at": "2025-07-22T13:59:55Z",
    "merged_at": "2025-07-22T13:59:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6255"
  },
  {
    "number": 6254,
    "title": "[None][feat] Support SharedTensor on MultimodalParams",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T09:08:07Z",
    "closed_at": "2025-08-11T00:48:24Z",
    "merged_at": "2025-08-11T00:48:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6254"
  },
  {
    "number": 6252,
    "title": "chore: bump version to 1.0.0rc5",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T07:57:14Z",
    "closed_at": "2025-07-22T08:24:28Z",
    "merged_at": "2025-07-22T08:24:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6252"
  },
  {
    "number": 6251,
    "title": "[feat] Update .coderabbit.yaml with review settings and code guidelines",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T07:42:23Z",
    "closed_at": "2025-07-23T21:22:49Z",
    "merged_at": "2025-07-23T21:22:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6251"
  },
  {
    "number": 6250,
    "title": "Improve TransferAgentTest.SyncMessage",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T07:36:15Z",
    "closed_at": "2025-07-24T15:41:36Z",
    "merged_at": "2025-07-24T15:41:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6250"
  },
  {
    "number": 6249,
    "title": "[None][chore] Dead code elimination, we no longer record/fetch through WindowBlockManager:: mContextBlocksByHash",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T07:21:47Z",
    "closed_at": "2025-08-10T13:10:10Z",
    "merged_at": "2025-08-10T13:10:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6249"
  },
  {
    "number": 6248,
    "title": "update disagg slurm scripts",
    "user": "lingjiew",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T06:55:32Z",
    "closed_at": "2025-07-29T09:02:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6248"
  },
  {
    "number": 6247,
    "title": "doc: update known issues",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T05:38:41Z",
    "closed_at": "2025-07-22T09:27:32Z",
    "merged_at": "2025-07-22T09:27:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6247"
  },
  {
    "number": 6246,
    "title": "[None][feat] Add support of scheduling attention dp request",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T03:42:01Z",
    "closed_at": "2025-08-02T00:38:01Z",
    "merged_at": "2025-08-02T00:38:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6246"
  },
  {
    "number": 6245,
    "title": "[None][feat] Implement advanced sampling for one model path mtp/eagle",
    "user": "nvxuanyuc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T03:37:42Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6245"
  },
  {
    "number": 6244,
    "title": "[None][feat] Support NVFP4 KV Cache",
    "user": "Tom-Zheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T03:06:31Z",
    "closed_at": "2025-09-01T01:24:52Z",
    "merged_at": "2025-09-01T01:24:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6244"
  },
  {
    "number": 6243,
    "title": "Update model-feature document",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T02:56:29Z",
    "closed_at": "2025-07-22T02:58:06Z",
    "merged_at": "2025-07-22T02:58:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6243"
  },
  {
    "number": 6242,
    "title": "chore: remove duplicate should_stop_processing check",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T02:43:21Z",
    "closed_at": "2025-07-23T06:11:24Z",
    "merged_at": "2025-07-23T06:11:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6242"
  },
  {
    "number": 6241,
    "title": "[PERF] Don't use hmac encryption for loopback interfaces",
    "user": "vadiklyutiy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T01:55:02Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6241"
  },
  {
    "number": 6240,
    "title": "Add Acceptance Rate calculation to benchmark_serving",
    "user": "zerollzeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-22T01:32:42Z",
    "closed_at": "2025-07-28T06:00:58Z",
    "merged_at": "2025-07-28T06:00:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6240"
  },
  {
    "number": 6239,
    "title": "fix: nvbug_5398806",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T23:52:29Z",
    "closed_at": "2025-07-23T03:45:12Z",
    "merged_at": "2025-07-23T03:45:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6239"
  },
  {
    "number": 6237,
    "title": "[fix][nvbugs/5399355] Fix Lamport buffer clear issue for MNNVL TwoShot Allreduce and add FP16 support.",
    "user": "timlee0212",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T22:39:27Z",
    "closed_at": "2025-07-25T00:01:40Z",
    "merged_at": "2025-07-25T00:01:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6237"
  },
  {
    "number": 6236,
    "title": "[nvbug/5376229]: Remove flash-attn dependency from test_ptp_quickstart_multimodal",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T22:07:58Z",
    "closed_at": "2025-07-22T20:42:20Z",
    "merged_at": "2025-07-22T20:42:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6236"
  },
  {
    "number": 6235,
    "title": "[Fix][nvbug 5401163][nvbug 5404726][Qwen3] Fix bug of MoE on tp > 1 with trtllm moe backend",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T22:04:13Z",
    "closed_at": "2025-07-24T13:47:37Z",
    "merged_at": "2025-07-24T13:47:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6235"
  },
  {
    "number": 6234,
    "title": "set NVIDIA_IMEX_CHANNELS for dlcluster slurm job only",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T20:21:04Z",
    "closed_at": "2025-07-22T18:27:54Z",
    "merged_at": "2025-07-22T18:27:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6234"
  },
  {
    "number": 6233,
    "title": "[Issue 6193] Fix gemma3vl weight loader",
    "user": "johncalesp",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T19:41:03Z",
    "closed_at": "2025-07-22T17:32:19Z",
    "merged_at": "2025-07-22T17:32:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6233"
  },
  {
    "number": 6232,
    "title": "[feat] Auto-enable ngram with concurrency <= 32.",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T19:09:22Z",
    "closed_at": "2025-07-31T22:45:52Z",
    "merged_at": "2025-07-31T22:45:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6232"
  },
  {
    "number": 6231,
    "title": "[TRTLLM-6744][feat] Remove input_sf swizzle for module WideEPMoE",
    "user": "StudyingShao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T18:55:57Z",
    "closed_at": "2025-08-08T03:13:42Z",
    "merged_at": "2025-08-08T03:13:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6231"
  },
  {
    "number": 6230,
    "title": "[fix] Fix flaky mistral E2E test",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T18:15:13Z",
    "closed_at": "2025-07-22T03:55:29Z",
    "merged_at": "2025-07-22T03:55:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6230"
  },
  {
    "number": 6229,
    "title": "Waive flaky tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T17:54:42Z",
    "closed_at": "2025-09-09T14:14:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6229"
  },
  {
    "number": 6228,
    "title": "[fix] Allow custom model config for Kimi-K2",
    "user": "meenchen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T17:38:08Z",
    "closed_at": "2025-08-05T14:02:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6228"
  },
  {
    "number": 6227,
    "title": "disable gc",
    "user": "zihaok",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T17:19:13Z",
    "closed_at": "2025-07-21T22:00:44Z",
    "merged_at": "2025-07-21T22:00:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6227"
  },
  {
    "number": 6226,
    "title": "Change the all-reduce strategy to NCCL",
    "user": "nzmora-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T17:09:39Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6226"
  },
  {
    "number": 6225,
    "title": "Bump version to 0.21.1",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T16:54:55Z",
    "closed_at": "2025-07-22T20:33:16Z",
    "merged_at": "2025-07-22T20:33:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6225"
  },
  {
    "number": 6224,
    "title": "[nvbugs/5401261][fix] Fix Triton backend disaggregated serving support",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T16:18:35Z",
    "closed_at": "2025-07-22T21:27:17Z",
    "merged_at": "2025-07-22T21:27:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6224"
  },
  {
    "number": 6223,
    "title": "[TRTLLM-6651][feat]  Enable Overlap scheduler +  Beam Search in TRTLLM Sampler",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T16:11:45Z",
    "closed_at": "2025-07-23T10:30:50Z",
    "merged_at": "2025-07-23T10:30:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6223"
  },
  {
    "number": 6222,
    "title": "[nvbug/5361223] doc: Update Llama4 deployment guide: update config & note concurrency",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T16:04:14Z",
    "closed_at": "2025-07-22T18:28:24Z",
    "merged_at": "2025-07-22T18:28:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6222"
  },
  {
    "number": 6221,
    "title": "fix: bindings unit tests for nanobind",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T15:43:13Z",
    "closed_at": "2025-07-22T13:51:43Z",
    "merged_at": "2025-07-22T13:51:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6221"
  },
  {
    "number": 6220,
    "title": "[TRTLLM-5830][feat] Improve LoRA cache memory control",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T15:41:55Z",
    "closed_at": "2025-07-31T06:26:39Z",
    "merged_at": "2025-07-31T06:26:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6220"
  },
  {
    "number": 6217,
    "title": "[TRTLLM-6650][feat] Enhance beam search support with CUDA graph integration",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T14:15:54Z",
    "closed_at": "2025-07-24T16:04:41Z",
    "merged_at": "2025-07-24T16:04:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6217"
  },
  {
    "number": 6216,
    "title": "[TRTLLM-6785][feat] BREAKING CHANGE Enable TRTLLM sampler by default",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T14:03:57Z",
    "closed_at": "2025-08-08T02:19:37Z",
    "merged_at": "2025-08-08T02:19:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6216"
  },
  {
    "number": 6215,
    "title": "Draft: Feat/support lora cuda graph",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T11:09:59Z",
    "closed_at": "2025-08-07T07:32:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6215"
  },
  {
    "number": 6213,
    "title": "test: update test list for RTX6KD",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T08:27:12Z",
    "closed_at": "2025-07-22T10:55:25Z",
    "merged_at": "2025-07-22T10:55:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6213"
  },
  {
    "number": 6212,
    "title": "[Infra] - Waive failed cases on recent post-merge",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T07:56:06Z",
    "closed_at": "2025-07-21T13:00:18Z",
    "merged_at": "2025-07-21T13:00:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6212"
  },
  {
    "number": 6211,
    "title": "chore: Mass integration of release/0.21 (part 4)",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T06:32:50Z",
    "closed_at": "2025-07-22T04:48:01Z",
    "merged_at": "2025-07-22T04:48:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6211"
  },
  {
    "number": 6210,
    "title": "[None][feat] Deepseek: Start Eagle work",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T06:13:06Z",
    "closed_at": "2025-08-22T16:57:17Z",
    "merged_at": "2025-08-22T16:57:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6210"
  },
  {
    "number": 6209,
    "title": "Revert \"[TRTLLM-5530][BREAKING CHANGE] refactor: unify KvCacheConfig …",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T04:35:13Z",
    "closed_at": "2025-07-21T06:55:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6209"
  },
  {
    "number": 6208,
    "title": "Revert \"[TRTLLM-5530][BREAKING CHANGE] refactor: unify KvCacheConfig in LLM class for pytorch backend\"",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-21T04:31:36Z",
    "closed_at": "2025-07-21T04:32:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6208"
  },
  {
    "number": 6206,
    "title": "bug: [https://nvbugs/5368507] Fix test_generate_with_seed.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-20T16:53:11Z",
    "closed_at": "2025-07-22T04:28:39Z",
    "merged_at": "2025-07-22T04:28:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6206"
  },
  {
    "number": 6205,
    "title": "[TRTLLM-6445] feat: Enable AllReduce-associated fusion patterns in Llama3/4.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-20T16:47:14Z",
    "closed_at": "2025-07-28T01:36:26Z",
    "merged_at": "2025-07-28T01:36:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6205"
  },
  {
    "number": 6203,
    "title": "Draft: Nanobind integration tests",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-20T13:41:58Z",
    "closed_at": "2025-08-25T08:26:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6203"
  },
  {
    "number": 6201,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-20T12:16:16Z",
    "closed_at": "2025-07-21T07:40:30Z",
    "merged_at": "2025-07-21T07:40:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6201"
  },
  {
    "number": 6200,
    "title": "[https://nvbugs/5378031] [feat] Hopper W4A8 MoE supports ModelOpt ckpt for PyT backend",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-20T07:46:46Z",
    "closed_at": "2025-08-13T13:24:40Z",
    "merged_at": "2025-08-13T13:24:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6200"
  },
  {
    "number": 6199,
    "title": "Qwen3: Fix eagle hidden states",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-20T05:05:13Z",
    "closed_at": "2025-08-06T21:05:19Z",
    "merged_at": "2025-08-06T21:05:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6199"
  },
  {
    "number": 6197,
    "title": "[nvbugs/5361178] feat: json_schema support in trtllm-serve using xgrammar ",
    "user": "mayani-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T23:59:15Z",
    "closed_at": "2025-07-25T18:32:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6197"
  },
  {
    "number": 6196,
    "title": "[AutoDeploy] merge feat/ad-2025-07-07",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T23:51:52Z",
    "closed_at": "2025-07-22T21:11:05Z",
    "merged_at": "2025-07-22T21:11:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6196"
  },
  {
    "number": 6195,
    "title": "enh: Lift expectation of single image per sample in Gemma3 VLM",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T23:32:46Z",
    "closed_at": "2025-07-21T00:43:07Z",
    "merged_at": "2025-07-21T00:43:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6195"
  },
  {
    "number": 6194,
    "title": "fix: Allreduce Strategy is not correctly set for MNNVL fallback.",
    "user": "timlee0212",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T21:40:35Z",
    "closed_at": "2025-07-21T22:29:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6194"
  },
  {
    "number": 6192,
    "title": "Zihaok disable gcthreshold",
    "user": "zihaok",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T18:19:28Z",
    "closed_at": "2025-07-18T18:19:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6192"
  },
  {
    "number": 6191,
    "title": "[doc][chore][qwen] fix typo of qwen document",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T17:28:24Z",
    "closed_at": "2025-07-18T22:47:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6191"
  },
  {
    "number": 6190,
    "title": "DRAFT Changes for multi stream executor",
    "user": "nvkgoyal",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T16:17:37Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6190"
  },
  {
    "number": 6189,
    "title": "fix: Ensure mlx5 library is installed for deep_ep and remove deprecated python bindings",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T16:10:58Z",
    "closed_at": "2025-07-20T02:38:51Z",
    "merged_at": "2025-07-20T02:38:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6189"
  },
  {
    "number": 6188,
    "title": "fix: Ensure that Python stub generation works against libnvidia-ml stubs",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T16:04:26Z",
    "closed_at": "2025-08-11T07:18:17Z",
    "merged_at": "2025-08-11T07:18:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6188"
  },
  {
    "number": 6185,
    "title": "feat: nanobind bindings",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T12:05:11Z",
    "closed_at": "2025-07-21T07:56:58Z",
    "merged_at": "2025-07-21T07:56:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6185"
  },
  {
    "number": 6184,
    "title": "[TRTLLM-6308][feat] Support Aggregate mode for phi4-mm",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T10:16:48Z",
    "closed_at": "2025-08-08T12:09:26Z",
    "merged_at": "2025-08-08T12:09:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6184"
  },
  {
    "number": 6181,
    "title": "[https://nvbugs/5393961][fix] record kv-cache size in MLACacheFormatter",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T09:29:29Z",
    "closed_at": "2025-07-18T21:06:45Z",
    "merged_at": "2025-07-18T21:06:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6181"
  },
  {
    "number": 6180,
    "title": "[feat] Sharding logic split to pattern detection and executor for EP and BMM (fixes #5916)",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T09:24:59Z",
    "closed_at": "2025-07-18T09:26:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6180"
  },
  {
    "number": 6178,
    "title": "[fix] Correct the returned value of has_spec_drafter",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T09:10:27Z",
    "closed_at": "2025-07-21T15:38:59Z",
    "merged_at": "2025-07-21T15:38:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6178"
  },
  {
    "number": 6177,
    "title": "[TRTLLM-6357][test] Add accuracy tests for Qwen3",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T09:08:16Z",
    "closed_at": "2025-08-01T17:33:34Z",
    "merged_at": "2025-08-01T17:33:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6177"
  },
  {
    "number": 6176,
    "title": "[Infra] - Waive failed tests in post-merge",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T09:01:46Z",
    "closed_at": "2025-07-18T09:34:34Z",
    "merged_at": "2025-07-18T09:34:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6176"
  },
  {
    "number": 6174,
    "title": "DON'T MERGE: log paused requests info",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T06:38:21Z",
    "closed_at": "2025-08-22T08:46:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6174"
  },
  {
    "number": 6173,
    "title": "[fix] Fix can_use_alltoall in fused_moe_wide_ep.py",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T06:35:47Z",
    "closed_at": "2025-07-21T02:53:08Z",
    "merged_at": "2025-07-21T02:53:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6173"
  },
  {
    "number": 6172,
    "title": "draft: DO NOT MERGE",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T06:04:17Z",
    "closed_at": "2025-07-18T11:34:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6172"
  },
  {
    "number": 6171,
    "title": "update broken link of PyTorchModelEngine in arch_overview",
    "user": "leslie-fang25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T06:01:35Z",
    "closed_at": "2025-07-18T11:53:39Z",
    "merged_at": "2025-07-18T11:53:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6171"
  },
  {
    "number": 6170,
    "title": "chore: add more log in FmhaDispatcher",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T05:55:50Z",
    "closed_at": "2025-07-18T08:53:03Z",
    "merged_at": "2025-07-18T08:53:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6170"
  },
  {
    "number": 6168,
    "title": "[None][infra] Update the allow list of CI trigger",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T04:51:58Z",
    "closed_at": "2025-07-18T07:38:39Z",
    "merged_at": "2025-07-18T07:38:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6168"
  },
  {
    "number": 6166,
    "title": "fix single_disagg_test",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T02:28:57Z",
    "closed_at": "2025-07-18T05:18:38Z",
    "merged_at": "2025-07-18T05:18:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6166"
  },
  {
    "number": 6165,
    "title": "infra: fix single-GPU stage failed will not raise error",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T02:14:44Z",
    "closed_at": "2025-07-18T14:39:33Z",
    "merged_at": "2025-07-18T14:39:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6165"
  },
  {
    "number": 6163,
    "title": "fix: Flush stale `PlanParams` with custom attention mask",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T01:25:27Z",
    "closed_at": "2025-07-21T01:55:09Z",
    "merged_at": "2025-07-21T01:55:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6163"
  },
  {
    "number": 6162,
    "title": "[linting] Enable ruff on more files (wave 2/N)",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T23:33:27Z",
    "closed_at": "2025-08-12T18:03:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6162"
  },
  {
    "number": 6161,
    "title": "[Doc][Qwen3] update qwen3 into support-matrix",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T22:09:54Z",
    "closed_at": "2025-07-18T03:23:30Z",
    "merged_at": "2025-07-18T03:23:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6161"
  },
  {
    "number": 6160,
    "title": "Revert \"feat: nanobind bindings (#5961)\"",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T22:09:34Z",
    "closed_at": "2025-07-18T02:12:55Z",
    "merged_at": "2025-07-18T02:12:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6160"
  },
  {
    "number": 6157,
    "title": "[Perf]: Add residual, norm for nemotron_nas models",
    "user": "NVShreyas",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T21:00:01Z",
    "closed_at": "2025-07-29T16:47:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6157"
  },
  {
    "number": 6153,
    "title": "fix: NVBug 5385576 py_batch_idx issue",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T18:05:02Z",
    "closed_at": "2025-07-18T14:36:43Z",
    "merged_at": "2025-07-18T14:36:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6153"
  },
  {
    "number": 6152,
    "title": "[feat] Enable TP and batching for PixtralVisionModel / Mistral3VLM",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T17:53:38Z",
    "closed_at": "2025-07-22T18:06:41Z",
    "merged_at": "2025-07-22T18:06:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6152"
  },
  {
    "number": 6151,
    "title": "[None][infra] Cherry-pick #6128 and #6130 from main branch",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T16:27:07Z",
    "closed_at": "2025-07-18T03:02:11Z",
    "merged_at": "2025-07-18T03:02:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6151"
  },
  {
    "number": 6150,
    "title": "doc: remove cuda_graph_config: {} from doc since cuda_graph enabled b…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T16:23:59Z",
    "closed_at": "2025-07-21T02:49:30Z",
    "merged_at": "2025-07-21T02:49:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6150"
  },
  {
    "number": 6149,
    "title": "fix: nanobind build due to undeclared CommType",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T16:15:53Z",
    "closed_at": "2025-07-21T14:01:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6149"
  },
  {
    "number": 6148,
    "title": "[git] Add .git-blame-ignore-revs file",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T16:03:56Z",
    "closed_at": "2025-07-17T23:34:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6148"
  },
  {
    "number": 6147,
    "title": "[nvbug/5393888][nvbug/5393042] Always use `py_seq_slot`",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T15:45:33Z",
    "closed_at": "2025-07-18T19:45:17Z",
    "merged_at": "2025-07-18T19:45:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6147"
  },
  {
    "number": 6146,
    "title": "[fix]: Skip prompt length checking for generation only requests",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T13:18:07Z",
    "closed_at": "2025-07-19T13:26:37Z",
    "merged_at": "2025-07-19T13:26:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6146"
  },
  {
    "number": 6145,
    "title": "test: Enable GB200 torch compile multi gpu tests",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T13:17:34Z",
    "closed_at": "2025-07-21T14:17:14Z",
    "merged_at": "2025-07-21T14:17:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6145"
  },
  {
    "number": 6143,
    "title": "[fix]: Revert commit 388b491",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T12:02:38Z",
    "closed_at": "2025-07-18T09:38:14Z",
    "merged_at": "2025-07-18T09:38:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6143"
  },
  {
    "number": 6141,
    "title": "DeepEP LL support variable hidden size and tokens num",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T10:31:02Z",
    "closed_at": "2025-07-20T01:32:42Z",
    "merged_at": "2025-07-20T01:32:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6141"
  },
  {
    "number": 6140,
    "title": "[https://nvbugs/5387375] fix(scaffolding): fix scaffolding aime test in test_e2e",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T10:01:45Z",
    "closed_at": "2025-07-18T02:34:38Z",
    "merged_at": "2025-07-18T02:34:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6140"
  },
  {
    "number": 6139,
    "title": "[TRTLLM-6537][infra] extend multi-gpu tests related file list",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T09:27:23Z",
    "closed_at": "2025-07-22T08:57:06Z",
    "merged_at": "2025-07-22T08:57:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6139"
  },
  {
    "number": 6138,
    "title": "[fix] Skip prompt length argument check for generation-only requests",
    "user": "Yuening-wa",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T09:13:13Z",
    "closed_at": "2025-07-17T14:44:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6138"
  },
  {
    "number": 6136,
    "title": "[nvbug/5322354] fix PD + MTP + overlap scheduler accuracy issue",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T08:46:34Z",
    "closed_at": "2025-07-23T06:53:38Z",
    "merged_at": "2025-07-23T06:53:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6136"
  },
  {
    "number": 6135,
    "title": "[TRTLLM-6549] chore: record delay introduced by disaggregated serving in kv cache measure",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T07:46:22Z",
    "closed_at": "2025-07-30T02:39:41Z",
    "merged_at": "2025-07-30T02:39:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6135"
  },
  {
    "number": 6134,
    "title": "Refactor KVCacheManager: Simplify token availability calculation and …",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T07:19:59Z",
    "closed_at": "2025-07-17T20:33:33Z",
    "merged_at": "2025-07-17T20:33:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6134"
  },
  {
    "number": 6133,
    "title": "[TRTLLM-6452][feat]: Two-model engine KV cache reuse support",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T07:13:10Z",
    "closed_at": "2025-07-19T05:17:15Z",
    "merged_at": "2025-07-19T05:17:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6133"
  },
  {
    "number": 6132,
    "title": "infra: [TRTLLM-6499] Split L0_Test into two pipeline by single GPU and multi GPU(For SBSA)",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T06:53:45Z",
    "closed_at": "2025-07-29T02:54:37Z",
    "merged_at": "2025-07-29T02:54:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6132"
  },
  {
    "number": 6131,
    "title": "CI: update multi gpu test trigger file list",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T06:20:45Z",
    "closed_at": "2025-07-17T06:48:23Z",
    "merged_at": "2025-07-17T06:48:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6131"
  },
  {
    "number": 6130,
    "title": "[Infra] - Add wiave list for pytest when using slurm",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T06:12:10Z",
    "closed_at": "2025-07-17T08:53:15Z",
    "merged_at": "2025-07-17T08:53:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6130"
  },
  {
    "number": 6129,
    "title": "fix: Fix DeepSeek R1 CI",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T06:08:25Z",
    "closed_at": "2025-07-17T10:24:49Z",
    "merged_at": "2025-07-17T10:24:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6129"
  },
  {
    "number": 6128,
    "title": "[None][infra] Set up the initial config for CodeRabbit",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T05:24:47Z",
    "closed_at": "2025-07-17T06:29:58Z",
    "merged_at": "2025-07-17T06:29:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6128"
  },
  {
    "number": 6127,
    "title": "Cherry-pick moe sort (and all its dependencies)",
    "user": "nvzhihanj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T05:23:46Z",
    "closed_at": "2025-07-28T06:13:18Z",
    "merged_at": "2025-07-28T06:13:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6127"
  },
  {
    "number": 6126,
    "title": "[None][perf] Add MOE support for dynamic cluster shapes and custom epilogue …",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T05:12:23Z",
    "closed_at": "2025-09-03T01:54:43Z",
    "merged_at": "2025-09-03T01:54:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6126"
  },
  {
    "number": 6125,
    "title": "[Infra][None] - Set up the initial config for CodeRabbit",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T05:09:52Z",
    "closed_at": "2025-07-17T05:22:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6125"
  },
  {
    "number": 6124,
    "title": "[TRTLLM-5331] perf: Replace allgaher with AllToAllPrepare (#5570)",
    "user": "nvzhihanj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T04:58:39Z",
    "closed_at": "2025-07-22T22:26:35Z",
    "merged_at": "2025-07-22T22:26:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6124"
  },
  {
    "number": 6123,
    "title": "[TRTLLM-5061] chore: add tags to API reference",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T04:54:46Z",
    "closed_at": "2025-07-24T06:32:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6123"
  },
  {
    "number": 6122,
    "title": "doc: sync llm api example",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T04:37:04Z",
    "closed_at": "2025-07-17T04:59:46Z",
    "merged_at": "2025-07-17T04:59:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6122"
  },
  {
    "number": 6121,
    "title": "fix: convert venv_prefix to str before comparison with base_prefix",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T04:25:47Z",
    "closed_at": "2025-07-17T07:04:54Z",
    "merged_at": "2025-07-17T07:04:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6121"
  },
  {
    "number": 6118,
    "title": "rebase release 0.21 for NIM 0.21",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T02:48:49Z",
    "closed_at": "2025-07-17T02:54:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6118"
  },
  {
    "number": 6117,
    "title": "Rebase 0.21-NIM to use changes from 0.21 branch",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T02:38:14Z",
    "closed_at": "2025-07-17T03:11:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6117"
  },
  {
    "number": 6116,
    "title": "feat: Support server reload",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T02:38:11Z",
    "closed_at": "2025-07-25T08:35:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6116"
  },
  {
    "number": 6115,
    "title": "test: fix PytestUnknownMarkWarning: Unknown pytest.mark.timeout",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T02:35:03Z",
    "closed_at": "2025-07-17T10:55:05Z",
    "merged_at": "2025-07-17T10:55:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6115"
  },
  {
    "number": 6114,
    "title": "fix: Update trtllm args issues with extra nested config (#5996)",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T02:23:33Z",
    "closed_at": "2025-07-17T20:13:22Z",
    "merged_at": "2025-07-17T20:13:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6114"
  },
  {
    "number": 6113,
    "title": "infra: fix SBSA test stage",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T02:12:08Z",
    "closed_at": "2025-07-17T03:56:03Z",
    "merged_at": "2025-07-17T03:56:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6113"
  },
  {
    "number": 6112,
    "title": "[TRTLLM-6070] docs: Add initial documentation for trtllm-bench CLI. (…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T02:03:15Z",
    "closed_at": "2025-07-17T02:08:09Z",
    "merged_at": "2025-07-17T02:08:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6112"
  },
  {
    "number": 6111,
    "title": "[fix] Fix Triton build (#6076)",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T01:49:12Z",
    "closed_at": "2025-07-17T02:42:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6111"
  },
  {
    "number": 6110,
    "title": "add safe chunked broadcast",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T00:50:49Z",
    "closed_at": "2025-09-18T23:52:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6110"
  },
  {
    "number": 6109,
    "title": "Add disagg launcher scripts",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T00:10:06Z",
    "closed_at": "2025-10-27T20:24:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6109"
  },
  {
    "number": 6108,
    "title": "[nvbug/5393849]: phi4-mini will generate garbage outputs with tp_size>1 with trt backend",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T00:07:19Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6108"
  },
  {
    "number": 6107,
    "title": "chores: unwaive a few tests for v1.0",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T22:44:10Z",
    "closed_at": "2025-07-17T09:59:51Z",
    "merged_at": "2025-07-17T09:59:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6107"
  },
  {
    "number": 6106,
    "title": "feat: add support for Modelopt fp8_pb_wo quantization scheme",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T22:32:13Z",
    "closed_at": "2025-07-18T02:35:12Z",
    "merged_at": "2025-07-18T02:35:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6106"
  },
  {
    "number": 6105,
    "title": "[fix] Fix Mistral3VLM weight-loading & enable in pre-merge",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T22:12:49Z",
    "closed_at": "2025-07-17T18:04:18Z",
    "merged_at": "2025-07-17T18:04:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6105"
  },
  {
    "number": 6104,
    "title": "[TRTLLM-6453][feat] Support chunked prefill on spec decode 2 model",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T20:03:23Z",
    "closed_at": "2025-07-25T01:50:11Z",
    "merged_at": "2025-07-25T01:50:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6104"
  },
  {
    "number": 6103,
    "title": "[fix] Fixes KV Cache overrides in trtllm-bench",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T18:35:33Z",
    "closed_at": "2025-07-17T19:44:44Z",
    "merged_at": "2025-07-17T19:44:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6103"
  },
  {
    "number": 6101,
    "title": "test: update max_beam_width to 1 due to torchsampler changes.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T16:49:37Z",
    "closed_at": "2025-07-17T10:05:46Z",
    "merged_at": "2025-07-17T10:05:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6101"
  },
  {
    "number": 6100,
    "title": "Draft:FP8 R1",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T15:34:10Z",
    "closed_at": "2025-08-01T04:35:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6100"
  },
  {
    "number": 6099,
    "title": "doc: merge main branch docs into 1.0 doc branch",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T15:02:09Z",
    "closed_at": "2025-07-17T01:03:49Z",
    "merged_at": "2025-07-17T01:03:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6099"
  },
  {
    "number": 6098,
    "title": "fix: Fix triton backend build [nvbug 5396469]",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T14:57:22Z",
    "closed_at": "2025-07-16T20:30:16Z",
    "merged_at": "2025-07-16T20:30:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6098"
  },
  {
    "number": 6097,
    "title": "[TRTLLM-1302][feat] Topk logprobs for TRT backend and top1 logprob for PyT backend",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T13:48:54Z",
    "closed_at": "2025-09-12T07:32:34Z",
    "merged_at": "2025-09-12T07:32:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6097"
  },
  {
    "number": 6096,
    "title": "[Infra] - Waive failed cases in post-merge on main ",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T13:43:19Z",
    "closed_at": "2025-07-16T14:41:18Z",
    "merged_at": "2025-07-16T14:41:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6096"
  },
  {
    "number": 6095,
    "title": "[TRTLLM-6471] Infra: unwaive nixl tests and some disagg-serve tests",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T13:31:07Z",
    "closed_at": "2025-07-18T16:48:44Z",
    "merged_at": "2025-07-18T16:48:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6095"
  },
  {
    "number": 6094,
    "title": "[fix] Update jenkins container images",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T09:29:03Z",
    "closed_at": "2025-07-17T15:22:25Z",
    "merged_at": "2025-07-17T15:22:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6094"
  },
  {
    "number": 6093,
    "title": "fix: Unable to load phi4-model with tp_size>1",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T08:39:54Z",
    "closed_at": "2025-07-18T02:16:36Z",
    "merged_at": "2025-07-18T02:16:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6093"
  },
  {
    "number": 6090,
    "title": "[None][chore] ucx establish connection with zmq",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T07:08:13Z",
    "closed_at": "2025-08-05T06:50:45Z",
    "merged_at": "2025-08-05T06:50:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6090"
  },
  {
    "number": 6089,
    "title": "Fix FP8 blockwise scaling GEMM support on Blackwell",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T06:06:27Z",
    "closed_at": "2025-07-16T06:23:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6089"
  },
  {
    "number": 6088,
    "title": "Cherry Pick: PR #6076",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T05:37:10Z",
    "closed_at": "2025-07-16T06:29:46Z",
    "merged_at": "2025-07-16T06:29:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6088"
  },
  {
    "number": 6087,
    "title": "optimize: ADP schedule optimization",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T04:54:22Z",
    "closed_at": "2025-07-17T04:12:22Z",
    "merged_at": "2025-07-17T04:12:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6087"
  },
  {
    "number": 6086,
    "title": "chore: Bump version to 1.0.0rc4",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T04:35:47Z",
    "closed_at": "2025-07-16T05:02:24Z",
    "merged_at": "2025-07-16T05:02:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6086"
  },
  {
    "number": 6085,
    "title": "[TRTLLM-6444] Add some UCX trouble shooting docs and print UCX related logs",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T03:34:58Z",
    "closed_at": "2025-07-24T08:21:01Z",
    "merged_at": "2025-07-24T08:21:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6085"
  },
  {
    "number": 6084,
    "title": "[TRTLLM-6471] Infra: Upgrade NIXL to 0.3.1 and unwaive test",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T03:28:58Z",
    "closed_at": "2025-07-29T09:18:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6084"
  },
  {
    "number": 6083,
    "title": "[Whisper] add whisper support",
    "user": "wu6u3tw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T03:21:01Z",
    "closed_at": "2025-07-16T19:14:11Z",
    "merged_at": "2025-07-16T19:14:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6083"
  },
  {
    "number": 6082,
    "title": "[None] - Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T02:54:51Z",
    "closed_at": "2025-07-16T05:14:17Z",
    "merged_at": "2025-07-16T05:14:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6082"
  },
  {
    "number": 6081,
    "title": "Doc: Cherry pick #5864 and #5810 to release/0.21",
    "user": "jiahanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T02:07:56Z",
    "closed_at": "2025-07-16T16:52:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6081"
  },
  {
    "number": 6080,
    "title": "feat: Add support for benchmarking individual gemms in MOE benchmark",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T01:46:46Z",
    "closed_at": "2025-07-17T21:00:12Z",
    "merged_at": "2025-07-17T21:00:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6080"
  },
  {
    "number": 6079,
    "title": "update spec_dec",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T01:45:23Z",
    "closed_at": "2025-07-16T09:50:44Z",
    "merged_at": "2025-07-16T09:50:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6079"
  },
  {
    "number": 6078,
    "title": "[None][feat] support JIT mha.cu for SPEC_DEC in runtime",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T01:08:18Z",
    "closed_at": "2025-09-23T21:56:18Z",
    "merged_at": "2025-09-23T21:56:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6078"
  },
  {
    "number": 6077,
    "title": "[Draft] Inter-request kv cache manager support for HSTU",
    "user": "geoffreyQiu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T00:53:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6077"
  },
  {
    "number": 6076,
    "title": "[fix] Fix Triton build",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-16T00:31:37Z",
    "closed_at": "2025-07-16T03:17:22Z",
    "merged_at": "2025-07-16T03:17:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6076"
  },
  {
    "number": 6075,
    "title": "fix TMA error with GEMM+AR on TP=2",
    "user": "xavier-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T23:55:40Z",
    "closed_at": "2025-07-18T02:26:09Z",
    "merged_at": "2025-07-18T02:26:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6075"
  },
  {
    "number": 6074,
    "title": "No onboarding of blocks that are outside of attention window",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T23:51:42Z",
    "closed_at": "2025-07-16T00:11:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6074"
  },
  {
    "number": 6073,
    "title": "[fix] Correct handling of NVFP4 block scaling factors in preprocessing for MoE",
    "user": "shengliangxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T23:30:05Z",
    "closed_at": "2025-07-16T19:11:45Z",
    "merged_at": "2025-07-16T19:11:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6073"
  },
  {
    "number": 6072,
    "title": "Add documentation for eagle3+disagg+dynamo",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T23:15:04Z",
    "closed_at": "2025-07-16T15:39:30Z",
    "merged_at": "2025-07-16T15:39:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6072"
  },
  {
    "number": 6071,
    "title": "Fix TMA error with GEMM+AR on TP=2",
    "user": "xavier-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T22:12:59Z",
    "closed_at": "2025-07-16T17:27:33Z",
    "merged_at": "2025-07-16T17:27:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6071"
  },
  {
    "number": 6070,
    "title": "[nvbug/5361223] doc: Update Llama4 deployment guide to have correct commands for 0.21 release",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T21:55:50Z",
    "closed_at": "2025-07-15T21:56:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6070"
  },
  {
    "number": 6069,
    "title": "[fix] Correct handling of NVFP4 block scaling factors in preprocessing for MoE",
    "user": "shengliangxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T20:45:09Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6069"
  },
  {
    "number": 6065,
    "title": "[Fix][Chore][Qwen3] fix bug of using fp4 on sm120",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T17:23:39Z",
    "closed_at": "2025-07-20T02:25:25Z",
    "merged_at": "2025-07-20T02:25:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6065"
  },
  {
    "number": 6064,
    "title": "feat: Remove padding in attention DP.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T15:54:02Z",
    "closed_at": "2025-07-18T15:30:35Z",
    "merged_at": "2025-07-18T15:30:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6064"
  },
  {
    "number": 6063,
    "title": "[FIX] Fix of build with ENABLE_MULTI_DEVICE=0. Fix Qwen-VL fail with request wo MM data",
    "user": "vadiklyutiy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T15:06:18Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6063"
  },
  {
    "number": 6062,
    "title": "fix: Add $HOME/.local/bin to PATH when running docker in local user mode",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T14:42:16Z",
    "closed_at": "2025-07-16T08:35:28Z",
    "merged_at": "2025-07-16T08:35:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6062"
  },
  {
    "number": 6061,
    "title": "[None][opt] ADP schedule balance optimization",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T14:38:29Z",
    "closed_at": "2025-08-06T01:38:02Z",
    "merged_at": "2025-08-06T01:38:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6061"
  },
  {
    "number": 6060,
    "title": "[WIP]feat: support Vora(Vision as LoRA) model in TensorRT-LLM library",
    "user": "effortprogrammer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T12:29:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6060"
  },
  {
    "number": 6059,
    "title": "[None][feat] : Add FP8 context MLA support for SM120",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T12:04:43Z",
    "closed_at": "2025-08-07T08:16:34Z",
    "merged_at": "2025-08-07T08:16:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6059"
  },
  {
    "number": 6058,
    "title": "chroe: upgrade modelopt to 0.33",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T10:47:27Z",
    "closed_at": "2025-07-16T05:10:48Z",
    "merged_at": "2025-07-16T05:10:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6058"
  },
  {
    "number": 6057,
    "title": "chore: Update required CUDAToolkit version to 12.9 in CMakeLists.txt",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T10:22:14Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6057"
  },
  {
    "number": 6056,
    "title": "User/zhanruis/0613 split l0 test v2",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T10:15:21Z",
    "closed_at": "2025-07-16T10:13:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6056"
  },
  {
    "number": 6055,
    "title": "refactor: Enhanced handling of decoder requests and logits within the batch manager",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T10:15:21Z",
    "closed_at": "2025-07-18T10:12:08Z",
    "merged_at": "2025-07-18T10:12:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6055"
  },
  {
    "number": 6054,
    "title": "doc: Refactor documents and examples of disaggregated serving and wide ep",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T10:14:41Z",
    "closed_at": "2025-07-23T01:20:57Z",
    "merged_at": "2025-07-23T01:20:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6054"
  },
  {
    "number": 6053,
    "title": "[fix] Move NCCL group in all-gather and reduce-scatter OPs outside the outer loop",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T09:57:30Z",
    "closed_at": "2025-07-15T15:25:32Z",
    "merged_at": "2025-07-15T15:25:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6053"
  },
  {
    "number": 6051,
    "title": "[nvbug/5359218][tests] add test llm api test case on lookahead with chunked prefill",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T08:51:45Z",
    "closed_at": "2025-07-16T08:04:08Z",
    "merged_at": "2025-07-16T08:04:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6051"
  },
  {
    "number": 6050,
    "title": "[TRTLLM-5877][infra] Add fmha tests and auto trigger rules",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T08:29:58Z",
    "closed_at": "2025-09-09T03:33:10Z",
    "merged_at": "2025-09-09T03:33:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6050"
  },
  {
    "number": 6049,
    "title": "add release notes for 0.21 release",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T07:46:16Z",
    "closed_at": "2025-07-16T08:54:15Z",
    "merged_at": "2025-07-16T08:54:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6049"
  },
  {
    "number": 6048,
    "title": "Fix: pad DeepEP fp4 recv tensors if empty",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T07:18:02Z",
    "closed_at": "2025-07-15T14:14:02Z",
    "merged_at": "2025-07-15T14:14:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6048"
  },
  {
    "number": 6047,
    "title": "[TRTLLM-3971][feat] low precision all2all",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T07:08:21Z",
    "closed_at": "2025-09-02T08:56:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6047"
  },
  {
    "number": 6046,
    "title": "test: add recursive updating pytorch config and change MOE backend format in perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T06:56:44Z",
    "closed_at": "2025-07-15T07:53:15Z",
    "merged_at": "2025-07-15T07:53:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6046"
  },
  {
    "number": 6045,
    "title": "Update waives.txt",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T06:56:44Z",
    "closed_at": "2025-08-27T18:36:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6045"
  },
  {
    "number": 6044,
    "title": "Update waives.txt",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T06:54:30Z",
    "closed_at": "2025-07-15T06:56:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6044"
  },
  {
    "number": 6043,
    "title": "Low precision all2all",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T06:45:29Z",
    "closed_at": "2025-07-15T07:04:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6043"
  },
  {
    "number": 6042,
    "title": "[test] touch requirements.txt, should invoke review from trt-llm-infra-codeowners",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T06:40:32Z",
    "closed_at": "2025-07-15T06:45:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6042"
  },
  {
    "number": 6041,
    "title": "[nvbug/5347489][nvbug/5388036] increase timeout in disagg worker test",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T06:29:40Z",
    "closed_at": "2025-07-16T05:52:13Z",
    "merged_at": "2025-07-16T05:52:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6041"
  },
  {
    "number": 6040,
    "title": "update CODEOWNERS file",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T06:27:05Z",
    "closed_at": "2025-07-15T06:37:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6040"
  },
  {
    "number": 6039,
    "title": "[TRTLLM-6495] doc: add disclaimer for 3rd party software installation.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T05:11:45Z",
    "closed_at": "2025-07-15T05:33:04Z",
    "merged_at": "2025-07-15T05:33:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6039"
  },
  {
    "number": 6037,
    "title": "[TRTLLM-6368] Update deepep dispatch API",
    "user": "yifeizhang-c",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T03:44:16Z",
    "closed_at": "2025-07-18T02:13:31Z",
    "merged_at": "2025-07-18T02:13:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6037"
  },
  {
    "number": 6036,
    "title": "[infra] add more log on reuse-uploading",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T03:11:54Z",
    "closed_at": "2025-07-15T09:18:38Z",
    "merged_at": "2025-07-15T09:18:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6036"
  },
  {
    "number": 6035,
    "title": "test: add llama_v3.3_70b_cases in perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T03:08:46Z",
    "closed_at": "2025-07-15T07:53:59Z",
    "merged_at": "2025-07-15T07:53:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6035"
  },
  {
    "number": 6034,
    "title": "doc: update EXAONE 4.0 news",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T01:01:28Z",
    "closed_at": "2025-07-15T01:26:52Z",
    "merged_at": "2025-07-15T01:26:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6034"
  },
  {
    "number": 6033,
    "title": "test: Add regression tests for Gemma3 VLM",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-15T00:26:16Z",
    "closed_at": "2025-07-15T18:37:56Z",
    "merged_at": "2025-07-15T18:37:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6033"
  },
  {
    "number": 6032,
    "title": "[fix] Release slots with spec decode + disagg (#5975)",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T23:58:01Z",
    "closed_at": "2025-07-17T04:58:18Z",
    "merged_at": "2025-07-17T04:58:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6032"
  },
  {
    "number": 6031,
    "title": "[][doc]add dev town hall link to readme",
    "user": "laikhtewari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T23:47:24Z",
    "closed_at": "2025-08-06T21:17:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6031"
  },
  {
    "number": 6030,
    "title": "[mock] [test-github-workflow] Check if PR-checklist workflow is triggered correctly",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T23:40:50Z",
    "closed_at": "2025-08-27T18:36:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6030"
  },
  {
    "number": 6029,
    "title": "[TRTLLM-6822][infra] Add PR-Checklist github action and modify PR template",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T23:08:50Z",
    "closed_at": "2025-08-28T01:45:23Z",
    "merged_at": "2025-08-28T01:45:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6029"
  },
  {
    "number": 6028,
    "title": "Doc: Update llama-3.3-70B guide",
    "user": "jiahanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T23:08:33Z",
    "closed_at": "2025-07-15T02:37:26Z",
    "merged_at": "2025-07-15T02:37:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6028"
  },
  {
    "number": 6027,
    "title": "GEMM+AR TP2 harness fix",
    "user": "xavier-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T22:39:22Z",
    "closed_at": "2025-07-14T22:52:53Z",
    "merged_at": "2025-07-14T22:52:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6027"
  },
  {
    "number": 6026,
    "title": "[Fix] check for ImportError or ModuleNotFoundError for deep_ep_utils",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T22:04:29Z",
    "closed_at": "2025-07-15T05:31:35Z",
    "merged_at": "2025-07-15T05:31:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6026"
  },
  {
    "number": 6025,
    "title": "[TRTLLM-5234][feature] Add a serve subcommand to trtllm-bench",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T21:34:21Z",
    "closed_at": "2025-07-31T17:36:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6025"
  },
  {
    "number": 6024,
    "title": "doc: Adding disaggregated serving page to features section for 1.0 docs [TRTLLM-6086]",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T20:33:07Z",
    "closed_at": "2025-07-29T20:54:04Z",
    "merged_at": "2025-07-29T20:54:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6024"
  },
  {
    "number": 6023,
    "title": "Draft: [TRTLLM-4719][enhance] Refactor to add scenarios to trtllm-bench",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T20:12:01Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6023"
  },
  {
    "number": 6022,
    "title": "[fix] Remove duplicated KVCache transmission check",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T20:00:53Z",
    "closed_at": "2025-07-17T16:02:20Z",
    "merged_at": "2025-07-17T16:02:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6022"
  },
  {
    "number": 6021,
    "title": "[chore] Clean up quickstart_advanced.py",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T19:41:47Z",
    "closed_at": "2025-07-21T19:00:59Z",
    "merged_at": "2025-07-21T19:00:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6021"
  },
  {
    "number": 6020,
    "title": "Sharding logic split to pattern detection and executor",
    "user": "greg-kwasniewski1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T19:32:05Z",
    "closed_at": "2025-07-14T19:37:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6020"
  },
  {
    "number": 6019,
    "title": "Add basic Nemo Ckpt Lora Loading in pytorch flow ",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T19:30:29Z",
    "closed_at": "2025-07-23T02:42:45Z",
    "merged_at": "2025-07-23T02:42:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6019"
  },
  {
    "number": 6018,
    "title": "Fix loading of aliased weights",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T19:03:25Z",
    "closed_at": "2025-07-14T19:03:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6018"
  },
  {
    "number": 6016,
    "title": "test: Relax Gemma3 unit test thresholds",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T17:59:32Z",
    "closed_at": "2025-07-28T13:24:34Z",
    "merged_at": "2025-07-28T13:24:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6016"
  },
  {
    "number": 6015,
    "title": "Dependencies upgrade",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T17:03:22Z",
    "closed_at": "2025-07-15T13:42:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6015"
  },
  {
    "number": 6014,
    "title": "[fix] fix eagle3 two model disaggregated serving test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T16:53:21Z",
    "closed_at": "2025-07-14T19:26:05Z",
    "merged_at": "2025-07-14T19:26:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6014"
  },
  {
    "number": 6013,
    "title": "[None][feat] Add vLLM KV Pool support for XQA kernel",
    "user": "Ransiki",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T16:15:22Z",
    "closed_at": "2025-08-06T01:29:37Z",
    "merged_at": "2025-08-06T01:29:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6013"
  },
  {
    "number": 6012,
    "title": "Quantization doc",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T15:41:55Z",
    "closed_at": "2025-07-15T13:51:48Z",
    "merged_at": "2025-07-15T13:51:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6012"
  },
  {
    "number": 6011,
    "title": "Update Quantization docs",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T15:26:59Z",
    "closed_at": "2025-07-14T15:27:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6011"
  },
  {
    "number": 6010,
    "title": "[Issue 5927][fix] Avoid memory calls during broadcast for single GPU",
    "user": "johncalesp",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T15:04:08Z",
    "closed_at": "2025-07-18T21:21:04Z",
    "merged_at": "2025-07-18T21:21:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6010"
  },
  {
    "number": 6008,
    "title": "fix: update model",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T14:24:48Z",
    "closed_at": "2025-07-14T14:46:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6008"
  },
  {
    "number": 6007,
    "title": "[TRTLLM-6352][feat] Migrate EAGLE3 and draft/target speculation to Drafter",
    "user": "ziyixiong-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T14:12:03Z",
    "closed_at": "2025-07-17T13:15:01Z",
    "merged_at": "2025-07-17T13:15:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6007"
  },
  {
    "number": 6006,
    "title": "chore: Cleanup disable_fp4_allgather.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T13:40:03Z",
    "closed_at": "2025-07-16T09:54:36Z",
    "merged_at": "2025-07-16T09:54:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6006"
  },
  {
    "number": 6005,
    "title": " W4A8 GEMM ",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T13:18:14Z",
    "closed_at": "2025-07-20T14:34:57Z",
    "merged_at": "2025-07-20T14:34:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6005"
  },
  {
    "number": 6004,
    "title": "[PERF] Move calculation Qwen2-VL's rotary_cos_sin to LLM worker process",
    "user": "vadiklyutiy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T13:02:04Z",
    "closed_at": "2025-07-31T00:35:25Z",
    "merged_at": "2025-07-31T00:35:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6004"
  },
  {
    "number": 6003,
    "title": "chore: [Breaking Change] Rename cuda_graph_config padding_enabled fie…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T11:43:04Z",
    "closed_at": "2025-07-15T06:50:04Z",
    "merged_at": "2025-07-15T06:50:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6003"
  },
  {
    "number": 6002,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T10:28:32Z",
    "closed_at": "2025-07-14T10:55:34Z",
    "merged_at": "2025-07-14T10:55:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6002"
  },
  {
    "number": 6001,
    "title": "[nvbug/5387226] chore: add propogation for trust_remote_code to AutoConfig",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T10:03:40Z",
    "closed_at": "2025-07-16T08:05:39Z",
    "merged_at": "2025-07-16T08:05:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6001"
  },
  {
    "number": 6000,
    "title": "[TRTLLM-6406, TRTLLM-5172] feat: Enable guided decoding with overlap scheduler",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T09:39:49Z",
    "closed_at": "2025-07-17T09:46:10Z",
    "merged_at": "2025-07-17T09:46:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6000"
  },
  {
    "number": 5999,
    "title": "Create a xml file for dashboard",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T09:31:33Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5999"
  },
  {
    "number": 5998,
    "title": "[nvbugs/5385972][nvbugs/5387423][Fix] Minor fix for llava_next/llava_onevision",
    "user": "MinaHuai",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T09:31:29Z",
    "closed_at": "2025-07-15T14:01:36Z",
    "merged_at": "2025-07-15T14:01:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5998"
  },
  {
    "number": 5997,
    "title": "[TRTLLM-5271][feat] best_of/n for pytorch workflow",
    "user": "evezhier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T08:59:02Z",
    "closed_at": "2025-08-04T12:08:07Z",
    "merged_at": "2025-08-04T12:08:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5997"
  },
  {
    "number": 5996,
    "title": "fix: Update trtllm args issues with extra nested config",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T08:55:24Z",
    "closed_at": "2025-07-16T16:41:45Z",
    "merged_at": "2025-07-16T16:41:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5996"
  },
  {
    "number": 5995,
    "title": "[None][doc]: remove the outdated features which marked as Experimental",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T08:30:26Z",
    "closed_at": "2025-08-07T02:01:42Z",
    "merged_at": "2025-08-07T02:01:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5995"
  },
  {
    "number": 5994,
    "title": "chore: set default device to cpu on Multimodal models",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T05:56:53Z",
    "closed_at": "2025-07-23T04:45:31Z",
    "merged_at": "2025-07-23T04:45:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5994"
  },
  {
    "number": 5993,
    "title": "test: Move some of the test from post merge to pre-merge, update dgx …",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T05:38:30Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5993"
  },
  {
    "number": 5992,
    "title": "Fix errors in wide-ep scripts",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T04:44:34Z",
    "closed_at": "2025-07-14T05:07:27Z",
    "merged_at": "2025-07-14T05:07:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5992"
  },
  {
    "number": 5991,
    "title": "[TRTLLM-6471] Infra: Upgrade NIXL to 0.3.1",
    "user": "bo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T03:46:29Z",
    "closed_at": "2025-07-16T05:54:42Z",
    "merged_at": "2025-07-16T05:54:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5991"
  },
  {
    "number": 5990,
    "title": "[None][feat] spec dec with external API + parallel speculation",
    "user": "jain-ria",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T03:02:57Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5990"
  },
  {
    "number": 5989,
    "title": "Cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/5947",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T02:52:44Z",
    "closed_at": "2025-07-15T16:33:13Z",
    "merged_at": "2025-07-15T16:33:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5989"
  },
  {
    "number": 5988,
    "title": "doc: add supported data modality and types on multimodal serve",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-14T01:45:26Z",
    "closed_at": "2025-07-22T06:32:41Z",
    "merged_at": "2025-07-22T06:32:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5988"
  },
  {
    "number": 5987,
    "title": "[TRTLLM-5996][feat] FP8 blockwise scaling GEMM support on Blackwell",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-13T23:27:03Z",
    "closed_at": "2025-08-04T07:32:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5987"
  },
  {
    "number": 5986,
    "title": "perf: Enable 128x256 tile shapes for FP4 MOE CUTLASS backend",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-13T23:10:40Z",
    "closed_at": "2025-07-14T21:04:15Z",
    "merged_at": "2025-07-14T21:04:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5986"
  },
  {
    "number": 5985,
    "title": "test 007 node",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-13T17:27:16Z",
    "closed_at": "2025-07-14T22:41:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5985"
  },
  {
    "number": 5984,
    "title": "feat: Simplify and Improve Whisper Example",
    "user": "MahmoudAshraf97",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-13T14:45:09Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5984"
  },
  {
    "number": 5983,
    "title": "[None][fix] Pass mode & directory",
    "user": "tshmilnvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-13T13:55:50Z",
    "closed_at": "2025-08-18T07:28:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5983"
  },
  {
    "number": 5982,
    "title": "BlockManager copy constructor fix",
    "user": "tshmilnvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-13T13:55:21Z",
    "closed_at": "2025-07-16T09:33:17Z",
    "merged_at": "2025-07-16T09:33:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5982"
  },
  {
    "number": 5981,
    "title": "Draft: test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-13T12:15:20Z",
    "closed_at": "2025-07-24T09:15:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5981"
  },
  {
    "number": 5980,
    "title": "[TRTLLM-4366][infra] Don't call reinstall_rockylinux_cuda when the base CUDA image is up to dated",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-13T10:40:50Z",
    "closed_at": "2025-09-09T06:15:39Z",
    "merged_at": "2025-09-09T06:15:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5980"
  },
  {
    "number": 5977,
    "title": "Expand CODEOWNERS coverage",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-12T00:03:54Z",
    "closed_at": "2025-07-15T05:53:00Z",
    "merged_at": "2025-07-15T05:53:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5977"
  },
  {
    "number": 5976,
    "title": "enh: Bidirectional mask with multiple images for Gemma3",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T22:53:05Z",
    "closed_at": "2025-07-14T14:39:18Z",
    "merged_at": "2025-07-14T14:39:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5976"
  },
  {
    "number": 5975,
    "title": "[fix] Release slots with spec decode + disagg",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T21:02:06Z",
    "closed_at": "2025-07-14T23:15:03Z",
    "merged_at": "2025-07-14T23:15:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5975"
  },
  {
    "number": 5974,
    "title": "[nvbug/5374773] chore: Add a runtime flag to enable fail fast when attn window is too large to fit at least one sequence in KV cache",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T20:40:05Z",
    "closed_at": "2025-07-25T22:10:41Z",
    "merged_at": "2025-07-25T22:10:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5974"
  },
  {
    "number": 5973,
    "title": "feat: Update Gemma3 Vision Encoder",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T19:06:03Z",
    "closed_at": "2025-07-14T14:38:10Z",
    "merged_at": "2025-07-14T14:38:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5973"
  },
  {
    "number": 5972,
    "title": "[https://nvbugs/5385167][fix] Potential fix for response queue size",
    "user": "arekay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T17:46:31Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5972"
  },
  {
    "number": 5971,
    "title": "[nvbug/5308432] fix: extend triton exit time for test_llava",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T17:23:25Z",
    "closed_at": "2025-07-12T03:56:37Z",
    "merged_at": "2025-07-12T03:56:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5971"
  },
  {
    "number": 5969,
    "title": "Fix GEMM+AR nvbugs 5219533,5127801,5072306",
    "user": "xavier-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T16:15:17Z",
    "closed_at": "2025-07-11T17:22:03Z",
    "merged_at": "2025-07-11T17:22:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5969"
  },
  {
    "number": 5967,
    "title": "[None] infra:Update dependencies for DLFW 25.06",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T14:06:54Z",
    "closed_at": "2025-07-21T03:55:18Z",
    "merged_at": "2025-07-21T03:55:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5967"
  },
  {
    "number": 5966,
    "title": "fix: Fix MoE benchmark",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T13:46:55Z",
    "closed_at": "2025-07-14T06:17:26Z",
    "merged_at": "2025-07-14T06:17:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5966"
  },
  {
    "number": 5965,
    "title": "chore: use a flexible .cache mounting for launching container",
    "user": "elvischenv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T13:16:33Z",
    "closed_at": "2025-07-14T12:55:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5965"
  },
  {
    "number": 5964,
    "title": "[nvbugs-5318143] fix: restrict PyTorch memory usage to avoid OOMs",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T12:21:31Z",
    "closed_at": "2025-07-14T22:49:42Z",
    "merged_at": "2025-07-14T22:49:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5964"
  },
  {
    "number": 5963,
    "title": "Use huge page mapping for host accessible memory on GB200",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T09:50:09Z",
    "closed_at": "2025-07-14T08:11:04Z",
    "merged_at": "2025-07-14T08:11:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5963"
  },
  {
    "number": 5962,
    "title": "fix: Unable to load phi4-model with tp_size>1",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T09:43:25Z",
    "closed_at": "2025-07-16T03:39:41Z",
    "merged_at": "2025-07-16T03:39:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5962"
  },
  {
    "number": 5961,
    "title": "feat: nanobind bindings",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T09:37:58Z",
    "closed_at": "2025-07-17T14:42:53Z",
    "merged_at": "2025-07-17T14:42:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5961"
  },
  {
    "number": 5960,
    "title": "Added code owners for LLM API",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T09:37:34Z",
    "closed_at": "2025-07-12T01:30:18Z",
    "merged_at": "2025-07-12T01:30:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5960"
  },
  {
    "number": 5959,
    "title": "tests: add QA test cases ",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T09:36:35Z",
    "closed_at": "2025-07-16T08:14:25Z",
    "merged_at": "2025-07-16T08:14:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5959"
  },
  {
    "number": 5958,
    "title": "[BUG5374319][fix] WAR for draft-target-model unit tests error",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T09:33:31Z",
    "closed_at": "2025-07-12T14:48:57Z",
    "merged_at": "2025-07-12T14:48:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5958"
  },
  {
    "number": 5957,
    "title": "[Issue/5952][feat] Support JSON Schema in OpenAI-Compatible API",
    "user": "noiji",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T09:03:20Z",
    "closed_at": "2025-07-26T02:11:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5957"
  },
  {
    "number": 5956,
    "title": "[TRTLLM-6164][TRTLLM-6165] chore: add runtime example for pytorch",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T08:41:37Z",
    "closed_at": "2025-07-14T06:09:39Z",
    "merged_at": "2025-07-14T06:09:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5956"
  },
  {
    "number": 5955,
    "title": "fix: set allreduce strategy to model config",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T08:33:34Z",
    "closed_at": "2025-07-14T08:59:11Z",
    "merged_at": "2025-07-14T08:59:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5955"
  },
  {
    "number": 5954,
    "title": "fix: fix index out of bounds error in spec decoding",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T08:29:03Z",
    "closed_at": "2025-07-14T01:41:27Z",
    "merged_at": "2025-07-14T01:41:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5954"
  },
  {
    "number": 5953,
    "title": "doc: update the link of the diagram",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T08:27:54Z",
    "closed_at": "2025-07-11T09:02:22Z",
    "merged_at": "2025-07-11T09:02:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5953"
  },
  {
    "number": 5951,
    "title": "[TRTLLM-6160] chore: add sampling examples for pytorch",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T07:50:50Z",
    "closed_at": "2025-07-14T06:28:32Z",
    "merged_at": "2025-07-14T06:28:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5951"
  },
  {
    "number": 5949,
    "title": "[BUG5388075][fix] Fix error in post-merge-tests",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T07:34:32Z",
    "closed_at": "2025-07-14T05:33:39Z",
    "merged_at": "2025-07-14T05:33:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5949"
  },
  {
    "number": 5948,
    "title": "[fix] Add detokenization-based stop word logic to LLM API",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T07:33:05Z",
    "closed_at": "2025-07-29T17:16:59Z",
    "merged_at": "2025-07-29T17:16:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5948"
  },
  {
    "number": 5947,
    "title": "[nvbugs/5333742] fix MTP illegal memory access in cuda graph warmup",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T07:11:07Z",
    "closed_at": "2025-07-12T13:55:44Z",
    "merged_at": "2025-07-12T13:55:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5947"
  },
  {
    "number": 5946,
    "title": "infra: [TRTLLM-6331] Support show all stage name list when stage name check failed",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T06:56:23Z",
    "closed_at": "2025-07-15T03:06:03Z",
    "merged_at": "2025-07-15T03:06:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5946"
  },
  {
    "number": 5945,
    "title": "infra: [TRTLLM-6313] Fix the package sanity stage 'Host Node Name' in…",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T06:46:41Z",
    "closed_at": "2025-07-15T06:39:32Z",
    "merged_at": "2025-07-15T06:39:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5945"
  },
  {
    "number": 5944,
    "title": "[nvbug/5373962][test] unwaive test_disaggregated_deepseek_v3_lite_fp8",
    "user": "reasonsolo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T05:54:04Z",
    "closed_at": "2025-07-18T02:43:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5944"
  },
  {
    "number": 5943,
    "title": "Add new debug hooks to trace the memory usage and where the process hangs at",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T05:37:21Z",
    "closed_at": "2025-08-22T08:46:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5943"
  },
  {
    "number": 5942,
    "title": "tests: add timeout_manager to tensorrt flow test cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T04:48:33Z",
    "closed_at": "2025-07-22T02:23:42Z",
    "merged_at": "2025-07-22T02:23:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5942"
  },
  {
    "number": 5941,
    "title": "fix: fast redux detection in trtllm gen routing kernel",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T04:15:19Z",
    "closed_at": "2025-07-13T08:35:07Z",
    "merged_at": "2025-07-13T08:35:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5941"
  },
  {
    "number": 5940,
    "title": "fix: Fix Build Error in Local User Container",
    "user": "jiaganc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T03:24:03Z",
    "closed_at": "2025-07-21T06:18:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5940"
  },
  {
    "number": 5939,
    "title": "[TRTLLM-6471] Infra: Upgrade NIXL to 0.3.1",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T03:04:31Z",
    "closed_at": "2025-08-13T09:49:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5939"
  },
  {
    "number": 5938,
    "title": "[TRTLLM-5673] Doc: ensure the disagg doc is up to date",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T02:14:32Z",
    "closed_at": "2025-07-11T08:39:06Z",
    "merged_at": "2025-07-11T08:39:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5938"
  },
  {
    "number": 5937,
    "title": "[refactor] Unify name of NGram speculative decoding",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T01:46:14Z",
    "closed_at": "2025-07-19T04:59:57Z",
    "merged_at": "2025-07-19T04:59:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5937"
  },
  {
    "number": 5936,
    "title": "[refactor] Simplification of Speculative decoding configs - Part 2",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-11T00:20:52Z",
    "closed_at": "2025-07-23T01:20:28Z",
    "merged_at": "2025-07-23T01:20:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5936"
  },
  {
    "number": 5935,
    "title": "feat:[AutoDeploy] Support llama4 FP8",
    "user": "meenchen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T23:50:28Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5935"
  },
  {
    "number": 5934,
    "title": "[fix] Performance Optimization for MNNVL TwoShot Kernel",
    "user": "timlee0212",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T23:42:01Z",
    "closed_at": "2025-07-17T02:49:52Z",
    "merged_at": "2025-07-17T02:49:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5934"
  },
  {
    "number": 5933,
    "title": "[None] [fix] improve kvcache allocation in PyTorch runtime",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T23:38:51Z",
    "closed_at": "2025-08-26T04:40:22Z",
    "merged_at": "2025-08-26T04:40:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5933"
  },
  {
    "number": 5932,
    "title": "[nvbugs/5384881] Fix modules_to_save kwd in config for release 0.20 ",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T20:40:17Z",
    "closed_at": "2025-07-10T21:50:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5932"
  },
  {
    "number": 5931,
    "title": "[fix] Remove SpecConfig and fix thread leak issues",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T20:03:07Z",
    "closed_at": "2025-07-12T12:03:24Z",
    "merged_at": "2025-07-12T12:03:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5931"
  },
  {
    "number": 5926,
    "title": "[nvbugs/5354884][fix] Update beam search workspace estimation to new upper bound",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T17:32:53Z",
    "closed_at": "2025-07-18T17:54:51Z",
    "merged_at": "2025-07-18T17:54:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5926"
  },
  {
    "number": 5925,
    "title": "[nvbugs/5321981] Cherrypick fix: Fix the Llama3.1 405B hanging issue. (#5698)",
    "user": "nvzhihanj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T17:29:47Z",
    "closed_at": "2025-07-10T23:51:44Z",
    "merged_at": "2025-07-10T23:51:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5925"
  },
  {
    "number": 5923,
    "title": "[feat] Support option 'served_model_name'",
    "user": "tlipoca9",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T17:23:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5923"
  },
  {
    "number": 5922,
    "title": "doc: Add instructions for running gemma in disaggregated serving",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T16:48:09Z",
    "closed_at": "2025-07-10T17:21:19Z",
    "merged_at": "2025-07-10T17:21:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5922"
  },
  {
    "number": 5921,
    "title": "test: Fix Gemma3 unit tests due to transformers upgrade",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T16:07:49Z",
    "closed_at": "2025-07-11T00:24:10Z",
    "merged_at": "2025-07-11T00:24:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5921"
  },
  {
    "number": 5920,
    "title": "Draft: fix: the issue of the lost request",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T15:20:37Z",
    "closed_at": "2025-08-13T09:49:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5920"
  },
  {
    "number": 5919,
    "title": "Feat: Add vectorized loading for finalize kernel in MoE Trtllm backend",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T14:32:40Z",
    "closed_at": "2025-07-17T04:38:30Z",
    "merged_at": "2025-07-17T04:38:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5919"
  },
  {
    "number": 5918,
    "title": "[nvbugs/5368410][fix] Disable moe allreduce for multi node",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T12:48:49Z",
    "closed_at": "2025-07-14T02:06:30Z",
    "merged_at": "2025-07-14T02:06:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5918"
  },
  {
    "number": 5917,
    "title": "fix: compatibility with CUDA < 12.9 on `__CUDA_ARCH_SPECIFIC__` macro",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T10:25:58Z",
    "closed_at": "2025-07-28T08:02:27Z",
    "merged_at": "2025-07-28T08:02:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5917"
  },
  {
    "number": 5915,
    "title": "[None] - Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T10:10:05Z",
    "closed_at": "2025-07-10T10:33:17Z",
    "merged_at": "2025-07-10T10:33:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5915"
  },
  {
    "number": 5914,
    "title": "add model-feature supported matrix doc",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T09:46:19Z",
    "closed_at": "2025-07-21T06:14:42Z",
    "merged_at": "2025-07-21T06:14:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5914"
  },
  {
    "number": 5913,
    "title": "[fix] improve head_dim calculation in Qwen config",
    "user": "gkswns0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T09:40:45Z",
    "closed_at": "2025-07-28T14:13:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5913"
  },
  {
    "number": 5911,
    "title": "[None][chore] expose tokens_per_block into KvCacheConfig",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T09:17:36Z",
    "closed_at": "2025-09-08T01:14:11Z",
    "merged_at": "2025-09-08T01:14:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5911"
  },
  {
    "number": 5910,
    "title": "infra: [TRTLLM-6201] remove use parameters",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T08:40:33Z",
    "closed_at": "2025-08-12T02:51:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5910"
  },
  {
    "number": 5909,
    "title": "chore: Mass integration of release/0.21 (part 3)",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T08:32:02Z",
    "closed_at": "2025-07-14T09:17:31Z",
    "merged_at": "2025-07-14T09:17:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5909"
  },
  {
    "number": 5908,
    "title": "test: add llama_v3.3_70b cases in perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T08:28:27Z",
    "closed_at": "2025-07-15T03:09:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5908"
  },
  {
    "number": 5907,
    "title": "chore: Port leftover 0.20",
    "user": "amirkl94",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T07:28:16Z",
    "closed_at": "2025-07-10T11:48:12Z",
    "merged_at": "2025-07-10T11:48:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5907"
  },
  {
    "number": 5906,
    "title": "tests: update sanity tests & fix tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T07:25:40Z",
    "closed_at": "2025-07-11T09:48:19Z",
    "merged_at": "2025-07-11T09:48:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5906"
  },
  {
    "number": 5905,
    "title": "fix: Make the bench serving script compatible with different usages",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T06:47:57Z",
    "closed_at": "2025-07-10T11:36:26Z",
    "merged_at": "2025-07-10T11:36:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5905"
  },
  {
    "number": 5904,
    "title": "[fix] Fix mistral unit tests due to transformers upgrade",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T05:56:15Z",
    "closed_at": "2025-07-10T17:45:27Z",
    "merged_at": "2025-07-10T17:45:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5904"
  },
  {
    "number": 5903,
    "title": "feat: Add deepseek-lite tests for RTX pro 6000",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T05:42:21Z",
    "closed_at": "2025-07-16T07:51:45Z",
    "merged_at": "2025-07-16T07:51:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5903"
  },
  {
    "number": 5902,
    "title": "[NvBug 5378370] fix: Fix alltoall for llama4 (apply_router_weight_on_input=True)",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T05:21:53Z",
    "closed_at": "2025-07-12T06:50:32Z",
    "merged_at": "2025-07-12T06:50:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5902"
  },
  {
    "number": 5901,
    "title": "test: Update Llama4 Scout FP4 & FP8 accuracy tests",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T05:19:05Z",
    "closed_at": "2025-07-17T01:41:18Z",
    "merged_at": "2025-07-17T01:41:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5901"
  },
  {
    "number": 5900,
    "title": "[fix] Fix MoE workspace info by storing Torch tensor itself instead of data_ptr",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T04:44:50Z",
    "closed_at": "2025-07-10T11:07:32Z",
    "merged_at": "2025-07-10T11:07:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5900"
  },
  {
    "number": 5899,
    "title": "test: Add Gemma3 unit tests to CI in release/0.21",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T04:23:55Z",
    "closed_at": "2025-07-10T07:47:49Z",
    "merged_at": "2025-07-10T07:47:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5899"
  },
  {
    "number": 5898,
    "title": "feat: Add support for Triton request cancellation",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T03:42:12Z",
    "closed_at": "2025-07-16T00:52:44Z",
    "merged_at": "2025-07-16T00:52:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5898"
  },
  {
    "number": 5897,
    "title": "[None][feat] Add opentelemetry tracing",
    "user": "zhanghaotong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T03:34:18Z",
    "closed_at": "2025-10-27T10:51:08Z",
    "merged_at": "2025-10-27T10:51:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5897"
  },
  {
    "number": 5896,
    "title": "fix: [nvbugs/5351130] Adjust DSV3-Lite tests free_gpu_memory_fraction to 0.75 to prevent OOM on CI.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T02:53:03Z",
    "closed_at": "2025-07-10T11:16:38Z",
    "merged_at": "2025-07-10T11:16:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5896"
  },
  {
    "number": 5895,
    "title": "chore: [Breaking Change] Rename cuda_graph_config padding_enabled fie…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T02:49:41Z",
    "closed_at": "2025-07-15T07:32:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5895"
  },
  {
    "number": 5894,
    "title": "[Don't merge][refactor] Simplification of Speculative decoding configs part2",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T02:40:24Z",
    "closed_at": "2025-07-11T00:01:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5894"
  },
  {
    "number": 5893,
    "title": "copy some existing torch docs to 1.0",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-10T00:50:28Z",
    "closed_at": "2025-07-10T01:27:01Z",
    "merged_at": "2025-07-10T01:27:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5893"
  },
  {
    "number": 5891,
    "title": "[TRTLLM] Request logging.",
    "user": "arekay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T23:03:29Z",
    "closed_at": "2025-08-27T17:18:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5891"
  },
  {
    "number": 5890,
    "title": "[WIP] Block diffusion",
    "user": "marcelroed",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T22:32:26Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5890"
  },
  {
    "number": 5888,
    "title": "[refactor] Move vision parts from processor to model for Gemma3",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T21:28:25Z",
    "closed_at": "2025-07-11T22:13:51Z",
    "merged_at": "2025-07-11T22:13:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5888"
  },
  {
    "number": 5887,
    "title": "Fix link in qwen example",
    "user": "flozi00",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T20:44:05Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5887"
  },
  {
    "number": 5886,
    "title": "Waive unittest failures introduced by PR#5345 (removal of `ScaffoldingOutput` class)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T18:50:21Z",
    "closed_at": "2025-07-10T01:53:33Z",
    "merged_at": "2025-07-10T01:53:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5886"
  },
  {
    "number": 5885,
    "title": "[TRTLLM-6264] Fix flaky test_e2e.py::test_openai_lora",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T16:31:48Z",
    "closed_at": "2025-07-11T23:20:41Z",
    "merged_at": "2025-07-11T23:20:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5885"
  },
  {
    "number": 5883,
    "title": "[Model load] Fix llama min-latency model load",
    "user": "arekay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T15:34:28Z",
    "closed_at": "2025-07-15T01:29:20Z",
    "merged_at": "2025-07-15T01:29:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5883"
  },
  {
    "number": 5882,
    "title": "[Nvbug/5383670] fix: switch test case to non-fp4 ckpt for more GPU coverage",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T15:18:12Z",
    "closed_at": "2025-07-14T11:21:46Z",
    "merged_at": "2025-07-14T11:21:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5882"
  },
  {
    "number": 5881,
    "title": "deepEP fp4 post quant all2all dispatch",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T12:16:30Z",
    "closed_at": "2025-07-11T00:18:54Z",
    "merged_at": "2025-07-11T00:18:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5881"
  },
  {
    "number": 5880,
    "title": "fix: adjust window sizes of VSWA at torch backend",
    "user": "jaedeok-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T11:51:34Z",
    "closed_at": "2025-07-15T09:41:54Z",
    "merged_at": "2025-07-15T09:41:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5880"
  },
  {
    "number": 5879,
    "title": "feat(eagle3):support qwen3 dense model",
    "user": "xq25478",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T10:57:58Z",
    "closed_at": "2025-07-18T17:24:32Z",
    "merged_at": "2025-07-18T17:24:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5879"
  },
  {
    "number": 5878,
    "title": "Fix: fuse_wfp4afp4 define error",
    "user": "zkyue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T10:32:32Z",
    "closed_at": "2025-07-24T09:15:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5878"
  },
  {
    "number": 5877,
    "title": "[TRTLLM-6071] doc: Add trtllm-eval 1.0 doc",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T10:25:02Z",
    "closed_at": "2025-07-16T12:38:33Z",
    "merged_at": "2025-07-16T12:38:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5877"
  },
  {
    "number": 5876,
    "title": "[TRTLLM-5530] chore: rename LLM.autotuner_enabled to enable_autotuner",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T09:44:08Z",
    "closed_at": "2025-07-10T03:31:35Z",
    "merged_at": "2025-07-10T03:31:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5876"
  },
  {
    "number": 5875,
    "title": "tests: Fix lora perf test",
    "user": "amirkl94",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T09:17:38Z",
    "closed_at": "2025-07-10T08:56:46Z",
    "merged_at": "2025-07-10T08:56:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5875"
  },
  {
    "number": 5874,
    "title": "cherry-pick: [fix: nvbugs/5355493] Correctly clamp max sequence len to max attention window",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T08:22:16Z",
    "closed_at": "2025-07-09T17:11:17Z",
    "merged_at": "2025-07-09T17:11:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5874"
  },
  {
    "number": 5873,
    "title": "doc: add the missing rst under 1.0/examples",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T07:23:03Z",
    "closed_at": "2025-07-09T08:27:45Z",
    "merged_at": "2025-07-09T08:27:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5873"
  },
  {
    "number": 5870,
    "title": "test: remove duplicate cases in perf sanity test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T05:25:49Z",
    "closed_at": "2025-07-09T05:36:22Z",
    "merged_at": "2025-07-09T05:36:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5870"
  },
  {
    "number": 5868,
    "title": "Fix: Enhance ModelConfig for kv cache size calculations",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T03:14:45Z",
    "closed_at": "2025-07-16T21:41:31Z",
    "merged_at": "2025-07-16T21:41:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5868"
  },
  {
    "number": 5867,
    "title": "[AutoDeploy] re-enable waive for flaky AD test",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T02:32:27Z",
    "closed_at": "2025-07-09T02:47:48Z",
    "merged_at": "2025-07-09T02:47:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5867"
  },
  {
    "number": 5866,
    "title": "avoid nesting NCCL group in allgather and reduce scatter OPs",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T02:27:42Z",
    "closed_at": "2025-07-10T03:33:00Z",
    "merged_at": "2025-07-10T03:33:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5866"
  },
  {
    "number": 5865,
    "title": "[https://nvbugs/5355316] fix: update torch.compile option to fix triton store_cubin error",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T02:01:20Z",
    "closed_at": "2025-07-10T03:16:57Z",
    "merged_at": "2025-07-10T03:16:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5865"
  },
  {
    "number": 5864,
    "title": "Doc: fix link in llama4 Maverick example",
    "user": "jiahanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T01:44:16Z",
    "closed_at": "2025-07-09T02:09:59Z",
    "merged_at": "2025-07-09T02:09:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5864"
  },
  {
    "number": 5863,
    "title": "Xqa swa sd",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-09T00:23:28Z",
    "closed_at": "2025-08-29T17:26:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5863"
  },
  {
    "number": 5859,
    "title": "[Chore] Replace MODEL_CACHE_DIR with LLM_MODELS_ROOT and unwaive triton_server/test_triton.py::test_gpt_ib[gpt-ib]",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T22:18:32Z",
    "closed_at": "2025-07-21T22:46:38Z",
    "merged_at": "2025-07-21T22:46:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5859"
  },
  {
    "number": 5857,
    "title": "fix: Skip rope scaling for local layers in Gemma3 VLM",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T19:52:00Z",
    "closed_at": "2025-07-09T02:10:34Z",
    "merged_at": "2025-07-09T02:10:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5857"
  },
  {
    "number": 5856,
    "title": "chore: remove support for llmapi + TRT backend in Triton",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T18:57:16Z",
    "closed_at": "2025-07-10T04:30:35Z",
    "merged_at": "2025-07-10T04:30:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5856"
  },
  {
    "number": 5855,
    "title": "fix: [5376140] [AutoDeploy] Update unit tests: skip all_close assert for dropout in attention, increase tolerance for rope op test",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T18:33:34Z",
    "closed_at": "2025-07-09T00:13:32Z",
    "merged_at": "2025-07-09T00:13:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5855"
  },
  {
    "number": 5853,
    "title": "feat: Custom masking utils for Gemma3 VLM",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T17:08:35Z",
    "closed_at": "2025-07-09T21:18:05Z",
    "merged_at": "2025-07-09T21:18:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5853"
  },
  {
    "number": 5852,
    "title": "Remove unnecessary benchmarking results",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T15:55:02Z",
    "closed_at": "2025-07-09T03:19:06Z",
    "merged_at": "2025-07-09T03:19:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5852"
  },
  {
    "number": 5851,
    "title": "draft: nanobind in pipeline",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T15:27:25Z",
    "closed_at": "2025-07-17T08:10:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5851"
  },
  {
    "number": 5850,
    "title": "[TRTLLM-5863][feat] Support Weight-Only-Quantization in PyTorch Workflow",
    "user": "Yuening-wa",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T14:25:35Z",
    "closed_at": "2025-07-21T07:17:35Z",
    "merged_at": "2025-07-21T07:17:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5850"
  },
  {
    "number": 5849,
    "title": "fix: [5328141] increase tolerance for test_fp8_block_scale_gemm",
    "user": "nekorobov",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T13:48:37Z",
    "closed_at": "2025-07-10T13:44:19Z",
    "merged_at": "2025-07-10T13:44:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5849"
  },
  {
    "number": 5848,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T12:41:51Z",
    "closed_at": "2025-07-09T09:02:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5848"
  },
  {
    "number": 5847,
    "title": "[TRTLLM-4279] feat: Multistream initial support for torch compile flow",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T12:26:00Z",
    "closed_at": "2025-07-21T11:10:23Z",
    "merged_at": "2025-07-21T11:10:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5847"
  },
  {
    "number": 5846,
    "title": "fix: use current_image_tags.properties in rename_docker_images.py",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T12:11:31Z",
    "closed_at": "2025-07-09T08:07:53Z",
    "merged_at": "2025-07-09T08:07:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5846"
  },
  {
    "number": 5845,
    "title": "test: reduce redundant test cases for TRTLLM Gen FP8 MoE",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T12:08:43Z",
    "closed_at": "2025-07-08T15:40:33Z",
    "merged_at": "2025-07-08T15:40:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5845"
  },
  {
    "number": 5844,
    "title": "fix: VSWA KV cache calculation",
    "user": "jaedeok-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T12:02:29Z",
    "closed_at": "2025-07-09T11:43:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5844"
  },
  {
    "number": 5843,
    "title": "[ci] Speedup beam search unit tests with fixtures for LLM",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T11:04:26Z",
    "closed_at": "2025-07-18T14:54:49Z",
    "merged_at": "2025-07-18T14:54:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5843"
  },
  {
    "number": 5842,
    "title": "fix: [https://nvbugspro.nvidia.com/bug/5375656] Unwaive for bug 5375656.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T10:57:13Z",
    "closed_at": "2025-07-09T02:17:05Z",
    "merged_at": "2025-07-09T02:17:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5842"
  },
  {
    "number": 5841,
    "title": "[fix] Catch inference failures in `trtllm-bench`",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T10:51:09Z",
    "closed_at": "2025-07-09T00:53:04Z",
    "merged_at": "2025-07-09T00:53:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5841"
  },
  {
    "number": 5840,
    "title": "doc: Update gb200 doc",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T09:58:29Z",
    "closed_at": "2025-07-08T11:22:06Z",
    "merged_at": "2025-07-08T11:22:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5840"
  },
  {
    "number": 5839,
    "title": "[https://nvbugs/5355316] fix(docker): add pytorch patch to fix triton store_cubin error",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T09:19:57Z",
    "closed_at": "2025-07-09T01:56:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5839"
  },
  {
    "number": 5838,
    "title": "update namelist in blossom-ci",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T09:18:34Z",
    "closed_at": "2025-07-08T10:07:08Z",
    "merged_at": "2025-07-08T10:07:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5838"
  },
  {
    "number": 5837,
    "title": "[Infra] - Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T09:13:09Z",
    "closed_at": "2025-07-08T09:54:06Z",
    "merged_at": "2025-07-08T09:54:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5837"
  },
  {
    "number": 5836,
    "title": "feat: Return context response immediately when stream_interval > 1",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T09:05:38Z",
    "closed_at": "2025-07-08T15:19:58Z",
    "merged_at": "2025-07-08T15:19:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5836"
  },
  {
    "number": 5835,
    "title": "[nvbug 5327706][fix] fix mgmn postprocess error",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T09:03:10Z",
    "closed_at": "2025-07-09T09:08:04Z",
    "merged_at": "2025-07-09T09:08:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5835"
  },
  {
    "number": 5834,
    "title": "[TRTLLM-6262] Fix Llama4 Scout FP4 crash issue",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T08:54:55Z",
    "closed_at": "2025-07-09T06:23:22Z",
    "merged_at": "2025-07-09T06:23:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5834"
  },
  {
    "number": 5833,
    "title": "[TRTLLM-6091][docs] Update docs/trtllm sampler 1.0",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T08:09:56Z",
    "closed_at": "2025-07-18T07:49:57Z",
    "merged_at": "2025-07-18T07:49:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5833"
  },
  {
    "number": 5832,
    "title": "[TRTLLM-5878] update nspect version",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T07:51:50Z",
    "closed_at": "2025-07-08T14:00:10Z",
    "merged_at": "2025-07-08T14:00:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5832"
  },
  {
    "number": 5831,
    "title": "Fix: ignore nvshmem_src_*.txz from `confidentiality-scan`",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T07:34:39Z",
    "closed_at": "2025-07-08T08:17:30Z",
    "merged_at": "2025-07-08T08:17:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5831"
  },
  {
    "number": 5830,
    "title": "feat: TRTLLM-6224 update xgrammar version to 0.1.19",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T07:33:05Z",
    "closed_at": "2025-07-09T01:59:14Z",
    "merged_at": "2025-07-09T01:59:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5830"
  },
  {
    "number": 5829,
    "title": "[fix] https://nvbugs/5333654 Verify the test on 0.21",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T07:06:43Z",
    "closed_at": "2025-07-08T11:59:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5829"
  },
  {
    "number": 5827,
    "title": " fix: timeout and broken pipe in disagg and worker tests",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T06:52:05Z",
    "closed_at": "2025-07-11T04:42:47Z",
    "merged_at": "2025-07-11T04:42:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5827"
  },
  {
    "number": 5826,
    "title": "test: add phi-4 multimodel and bielik-11b-v2.2 models for perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T06:38:42Z",
    "closed_at": "2025-07-21T01:29:19Z",
    "merged_at": "2025-07-21T01:29:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5826"
  },
  {
    "number": 5825,
    "title": "[NvBug 5370718, 5371538] fix: Fix incremental detokenization",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T06:36:20Z",
    "closed_at": "2025-07-10T08:30:00Z",
    "merged_at": "2025-07-10T08:30:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5825"
  },
  {
    "number": 5823,
    "title": "Fix : fix moe regression for sm120",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T03:46:10Z",
    "closed_at": "2025-07-09T13:25:11Z",
    "merged_at": "2025-07-09T13:25:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5823"
  },
  {
    "number": 5822,
    "title": "blog: add qwen3 disagg perf metrics",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T03:42:39Z",
    "closed_at": "2025-07-11T07:41:46Z",
    "merged_at": "2025-07-11T07:41:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5822"
  },
  {
    "number": 5821,
    "title": "fix: [https://nvbugs/5351130][https://nvbugs/5333654] Unwaive for bug 5351130 and 5333654.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T03:15:52Z",
    "closed_at": "2025-07-08T10:12:48Z",
    "merged_at": "2025-07-08T10:12:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5821"
  },
  {
    "number": 5819,
    "title": "chore: bump version to 1.0.0rc3",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T02:01:45Z",
    "closed_at": "2025-07-08T07:04:40Z",
    "merged_at": "2025-07-08T07:04:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5819"
  },
  {
    "number": 5818,
    "title": "Revert \"chore: [Breaking Change] Rename cuda_graph_config padding_enabled fie…\"",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T01:45:24Z",
    "closed_at": "2025-07-08T04:15:30Z",
    "merged_at": "2025-07-08T04:15:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5818"
  },
  {
    "number": 5817,
    "title": "[Feature] Add a runtime flag to enable fail fast when attn window is too large to fit at least one sequence in KV cache",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T00:54:29Z",
    "closed_at": "2025-07-11T20:24:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5817"
  },
  {
    "number": 5816,
    "title": "Fix a quote error introduced in #5534",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T00:42:08Z",
    "closed_at": "2025-07-08T10:48:33Z",
    "merged_at": "2025-07-08T10:48:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5816"
  },
  {
    "number": 5815,
    "title": "Fix lost requests for disaggregated serving",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-08T00:26:18Z",
    "closed_at": "2025-07-08T23:42:46Z",
    "merged_at": "2025-07-08T23:42:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5815"
  },
  {
    "number": 5814,
    "title": "[nvbug/5308432] fix: triton_backend test_llava timeout",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T23:41:18Z",
    "closed_at": "2025-07-08T16:53:11Z",
    "merged_at": "2025-07-08T16:53:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5814"
  },
  {
    "number": 5813,
    "title": "Add is_fp8_output key to XQA kernel cubin hashing (solves Eagle3-one-engine Hopper fp8 bug)",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T23:33:36Z",
    "closed_at": "2025-07-09T01:26:27Z",
    "merged_at": "2025-07-09T01:26:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5813"
  },
  {
    "number": 5812,
    "title": "fix: count num of chunked prefill requests into batch size",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T23:27:30Z",
    "closed_at": "2025-07-08T12:47:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5812"
  },
  {
    "number": 5811,
    "title": "Waive some `test_llama_eagle3` unittests",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T23:23:21Z",
    "closed_at": "2025-07-08T06:35:27Z",
    "merged_at": "2025-07-08T06:35:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5811"
  },
  {
    "number": 5810,
    "title": "Doc: Add llama4 Maverick eagle3 and max-throughput and low_latency benchmark guide",
    "user": "jiahanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T22:20:07Z",
    "closed_at": "2025-07-09T01:10:03Z",
    "merged_at": "2025-07-09T01:10:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5810"
  },
  {
    "number": 5809,
    "title": "[Bugfix] LLama4: fix for llama4 multimodal support",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T22:19:48Z",
    "closed_at": "2025-07-09T04:03:40Z",
    "merged_at": "2025-07-09T04:03:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5809"
  },
  {
    "number": 5808,
    "title": "[Disaggregated] Add retry knobs and handling",
    "user": "arekay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T22:15:39Z",
    "closed_at": "2025-07-18T23:27:59Z",
    "merged_at": "2025-07-18T23:27:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5808"
  },
  {
    "number": 5806,
    "title": "[None][infra] Set the label community action to only run on upstream TRTLLM",
    "user": "poweiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T19:34:13Z",
    "closed_at": "2025-07-08T06:33:43Z",
    "merged_at": "2025-07-08T06:33:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5806"
  },
  {
    "number": 5805,
    "title": "[refactor] Clean up drafter/resource manager creation logic",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T19:31:50Z",
    "closed_at": "2025-07-16T19:45:46Z",
    "merged_at": "2025-07-16T19:45:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5805"
  },
  {
    "number": 5804,
    "title": "[TRTLLM-6081] doc: KV cache feature documentation",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T18:44:21Z",
    "closed_at": "2025-08-28T15:15:42Z",
    "merged_at": "2025-08-28T15:15:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5804"
  },
  {
    "number": 5802,
    "title": "feat: binding type build argument (pybind, nanobind)",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T17:17:46Z",
    "closed_at": "2025-07-10T15:48:51Z",
    "merged_at": "2025-07-10T15:48:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5802"
  },
  {
    "number": 5801,
    "title": "[5305318] fix: Fix the accuracy issue when reduce_fusion is enabled for GEMMA model.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T16:26:52Z",
    "closed_at": "2025-07-08T11:51:06Z",
    "merged_at": "2025-07-08T11:51:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5801"
  },
  {
    "number": 5800,
    "title": "fix cancel request logic",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T15:58:22Z",
    "closed_at": "2025-07-14T02:23:20Z",
    "merged_at": "2025-07-14T02:23:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5800"
  },
  {
    "number": 5799,
    "title": "[None][refactor] decoding inputs, part 2",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T15:26:06Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5799"
  },
  {
    "number": 5798,
    "title": "[None][infra] Add nightly pipeline to generate lock files",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T14:13:56Z",
    "closed_at": "2025-09-16T22:00:04Z",
    "merged_at": "2025-09-16T22:00:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5798"
  },
  {
    "number": 5797,
    "title": "[None][infra] add nspect allow list for false positive secrets",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T14:01:37Z",
    "closed_at": "2025-09-16T20:23:02Z",
    "merged_at": "2025-09-16T20:23:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5797"
  },
  {
    "number": 5796,
    "title": "doc: update  cuda_graph_config usage part in DS R1 docs  ",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T13:32:20Z",
    "closed_at": "2025-07-08T07:54:46Z",
    "merged_at": "2025-07-08T07:54:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5796"
  },
  {
    "number": 5795,
    "title": "chore: [Breaking Change] Rename cuda_graph_config padding_enabled fie…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T13:18:27Z",
    "closed_at": "2025-07-08T00:52:36Z",
    "merged_at": "2025-07-08T00:52:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5795"
  },
  {
    "number": 5794,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T13:02:24Z",
    "closed_at": "2025-07-07T13:30:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5794"
  },
  {
    "number": 5793,
    "title": "[FEAT]: add otel trace",
    "user": "zhanghaotong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T10:00:36Z",
    "closed_at": "2025-07-10T03:06:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5793"
  },
  {
    "number": 5790,
    "title": "doc: update 1.0 doc outline by adding models and renaming software ar…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T08:49:21Z",
    "closed_at": "2025-07-09T07:04:30Z",
    "merged_at": "2025-07-09T07:04:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5790"
  },
  {
    "number": 5789,
    "title": "[nvbugs/5326453] Avoid nesting NCCL grouping in allgather OP",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T08:35:34Z",
    "closed_at": "2025-07-08T06:39:28Z",
    "merged_at": "2025-07-08T06:39:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5789"
  },
  {
    "number": 5788,
    "title": "infra: add a customizable container config for engineer development efficiency",
    "user": "zhenhuaw-me",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T08:20:43Z",
    "closed_at": "2025-07-08T12:50:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5788"
  },
  {
    "number": 5786,
    "title": "feat: Refactor the fetching request logic",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T07:04:59Z",
    "closed_at": "2025-07-22T01:16:28Z",
    "merged_at": "2025-07-22T01:16:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5786"
  },
  {
    "number": 5785,
    "title": "[None][feat] Core Metrics Implementation",
    "user": "hcyezhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T06:32:32Z",
    "closed_at": "2025-08-09T06:48:54Z",
    "merged_at": "2025-08-09T06:48:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5785"
  },
  {
    "number": 5784,
    "title": "doc: add Deprecation Policy section",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T06:17:14Z",
    "closed_at": "2025-07-21T10:47:23Z",
    "merged_at": "2025-07-21T10:47:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5784"
  },
  {
    "number": 5782,
    "title": "[NvBug 5362426] fix: Fix prompt adapter TP2 case",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T03:50:11Z",
    "closed_at": "2025-07-08T07:01:36Z",
    "merged_at": "2025-07-08T07:01:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5782"
  },
  {
    "number": 5781,
    "title": "tests: waive failed cases on main",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T03:42:35Z",
    "closed_at": "2025-07-08T09:44:13Z",
    "merged_at": "2025-07-08T09:44:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5781"
  },
  {
    "number": 5779,
    "title": "[https://nvbugspro.nvidia.com/bug/5355054] fallback to cubins for fp8 fmha kernels on Ada.",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T02:59:02Z",
    "closed_at": "2025-07-08T02:35:38Z",
    "merged_at": "2025-07-08T02:35:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5779"
  },
  {
    "number": 5777,
    "title": "[Don't merge][refactor] Simplification of Speculative decoding configs",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T02:54:52Z",
    "closed_at": "2025-07-10T13:43:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5777"
  },
  {
    "number": 5776,
    "title": "cherry pick #5416",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T02:07:31Z",
    "closed_at": "2025-07-07T09:19:38Z",
    "merged_at": "2025-07-07T09:19:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5776"
  },
  {
    "number": 5775,
    "title": "[Infra] - Fix a syntax issue in the image check",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T01:58:38Z",
    "closed_at": "2025-07-07T02:20:00Z",
    "merged_at": "2025-07-07T02:20:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5775"
  },
  {
    "number": 5774,
    "title": "fix: Adjust free GPU memory fraction in KvCacheConfig for DeepSeek R1 tests",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T01:21:03Z",
    "closed_at": "2025-07-07T09:38:55Z",
    "merged_at": "2025-07-07T09:38:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5774"
  },
  {
    "number": 5773,
    "title": "fix: Skip rope scaling for local layers in Gemma3 VLM",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-07T00:09:53Z",
    "closed_at": "2025-07-07T05:36:24Z",
    "merged_at": "2025-07-07T05:36:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5773"
  },
  {
    "number": 5772,
    "title": "fix: [nvbug/5368507] Fix test_generate_with_seed CI failure.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-06T17:06:10Z",
    "closed_at": "2025-07-07T06:58:32Z",
    "merged_at": "2025-07-07T06:58:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5772"
  },
  {
    "number": 5771,
    "title": "Refactor the rest routing part for the routing kernels in the MoE TRT-LLM backend",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-06T15:38:51Z",
    "closed_at": "2025-07-11T08:37:57Z",
    "merged_at": "2025-07-11T08:37:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5771"
  },
  {
    "number": 5770,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-06T12:16:01Z",
    "closed_at": "2025-07-07T08:06:07Z",
    "merged_at": "2025-07-07T08:06:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5770"
  },
  {
    "number": 5769,
    "title": "[Test] - Waive or fix few known test failures",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-06T07:22:20Z",
    "closed_at": "2025-07-06T13:14:17Z",
    "merged_at": "2025-07-06T13:14:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5769"
  },
  {
    "number": 5768,
    "title": "feat: Enable Gemma3 with FlashInfer backend",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-05T23:35:59Z",
    "closed_at": "2025-07-11T02:19:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5768"
  },
  {
    "number": 5765,
    "title": "Improve documentation of Kv_block_array",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-05T10:14:00Z",
    "closed_at": "2025-07-05T20:25:27Z",
    "merged_at": "2025-07-05T20:25:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5765"
  },
  {
    "number": 5764,
    "title": "[TRTLLM-5881] feat: Integrate TRT-LLM Gen FP4 block scale MoE with Pytorch workflow kernel autotuner",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T19:17:02Z",
    "closed_at": "2025-07-09T07:21:58Z",
    "merged_at": "2025-07-09T07:21:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5764"
  },
  {
    "number": 5763,
    "title": "Fix docker cache mount",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T14:17:17Z",
    "closed_at": "2025-07-07T07:18:56Z",
    "merged_at": "2025-07-07T07:18:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5763"
  },
  {
    "number": 5762,
    "title": "[nvbugs/5369799] fix: Update disaggregation handling in sampler",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T11:56:48Z",
    "closed_at": "2025-07-18T17:40:46Z",
    "merged_at": "2025-07-18T17:40:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5762"
  },
  {
    "number": 5761,
    "title": "[nvbugs/5345391] fix: chunked prefill + overlap scheduling",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T11:08:39Z",
    "closed_at": "2025-07-09T15:59:45Z",
    "merged_at": "2025-07-09T15:59:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5761"
  },
  {
    "number": 5760,
    "title": "Add wide-ep benchmarking scripts",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T10:27:02Z",
    "closed_at": "2025-07-05T11:29:39Z",
    "merged_at": "2025-07-05T11:29:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5760"
  },
  {
    "number": 5759,
    "title": "[Infra] - Waive L0 flaky test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T09:54:43Z",
    "closed_at": "2025-07-04T10:25:12Z",
    "merged_at": "2025-07-04T10:25:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5759"
  },
  {
    "number": 5757,
    "title": "Fix --image_path param error in multimodal run.py tests",
    "user": "pandalee99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T08:56:29Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5757"
  },
  {
    "number": 5756,
    "title": "[Infra] - Always use x86 image for the Jenkins agent",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T08:09:38Z",
    "closed_at": "2025-07-06T02:25:20Z",
    "merged_at": "2025-07-06T02:25:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5756"
  },
  {
    "number": 5755,
    "title": "fix: check file exists in dev container script",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T08:06:24Z",
    "closed_at": "2025-07-04T08:29:18Z",
    "merged_at": "2025-07-04T08:29:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5755"
  },
  {
    "number": 5754,
    "title": "Fix cancel request bug in attentiondp",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T07:59:12Z",
    "closed_at": "2025-07-08T07:38:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5754"
  },
  {
    "number": 5753,
    "title": "[Infra] - Always use x86 image for the Jenkins agent and few clean-ups",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T07:45:35Z",
    "closed_at": "2025-07-06T02:25:58Z",
    "merged_at": "2025-07-06T02:25:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5753"
  },
  {
    "number": 5752,
    "title": "[TRTLLM-5530][BREAKING CHANGE] refactor: unify KvCacheConfig in LLM class for pytorch backend",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T07:16:09Z",
    "closed_at": "2025-07-16T08:42:59Z",
    "merged_at": "2025-07-16T08:42:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5752"
  },
  {
    "number": 5751,
    "title": "[TRTLLM-5530][BREAKING CHANGE] refactor: LLM arglist rename mixed_sampler to enable_mixed_sampler",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T06:43:14Z",
    "closed_at": "2025-07-07T09:05:15Z",
    "merged_at": "2025-07-07T09:05:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5751"
  },
  {
    "number": 5750,
    "title": "[Infra][nvbugs/5370968] - Unwaive l0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T06:33:30Z",
    "closed_at": "2025-07-04T07:27:53Z",
    "merged_at": "2025-07-04T07:27:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5750"
  },
  {
    "number": 5749,
    "title": "chore: log stack trace on error in openai server",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T05:58:47Z",
    "closed_at": "2025-07-07T06:54:36Z",
    "merged_at": "2025-07-07T06:54:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5749"
  },
  {
    "number": 5748,
    "title": "[Infra] - Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T05:50:49Z",
    "closed_at": "2025-07-04T07:04:49Z",
    "merged_at": "2025-07-04T07:04:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5748"
  },
  {
    "number": 5747,
    "title": "Update transformers to 4.53.0",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T04:15:19Z",
    "closed_at": "2025-07-09T16:32:24Z",
    "merged_at": "2025-07-09T16:32:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5747"
  },
  {
    "number": 5746,
    "title": "Fix: pass allreduce strategy to pytorchConfig",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T03:49:54Z",
    "closed_at": "2025-07-04T12:32:14Z",
    "merged_at": "2025-07-04T12:32:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5746"
  },
  {
    "number": 5745,
    "title": "Add dummy all_reduce for kernel breakdown",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T03:19:49Z",
    "closed_at": "2025-07-05T04:08:59Z",
    "merged_at": "2025-07-05T04:08:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5745"
  },
  {
    "number": 5744,
    "title": "[nvbug5266240] chore: unwaive test_llm_with_dummy_weights",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T03:18:05Z",
    "closed_at": "2025-07-07T14:13:34Z",
    "merged_at": "2025-07-07T14:13:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5744"
  },
  {
    "number": 5743,
    "title": "[feat] Adds optional module cache for TRT-LLM Gen Gemm interfaces",
    "user": "davidclark-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T02:55:36Z",
    "closed_at": "2025-07-07T20:34:55Z",
    "merged_at": "2025-07-07T20:34:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5743"
  },
  {
    "number": 5742,
    "title": "feat: moe prepare support topk % 4 != 0",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T02:50:14Z",
    "closed_at": "2025-07-22T02:42:46Z",
    "merged_at": "2025-07-22T02:42:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5742"
  },
  {
    "number": 5740,
    "title": "Waive tests : test_openai_lora, test_trtllm_serve_lora_example and test_openai_chat_structural_tag_example",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T01:21:48Z",
    "closed_at": "2025-07-04T02:01:08Z",
    "merged_at": "2025-07-04T02:01:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5740"
  },
  {
    "number": 5739,
    "title": "[feat] Support nvidia/Cosmos-Reason1-7B",
    "user": "meatybobby",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T01:17:09Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5739"
  },
  {
    "number": 5738,
    "title": "Custom attention mask in TRTLLM attention backend",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-04T01:12:51Z",
    "closed_at": "2025-07-11T23:27:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5738"
  },
  {
    "number": 5737,
    "title": "[AutoDeploy] merge feat/ad-2025-06-29",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T22:55:37Z",
    "closed_at": "2025-07-04T01:21:18Z",
    "merged_at": "2025-07-04T01:21:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5737"
  },
  {
    "number": 5736,
    "title": "chores: merge examples for v1.0 doc",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T22:46:11Z",
    "closed_at": "2025-07-09T04:00:43Z",
    "merged_at": "2025-07-09T04:00:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5736"
  },
  {
    "number": 5735,
    "title": "[nvbug/5341178][fix] Fix OOM in Llama 4 accuracy test",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T21:54:45Z",
    "closed_at": "2025-07-04T02:55:34Z",
    "merged_at": "2025-07-04T02:55:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5735"
  },
  {
    "number": 5734,
    "title": "[TRTLLM-6070] docs: Add initial documentation for trtllm-bench CLI.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T21:17:56Z",
    "closed_at": "2025-07-17T01:15:06Z",
    "merged_at": "2025-07-17T01:15:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5734"
  },
  {
    "number": 5732,
    "title": "[TRTLLM-5847][feat] Support n-gram speculative decoding with disagg",
    "user": "raayandhar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T21:00:22Z",
    "closed_at": "2025-07-08T13:39:59Z",
    "merged_at": "2025-07-08T13:39:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5732"
  },
  {
    "number": 5727,
    "title": "chore: some refactor on WideEP",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T19:07:23Z",
    "closed_at": "2025-07-09T06:26:57Z",
    "merged_at": "2025-07-09T06:26:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5727"
  },
  {
    "number": 5726,
    "title": "[ci] speedup fused moe tests",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T18:47:34Z",
    "closed_at": "2025-07-07T15:03:15Z",
    "merged_at": "2025-07-07T15:03:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5726"
  },
  {
    "number": 5725,
    "title": "[mock] Testing the auto-assign PR functionality",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T18:10:05Z",
    "closed_at": "2025-08-27T18:36:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5725"
  },
  {
    "number": 5724,
    "title": "Cherry pick \"[NVBUG:5355009] Modify check for fuse_fp4_quant on SM120",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T18:04:16Z",
    "closed_at": "2025-07-04T07:53:21Z",
    "merged_at": "2025-07-04T07:53:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5724"
  },
  {
    "number": 5723,
    "title": "[feat] Add TRTLLM MoE nvfp4 cubins for mid-high concurrency; attention_dp for TRTLLM MoE",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T16:31:40Z",
    "closed_at": "2025-07-10T06:06:51Z",
    "merged_at": "2025-07-10T06:06:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5723"
  },
  {
    "number": 5722,
    "title": "fix: auto-load sliding window config for VSWA models",
    "user": "jaedeok-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T16:21:39Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5722"
  },
  {
    "number": 5721,
    "title": "[TRTLLM-6183] Optimize EPLB strategy with expert cooccurrence",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T15:45:47Z",
    "closed_at": "2025-07-18T11:32:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5721"
  },
  {
    "number": 5720,
    "title": "[fix: nvbugs/5355493] Correctly clamp max sequence len to max attention window",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T15:18:22Z",
    "closed_at": "2025-07-04T06:16:26Z",
    "merged_at": "2025-07-04T06:16:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5720"
  },
  {
    "number": 5719,
    "title": "[#5249][AutoDeploy] Refactor the signatures of AD graph transformations to return None",
    "user": "galagam",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T15:16:22Z",
    "closed_at": "2025-07-03T15:22:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5719"
  },
  {
    "number": 5718,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T14:57:32Z",
    "closed_at": "2025-07-04T08:24:49Z",
    "merged_at": "2025-07-04T08:24:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5718"
  },
  {
    "number": 5717,
    "title": "[BREAKING CHANGE]: change default backend to PyTorch in trtllm-serve",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T14:28:57Z",
    "closed_at": "2025-07-21T13:09:43Z",
    "merged_at": "2025-07-21T13:09:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5717"
  },
  {
    "number": 5715,
    "title": "Move test to pre merge",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T12:38:05Z",
    "closed_at": "2025-07-13T17:35:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5715"
  },
  {
    "number": 5714,
    "title": "[ci] parallelize torch unittests",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T12:31:20Z",
    "closed_at": "2025-07-09T08:05:58Z",
    "merged_at": "2025-07-09T08:05:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5714"
  },
  {
    "number": 5713,
    "title": "hopper-style context MLA",
    "user": "zhou-yuxin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T12:12:20Z",
    "closed_at": "2025-07-23T06:37:21Z",
    "merged_at": "2025-07-23T06:37:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5713"
  },
  {
    "number": 5712,
    "title": "Perf: reduce DeepEPLowLatency memory and time",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T11:44:39Z",
    "closed_at": "2025-07-04T06:46:29Z",
    "merged_at": "2025-07-04T06:46:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5712"
  },
  {
    "number": 5711,
    "title": "delete duplicate eagle3 and ngram tests",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T11:43:17Z",
    "closed_at": "2025-07-03T12:47:26Z",
    "merged_at": "2025-07-03T12:47:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5711"
  },
  {
    "number": 5710,
    "title": "fix: Improve chunking test and skip empty kernel calls",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T10:34:25Z",
    "closed_at": "2025-07-04T07:08:15Z",
    "merged_at": "2025-07-04T07:08:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5710"
  },
  {
    "number": 5709,
    "title": "infra: [TRTLLM-6242] install cuda-toolkit to fix sanity check",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T10:20:18Z",
    "closed_at": "2025-07-14T09:52:13Z",
    "merged_at": "2025-07-14T09:52:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5709"
  },
  {
    "number": 5707,
    "title": "[TRTLLM-5061] chore: add status tags to LLM API reference",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T09:44:46Z",
    "closed_at": "2025-07-28T07:57:07Z",
    "merged_at": "2025-07-28T07:57:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5707"
  },
  {
    "number": 5706,
    "title": "chore [TRTLLM-6161]: add LLM speculative decoding example",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T09:43:33Z",
    "closed_at": "2025-07-08T23:33:12Z",
    "merged_at": "2025-07-08T23:33:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5706"
  },
  {
    "number": 5705,
    "title": "[https://nvbugs/5365714] fix(scaffolding): use default LLM rather than trt backend LLM",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T09:41:56Z",
    "closed_at": "2025-07-03T14:54:20Z",
    "merged_at": "2025-07-03T14:54:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5705"
  },
  {
    "number": 5704,
    "title": "tests: waive failures on main",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T09:36:08Z",
    "closed_at": "2025-07-04T03:39:12Z",
    "merged_at": "2025-07-04T03:39:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5704"
  },
  {
    "number": 5703,
    "title": "[Doc] update the document of qwen3 and cuda_graph usage",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T09:34:39Z",
    "closed_at": "2025-07-07T01:44:25Z",
    "merged_at": "2025-07-07T01:44:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5703"
  },
  {
    "number": 5702,
    "title": "[Infra] - Waive a failed case on main",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T09:07:38Z",
    "closed_at": "2025-07-03T10:09:28Z",
    "merged_at": "2025-07-03T10:09:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5702"
  },
  {
    "number": 5701,
    "title": "chore: Mass integration of release/0.21",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T08:38:53Z",
    "closed_at": "2025-07-04T05:14:14Z",
    "merged_at": "2025-07-04T05:14:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5701"
  },
  {
    "number": 5700,
    "title": "[fix] https://nvbugs/5333654 Unwaive to check ci status and improve torch compile multi-gpu coverage",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T08:24:31Z",
    "closed_at": "2025-07-08T04:42:15Z",
    "merged_at": "2025-07-08T04:42:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5700"
  },
  {
    "number": 5699,
    "title": "[TRTLLM-5878] add stage for image registration to nspect",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T07:31:25Z",
    "closed_at": "2025-07-06T15:52:55Z",
    "merged_at": "2025-07-06T15:52:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5699"
  },
  {
    "number": 5698,
    "title": "[5321981] fix: Fix the Llama3.1 405B hanging issue.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T06:25:01Z",
    "closed_at": "2025-07-04T04:29:20Z",
    "merged_at": "2025-07-04T04:29:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5698"
  },
  {
    "number": 5696,
    "title": "feat: EXAONE4.0 support",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T04:23:59Z",
    "closed_at": "2025-07-14T13:28:10Z",
    "merged_at": "2025-07-14T13:28:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5696"
  },
  {
    "number": 5695,
    "title": "chore: refine the default value by using pydantic default instead of …",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T03:43:49Z",
    "closed_at": "2025-07-03T13:41:30Z",
    "merged_at": "2025-07-03T13:41:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5695"
  },
  {
    "number": 5694,
    "title": "[Infra] - Fix test stage check for the package sanity check stage",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T03:29:30Z",
    "closed_at": "2025-07-03T08:39:46Z",
    "merged_at": "2025-07-03T08:39:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5694"
  },
  {
    "number": 5693,
    "title": "test: fix some test failure and add llama_nemotron models in perf sanity test, add more torch cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T02:49:51Z",
    "closed_at": "2025-07-07T03:11:41Z",
    "merged_at": "2025-07-07T03:11:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5693"
  },
  {
    "number": 5692,
    "title": "chore: move remaining TRT code into _tensorrt_engine",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T02:25:49Z",
    "closed_at": "2025-09-02T01:07:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5692"
  },
  {
    "number": 5691,
    "title": "fix: Fix internal MOE build",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T02:21:08Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5691"
  },
  {
    "number": 5690,
    "title": "test: Waive NIXL tests",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-03T00:03:43Z",
    "closed_at": "2025-08-13T09:49:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5690"
  },
  {
    "number": 5689,
    "title": "Mtp optimizations round1",
    "user": "ameynaik-hub",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T23:40:02Z",
    "closed_at": "2025-07-25T17:48:27Z",
    "merged_at": "2025-07-25T17:48:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5689"
  },
  {
    "number": 5688,
    "title": "Waive `disaggregated/test_workers.py::test_workers_conditional_disaggregation[TinyLlama-1.1B-Chat-v1.0]`",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T22:15:10Z",
    "closed_at": "2025-07-14T08:21:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5688"
  },
  {
    "number": 5686,
    "title": "[feat] add support for Eclairv2 model",
    "user": "aw632",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T20:44:45Z",
    "closed_at": "2025-07-30T23:38:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5686"
  },
  {
    "number": 5684,
    "title": "support TRTLLM_DEEP_EP_TOKEN_LIMIT to allow run deep-ep on memory-con…",
    "user": "ttyio",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T18:43:43Z",
    "closed_at": "2025-07-15T15:34:22Z",
    "merged_at": "2025-07-15T15:34:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5684"
  },
  {
    "number": 5683,
    "title": "Doc: Update invalid hugging face URLs",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T18:38:48Z",
    "closed_at": "2025-07-03T07:37:01Z",
    "merged_at": "2025-07-03T07:37:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5683"
  },
  {
    "number": 5682,
    "title": "Doc: Update invalid hugging face URLs",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T18:24:16Z",
    "closed_at": "2025-07-02T18:26:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5682"
  },
  {
    "number": 5681,
    "title": "[fix] improve fp4_block_scale_moe_runner type check",
    "user": "Alcanderian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T16:46:14Z",
    "closed_at": "2025-07-08T05:32:14Z",
    "merged_at": "2025-07-08T05:32:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5681"
  },
  {
    "number": 5680,
    "title": "chore: update doc by replacing use_cuda_graph with cuda_graph_config",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T15:50:11Z",
    "closed_at": "2025-07-04T06:39:15Z",
    "merged_at": "2025-07-04T06:39:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5680"
  },
  {
    "number": 5679,
    "title": "refactor: decoding inputs",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T15:25:54Z",
    "closed_at": "2025-07-06T06:21:02Z",
    "merged_at": "2025-07-06T06:21:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5679"
  },
  {
    "number": 5678,
    "title": "[TRTLLM-6224][infra] Upgrade dependencies to DLFW 25.06 and CUDA 12.9.1",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T14:07:45Z",
    "closed_at": "2025-08-03T03:18:59Z",
    "merged_at": "2025-08-03T03:18:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5678"
  },
  {
    "number": 5676,
    "title": "[TRTLLM-6100] fix: Nvbug 5356427: autotuned TRTLLM Gen fp8 block scale MoE illegal memory access",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T11:30:29Z",
    "closed_at": "2025-07-04T02:38:08Z",
    "merged_at": "2025-07-04T02:38:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5676"
  },
  {
    "number": 5675,
    "title": "MTP and derivatives: Align sample state with trtllm sampler sample state",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T10:48:10Z",
    "closed_at": "2025-07-03T17:55:49Z",
    "merged_at": "2025-07-03T17:55:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5675"
  },
  {
    "number": 5674,
    "title": "[Infra] - Waive failed cases on release/0.21",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T10:31:26Z",
    "closed_at": "2025-07-03T02:23:54Z",
    "merged_at": "2025-07-03T02:23:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5674"
  },
  {
    "number": 5672,
    "title": "[Infra][TRTLLM-6013] - Fix stage name in single stage test rerun report",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T10:00:23Z",
    "closed_at": "2025-07-15T03:27:22Z",
    "merged_at": "2025-07-15T03:27:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5672"
  },
  {
    "number": 5671,
    "title": "[Infra] - Waive failed tests for main 0702",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T09:59:26Z",
    "closed_at": "2025-07-03T02:05:07Z",
    "merged_at": "2025-07-03T02:05:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5671"
  },
  {
    "number": 5669,
    "title": "fix: Fix missing arg to alltoall_prepare_maybe_dispatch",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T09:22:57Z",
    "closed_at": "2025-07-03T01:41:55Z",
    "merged_at": "2025-07-03T01:41:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5669"
  },
  {
    "number": 5668,
    "title": "[TRTLLM-5966][feat] Initial steps towards Helix parallelism support",
    "user": "MatthiasKohl",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T09:16:25Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5668"
  },
  {
    "number": 5667,
    "title": "[Infra] - Set default timeout to 1hr and remove some specific settings",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T08:50:53Z",
    "closed_at": "2025-07-02T12:37:54Z",
    "merged_at": "2025-07-02T12:37:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5667"
  },
  {
    "number": 5666,
    "title": "chore: Remove unused isFullContextRequest method",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T07:41:14Z",
    "closed_at": "2025-07-03T13:08:09Z",
    "merged_at": "2025-07-03T13:08:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5666"
  },
  {
    "number": 5664,
    "title": "fix [nvbug/5351244]: address remote mpi session submit",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T07:09:40Z",
    "closed_at": "2025-07-10T12:22:41Z",
    "merged_at": "2025-07-10T12:22:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5664"
  },
  {
    "number": 5663,
    "title": "feat: Enable alltoall for Cutlass MoE Backend",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T06:52:37Z",
    "closed_at": "2025-08-15T07:05:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5663"
  },
  {
    "number": 5662,
    "title": "add supported models doc",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T03:52:14Z",
    "closed_at": "2025-07-07T05:48:30Z",
    "merged_at": "2025-07-07T05:48:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5662"
  },
  {
    "number": 5661,
    "title": "[chore] 2025-07-02 update github CI allowlist",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T03:41:43Z",
    "closed_at": "2025-07-02T05:57:24Z",
    "merged_at": "2025-07-02T05:57:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5661"
  },
  {
    "number": 5660,
    "title": "fix: Set init value for moe expert id",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-02T02:59:26Z",
    "closed_at": "2025-07-03T11:05:32Z",
    "merged_at": "2025-07-03T11:05:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5660"
  },
  {
    "number": 5659,
    "title": "Avoiding the kernel launch if no finished context requests",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T23:36:24Z",
    "closed_at": "2025-07-04T08:12:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5659"
  },
  {
    "number": 5658,
    "title": "[None][infra] Update the auto-community label action to be triggered every hour",
    "user": "poweiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T23:29:01Z",
    "closed_at": "2025-07-03T16:56:30Z",
    "merged_at": "2025-07-03T16:56:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5658"
  },
  {
    "number": 5657,
    "title": "[None][infra] Update the auto-community label action to be triggered every hour",
    "user": "poweiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T23:24:34Z",
    "closed_at": "2025-07-01T23:29:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5657"
  },
  {
    "number": 5655,
    "title": "[DO NOT MERGE] TEST PR for auto-community label bot",
    "user": "poweiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T22:31:45Z",
    "closed_at": "2025-07-01T22:32:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5655"
  },
  {
    "number": 5654,
    "title": "test: Validate and add accuracy& perf tests for Ministral-8B-Instruct[-FP8](pytorch only)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T22:26:23Z",
    "closed_at": "2025-07-09T01:16:21Z",
    "merged_at": "2025-07-09T01:16:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5654"
  },
  {
    "number": 5653,
    "title": "fix: add missing self. from PR #5346",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T20:58:07Z",
    "closed_at": "2025-07-02T00:38:55Z",
    "merged_at": "2025-07-02T00:38:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5653"
  },
  {
    "number": 5651,
    "title": "[NVBUG:5355009] Modify check for fuse_fp4_quant on SM120",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T17:22:56Z",
    "closed_at": "2025-07-03T13:08:16Z",
    "merged_at": "2025-07-03T13:08:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5651"
  },
  {
    "number": 5650,
    "title": "[feat] Add TensorRT-Engine Qwen3 (dense) model support",
    "user": "gkswns0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T17:03:12Z",
    "closed_at": "2025-07-10T02:26:06Z",
    "merged_at": "2025-07-10T02:26:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5650"
  },
  {
    "number": 5649,
    "title": "feat: Add Qwen3 model support",
    "user": "gkswns0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T16:39:48Z",
    "closed_at": "2025-07-01T16:41:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5649"
  },
  {
    "number": 5647,
    "title": "Refactor the control message transceiver with ZeroMQ",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T13:10:32Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5647"
  },
  {
    "number": 5646,
    "title": "[TRTLLM-4923][feat] Enable CUDA graphs for Nemotron-H",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T12:43:38Z",
    "closed_at": "2025-07-03T08:07:51Z",
    "merged_at": "2025-07-03T08:07:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5646"
  },
  {
    "number": 5645,
    "title": "chore: bump version to 1.0.0rc2",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T11:39:51Z",
    "closed_at": "2025-07-03T04:35:29Z",
    "merged_at": "2025-07-03T04:35:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5645"
  },
  {
    "number": 5644,
    "title": "feat: TRTLLM-5574 Add phi-4-multimodal pytorch-backend support",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T10:22:35Z",
    "closed_at": "2025-07-16T22:30:59Z",
    "merged_at": "2025-07-16T22:30:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5644"
  },
  {
    "number": 5643,
    "title": "[ci] small multigpu speedups",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T10:14:52Z",
    "closed_at": "2025-07-03T12:06:11Z",
    "merged_at": "2025-07-03T12:06:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5643"
  },
  {
    "number": 5642,
    "title": "[Bug] attention DP doesn't work with embedding TP",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T10:03:35Z",
    "closed_at": "2025-07-02T00:57:46Z",
    "merged_at": "2025-07-02T00:57:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5642"
  },
  {
    "number": 5641,
    "title": "[helper] Fill build cache for #5476",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T09:22:04Z",
    "closed_at": "2025-07-02T04:18:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5641"
  },
  {
    "number": 5640,
    "title": "test: Move some of the test from post merge to pre-merge, update dgx b200 test case",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T09:17:41Z",
    "closed_at": "2025-07-04T04:26:53Z",
    "merged_at": "2025-07-04T04:26:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5640"
  },
  {
    "number": 5639,
    "title": "[refactor] Simplification of Speculative decoding configs",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T09:15:01Z",
    "closed_at": "2025-07-10T15:37:30Z",
    "merged_at": "2025-07-10T15:37:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5639"
  },
  {
    "number": 5638,
    "title": "doc: Fix outdated config in DeepSeek best perf practice doc",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T08:32:39Z",
    "closed_at": "2025-07-01T08:58:50Z",
    "merged_at": "2025-07-01T08:58:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5638"
  },
  {
    "number": 5637,
    "title": "fix: Add back allreduce_strategy parameter into TorchLlmArgs",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T07:41:09Z",
    "closed_at": "2025-07-02T01:49:20Z",
    "merged_at": "2025-07-02T01:49:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5637"
  },
  {
    "number": 5636,
    "title": "[fix] WAR to fix the illegal memory access issue in moe gemm on SM120",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T06:24:18Z",
    "closed_at": "2025-07-10T01:20:31Z",
    "merged_at": "2025-07-10T01:20:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5636"
  },
  {
    "number": 5635,
    "title": "Rebase fp8 blockwise gemm autotune",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T06:01:01Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5635"
  },
  {
    "number": 5634,
    "title": "[fix] Update to properly set cuda graphs in trtllm-bench overrides.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T04:51:52Z",
    "closed_at": "2025-07-04T20:19:16Z",
    "merged_at": "2025-07-04T20:19:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5634"
  },
  {
    "number": 5632,
    "title": "[fix] (benchmark) Correct file creation error in text_dataset_dump for paths without a directory",
    "user": "SuperGoodGame",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T03:50:23Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5632"
  },
  {
    "number": 5631,
    "title": "[Infra] - Add some timeout and unwaive a test which dev fixed",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T02:58:46Z",
    "closed_at": "2025-07-01T09:01:33Z",
    "merged_at": "2025-07-01T09:01:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5631"
  },
  {
    "number": 5630,
    "title": "[Github] Action to Auto-assign PR Reviewers (that respects CODEOWNERS.md and overrides)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T01:15:58Z",
    "closed_at": "2025-08-27T18:51:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5630"
  },
  {
    "number": 5629,
    "title": "chores: [TRTLLM-6072] 1.0 LLMAPI doc updates",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-01T00:45:11Z",
    "closed_at": "2025-07-01T01:58:45Z",
    "merged_at": "2025-07-01T01:58:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5629"
  },
  {
    "number": 5628,
    "title": "[Test] Update transformers to 4.53.0",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T23:06:12Z",
    "closed_at": "2025-07-14T12:04:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5628"
  },
  {
    "number": 5627,
    "title": "[feat] Implement pytorch sampler for MTP",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T22:30:43Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5627"
  },
  {
    "number": 5626,
    "title": "[test] [mock] touch various files and check if correctly auto-assigns reviewers",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T22:29:34Z",
    "closed_at": "2025-07-03T18:06:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5626"
  },
  {
    "number": 5625,
    "title": "[https://nvbugspro.nvidia.com/bug/5351333][fix] Update to chunking calculation.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T21:48:25Z",
    "closed_at": "2025-07-02T09:48:02Z",
    "merged_at": "2025-07-02T09:48:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5625"
  },
  {
    "number": 5624,
    "title": "[https://nvbugs/5318059][test] Unwaive test",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T21:21:47Z",
    "closed_at": "2025-07-01T08:54:45Z",
    "merged_at": "2025-07-01T08:54:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5624"
  },
  {
    "number": 5623,
    "title": "[None][fix] Ignore context for presence and frequency penalties. Matches calculation in vllm.",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T21:09:38Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5623"
  },
  {
    "number": 5622,
    "title": "[feat] Compatibility with other LLM engines: Support negative seed and top_k=-1",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T20:58:52Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5622"
  },
  {
    "number": 5621,
    "title": "[TRTLLM-3576][fix] Raise exception when exceeds input tokens and clamp max tokens",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T20:54:05Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5621"
  },
  {
    "number": 5620,
    "title": "[fix] fix log probs and add for mtp and completion requests",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T20:49:25Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5620"
  },
  {
    "number": 5619,
    "title": "fix: draft tokens `TorchSampler` fast path",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T20:32:21Z",
    "closed_at": "2025-06-30T20:59:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5619"
  },
  {
    "number": 5617,
    "title": "refactor: Clean up DecodingInput and DecodingOutput",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T15:08:40Z",
    "closed_at": "2025-07-01T12:31:42Z",
    "merged_at": "2025-07-01T12:31:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5617"
  },
  {
    "number": 5616,
    "title": "[TRTLLM-5826][feat] Support pytorch LoRA adapter eviction",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T14:59:10Z",
    "closed_at": "2025-07-20T05:00:14Z",
    "merged_at": "2025-07-20T05:00:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5616"
  },
  {
    "number": 5615,
    "title": "[TRTLLM-5812][feat] support FP8 row-wise dense GEMM in torch flow",
    "user": "DylanChen-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T13:47:57Z",
    "closed_at": "2025-07-07T10:04:57Z",
    "merged_at": "2025-07-07T10:04:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5615"
  },
  {
    "number": 5614,
    "title": "Chunked Experts",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T13:21:36Z",
    "closed_at": "2025-08-01T03:29:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5614"
  },
  {
    "number": 5613,
    "title": "feat: fuse w4a8 moe pre-quant scale on Hopper",
    "user": "xiaoweiw-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T12:37:10Z",
    "closed_at": "2025-07-02T03:02:41Z",
    "merged_at": "2025-07-02T03:02:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5613"
  },
  {
    "number": 5611,
    "title": "feat: use session abstraction in data transceiver and cache formatter",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T10:17:40Z",
    "closed_at": "2025-07-16T05:52:44Z",
    "merged_at": "2025-07-16T05:52:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5611"
  },
  {
    "number": 5610,
    "title": "chore: enhance yaml loading arbitrary options in LlmArgs",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T10:09:08Z",
    "closed_at": "2025-07-02T06:21:38Z",
    "merged_at": "2025-07-02T06:21:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5610"
  },
  {
    "number": 5609,
    "title": "fix:https://nvbugs/5362398",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T10:00:39Z",
    "closed_at": "2025-06-30T17:29:41Z",
    "merged_at": "2025-06-30T17:29:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5609"
  },
  {
    "number": 5608,
    "title": "fix [nvbug5351244]: test_mpi_session submit sync/async",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T09:44:04Z",
    "closed_at": "2025-06-30T16:48:59Z",
    "merged_at": "2025-06-30T16:48:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5608"
  },
  {
    "number": 5607,
    "title": "[nvbugs/5359696] ci: add failing test",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T09:24:50Z",
    "closed_at": "2025-07-14T08:22:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5607"
  },
  {
    "number": 5606,
    "title": "fix: [https://nvbugspro.nvidia.com/bug/5345215] Unwaive for bug 5345215.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T09:06:15Z",
    "closed_at": "2025-07-08T04:11:08Z",
    "merged_at": "2025-07-08T04:11:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5606"
  },
  {
    "number": 5605,
    "title": "[TRTLLM-5989, TRTLLM-5991, TRTLLM-5993] doc: Update container instructions (#5490)",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T08:01:06Z",
    "closed_at": "2025-06-30T11:27:49Z",
    "merged_at": "2025-06-30T11:27:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5605"
  },
  {
    "number": 5604,
    "title": "[ci] move eagle1 and medusa tests to post-merge",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T07:54:25Z",
    "closed_at": "2025-06-30T11:32:28Z",
    "merged_at": "2025-06-30T11:32:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5604"
  },
  {
    "number": 5603,
    "title": "[fix][ci] missing class names in post-merge test reports",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T07:49:11Z",
    "closed_at": "2025-06-30T14:13:29Z",
    "merged_at": "2025-06-30T14:13:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5603"
  },
  {
    "number": 5602,
    "title": "fix: [https://nvbugs/5355219] Fix bug of Qwen3 235B CI on dgx_gb200",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T07:47:46Z",
    "closed_at": "2025-07-02T02:07:02Z",
    "merged_at": "2025-07-02T02:07:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5602"
  },
  {
    "number": 5600,
    "title": "doc: Minor update to DeepSeek R1 best practice",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T07:04:56Z",
    "closed_at": "2025-06-30T07:49:06Z",
    "merged_at": "2025-06-30T07:49:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5600"
  },
  {
    "number": 5596,
    "title": "feat: nvfp4 attn compile init commit",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T06:40:00Z",
    "closed_at": "2025-08-05T07:00:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5596"
  },
  {
    "number": 5595,
    "title": "chore [TRTLLM-6009]: remove ptuning knobs from TorchLlmArgs",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T06:31:23Z",
    "closed_at": "2025-06-30T12:40:23Z",
    "merged_at": "2025-06-30T12:40:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5595"
  },
  {
    "number": 5589,
    "title": "[TRTLLM-5930] doc:refactor doc structure for 1.0 release",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T05:37:29Z",
    "closed_at": "2025-07-06T15:59:05Z",
    "merged_at": "2025-07-06T15:59:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5589"
  },
  {
    "number": 5587,
    "title": "[Infra][main] Cherry-pick from release/0.21: Update nccl to 2.27.5 (#5539)",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T05:06:53Z",
    "closed_at": "2025-06-30T10:12:08Z",
    "merged_at": "2025-06-30T10:12:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5587"
  },
  {
    "number": 5586,
    "title": "[fix] increase evaluation batch size for vicuda tests",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T03:49:58Z",
    "closed_at": "2025-06-30T06:08:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5586"
  },
  {
    "number": 5585,
    "title": "chore: remove cuda_graph_ prefix from cuda_graph_config filed members.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T03:43:14Z",
    "closed_at": "2025-06-30T16:23:15Z",
    "merged_at": "2025-06-30T16:23:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5585"
  },
  {
    "number": 5584,
    "title": "None - Few clean-ups and WAR one dependency issue",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T03:25:07Z",
    "closed_at": "2025-07-06T07:25:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5584"
  },
  {
    "number": 5583,
    "title": "fix _pad_attention_dp_dummy_request",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T03:24:49Z",
    "closed_at": "2025-07-07T06:13:55Z",
    "merged_at": "2025-07-07T06:13:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5583"
  },
  {
    "number": 5582,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-30T01:26:57Z",
    "closed_at": "2025-07-01T06:57:04Z",
    "merged_at": "2025-07-01T06:57:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5582"
  },
  {
    "number": 5580,
    "title": "feat: KV events for sliding window attention",
    "user": "jthomson04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T21:44:25Z",
    "closed_at": "2025-07-04T22:05:21Z",
    "merged_at": "2025-07-04T22:05:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5580"
  },
  {
    "number": 5579,
    "title": "[fix] speedup modeling unittests",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T21:09:19Z",
    "closed_at": "2025-06-30T03:30:45Z",
    "merged_at": "2025-06-30T03:30:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5579"
  },
  {
    "number": 5578,
    "title": "[ci] remove MMLU if followed by GSM8K",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T19:23:22Z",
    "closed_at": "2025-06-30T02:29:54Z",
    "merged_at": "2025-06-30T02:29:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5578"
  },
  {
    "number": 5577,
    "title": "refactor: Remove IGptDecoderBatched interface",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T15:14:32Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5577"
  },
  {
    "number": 5576,
    "title": "[None][refactor] Improve lookahead decoding interfaces",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T14:57:20Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5576"
  },
  {
    "number": 5575,
    "title": "Add Dependabot configuration for devcontainer updates",
    "user": "unkayk",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T14:00:09Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5575"
  },
  {
    "number": 5574,
    "title": "perf: Use tokenizers API to optimize incremental detokenization perf",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T13:22:41Z",
    "closed_at": "2025-07-01T13:35:25Z",
    "merged_at": "2025-07-01T13:35:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5574"
  },
  {
    "number": 5573,
    "title": "Add Dependabot configuration for devcontainer updates",
    "user": "unkayk",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T12:48:37Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5573"
  },
  {
    "number": 5572,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T12:14:42Z",
    "closed_at": "2025-07-01T02:15:44Z",
    "merged_at": "2025-07-01T02:15:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5572"
  },
  {
    "number": 5571,
    "title": "[CI] reduce mamba2 ssm test parameterization",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T10:37:11Z",
    "closed_at": "2025-06-29T12:56:23Z",
    "merged_at": "2025-06-29T12:56:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5571"
  },
  {
    "number": 5570,
    "title": "[TRTLLM-5331] large-scale EP: perf - Replace allgather with AllToAllPrepare",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-29T10:26:18Z",
    "closed_at": "2025-06-30T05:06:09Z",
    "merged_at": "2025-06-30T05:06:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5570"
  },
  {
    "number": 5569,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-28T23:01:29Z",
    "closed_at": "2025-07-01T03:02:56Z",
    "merged_at": "2025-07-01T03:02:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5569"
  },
  {
    "number": 5568,
    "title": "[nvbug/5354946][fix] Fix mtp vanilla draft inputs",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-28T16:26:23Z",
    "closed_at": "2025-06-30T07:59:12Z",
    "merged_at": "2025-06-30T07:59:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5568"
  },
  {
    "number": 5567,
    "title": "Refactor the topk parallelization part for the routing kernels",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-28T15:37:24Z",
    "closed_at": "2025-07-07T07:53:26Z",
    "merged_at": "2025-07-07T07:53:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5567"
  },
  {
    "number": 5566,
    "title": "[feat][test] reuse MPI pool executor across tests",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-28T04:02:59Z",
    "closed_at": "2025-06-29T14:23:13Z",
    "merged_at": "2025-06-29T14:23:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5566"
  },
  {
    "number": 5565,
    "title": "[Infra] - Add import pytest",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-28T03:26:50Z",
    "closed_at": "2025-06-29T03:06:15Z",
    "merged_at": "2025-06-29T03:06:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5565"
  },
  {
    "number": 5564,
    "title": "fix: Investigate Gemma3 1B decoder output discrepancy",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-28T01:28:04Z",
    "closed_at": "2025-07-03T01:55:25Z",
    "merged_at": "2025-07-03T01:55:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5564"
  },
  {
    "number": 5563,
    "title": "Fix GEMM+AR fusion on blackwell",
    "user": "xavier-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-28T00:14:24Z",
    "closed_at": "2025-07-09T00:48:47Z",
    "merged_at": "2025-07-09T00:48:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5563"
  },
  {
    "number": 5562,
    "title": "test: Deprecate gpt_model_type \"v1\" static batching from triton_backe…",
    "user": "mc-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T23:48:34Z",
    "closed_at": "2025-07-16T00:02:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5562"
  },
  {
    "number": 5561,
    "title": "Implement --served_model_name and improve command line parsing",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T23:47:10Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5561"
  },
  {
    "number": 5560,
    "title": "[TRTLLM-4926][feat] Reimplement metrics endpoint with stats about requests",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T23:41:49Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5560"
  },
  {
    "number": 5559,
    "title": "[None][fix] Use decorator for request cancelation and handle CancelledError",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T23:19:55Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5559"
  },
  {
    "number": 5558,
    "title": "[nvbug/5337601][fix] Fix disagg + speculative decoding",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T23:09:02Z",
    "closed_at": "2025-07-04T16:52:35Z",
    "merged_at": "2025-07-04T16:52:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5558"
  },
  {
    "number": 5557,
    "title": "refactor: [TRTLLM-6150] Refactor moe permute and finalize op by removing duplicated code",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T23:00:27Z",
    "closed_at": "2025-06-30T15:48:05Z",
    "merged_at": "2025-06-30T15:48:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5557"
  },
  {
    "number": 5556,
    "title": "[AutoDeploy] merge feat/ad-2025-06-24",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T20:57:36Z",
    "closed_at": "2025-06-28T19:52:14Z",
    "merged_at": "2025-06-28T19:52:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5556"
  },
  {
    "number": 5554,
    "title": "[TRTLLM-6104] feat: add request_perf_metrics to triton LLMAPI backend",
    "user": "xuanzic",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T20:43:40Z",
    "closed_at": "2025-07-01T04:34:42Z",
    "merged_at": "2025-07-01T04:34:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5554"
  },
  {
    "number": 5553,
    "title": "[cherry-pick] [CI] Waive `test_fp8_block_scales_4gpus[ep4-mtp_nextn=0-fp8kv=True-attention_dp=True-cuda_graph=True-overlap_scheduler=True-torch_compile=False]`",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T16:44:24Z",
    "closed_at": "2025-06-27T17:15:04Z",
    "merged_at": "2025-06-27T17:15:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5553"
  },
  {
    "number": 5551,
    "title": "[TRTLLM-6143] feat: Improve dev container tagging",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T13:56:26Z",
    "closed_at": "2025-07-02T12:56:35Z",
    "merged_at": "2025-07-02T12:56:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5551"
  },
  {
    "number": 5550,
    "title": "feat: Optimize TRTLLM Sampler perf single beam single step",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T12:11:10Z",
    "closed_at": "2025-07-07T13:44:48Z",
    "merged_at": "2025-07-07T13:44:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5550"
  },
  {
    "number": 5549,
    "title": "tests: add test_chunked_prefill for llama4",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T10:22:36Z",
    "closed_at": "2025-07-25T03:02:00Z",
    "merged_at": "2025-07-25T03:02:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5549"
  },
  {
    "number": 5548,
    "title": "ci: waive flaky test test_llama_eagle3",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T10:11:00Z",
    "closed_at": "2025-06-27T11:16:08Z",
    "merged_at": "2025-06-27T11:16:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5548"
  },
  {
    "number": 5547,
    "title": "[fix][ci] correct unittests test prefix",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T10:07:20Z",
    "closed_at": "2025-06-27T12:34:44Z",
    "merged_at": "2025-06-27T12:34:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5547"
  },
  {
    "number": 5546,
    "title": "Deduplicate waive list",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T09:53:33Z",
    "closed_at": "2025-06-30T03:12:27Z",
    "merged_at": "2025-06-30T03:12:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5546"
  },
  {
    "number": 5545,
    "title": "test: Use multiple workers for multi-GPU engine building",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T09:31:44Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5545"
  },
  {
    "number": 5544,
    "title": "rcca: test default kv_cache_reuse option for pytorch multimodal",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T09:07:29Z",
    "closed_at": "2025-07-01T04:12:48Z",
    "merged_at": "2025-07-01T04:12:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5544"
  },
  {
    "number": 5543,
    "title": "[TRTLLM-8189][chore] enhance GenerationExecutor with RPC (part1)",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T09:00:51Z",
    "closed_at": "2025-10-05T09:28:20Z",
    "merged_at": "2025-10-05T09:28:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5543"
  },
  {
    "number": 5542,
    "title": "[TRTLLM-5846] NGram with iter_stats",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T08:13:27Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5542"
  },
  {
    "number": 5541,
    "title": "[nvbug 5304752][fix] enhance _check_arguments to filter illegal requests for pytorch backend",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T07:47:41Z",
    "closed_at": "2025-07-07T11:26:14Z",
    "merged_at": "2025-07-07T11:26:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5541"
  },
  {
    "number": 5540,
    "title": "doc: Add pd dynamic scaling readme",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T07:32:48Z",
    "closed_at": "2025-07-02T06:18:51Z",
    "merged_at": "2025-07-02T06:18:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5540"
  },
  {
    "number": 5539,
    "title": "[Infra][release/0.21]Update nccl to 2.27.5",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T07:03:09Z",
    "closed_at": "2025-06-29T12:50:16Z",
    "merged_at": "2025-06-29T12:50:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5539"
  },
  {
    "number": 5538,
    "title": "feat: Use inference mode in update_requests to improve perf of TRTLLM Sampler",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T05:59:01Z",
    "closed_at": "2025-06-27T10:40:54Z",
    "merged_at": "2025-06-27T10:40:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5538"
  },
  {
    "number": 5537,
    "title": "[Infra][release/0.21] - waive failed tests",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T04:12:25Z",
    "closed_at": "2025-06-27T05:58:14Z",
    "merged_at": "2025-06-27T05:58:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5537"
  },
  {
    "number": 5536,
    "title": "[Infra] - Waive failed case in post-merge",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T04:03:10Z",
    "closed_at": "2025-06-27T05:55:49Z",
    "merged_at": "2025-06-27T05:55:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5536"
  },
  {
    "number": 5535,
    "title": "feat: Add support for MXFP8xMXFP4 in pytorch",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T03:32:36Z",
    "closed_at": "2025-07-06T22:32:06Z",
    "merged_at": "2025-07-06T22:32:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5535"
  },
  {
    "number": 5534,
    "title": "Refactor: move DeepEP from Docker images to wheel building",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T02:46:50Z",
    "closed_at": "2025-07-07T13:57:04Z",
    "merged_at": "2025-07-07T13:57:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5534"
  },
  {
    "number": 5532,
    "title": "[nvbug/5302638][nvbugs/5310314] fix _handle_cancelled_requests",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T01:43:41Z",
    "closed_at": "2025-07-07T08:51:25Z",
    "merged_at": "2025-07-07T08:51:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5532"
  },
  {
    "number": 5531,
    "title": "[fix] fix tileN cannot % 16==0 & support sm89 deepgemm bmm",
    "user": "CarstyYou",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T01:19:04Z",
    "closed_at": "2025-07-10T07:16:19Z",
    "merged_at": "2025-07-10T07:16:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5531"
  },
  {
    "number": 5530,
    "title": "[enh] [GH/CI] [WIP] [TEST] Auto-assign PR reviewers using module-owners information randomly ",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-27T00:43:51Z",
    "closed_at": "2025-08-27T18:36:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5530"
  },
  {
    "number": 5529,
    "title": "feat(models): Mistral3.1 VLM pytorch backend support",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T22:59:33Z",
    "closed_at": "2025-07-09T20:17:41Z",
    "merged_at": "2025-07-09T20:17:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5529"
  },
  {
    "number": 5528,
    "title": "Add testing for trtllm-llmapi-launch with tritonserver",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T22:12:30Z",
    "closed_at": "2025-06-27T03:19:52Z",
    "merged_at": "2025-06-27T03:19:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5528"
  },
  {
    "number": 5527,
    "title": "[https://nvbugs/5302040][feat] Add whisper support (Bert Attention on SM100 and GPTAttention for cross attention on SM100)",
    "user": "wu6u3tw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T21:52:52Z",
    "closed_at": "2025-08-13T18:19:14Z",
    "merged_at": "2025-08-13T18:19:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5527"
  },
  {
    "number": 5526,
    "title": "Update allow list 2025_06_26",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T21:49:24Z",
    "closed_at": "2025-06-26T22:25:09Z",
    "merged_at": "2025-06-26T22:25:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5526"
  },
  {
    "number": 5525,
    "title": "[nvbug/5337601][fix] Fix disagg + speculative decoding",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T18:34:01Z",
    "closed_at": "2025-06-27T23:11:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5525"
  },
  {
    "number": 5524,
    "title": "[TRTLLM-5366][feat]Add support for sm121",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T18:09:24Z",
    "closed_at": "2025-07-08T21:27:00Z",
    "merged_at": "2025-07-08T21:27:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5524"
  },
  {
    "number": 5522,
    "title": "feat: add MultimodalParams & putting all multimodal params into it and refactor HyperCLOVAX & Qwen2/2.5-VL",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T15:02:37Z",
    "closed_at": "2025-07-08T01:03:13Z",
    "merged_at": "2025-07-08T01:03:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5522"
  },
  {
    "number": 5521,
    "title": "[None] [feat] Add Tencent HunYuanMoEV1 model support",
    "user": "qianbiaoxiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T14:46:32Z",
    "closed_at": "2025-08-14T22:56:45Z",
    "merged_at": "2025-08-14T22:56:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5521"
  },
  {
    "number": 5520,
    "title": "fix: MoE autotune fallback failed to query default heuristic",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T13:45:37Z",
    "closed_at": "2025-06-26T16:28:48Z",
    "merged_at": "2025-06-26T16:28:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5520"
  },
  {
    "number": 5519,
    "title": "fix: [https://nvbugspro.nvidia.com/bug/5349343] Fix mPtrExpertCounts allocation in MoE TRT-LLM backend (nvfp4)",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T12:39:01Z",
    "closed_at": "2025-06-27T12:17:41Z",
    "merged_at": "2025-06-27T12:17:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5519"
  },
  {
    "number": 5518,
    "title": "[don't review] Fp8 blockwise gemm autotune",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T09:32:59Z",
    "closed_at": "2025-07-01T06:00:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5518"
  },
  {
    "number": 5517,
    "title": "fix: fix regression in LOCAL_USER",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T08:56:24Z",
    "closed_at": "2025-06-26T09:10:55Z",
    "merged_at": "2025-06-26T09:10:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5517"
  },
  {
    "number": 5516,
    "title": "doc: Fix benchmark cmd in disagg scripts",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T08:47:08Z",
    "closed_at": "2025-06-26T09:23:25Z",
    "merged_at": "2025-06-26T09:23:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5516"
  },
  {
    "number": 5515,
    "title": "doc: Fix benchmark cmd in disagg scripts ",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T08:42:57Z",
    "closed_at": "2025-06-26T09:04:55Z",
    "merged_at": "2025-06-26T09:04:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5515"
  },
  {
    "number": 5514,
    "title": "fix: Fix block scale fp8 support for deepseek v3 on Blackwell.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T08:17:46Z",
    "closed_at": "2025-06-27T03:03:38Z",
    "merged_at": "2025-06-27T03:03:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5514"
  },
  {
    "number": 5513,
    "title": "feature: unify new_tokens format sample state to trtllm samper tokens format",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T08:05:51Z",
    "closed_at": "2025-06-30T18:58:59Z",
    "merged_at": "2025-06-30T18:58:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5513"
  },
  {
    "number": 5512,
    "title": "tests: waive failed tests on main",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T07:59:17Z",
    "closed_at": "2025-06-27T02:13:22Z",
    "merged_at": "2025-06-27T02:13:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5512"
  },
  {
    "number": 5511,
    "title": "feat: Add support for TRTLLM CustomDataset",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T07:23:30Z",
    "closed_at": "2025-06-26T10:27:37Z",
    "merged_at": "2025-06-26T10:27:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5511"
  },
  {
    "number": 5508,
    "title": "WIP: [feat] Add Vertex AI compatible prediction route,  /vertex_generate",
    "user": "harrisonlimh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T06:27:14Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5508"
  },
  {
    "number": 5507,
    "title": "chore: Mass integration of release/0.21",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T06:03:30Z",
    "closed_at": "2025-07-01T12:12:55Z",
    "merged_at": "2025-07-01T12:12:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5507"
  },
  {
    "number": 5506,
    "title": "[CI] move flashinfer llama tests to post merge",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T04:07:51Z",
    "closed_at": "2025-06-26T11:27:32Z",
    "merged_at": "2025-06-26T11:27:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5506"
  },
  {
    "number": 5505,
    "title": "Cache transceiver support  VSWA",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T03:44:55Z",
    "closed_at": "2025-07-04T16:18:43Z",
    "merged_at": "2025-07-04T16:18:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5505"
  },
  {
    "number": 5504,
    "title": "perf: Avoid reswizzle_sf after allgather.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T03:20:24Z",
    "closed_at": "2025-06-29T13:25:50Z",
    "merged_at": "2025-06-29T13:25:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5504"
  },
  {
    "number": 5503,
    "title": "[fix]: Fix main test skip issue",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T03:18:07Z",
    "closed_at": "2025-07-01T01:39:49Z",
    "merged_at": "2025-07-01T01:39:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5503"
  },
  {
    "number": 5502,
    "title": "[None][fix] fix ssm_state for cuda graph",
    "user": "shaochangxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T03:16:27Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5502"
  },
  {
    "number": 5501,
    "title": "[Infra] - Add timeout setting for long tests found in post-merge",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T02:53:03Z",
    "closed_at": "2025-06-26T03:31:39Z",
    "merged_at": "2025-06-26T03:31:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5501"
  },
  {
    "number": 5499,
    "title": "chore: create PyExecutor workers from TorchLlmArgs part 1",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T00:58:19Z",
    "closed_at": "2025-08-26T15:13:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5499"
  },
  {
    "number": 5498,
    "title": "[CI] Run shorter deepseek tests in pre-merge",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-26T00:29:34Z",
    "closed_at": "2025-06-29T19:14:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5498"
  },
  {
    "number": 5497,
    "title": "[TRTLLM-6104] feat: add request_perf_metrics to LLMAPI",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T23:06:43Z",
    "closed_at": "2025-06-27T15:03:05Z",
    "merged_at": "2025-06-27T15:03:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5497"
  },
  {
    "number": 5496,
    "title": "[nvbug/5354825] Fix nougat test image url",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T21:26:41Z",
    "closed_at": "2025-06-26T02:10:19Z",
    "merged_at": "2025-06-26T02:10:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5496"
  },
  {
    "number": 5494,
    "title": "[CI] Waive `test_fp8_block_scales_4gpus[ep4-mtp_nextn=0-fp8kv=True-attention_dp=True-cuda_graph=True-overlap_scheduler=True-torch_compile=False]`",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T17:40:16Z",
    "closed_at": "2025-06-26T03:17:12Z",
    "merged_at": "2025-06-26T03:17:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5494"
  },
  {
    "number": 5493,
    "title": "fix: constrain grepping in docker/Makefile",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T17:12:12Z",
    "closed_at": "2025-06-26T11:44:41Z",
    "merged_at": "2025-06-26T11:44:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5493"
  },
  {
    "number": 5491,
    "title": "Update trtllm-bench to support new Pytorch default.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T16:42:56Z",
    "closed_at": "2025-06-27T00:05:43Z",
    "merged_at": "2025-06-27T00:05:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5491"
  },
  {
    "number": 5490,
    "title": "[TRTLLM-5989, TRTLLM-5991, TRTLLM-5993] doc: Update container instructions",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T16:01:14Z",
    "closed_at": "2025-06-27T14:09:41Z",
    "merged_at": "2025-06-27T14:09:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5490"
  },
  {
    "number": 5489,
    "title": "[TRTLLM-1316] refactor: Remove unnecessary pipeline parallelism logic from postProcessRequest",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T15:43:29Z",
    "closed_at": "2025-07-02T08:13:31Z",
    "merged_at": "2025-07-02T08:13:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5489"
  },
  {
    "number": 5488,
    "title": "[None][feat] Nixl support for GDS",
    "user": "tshmilnvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T15:03:14Z",
    "closed_at": "2025-09-09T05:00:38Z",
    "merged_at": "2025-09-09T05:00:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5488"
  },
  {
    "number": 5487,
    "title": "[nvbug/5341178][fix] Fix OOM in Llama 4 accuracy test",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T14:38:24Z",
    "closed_at": "2025-07-07T13:55:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5487"
  },
  {
    "number": 5486,
    "title": "[feat] merge shared expert into MoE module in TRTLLM GEN flow",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T14:35:57Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5486"
  },
  {
    "number": 5485,
    "title": "[5356427] fix: Remove the seq_len of 4096 from FP8 block scale MoE tuning configs.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T14:08:56Z",
    "closed_at": "2025-06-26T00:38:36Z",
    "merged_at": "2025-06-26T00:38:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5485"
  },
  {
    "number": 5484,
    "title": "CI: enable test cases on single device type",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T14:01:51Z",
    "closed_at": "2025-06-26T00:03:44Z",
    "merged_at": "2025-06-26T00:03:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5484"
  },
  {
    "number": 5482,
    "title": "CI: reduce BF16 test cases in B200",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T13:17:35Z",
    "closed_at": "2025-06-25T23:18:21Z",
    "merged_at": "2025-06-25T23:18:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5482"
  },
  {
    "number": 5481,
    "title": "Fix execute_process: check results using EQUAL",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T11:45:29Z",
    "closed_at": "2025-06-27T03:57:05Z",
    "merged_at": "2025-06-27T03:57:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5481"
  },
  {
    "number": 5480,
    "title": "Breaking change: perf: [TRTLLM-4662] Enable cuda graph by default",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T10:52:11Z",
    "closed_at": "2025-07-14T08:42:24Z",
    "merged_at": "2025-07-14T08:42:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5480"
  },
  {
    "number": 5479,
    "title": "test: set enable_attention_dp=True for some llama models",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T10:46:07Z",
    "closed_at": "2025-06-26T02:52:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5479"
  },
  {
    "number": 5477,
    "title": "[Infra] - Waive failed tests on release/0.21",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T09:59:22Z",
    "closed_at": "2025-06-25T11:01:55Z",
    "merged_at": "2025-06-25T11:01:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5477"
  },
  {
    "number": 5476,
    "title": "feat: reduce unnecessary kernel generation",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T09:50:52Z",
    "closed_at": "2025-07-04T06:37:50Z",
    "merged_at": "2025-07-04T06:37:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5476"
  },
  {
    "number": 5475,
    "title": "[TRTLLM-3602][feat] support nvfp4 model and fp8 kv cache for MLA chunked prefill (Blackwell)",
    "user": "jmydurant",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T09:37:43Z",
    "closed_at": "2025-06-26T14:18:09Z",
    "merged_at": "2025-06-26T14:18:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5475"
  },
  {
    "number": 5474,
    "title": "Revert \"feature: unify new_tokens format sample state to trtllm samper new_tokens format (#4401)\"",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T09:35:16Z",
    "closed_at": "2025-06-26T03:56:05Z",
    "merged_at": "2025-06-26T03:56:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5474"
  },
  {
    "number": 5473,
    "title": "[fix][ci] move torch tests to run under torch stage",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T09:11:07Z",
    "closed_at": "2025-06-26T11:31:39Z",
    "merged_at": "2025-06-26T11:31:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5473"
  },
  {
    "number": 5472,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T09:06:21Z",
    "closed_at": "2025-06-26T04:46:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5472"
  },
  {
    "number": 5471,
    "title": "CI: waive test_ad_build_small_multi",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T08:20:35Z",
    "closed_at": "2025-06-25T08:44:42Z",
    "merged_at": "2025-06-25T08:44:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5471"
  },
  {
    "number": 5470,
    "title": "[fix][test] remove test in global scope",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T08:00:54Z",
    "closed_at": "2025-06-25T20:42:27Z",
    "merged_at": "2025-06-25T20:42:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5470"
  },
  {
    "number": 5469,
    "title": "chore: consolidate all the per-file fixtures in the RemoteOpenAIServer tests to a folder based conftest",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T07:51:12Z",
    "closed_at": "2025-07-09T09:14:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5469"
  },
  {
    "number": 5468,
    "title": "[perf] improve XQA-MLA perf",
    "user": "lowsfer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T07:47:07Z",
    "closed_at": "2025-06-26T10:09:13Z",
    "merged_at": "2025-06-26T10:09:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5468"
  },
  {
    "number": 5467,
    "title": "chore:  consolidate all the per-file fixtures in the RemoteOpenAIServer tests to a folder based conftest",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T07:44:04Z",
    "closed_at": "2025-06-25T07:51:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5467"
  },
  {
    "number": 5466,
    "title": "CI: update multi gpu test triggering file list",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T07:35:18Z",
    "closed_at": "2025-06-25T07:51:03Z",
    "merged_at": "2025-06-25T07:51:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5466"
  },
  {
    "number": 5465,
    "title": "[nvbug 5300551] test: increase block count in eviction test",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T07:32:32Z",
    "closed_at": "2025-07-01T02:48:26Z",
    "merged_at": "2025-07-01T02:48:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5465"
  },
  {
    "number": 5464,
    "title": "Fix:  fix nvbug 5356427",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T07:13:22Z",
    "closed_at": "2025-06-25T14:24:26Z",
    "merged_at": "2025-06-25T14:24:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5464"
  },
  {
    "number": 5463,
    "title": "[nvbugs/5336321][fix] Enable attention dp = False test case, Fix TRTLLM Gen Moe workspace allocation",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T06:31:13Z",
    "closed_at": "2025-07-04T14:23:42Z",
    "merged_at": "2025-07-04T14:23:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5463"
  },
  {
    "number": 5462,
    "title": "tests: Set kv cache free memory fraction in test case",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T06:26:52Z",
    "closed_at": "2025-06-25T08:27:46Z",
    "merged_at": "2025-06-25T08:27:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5462"
  },
  {
    "number": 5461,
    "title": "test: set enable_attention_dp=True in default deepseek settings",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T05:14:53Z",
    "closed_at": "2025-06-25T06:21:14Z",
    "merged_at": "2025-06-25T06:21:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5461"
  },
  {
    "number": 5460,
    "title": "chore: bump version to 1.0.0rc1",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T04:48:48Z",
    "closed_at": "2025-06-25T08:21:34Z",
    "merged_at": "2025-06-25T08:21:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5460"
  },
  {
    "number": 5459,
    "title": "feat : support duplicate_kv_weight for qwen3 blockwise scale",
    "user": "dongjiyingdjy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T03:42:36Z",
    "closed_at": "2025-06-30T03:49:22Z",
    "merged_at": "2025-06-30T03:49:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5459"
  },
  {
    "number": 5458,
    "title": "tests: waive tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T02:50:44Z",
    "closed_at": "2025-06-26T06:53:55Z",
    "merged_at": "2025-06-26T06:53:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5458"
  },
  {
    "number": 5457,
    "title": "Move 3 disaggregated cases from 4 GPUs devices to 1 GPU device",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T01:50:22Z",
    "closed_at": "2025-06-25T13:38:15Z",
    "merged_at": "2025-06-25T13:38:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5457"
  },
  {
    "number": 5456,
    "title": "debug",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T00:55:40Z",
    "closed_at": "2025-07-14T08:23:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5456"
  },
  {
    "number": 5455,
    "title": "waive test_moe.py::test_moe_fp8[autotune]",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-25T00:22:43Z",
    "closed_at": "2025-06-25T01:14:44Z",
    "merged_at": "2025-06-25T01:14:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5455"
  },
  {
    "number": 5454,
    "title": "[AutoDeploy] Merge feat/ad_2025_06_13 feature branch",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T23:01:10Z",
    "closed_at": "2025-06-25T01:30:13Z",
    "merged_at": "2025-06-25T01:30:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5454"
  },
  {
    "number": 5453,
    "title": "nvbugs-5331031; nvbugs-5344203 - address intermittent issues with Mistral Small multimodal for BS=8",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T23:00:19Z",
    "closed_at": "2025-06-25T03:45:14Z",
    "merged_at": "2025-06-25T03:45:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5453"
  },
  {
    "number": 5452,
    "title": "Add trtllm-bench reviewers.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T22:04:16Z",
    "closed_at": "2025-06-26T10:48:01Z",
    "merged_at": "2025-06-26T10:48:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5452"
  },
  {
    "number": 5449,
    "title": "feature: support to nemotron-h-4b",
    "user": "vegaluisjose",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T20:31:08Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5449"
  },
  {
    "number": 5448,
    "title": "[chore] Disable block reuse when draft model speculation is being used",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T20:29:39Z",
    "closed_at": "2025-06-25T19:51:21Z",
    "merged_at": "2025-06-25T19:51:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5448"
  },
  {
    "number": 5447,
    "title": "[CI] Waive `test_moe_fp8[autotune-512-512-expert_info3-16]` causing cuda out-of-bounds error",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T19:00:54Z",
    "closed_at": "2025-06-25T00:42:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5447"
  },
  {
    "number": 5445,
    "title": "[nvbugs/5309940] Add support for input output token counts",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T16:14:51Z",
    "closed_at": "2025-06-27T20:39:40Z",
    "merged_at": "2025-06-27T20:39:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5445"
  },
  {
    "number": 5444,
    "title": "[TRTLLM-5059][feat] Add KV cache reuse support for multimodal models",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T15:03:36Z",
    "closed_at": "2025-07-21T23:11:58Z",
    "merged_at": "2025-07-21T23:11:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5444"
  },
  {
    "number": 5443,
    "title": "[fix][ci] dont build wheel for cpp tests",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T14:35:17Z",
    "closed_at": "2025-06-25T21:13:47Z",
    "merged_at": "2025-06-25T21:13:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5443"
  },
  {
    "number": 5442,
    "title": "[draft][nvbug 5302124][fix] Support recursive update with extra_llm_api_options",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T14:21:29Z",
    "closed_at": "2025-08-05T06:20:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5442"
  },
  {
    "number": 5441,
    "title": "[NVBUGs/5301636] ci: Update waives.txt to unskip L0 test for verification",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T13:32:24Z",
    "closed_at": "2025-06-26T12:55:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5441"
  },
  {
    "number": 5440,
    "title": "Draft: chore: add the sequence block of context request",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T11:53:17Z",
    "closed_at": "2025-06-25T01:24:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5440"
  },
  {
    "number": 5439,
    "title": "draft: migration from pybind11 to nanobind",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T11:42:41Z",
    "closed_at": "2025-07-21T14:02:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5439"
  },
  {
    "number": 5437,
    "title": "test: Reduce number of C++ test cases",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T10:29:34Z",
    "closed_at": "2025-07-01T07:40:49Z",
    "merged_at": "2025-07-01T07:40:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5437"
  },
  {
    "number": 5436,
    "title": "[nvbug/5354956] fix: unexpected keyword argument 'streaming'",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T10:18:17Z",
    "closed_at": "2025-06-25T12:37:25Z",
    "merged_at": "2025-06-25T12:37:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5436"
  },
  {
    "number": 5435,
    "title": "[TRTLLM-5965] perf: Optimize MoE sort kernels for large-scale EP",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T10:18:08Z",
    "closed_at": "2025-06-29T17:02:07Z",
    "merged_at": "2025-06-29T17:02:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5435"
  },
  {
    "number": 5434,
    "title": "infra: [TRTLLM-6054][TRTLLM-5804] Fix two known NSPECT high vulnerability issues and reduce image size",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T10:12:56Z",
    "closed_at": "2025-07-10T14:24:46Z",
    "merged_at": "2025-07-10T14:24:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5434"
  },
  {
    "number": 5433,
    "title": "tests: Set kv cache free memory fraction in test case",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T10:04:19Z",
    "closed_at": "2025-06-25T04:31:58Z",
    "merged_at": "2025-06-25T04:31:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5433"
  },
  {
    "number": 5432,
    "title": "perf: better heuristic for allreduce",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T09:50:09Z",
    "closed_at": "2025-07-02T02:56:07Z",
    "merged_at": "2025-07-02T02:56:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5432"
  },
  {
    "number": 5431,
    "title": "[TRTLLM-5277] chore: refine llmapi examples for 1.0 (part1)",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T09:44:59Z",
    "closed_at": "2025-07-01T11:06:41Z",
    "merged_at": "2025-07-01T11:06:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5431"
  },
  {
    "number": 5430,
    "title": "[NVBUGs/5301636] ci: Update waives.txt to unskip L0 test for verification",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T09:20:36Z",
    "closed_at": "2025-06-26T12:55:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5430"
  },
  {
    "number": 5429,
    "title": "shutil.copy bug fix in quantize_mixed_precision_moe.py",
    "user": "Wokzy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T08:58:54Z",
    "closed_at": "2025-07-08T05:23:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5429"
  },
  {
    "number": 5428,
    "title": "feat: Make benchmark_serving part of the library",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T08:50:53Z",
    "closed_at": "2025-06-25T15:13:57Z",
    "merged_at": "2025-06-25T15:13:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5428"
  },
  {
    "number": 5427,
    "title": "start OAIServer with `max_beam_width=1` for TorchSampler",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T08:38:57Z",
    "closed_at": "2025-06-25T07:52:06Z",
    "merged_at": "2025-06-25T07:52:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5427"
  },
  {
    "number": 5426,
    "title": "[https://jirasw.nvidia.com/browse/TRTLLM-4645] support mutliCtasKvMode for high-throughput MLA kernels",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T08:20:45Z",
    "closed_at": "2025-06-25T08:31:10Z",
    "merged_at": "2025-06-25T08:31:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5426"
  },
  {
    "number": 5425,
    "title": "Fp8 blockwise gemm autotune",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T08:02:55Z",
    "closed_at": "2025-06-24T08:03:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5425"
  },
  {
    "number": 5424,
    "title": "[Infra] - Waive failed tests in post-merge and increase some timeout setting",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T07:44:35Z",
    "closed_at": "2025-06-24T09:19:32Z",
    "merged_at": "2025-06-24T09:19:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5424"
  },
  {
    "number": 5423,
    "title": "[fix][ci] trigger multigpu tests for deepseek changes",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T07:10:35Z",
    "closed_at": "2025-06-26T06:30:01Z",
    "merged_at": "2025-06-26T06:30:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5423"
  },
  {
    "number": 5422,
    "title": "Fix none response in PD",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T06:44:26Z",
    "closed_at": "2025-07-04T06:25:10Z",
    "merged_at": "2025-07-04T06:25:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5422"
  },
  {
    "number": 5421,
    "title": "tests: fix typos in qa test",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T06:39:01Z",
    "closed_at": "2025-06-25T02:42:34Z",
    "merged_at": "2025-06-25T02:42:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5421"
  },
  {
    "number": 5420,
    "title": "[feat] Optimizations on weight-only batched gemv kernel",
    "user": "Njuapp",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T05:19:01Z",
    "closed_at": "2025-06-30T02:20:16Z",
    "merged_at": "2025-06-30T02:20:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5420"
  },
  {
    "number": 5419,
    "title": "Feat/apply autotuner to cute dsl fp8 gemm",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T04:24:19Z",
    "closed_at": "2025-06-24T04:24:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5419"
  },
  {
    "number": 5418,
    "title": "chore: split _build_model method for TorchLlm and TrtLlm",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T03:49:36Z",
    "closed_at": "2025-06-25T20:32:46Z",
    "merged_at": "2025-06-25T20:32:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5418"
  },
  {
    "number": 5417,
    "title": "[nvbug 5273941] fix: broken cyclic reference detect",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T03:33:53Z",
    "closed_at": "2025-06-25T23:35:35Z",
    "merged_at": "2025-06-25T23:35:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5417"
  },
  {
    "number": 5416,
    "title": "Fix test Pytorch model engine",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T03:32:59Z",
    "closed_at": "2025-06-24T18:09:28Z",
    "merged_at": "2025-06-24T18:09:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5416"
  },
  {
    "number": 5415,
    "title": "fix: Enable num_return_sequences (`n`) support in PyTorch backend",
    "user": "jaedeok-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T03:29:46Z",
    "closed_at": "2025-08-05T01:21:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5415"
  },
  {
    "number": 5414,
    "title": "add fmha test",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T03:28:45Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5414"
  },
  {
    "number": 5413,
    "title": "Fix load balancing router bug",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T03:11:07Z",
    "closed_at": "2025-07-30T06:13:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5413"
  },
  {
    "number": 5412,
    "title": "Make moe permute and final as custom op",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T02:56:10Z",
    "closed_at": "2025-06-27T22:48:33Z",
    "merged_at": "2025-06-27T22:48:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5412"
  },
  {
    "number": 5411,
    "title": "fix (NvBug 5354925): Fix static EPLB",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T02:02:57Z",
    "closed_at": "2025-06-25T05:14:41Z",
    "merged_at": "2025-06-25T05:14:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5411"
  },
  {
    "number": 5410,
    "title": "feat: Expose bias and FP8_MXFP4 MOE CUTLASS backend features to pytorch ",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-24T01:16:59Z",
    "closed_at": "2025-06-27T04:29:34Z",
    "merged_at": "2025-06-27T04:29:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5410"
  },
  {
    "number": 5409,
    "title": "chore: delete mamba hybrid, since it is now called NemotronH",
    "user": "vegaluisjose",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-23T23:21:45Z",
    "closed_at": "2025-06-24T08:27:31Z",
    "merged_at": "2025-06-24T08:27:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5409"
  },
  {
    "number": 5407,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-23T17:29:44Z",
    "closed_at": "2025-06-24T02:01:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5407"
  },
  {
    "number": 5406,
    "title": "[DRAFT] refactor: PyExecutor uses a list-type for response handling",
    "user": "jaedeok-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-23T17:07:23Z",
    "closed_at": "2025-06-27T02:53:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5406"
  },
  {
    "number": 5405,
    "title": "Add unit test for routing kernels",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-23T15:46:24Z",
    "closed_at": "2025-06-26T01:49:11Z",
    "merged_at": "2025-06-26T01:49:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5405"
  },
  {
    "number": 5404,
    "title": "[#5403][perf] Conditionally enable SWAP AB for speculative decoding",
    "user": "zoheth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-23T10:35:39Z",
    "closed_at": "2025-07-01T10:32:37Z",
    "merged_at": "2025-07-01T10:32:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5404"
  },
  {
    "number": 5401,
    "title": "[DON'T MERGE] NGram V2 test",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-23T02:11:39Z",
    "closed_at": "2025-06-27T08:13:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5401"
  },
  {
    "number": 5400,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-22T12:23:39Z",
    "closed_at": "2025-06-24T05:39:57Z",
    "merged_at": "2025-06-24T05:39:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5400"
  },
  {
    "number": 5399,
    "title": "remove libnuma conan dependency",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-22T09:50:42Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5399"
  },
  {
    "number": 5398,
    "title": "Add sleep function for disagg gen-only benchmarking",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-22T02:46:05Z",
    "closed_at": "2025-06-25T23:32:17Z",
    "merged_at": "2025-06-25T23:32:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5398"
  },
  {
    "number": 5397,
    "title": "test: add more tests for GB200 with 8 GPUs/2 nodes in L0 tests",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-21T16:08:59Z",
    "closed_at": "2025-07-01T05:06:47Z",
    "merged_at": "2025-07-01T05:06:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5397"
  },
  {
    "number": 5396,
    "title": "[1/N][TRTLLM-5195][feat] Share PyTorch tensor between processes",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-21T14:10:13Z",
    "closed_at": "2025-07-09T20:12:54Z",
    "merged_at": "2025-07-09T20:12:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5396"
  },
  {
    "number": 5394,
    "title": "[TRTLLM-6019] feat: Remove cutlass min latency code from AutoTuner.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-21T13:26:23Z",
    "closed_at": "2025-06-26T05:12:03Z",
    "merged_at": "2025-06-26T05:12:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5394"
  },
  {
    "number": 5388,
    "title": "[test] speedup TRT accuracy tests",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-20T20:49:18Z",
    "closed_at": "2025-06-21T19:44:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5388"
  },
  {
    "number": 5387,
    "title": "[doc] update mtp documents",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-20T16:07:38Z",
    "closed_at": "2025-06-21T08:05:53Z",
    "merged_at": "2025-06-21T08:05:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5387"
  },
  {
    "number": 5385,
    "title": "feat: Remove not used padding_idx in models",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-20T12:28:51Z",
    "closed_at": "2025-06-25T09:19:59Z",
    "merged_at": "2025-06-25T09:19:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5385"
  },
  {
    "number": 5384,
    "title": "refactor: remove batch_manager::KvCacheConfig and use executor::KvCacheConfig instead",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-20T08:50:37Z",
    "closed_at": "2025-06-26T11:45:52Z",
    "merged_at": "2025-06-26T11:45:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5384"
  },
  {
    "number": 5382,
    "title": "[feat]: Detokenize option in /v1/completions request",
    "user": "Wokzy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-20T07:46:26Z",
    "closed_at": "2025-07-08T11:36:05Z",
    "merged_at": "2025-07-08T11:36:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5382"
  },
  {
    "number": 5378,
    "title": "Fix: missing clientId when serialize and deserialize response (cherry-pick #5231)",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-20T03:45:10Z",
    "closed_at": "2025-06-24T02:00:38Z",
    "merged_at": "2025-06-24T02:00:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5378"
  },
  {
    "number": 5376,
    "title": "[TRTLLM-5831][feat] Add LoRA support for pytorch backend in trtllm-serve",
    "user": "talorabr",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T16:39:55Z",
    "closed_at": "2025-06-29T12:46:30Z",
    "merged_at": "2025-06-29T12:46:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5376"
  },
  {
    "number": 5375,
    "title": "chore: Add scripts to profile cutlass moe",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T15:04:28Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5375"
  },
  {
    "number": 5374,
    "title": "feat: Misc Opt for large scale EP",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T14:57:25Z",
    "closed_at": "2025-06-20T05:11:32Z",
    "merged_at": "2025-06-20T05:11:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5374"
  },
  {
    "number": 5373,
    "title": "Fix permission for local user issues in NGC docker container.",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T14:10:54Z",
    "closed_at": "2025-06-25T12:10:20Z",
    "merged_at": "2025-06-25T12:10:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5373"
  },
  {
    "number": 5372,
    "title": "[TRTLLM-5493] Add core infrastructure to enable loading of custom checkpoint formats",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T13:28:18Z",
    "closed_at": "2025-07-16T16:50:31Z",
    "merged_at": "2025-07-16T16:50:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5372"
  },
  {
    "number": 5371,
    "title": "[TRTLLM-5838][fix] fix max batch size and max tokens in kv cache estimations for Nemotron-H",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T12:03:35Z",
    "closed_at": "2025-07-09T08:30:15Z",
    "merged_at": "2025-07-09T08:30:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5371"
  },
  {
    "number": 5369,
    "title": "fix: fix bug of qwen3 + eagle3 + finalize_moe_fusion",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T11:48:07Z",
    "closed_at": "2025-06-25T01:20:23Z",
    "merged_at": "2025-06-25T01:20:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5369"
  },
  {
    "number": 5368,
    "title": "doc: cherry pick #5334",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T11:47:37Z",
    "closed_at": "2025-06-19T12:03:59Z",
    "merged_at": "2025-06-19T12:03:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5368"
  },
  {
    "number": 5367,
    "title": "Mxfp4 moe",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T10:22:45Z",
    "closed_at": "2025-06-25T08:21:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5367"
  },
  {
    "number": 5366,
    "title": "doc: subsequent modifications of blog 5",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T10:07:32Z",
    "closed_at": "2025-06-19T10:23:13Z",
    "merged_at": "2025-06-19T10:23:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5366"
  },
  {
    "number": 5365,
    "title": "tests: update benchmark test lists",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T10:06:56Z",
    "closed_at": "2025-06-24T07:23:39Z",
    "merged_at": "2025-06-24T07:23:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5365"
  },
  {
    "number": 5364,
    "title": "feat: TRTLLM-5941 Upgrade xgrammar to 0.1.18",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T09:18:02Z",
    "closed_at": "2025-06-25T06:10:51Z",
    "merged_at": "2025-06-25T06:10:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5364"
  },
  {
    "number": 5363,
    "title": "[Infra]cherry pick sanity check yml change for 5080 and 5090 from main",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T07:19:59Z",
    "closed_at": "2025-06-19T07:33:57Z",
    "merged_at": "2025-06-19T07:33:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5363"
  },
  {
    "number": 5362,
    "title": "[Infra]Fix l0_sanity_check.yml which also has gb202 and gb203 (#5360)",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T07:10:39Z",
    "closed_at": "2025-06-19T07:29:06Z",
    "merged_at": "2025-06-19T07:29:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5362"
  },
  {
    "number": 5361,
    "title": "fix: Fix DS-R1 nvfp4 test case naming",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T06:53:20Z",
    "closed_at": "2025-06-19T07:50:43Z",
    "merged_at": "2025-06-19T07:50:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5361"
  },
  {
    "number": 5360,
    "title": "[Infra]Fix l0_sanity_check.yml which also as gb202 and gb203",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T06:47:00Z",
    "closed_at": "2025-06-19T07:05:57Z",
    "merged_at": "2025-06-19T07:05:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5360"
  },
  {
    "number": 5358,
    "title": "Make moe permute and final as custom op",
    "user": "limin2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T06:27:12Z",
    "closed_at": "2025-06-24T02:56:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5358"
  },
  {
    "number": 5357,
    "title": "tests: cherry-pick from main branch, add qwen3 test cases and amend test name in perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T06:27:00Z",
    "closed_at": "2025-06-19T06:34:05Z",
    "merged_at": "2025-06-19T06:34:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5357"
  },
  {
    "number": 5356,
    "title": "test: amend test case name in perf cluster test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T06:11:59Z",
    "closed_at": "2025-06-19T06:50:12Z",
    "merged_at": "2025-06-19T06:50:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5356"
  },
  {
    "number": 5355,
    "title": "fix: Fix skip by mpi size fixture",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T05:41:25Z",
    "closed_at": "2025-06-21T18:51:02Z",
    "merged_at": "2025-06-21T18:51:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5355"
  },
  {
    "number": 5354,
    "title": "[feat]: support logit_bias",
    "user": "xq25478",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T05:41:02Z",
    "closed_at": "2025-07-25T09:37:41Z",
    "merged_at": "2025-07-25T09:37:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5354"
  },
  {
    "number": 5353,
    "title": "blog: Disaggregated Serving in TensorRT-LLM",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T05:30:19Z",
    "closed_at": "2025-06-19T10:02:16Z",
    "merged_at": "2025-06-19T10:02:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5353"
  },
  {
    "number": 5352,
    "title": "feat(eagle):support qwen in eagle1/2",
    "user": "xq25478",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T05:00:24Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5352"
  },
  {
    "number": 5350,
    "title": "feat(model):support qwen3 dense in trt flow",
    "user": "xq25478",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T04:07:05Z",
    "closed_at": "2025-07-24T02:59:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5350"
  },
  {
    "number": 5349,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T03:27:18Z",
    "closed_at": "2025-06-19T04:01:12Z",
    "merged_at": "2025-06-19T04:01:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5349"
  },
  {
    "number": 5348,
    "title": "test: Add LLGuidance test and refine guided decoding",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T03:14:56Z",
    "closed_at": "2025-06-25T06:12:57Z",
    "merged_at": "2025-06-25T06:12:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5348"
  },
  {
    "number": 5347,
    "title": "[TRTLLM-5350] Add Phi-4-Mini-Instruct in Pytorch backend for LLM API accuracy tests",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T02:53:51Z",
    "closed_at": "2025-08-30T00:11:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5347"
  },
  {
    "number": 5346,
    "title": "feat: add LLmArgs option to force using dynamic quantization",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T02:32:44Z",
    "closed_at": "2025-07-01T19:16:10Z",
    "merged_at": "2025-07-01T19:16:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5346"
  },
  {
    "number": 5345,
    "title": "feat(scaffolding): add streaming scaffolding_llm.generate_async support",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T01:41:13Z",
    "closed_at": "2025-07-08T06:08:40Z",
    "merged_at": "2025-07-08T06:08:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5345"
  },
  {
    "number": 5344,
    "title": "Refactor CutlassFusedMoE",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T01:31:45Z",
    "closed_at": "2025-06-19T07:04:08Z",
    "merged_at": "2025-06-19T07:04:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5344"
  },
  {
    "number": 5343,
    "title": "[fix] Add 1 and draft_token_num to seq_len when overlap scheduling is enabled during memory estimation",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-19T00:17:46Z",
    "closed_at": "2025-06-24T03:43:43Z",
    "merged_at": "2025-06-24T03:43:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5343"
  },
  {
    "number": 5342,
    "title": "[WAR][nvbug/5321947] Add an async sleep to unblock event loop.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T22:08:09Z",
    "closed_at": "2025-06-19T09:25:19Z",
    "merged_at": "2025-06-19T09:25:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5342"
  },
  {
    "number": 5341,
    "title": "[fix][test] parametrize deepseek eval",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T22:06:47Z",
    "closed_at": "2025-06-25T21:20:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5341"
  },
  {
    "number": 5340,
    "title": "fix: pass the correct KV retention config for block eviction",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T20:02:50Z",
    "closed_at": "2025-10-23T22:22:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5340"
  },
  {
    "number": 5339,
    "title": "ci_template_gather_2",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T16:50:48Z",
    "closed_at": "2025-06-19T05:56:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5339"
  },
  {
    "number": 5338,
    "title": "[TRTLLM-5825][fix] Fix torch LoRA TP",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T16:08:54Z",
    "closed_at": "2025-06-19T06:12:00Z",
    "merged_at": "2025-06-19T06:12:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5338"
  },
  {
    "number": 5337,
    "title": "Fix CI build time increase",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T15:32:39Z",
    "closed_at": "2025-06-19T05:49:42Z",
    "merged_at": "2025-06-19T05:49:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5337"
  },
  {
    "number": 5336,
    "title": "[nvbugs/5323043] test: Fix triton_extensive test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T15:14:16Z",
    "closed_at": "2025-09-08T16:26:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5336"
  },
  {
    "number": 5335,
    "title": "[fix][test] remove some cpp test cases from h100",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T14:48:08Z",
    "closed_at": "2025-06-18T17:40:27Z",
    "merged_at": "2025-06-18T17:40:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5335"
  },
  {
    "number": 5334,
    "title": "doc: Include NGC release containers in quick-start-guide.md",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T13:42:33Z",
    "closed_at": "2025-06-19T07:41:57Z",
    "merged_at": "2025-06-19T07:41:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5334"
  },
  {
    "number": 5333,
    "title": "[TRTLLM-3442] feat: added beam search support to the PyTorch Workflow",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T13:23:07Z",
    "closed_at": "2025-07-04T16:35:13Z",
    "merged_at": "2025-07-04T16:35:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5333"
  },
  {
    "number": 5332,
    "title": "[Infra] Cherry-pick for 5080 / 5090 gpu name fixing",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T12:37:34Z",
    "closed_at": "2025-06-18T12:54:56Z",
    "merged_at": "2025-06-18T12:54:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5332"
  },
  {
    "number": 5330,
    "title": "[nvbug/5333818] fix: fix bug of nvbug 5333818",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T11:48:36Z",
    "closed_at": "2025-09-15T02:17:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5330"
  },
  {
    "number": 5329,
    "title": "chore: Update README.md to expose meet-up info",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T11:16:11Z",
    "closed_at": "2025-06-18T12:04:28Z",
    "merged_at": "2025-06-18T12:04:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5329"
  },
  {
    "number": 5328,
    "title": "[TRTLLM-5974][feat] Support disaggregated serving in TRTLLM Sampler",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T10:59:30Z",
    "closed_at": "2025-06-25T15:41:36Z",
    "merged_at": "2025-06-25T15:41:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5328"
  },
  {
    "number": 5326,
    "title": "chore: bump version to 1.0.0rc0",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T10:15:47Z",
    "closed_at": "2025-06-19T04:02:28Z",
    "merged_at": "2025-06-19T04:02:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5326"
  },
  {
    "number": 5325,
    "title": "chore: bump version to 0.21.0",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T10:10:07Z",
    "closed_at": "2025-06-19T04:58:44Z",
    "merged_at": "2025-06-19T04:58:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5325"
  },
  {
    "number": 5324,
    "title": "[doc] Update Perf-Overview.MD with V0.20 Release Data (cherry-pick #5176)",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T09:22:28Z",
    "closed_at": "2025-06-18T09:44:04Z",
    "merged_at": "2025-06-18T09:44:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5324"
  },
  {
    "number": 5323,
    "title": "Remove duplicated test cases",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T09:20:41Z",
    "closed_at": "2025-06-18T13:20:21Z",
    "merged_at": "2025-06-18T13:20:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5323"
  },
  {
    "number": 5322,
    "title": "test release branch pipeline",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T09:02:46Z",
    "closed_at": "2025-06-18T10:36:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5322"
  },
  {
    "number": 5321,
    "title": "refactor: Rename maxBatchSize to maxNumSequences in decoder",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T08:15:16Z",
    "closed_at": "2025-08-11T09:38:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5321"
  },
  {
    "number": 5320,
    "title": "keep sm90 headsize 128 cubins",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T07:35:39Z",
    "closed_at": "2025-06-26T04:14:02Z",
    "merged_at": "2025-06-26T04:14:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5320"
  },
  {
    "number": 5319,
    "title": "Fix rerun step",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T07:32:17Z",
    "closed_at": "2025-06-18T08:38:46Z",
    "merged_at": "2025-06-18T08:38:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5319"
  },
  {
    "number": 5318,
    "title": "perf: Optimize swizzle_sf, unswizzle_sf, reswizzle_sf",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T07:05:26Z",
    "closed_at": "2025-06-26T06:03:57Z",
    "merged_at": "2025-06-26T06:03:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5318"
  },
  {
    "number": 5317,
    "title": "[Infra]Update 5080 and 5090 case condition since we will upgrade driver",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T07:05:21Z",
    "closed_at": "2025-06-18T12:01:37Z",
    "merged_at": "2025-06-18T12:01:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5317"
  },
  {
    "number": 5316,
    "title": "refactor: Speculative decoding buffers part 2",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T06:58:46Z",
    "closed_at": "2025-06-27T15:41:48Z",
    "merged_at": "2025-06-27T15:41:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5316"
  },
  {
    "number": 5315,
    "title": "refactor: manage cache indirection in decoder state",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T06:27:57Z",
    "closed_at": "2025-06-24T07:15:59Z",
    "merged_at": "2025-06-24T07:15:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5315"
  },
  {
    "number": 5314,
    "title": "Chore: remove unused variables",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T06:26:27Z",
    "closed_at": "2025-06-24T14:27:33Z",
    "merged_at": "2025-06-24T14:27:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5314"
  },
  {
    "number": 5313,
    "title": "Update README.md",
    "user": "laikhtewari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T05:13:04Z",
    "closed_at": "2025-06-18T11:17:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5313"
  },
  {
    "number": 5312,
    "title": "[TRTLLM-5208][BREAKING CHANGE] chore: promote PyTorch to be the default LLM backend",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T05:05:01Z",
    "closed_at": "2025-06-19T19:01:10Z",
    "merged_at": "2025-06-19T19:01:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5312"
  },
  {
    "number": 5311,
    "title": "Waive L0",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T04:58:28Z",
    "closed_at": "2025-06-18T08:40:41Z",
    "merged_at": "2025-06-18T08:40:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5311"
  },
  {
    "number": 5309,
    "title": "chore: bump version to 0.21.0rc3",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T04:34:30Z",
    "closed_at": "2025-06-18T07:59:53Z",
    "merged_at": "2025-06-18T07:59:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5309"
  },
  {
    "number": 5308,
    "title": "Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T03:47:15Z",
    "closed_at": "2025-06-18T04:43:45Z",
    "merged_at": "2025-06-18T04:43:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5308"
  },
  {
    "number": 5307,
    "title": "test: cherry-pick deepseek rcca cases in main branch",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T03:16:24Z",
    "closed_at": "2025-06-18T06:26:27Z",
    "merged_at": "2025-06-18T06:26:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5307"
  },
  {
    "number": 5305,
    "title": "test: update qa test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T02:55:50Z",
    "closed_at": "2025-06-18T03:29:12Z",
    "merged_at": "2025-06-18T03:29:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5305"
  },
  {
    "number": 5303,
    "title": "fix disaggregated serving MTP accuracy tests",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T02:41:20Z",
    "closed_at": "2025-06-18T07:43:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5303"
  },
  {
    "number": 5302,
    "title": "test: add qwen3 cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T02:28:00Z",
    "closed_at": "2025-06-19T06:38:36Z",
    "merged_at": "2025-06-19T06:38:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5302"
  },
  {
    "number": 5301,
    "title": "CI: fix TensorRT H200 tests",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T02:13:45Z",
    "closed_at": "2025-06-18T06:40:58Z",
    "merged_at": "2025-06-18T06:40:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5301"
  },
  {
    "number": 5300,
    "title": "tests: fix marker",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-18T01:58:06Z",
    "closed_at": "2025-06-18T07:42:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5300"
  },
  {
    "number": 5299,
    "title": "Draft: [nvbug/5321947] fix: Update async iteration status to use a thread.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T23:14:36Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5299"
  },
  {
    "number": 5298,
    "title": "Revert \"[infra] Report CI authorization errors to PR\"",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T21:28:07Z",
    "closed_at": "2025-06-17T21:28:33Z",
    "merged_at": "2025-06-17T21:28:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5298"
  },
  {
    "number": 5297,
    "title": "don't merge: test ci",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T21:25:32Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5297"
  },
  {
    "number": 5296,
    "title": "[refactor] Remove SpecConfig boilerplate",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T20:39:00Z",
    "closed_at": "2025-07-07T13:54:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5296"
  },
  {
    "number": 5295,
    "title": "chore: Refine printed info of CHECK_TYPE.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T19:55:35Z",
    "closed_at": "2025-06-18T07:35:42Z",
    "merged_at": "2025-06-18T07:35:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5295"
  },
  {
    "number": 5293,
    "title": "chore: skip test_llm_gpt2_medium_fp8 for fp8_pc_pt + quant_lm_head",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T18:22:13Z",
    "closed_at": "2025-06-18T18:13:12Z",
    "merged_at": "2025-06-18T18:13:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5293"
  },
  {
    "number": 5291,
    "title": "[ModelLoad] Concurrent load model",
    "user": "arekay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T16:53:29Z",
    "closed_at": "2025-07-03T14:18:05Z",
    "merged_at": "2025-07-03T14:18:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5291"
  },
  {
    "number": 5290,
    "title": "Please jump to [PR 5337](https://github.com/NVIDIA/TensorRT-LLM/pull/5337).",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T16:03:03Z",
    "closed_at": "2025-06-19T05:51:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5290"
  },
  {
    "number": 5289,
    "title": "test ci build time",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T15:02:23Z",
    "closed_at": "2025-06-19T05:57:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5289"
  },
  {
    "number": 5288,
    "title": "Origin/user/yunruis/ci template group 80",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T14:25:04Z",
    "closed_at": "2025-06-19T05:57:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5288"
  },
  {
    "number": 5287,
    "title": "User/yunruis/ci template group 50",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T14:20:58Z",
    "closed_at": "2025-06-19T05:57:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5287"
  },
  {
    "number": 5285,
    "title": "Fix: fix the deterministic issue in the MTP Eagle path",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T12:12:06Z",
    "closed_at": "2025-06-19T10:08:40Z",
    "merged_at": "2025-06-19T10:08:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5285"
  },
  {
    "number": 5284,
    "title": "feat: Support stream_interval",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T12:09:09Z",
    "closed_at": "2025-06-19T13:57:10Z",
    "merged_at": "2025-06-19T13:57:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5284"
  },
  {
    "number": 5283,
    "title": "update LlmRequest.is_dummy property",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T12:01:50Z",
    "closed_at": "2025-06-18T02:52:13Z",
    "merged_at": "2025-06-18T02:52:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5283"
  },
  {
    "number": 5282,
    "title": "Draft: add script to protect perf. DO-NOT-MERGE",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T11:18:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5282"
  },
  {
    "number": 5281,
    "title": "ci: unwaive llmapi launch test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T11:04:51Z",
    "closed_at": "2025-06-27T06:10:46Z",
    "merged_at": "2025-06-27T06:10:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5281"
  },
  {
    "number": 5280,
    "title": "[fix][test] move deepseek single gpu tests to post merge",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T10:47:27Z",
    "closed_at": "2025-06-18T03:59:40Z",
    "merged_at": "2025-06-18T03:59:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5280"
  },
  {
    "number": 5279,
    "title": "User/zhanruis/test ccache flag",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T10:21:05Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5279"
  },
  {
    "number": 5278,
    "title": "User/zhanruis/0617 test 8 build with 96 g mem",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T10:12:58Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5278"
  },
  {
    "number": 5277,
    "title": "[TRTLLM-5179] - Update bot help messages",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T09:17:29Z",
    "closed_at": "2025-07-18T07:25:05Z",
    "merged_at": "2025-07-18T07:25:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5277"
  },
  {
    "number": 5275,
    "title": "CI: extend model weights load time for dsv3 in stress test.",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T09:05:47Z",
    "closed_at": "2025-06-18T03:51:49Z",
    "merged_at": "2025-06-18T03:51:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5275"
  },
  {
    "number": 5274,
    "title": "delete cubins",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T08:47:26Z",
    "closed_at": "2025-06-17T14:10:49Z",
    "merged_at": "2025-06-17T14:10:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5274"
  },
  {
    "number": 5273,
    "title": "test: correct unittest rerun behavior",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T08:30:35Z",
    "closed_at": "2025-06-18T08:37:19Z",
    "merged_at": "2025-06-18T08:37:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5273"
  },
  {
    "number": 5272,
    "title": "CI: move multi-gpu test cases of tensorrt backend to h200",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T08:17:13Z",
    "closed_at": "2025-06-17T09:37:37Z",
    "merged_at": "2025-06-17T09:37:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5272"
  },
  {
    "number": 5271,
    "title": "Draft: feat: Optimize CreateNewDecoderRequest",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T08:13:50Z",
    "closed_at": "2025-08-11T08:23:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5271"
  },
  {
    "number": 5270,
    "title": "feat: Dynamically remove servers in PD",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T07:54:01Z",
    "closed_at": "2025-06-25T01:50:04Z",
    "merged_at": "2025-06-25T01:50:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5270"
  },
  {
    "number": 5269,
    "title": "feat: Upgrade xgrammar to 0.1.18",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T07:49:17Z",
    "closed_at": "2025-06-25T06:13:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5269"
  },
  {
    "number": 5268,
    "title": "ci: Split long running jobs into multiple jobs",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T07:34:30Z",
    "closed_at": "2025-06-18T22:24:29Z",
    "merged_at": "2025-06-18T22:24:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5268"
  },
  {
    "number": 5266,
    "title": "Combine MNNVL allreduce with trtllm.allreduce and unify strategy selection logic.",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T05:12:53Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5266"
  },
  {
    "number": 5265,
    "title": "Fix : fix build for sm120",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T05:05:28Z",
    "closed_at": "2025-06-27T12:42:48Z",
    "merged_at": "2025-06-27T12:42:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5265"
  },
  {
    "number": 5264,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T03:59:33Z",
    "closed_at": "2025-06-17T05:02:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5264"
  },
  {
    "number": 5262,
    "title": "fix: remove cudaStreamSynchronize when using relaxed acceptance",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T03:41:27Z",
    "closed_at": "2025-07-28T01:18:41Z",
    "merged_at": "2025-07-28T01:18:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5262"
  },
  {
    "number": 5261,
    "title": "chore: remove torch_compile prefix for TorchCompileConfig field members",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T02:36:25Z",
    "closed_at": "2025-06-19T01:21:52Z",
    "merged_at": "2025-06-19T01:21:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5261"
  },
  {
    "number": 5259,
    "title": "Fix: https://nvbugs/5345720",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-17T00:53:24Z",
    "closed_at": "2025-06-17T04:03:10Z",
    "merged_at": "2025-06-17T04:03:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5259"
  },
  {
    "number": 5258,
    "title": "[enhance] Add the ability to write a request timeline.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T23:37:41Z",
    "closed_at": "2025-07-11T00:15:30Z",
    "merged_at": "2025-07-11T00:15:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5258"
  },
  {
    "number": 5253,
    "title": "fix: only set _mpi_session if world_size is > 1",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T18:16:18Z",
    "closed_at": "2025-06-18T02:21:41Z",
    "merged_at": "2025-06-18T02:21:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5253"
  },
  {
    "number": 5252,
    "title": "chore: Waive CI failure.",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T18:11:34Z",
    "closed_at": "2025-06-16T18:47:06Z",
    "merged_at": "2025-06-16T18:47:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5252"
  },
  {
    "number": 5251,
    "title": "[chore] Remove BaseDraftTokenManager",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T18:10:50Z",
    "closed_at": "2025-06-17T15:57:52Z",
    "merged_at": "2025-06-17T15:57:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5251"
  },
  {
    "number": 5250,
    "title": "doc:update contributing md for internal developers",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T15:59:40Z",
    "closed_at": "2025-06-18T06:20:30Z",
    "merged_at": "2025-06-18T06:20:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5250"
  },
  {
    "number": 5248,
    "title": "[infra] Make test_chunked_prefill faster",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T15:34:17Z",
    "closed_at": "2025-06-16T20:19:48Z",
    "merged_at": "2025-06-16T20:19:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5248"
  },
  {
    "number": 5246,
    "title": "refactor: Introduce ResourceManagerType enum for resource management",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T14:13:38Z",
    "closed_at": "2025-06-18T07:56:00Z",
    "merged_at": "2025-06-18T07:56:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5246"
  },
  {
    "number": 5245,
    "title": "None - Some clean-ups for the automation pipeline",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T14:07:01Z",
    "closed_at": "2025-06-17T13:08:25Z",
    "merged_at": "2025-06-17T13:08:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5245"
  },
  {
    "number": 5244,
    "title": "draft: revert ci build jobs 4->8",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T13:11:55Z",
    "closed_at": "2025-06-19T05:57:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5244"
  },
  {
    "number": 5243,
    "title": "Add disagg slurm scripts",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T13:11:42Z",
    "closed_at": "2025-06-18T15:17:55Z",
    "merged_at": "2025-06-18T15:17:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5243"
  },
  {
    "number": 5241,
    "title": "[fix][test] remove duplicate test runs",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T10:16:34Z",
    "closed_at": "2025-06-18T17:59:54Z",
    "merged_at": "2025-06-18T17:59:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5241"
  },
  {
    "number": 5240,
    "title": "fix mla test",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T08:42:40Z",
    "closed_at": "2025-06-17T07:26:26Z",
    "merged_at": "2025-06-17T07:26:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5240"
  },
  {
    "number": 5239,
    "title": "refactor: Unify decoder test with e2e worklfow",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T08:35:51Z",
    "closed_at": "2025-06-17T10:04:58Z",
    "merged_at": "2025-06-17T10:04:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5239"
  },
  {
    "number": 5238,
    "title": "[test] split nemotron test cases from examples_test_list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T08:20:38Z",
    "closed_at": "2025-06-16T08:36:34Z",
    "merged_at": "2025-06-16T08:36:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5238"
  },
  {
    "number": 5237,
    "title": "test: add more pytorch cases in perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T08:18:14Z",
    "closed_at": "2025-06-17T03:11:29Z",
    "merged_at": "2025-06-17T03:11:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5237"
  },
  {
    "number": 5236,
    "title": "[TRTLLM-4501][feat] AutoTuner tuning config refactor and add tuning for kernel configs.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T07:48:01Z",
    "closed_at": "2025-10-31T09:18:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5236"
  },
  {
    "number": 5235,
    "title": "Update DeepSeek R1 perf numbers to latest release/0.20 results",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T07:26:12Z",
    "closed_at": "2025-06-16T09:42:14Z",
    "merged_at": "2025-06-16T09:42:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5235"
  },
  {
    "number": 5234,
    "title": "chore:[BREAKING CHANGE] use cacheTransceiverConfig as knobs for disagg service",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T07:07:35Z",
    "closed_at": "2025-07-17T09:42:07Z",
    "merged_at": "2025-07-17T09:42:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5234"
  },
  {
    "number": 5233,
    "title": "Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T07:02:25Z",
    "closed_at": "2025-06-16T07:19:03Z",
    "merged_at": "2025-06-16T07:19:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5233"
  },
  {
    "number": 5232,
    "title": "move some test cases of TensorRT backend back",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T06:56:56Z",
    "closed_at": "2025-06-17T09:03:12Z",
    "merged_at": "2025-06-17T09:03:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5232"
  },
  {
    "number": 5231,
    "title": "Fix: missing clientId when serialize and deserialize response",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T06:55:58Z",
    "closed_at": "2025-06-19T15:05:12Z",
    "merged_at": "2025-06-19T15:05:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5231"
  },
  {
    "number": 5230,
    "title": "chore: enable moe_backend on Qwen3 test",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T06:48:05Z",
    "closed_at": "2025-06-19T05:40:45Z",
    "merged_at": "2025-06-19T05:40:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5230"
  },
  {
    "number": 5229,
    "title": "test: Deprecate gpt_model_type \"v1\" static batching from triton_backend L0_backend_trtllm",
    "user": "yinggeh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T06:15:59Z",
    "closed_at": "2025-06-17T06:47:03Z",
    "merged_at": "2025-06-17T06:47:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5229"
  },
  {
    "number": 5228,
    "title": "Update internal cutlass commit.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T06:00:36Z",
    "closed_at": "2025-06-17T02:47:46Z",
    "merged_at": "2025-06-17T02:47:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5228"
  },
  {
    "number": 5227,
    "title": "update setup.py for special cases",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T05:57:35Z",
    "closed_at": "2025-06-17T08:41:07Z",
    "merged_at": "2025-06-17T08:41:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5227"
  },
  {
    "number": 5226,
    "title": "feat: large-scale EP(part 8: Online EP load balancer integration for PCIe fp8)",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T05:39:15Z",
    "closed_at": "2025-06-26T05:25:13Z",
    "merged_at": "2025-06-26T05:25:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5226"
  },
  {
    "number": 5225,
    "title": "test: add deepseek_v3_lite rcca cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T05:13:53Z",
    "closed_at": "2025-06-16T05:39:26Z",
    "merged_at": "2025-06-16T05:39:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5225"
  },
  {
    "number": 5224,
    "title": "Re-implement LlmResponse in Python to reduce host overhead of pybind",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T02:54:41Z",
    "closed_at": "2025-06-17T13:28:10Z",
    "merged_at": "2025-06-17T13:28:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5224"
  },
  {
    "number": 5223,
    "title": "[feat] Fusion finalize and allreduce for qwenmoe model",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T01:30:36Z",
    "closed_at": "2025-06-19T00:03:58Z",
    "merged_at": "2025-06-19T00:03:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5223"
  },
  {
    "number": 5222,
    "title": "opensource: Opensource MOE MXFP8-MXFP4 implementation",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-16T00:27:56Z",
    "closed_at": "2025-06-26T04:18:20Z",
    "merged_at": "2025-06-26T04:18:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5222"
  },
  {
    "number": 5221,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-15T18:33:31Z",
    "closed_at": "2025-06-16T08:11:54Z",
    "merged_at": "2025-06-16T08:11:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5221"
  },
  {
    "number": 5220,
    "title": "[TRTLLM-5990][doc] trtllm-serve doc improvement.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-15T16:32:00Z",
    "closed_at": "2025-08-05T05:04:02Z",
    "merged_at": "2025-08-05T05:04:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5220"
  },
  {
    "number": 5219,
    "title": "[TRTLLM-4406][AutoDeploy] Rename auto-deploy custom ops",
    "user": "nzmora-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-15T16:14:26Z",
    "closed_at": "2025-06-15T17:10:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5219"
  },
  {
    "number": 5218,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-15T12:17:05Z",
    "closed_at": "2025-06-17T05:13:24Z",
    "merged_at": "2025-06-17T05:13:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5218"
  },
  {
    "number": 5217,
    "title": "[TRTLLM-4406][AutoDeploy] Rename auto-deploy custom ops",
    "user": "nzmora-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-15T11:47:47Z",
    "closed_at": "2025-06-15T16:15:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5217"
  },
  {
    "number": 5215,
    "title": "[TRTLLM-5330] perf: Optimize MoE supplementary kernels for large-scale EP",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-15T04:21:17Z",
    "closed_at": "2025-06-17T07:23:24Z",
    "merged_at": "2025-06-17T07:23:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5215"
  },
  {
    "number": 5214,
    "title": "feat: Add LLGuidance Support for PyTorch Backend",
    "user": "jellysnack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-14T22:26:59Z",
    "closed_at": "2025-06-18T11:33:35Z",
    "merged_at": "2025-06-18T11:33:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5214"
  },
  {
    "number": 5213,
    "title": "Add MTP support for Online EPLB",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-14T13:15:06Z",
    "closed_at": "2025-06-24T23:58:13Z",
    "merged_at": "2025-06-24T23:58:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5213"
  },
  {
    "number": 5212,
    "title": "Fix: Double build time limit since #5027 halfs `NUM_JOBS`",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-14T01:12:28Z",
    "closed_at": "2025-06-17T00:57:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5212"
  },
  {
    "number": 5211,
    "title": "[None][fix] Overlap: Skip last iter on length",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T22:23:54Z",
    "closed_at": "2025-09-15T12:07:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5211"
  },
  {
    "number": 5209,
    "title": "[fix] Fix Llama4 min-latency import error",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T17:51:55Z",
    "closed_at": "2025-06-16T02:03:07Z",
    "merged_at": "2025-06-16T02:03:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5209"
  },
  {
    "number": 5208,
    "title": "test",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T17:13:51Z",
    "closed_at": "2025-06-13T18:22:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5208"
  },
  {
    "number": 5207,
    "title": "[TRTLLM-5770] feat: Integrate TRT-LLM Gen FP8 block scale MoE with Pytorch workflow kernel autotuner",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T16:31:28Z",
    "closed_at": "2025-06-17T13:01:57Z",
    "merged_at": "2025-06-17T13:01:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5207"
  },
  {
    "number": 5206,
    "title": "[feat] Add EAGLE3 support for Qwen3",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T16:18:55Z",
    "closed_at": "2025-06-17T09:07:06Z",
    "merged_at": "2025-06-17T09:07:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5206"
  },
  {
    "number": 5205,
    "title": "fix: Fix waive list",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T14:38:55Z",
    "closed_at": "2025-06-13T15:33:23Z",
    "merged_at": "2025-06-13T15:33:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5205"
  },
  {
    "number": 5204,
    "title": "[TRTLLM-6291] feat: Add user-provided speculative decoding support",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T14:16:48Z",
    "closed_at": "2025-07-07T14:30:44Z",
    "merged_at": "2025-07-07T14:30:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5204"
  },
  {
    "number": 5203,
    "title": "feat: Enable EPLB to existing MoE models",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T11:54:51Z",
    "closed_at": "2025-06-15T03:48:07Z",
    "merged_at": "2025-06-15T03:48:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5203"
  },
  {
    "number": 5202,
    "title": "[fix][test] Speedup Nemotron NAS unittests",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T10:53:29Z",
    "closed_at": "2025-06-15T08:26:03Z",
    "merged_at": "2025-06-15T08:26:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5202"
  },
  {
    "number": 5201,
    "title": "[nvbug 5004744][fix] rewrite completion API to avoid repetitive tokens",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T10:49:58Z",
    "closed_at": "2025-07-07T06:06:49Z",
    "merged_at": "2025-07-07T06:06:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5201"
  },
  {
    "number": 5200,
    "title": "fix: fix license bug",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T10:39:10Z",
    "closed_at": "2025-06-13T10:58:16Z",
    "merged_at": "2025-06-13T10:58:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5200"
  },
  {
    "number": 5199,
    "title": "infra: [TRTLLM-5879] Spilt single GPU test and multi GPU test into 2 pipelines",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T10:10:52Z",
    "closed_at": "2025-07-16T10:04:05Z",
    "merged_at": "2025-07-16T10:04:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5199"
  },
  {
    "number": 5198,
    "title": "[TRTLLM-5633] - Merge current waive list with the TOT waive list",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T09:55:06Z",
    "closed_at": "2025-07-30T09:50:05Z",
    "merged_at": "2025-07-30T09:50:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5198"
  },
  {
    "number": 5197,
    "title": "tests: add ds r1 tp4 test",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T09:17:56Z",
    "closed_at": "2025-06-19T04:48:33Z",
    "merged_at": "2025-06-19T04:48:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5197"
  },
  {
    "number": 5196,
    "title": "tests: add multi nodes tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T09:01:07Z",
    "closed_at": "2025-06-18T10:08:05Z",
    "merged_at": "2025-06-18T10:08:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5196"
  },
  {
    "number": 5195,
    "title": "test: add deepseek rcca cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T08:40:32Z",
    "closed_at": "2025-06-15T08:20:16Z",
    "merged_at": "2025-06-15T08:20:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5195"
  },
  {
    "number": 5194,
    "title": "add doc for open-sourced cutlass kernels",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T08:28:30Z",
    "closed_at": "2025-06-13T10:51:28Z",
    "merged_at": "2025-06-13T10:51:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5194"
  },
  {
    "number": 5193,
    "title": "test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T08:04:38Z",
    "closed_at": "2025-06-13T08:25:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5193"
  },
  {
    "number": 5192,
    "title": "refactor: dummy request creation",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T08:01:51Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5192"
  },
  {
    "number": 5191,
    "title": "[fix] Fix comment to pass guardwords check",
    "user": "MatthiasKohl",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T07:09:40Z",
    "closed_at": "2025-06-13T07:49:59Z",
    "merged_at": "2025-06-13T07:49:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5191"
  },
  {
    "number": 5190,
    "title": "[TRTLLM-5516] perf: replicate dummy request for cuda graph padding (cherry-pick #4729)",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T06:34:44Z",
    "closed_at": "2025-06-13T16:36:16Z",
    "merged_at": "2025-06-13T16:36:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5190"
  },
  {
    "number": 5189,
    "title": "[chore] Allow configuring linking of NVRTC wrapper",
    "user": "AlessioNetti",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T06:28:38Z",
    "closed_at": "2025-06-26T05:26:10Z",
    "merged_at": "2025-06-26T05:26:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5189"
  },
  {
    "number": 5188,
    "title": "optimize memset before alltoall communication",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T06:26:46Z",
    "closed_at": "2025-06-14T02:49:48Z",
    "merged_at": "2025-06-14T02:49:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5188"
  },
  {
    "number": 5187,
    "title": "test: add llama4 models for perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T05:47:11Z",
    "closed_at": "2025-06-16T03:24:35Z",
    "merged_at": "2025-06-16T03:24:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5187"
  },
  {
    "number": 5186,
    "title": "CI: move all test cases of TensorRT backend into post merge",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T05:18:16Z",
    "closed_at": "2025-06-13T12:48:48Z",
    "merged_at": "2025-06-13T12:48:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5186"
  },
  {
    "number": 5185,
    "title": "add dgx b200 8gpu test case in post merge",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T04:56:11Z",
    "closed_at": "2025-06-25T20:31:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5185"
  },
  {
    "number": 5184,
    "title": "[TRTLLM-5653][infra] Run docs build only if PR contains only doc changes",
    "user": "zhanga5",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T04:20:43Z",
    "closed_at": "2025-07-14T13:40:34Z",
    "merged_at": "2025-07-14T13:40:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5184"
  },
  {
    "number": 5183,
    "title": "feat: MoE trtllm backend kernel update",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T04:18:16Z",
    "closed_at": "2025-06-16T06:46:13Z",
    "merged_at": "2025-06-16T06:46:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5183"
  },
  {
    "number": 5182,
    "title": "Add debug hook to support dump tensor data and add new debug functions easily",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T03:35:51Z",
    "closed_at": "2025-06-24T09:45:28Z",
    "merged_at": "2025-06-24T09:45:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5182"
  },
  {
    "number": 5181,
    "title": "Removed <think> on head of reasoning_content for DeepSeek-R1 model",
    "user": "k-l-lambda",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T03:26:26Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5181"
  },
  {
    "number": 5180,
    "title": "tests: update tests for b200",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T02:14:43Z",
    "closed_at": "2025-06-13T03:25:34Z",
    "merged_at": "2025-06-13T03:25:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5180"
  },
  {
    "number": 5179,
    "title": "test: Add json_mode_eval for guided decoding evaluation",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T01:42:33Z",
    "closed_at": "2025-06-16T02:03:56Z",
    "merged_at": "2025-06-16T02:03:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5179"
  },
  {
    "number": 5178,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T01:29:52Z",
    "closed_at": "2025-06-13T08:38:51Z",
    "merged_at": "2025-06-13T08:38:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5178"
  },
  {
    "number": 5177,
    "title": "enh: Add script to map tests <-> jenkins stages & vice-versa",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T00:48:20Z",
    "closed_at": "2025-07-18T16:50:41Z",
    "merged_at": "2025-07-18T16:50:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5177"
  },
  {
    "number": 5176,
    "title": "[doc] Update Perf-Overview.MD with V0.20 Release Data",
    "user": "zbpatel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-13T00:39:34Z",
    "closed_at": "2025-06-18T09:36:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5176"
  },
  {
    "number": 5175,
    "title": "[infra] Report CI authorization errors to PR",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T22:51:40Z",
    "closed_at": "2025-06-17T21:26:50Z",
    "merged_at": "2025-06-17T21:26:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5175"
  },
  {
    "number": 5174,
    "title": "[infra] Report CI authorization errors to PR",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T22:49:56Z",
    "closed_at": "2025-06-12T22:51:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5174"
  },
  {
    "number": 5173,
    "title": "[feat] Add progress bar to benchmark",
    "user": "arekay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T21:55:58Z",
    "closed_at": "2025-06-26T10:39:46Z",
    "merged_at": "2025-06-26T10:39:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5173"
  },
  {
    "number": 5172,
    "title": "feat: add multi-node support for Triton with pytorch backend",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T21:25:14Z",
    "closed_at": "2025-06-13T20:27:59Z",
    "merged_at": "2025-06-13T20:27:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5172"
  },
  {
    "number": 5171,
    "title": "infra: move internal_cutlass to conan",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T18:23:48Z",
    "closed_at": "2025-06-17T15:24:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5171"
  },
  {
    "number": 5170,
    "title": "[TRTLLM-5508][feat] check input tokens + improve error handling",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T17:57:26Z",
    "closed_at": "2025-08-05T17:27:44Z",
    "merged_at": "2025-08-05T17:27:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5170"
  },
  {
    "number": 5169,
    "title": "fix: Remove stream and wakeup features from ucxx init",
    "user": "tvegas1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T17:44:35Z",
    "closed_at": "2025-06-13T06:35:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5169"
  },
  {
    "number": 5168,
    "title": "Fix GEMM+AR fusion on blackwell",
    "user": "xavier-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T16:39:45Z",
    "closed_at": "2025-06-28T00:14:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5168"
  },
  {
    "number": 5167,
    "title": "fix: Making env variables non-static.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T15:54:53Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5167"
  },
  {
    "number": 5166,
    "title": "tests: Move stress tests to be Post-Merge only",
    "user": "amirkl94",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T13:52:05Z",
    "closed_at": "2025-06-29T06:44:48Z",
    "merged_at": "2025-06-29T06:44:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5166"
  },
  {
    "number": 5165,
    "title": "refactor: remove TrtGptModelOptionalParams",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T13:48:34Z",
    "closed_at": "2025-06-20T08:31:41Z",
    "merged_at": "2025-06-20T08:31:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5165"
  },
  {
    "number": 5164,
    "title": "[https://nvbugspro.nvidia.com/bug/5295470] support headDim 256 for blackwell fmha kernels",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T13:32:02Z",
    "closed_at": "2025-06-13T15:01:02Z",
    "merged_at": "2025-06-13T15:01:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5164"
  },
  {
    "number": 5163,
    "title": "[DON'T MERGE] NGrams V3 draft",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T13:21:51Z",
    "closed_at": "2025-08-07T01:14:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5163"
  },
  {
    "number": 5162,
    "title": "[feat] Optimize TRTLLM Sampler to use pure python type for decoder buffer logits.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T12:27:01Z",
    "closed_at": "2025-06-24T11:21:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5162"
  },
  {
    "number": 5161,
    "title": "add llama_nemotron_nano_vl support",
    "user": "mwawrzos",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T11:25:07Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5161"
  },
  {
    "number": 5160,
    "title": "[fix] Reenable test return logits",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T10:03:31Z",
    "closed_at": "2025-06-13T04:07:22Z",
    "merged_at": "2025-06-13T04:07:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5160"
  },
  {
    "number": 5159,
    "title": "[TRTLLM-5758] test: Add Bielik-11B-v2.2 Model Support",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T09:49:03Z",
    "closed_at": "2025-06-18T07:12:49Z",
    "merged_at": "2025-06-18T07:12:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5159"
  },
  {
    "number": 5158,
    "title": "doc: add document of benchmarking for Qwen3",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T09:39:25Z",
    "closed_at": "2025-06-17T08:18:55Z",
    "merged_at": "2025-06-17T08:18:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5158"
  },
  {
    "number": 5157,
    "title": "doc: Add document of benchmarking for Qwen3",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T09:37:06Z",
    "closed_at": "2025-06-12T09:37:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5157"
  },
  {
    "number": 5156,
    "title": "infra: add Qwen3 into gb202 test",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T09:36:05Z",
    "closed_at": "2025-06-18T05:41:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5156"
  },
  {
    "number": 5155,
    "title": "test: add more cases for llama_v3.3/3.1 70b fp8 and set enable_attention_dp to false to non-deepseek models",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T08:32:55Z",
    "closed_at": "2025-06-16T03:23:20Z",
    "merged_at": "2025-06-16T03:23:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5155"
  },
  {
    "number": 5153,
    "title": "test: waive the NIXL related tests",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T07:54:07Z",
    "closed_at": "2025-06-12T09:02:27Z",
    "merged_at": "2025-06-12T09:02:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5153"
  },
  {
    "number": 5151,
    "title": "Feat/pytorch vswa kvcachemanager",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T07:41:39Z",
    "closed_at": "2025-07-02T07:58:01Z",
    "merged_at": "2025-07-02T07:58:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5151"
  },
  {
    "number": 5150,
    "title": "doc:add release notes for v0.20.0",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T07:37:43Z",
    "closed_at": "2025-06-16T01:27:58Z",
    "merged_at": "2025-06-16T01:27:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5150"
  },
  {
    "number": 5149,
    "title": "test: set enable_attention_dp to False for non-deepseek models and add more cases for llama_v3.1/3.3 70b fp8 models",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T06:37:22Z",
    "closed_at": "2025-06-12T06:59:16Z",
    "merged_at": "2025-06-12T06:59:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5149"
  },
  {
    "number": 5148,
    "title": "feat: develop all to all kernel for moe prepare stage with mnnvl",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T06:12:58Z",
    "closed_at": "2025-06-29T10:23:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5148"
  },
  {
    "number": 5147,
    "title": "Move allreduce_strategy from committed api to reference",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T06:10:22Z",
    "closed_at": "2025-06-12T13:00:21Z",
    "merged_at": "2025-06-12T13:00:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5147"
  },
  {
    "number": 5146,
    "title": "[https://nvbugspro.nvidia.com/bug/5329655] [feat] Pytorch path add spec dec param to attention op ",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T05:12:29Z",
    "closed_at": "2025-07-02T08:54:44Z",
    "merged_at": "2025-07-02T08:54:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5146"
  },
  {
    "number": 5145,
    "title": "doc:fix invalid links for trtllm-serve doc",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T04:00:27Z",
    "closed_at": "2025-06-12T08:17:33Z",
    "merged_at": "2025-06-12T08:17:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5145"
  },
  {
    "number": 5143,
    "title": "fix:remove duplicated trust_remote_code knob from trtllm-serve",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T03:02:15Z",
    "closed_at": "2025-06-12T11:48:25Z",
    "merged_at": "2025-06-12T11:48:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5143"
  },
  {
    "number": 5142,
    "title": "Add Wechat_Group_QR_Code.png to docs/source/media and main page of TR…",
    "user": "AdamzNV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T02:55:03Z",
    "closed_at": "2025-06-19T19:28:01Z",
    "merged_at": "2025-06-19T19:28:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5142"
  },
  {
    "number": 5141,
    "title": "[feat] Add llm args to tune python gc threshold",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T01:53:34Z",
    "closed_at": "2025-06-16T09:45:23Z",
    "merged_at": "2025-06-16T09:45:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5141"
  },
  {
    "number": 5140,
    "title": "linting(python): Enable ruff on more files (wave 1/N)",
    "user": "2ez4bz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T01:48:50Z",
    "closed_at": "2025-06-14T11:19:35Z",
    "merged_at": "2025-06-14T11:19:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5140"
  },
  {
    "number": 5139,
    "title": "[TRTLLM-5589] feat: Minor optimizations for tunable FP8 batched GEMM op.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-12T01:30:40Z",
    "closed_at": "2025-06-18T06:33:46Z",
    "merged_at": "2025-06-18T06:33:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5139"
  },
  {
    "number": 5138,
    "title": "[fix] Fixes building TRTLLM wheel with NIXL",
    "user": "tanmayv25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T23:52:00Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5138"
  },
  {
    "number": 5137,
    "title": "enh(doc): Add `ci-overview` in `docs/source/reference/`",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T22:59:04Z",
    "closed_at": "2025-06-12T09:48:14Z",
    "merged_at": "2025-06-12T09:48:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5137"
  },
  {
    "number": 5136,
    "title": "Fix logprobs issues.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T20:29:33Z",
    "closed_at": "2025-06-12T07:07:02Z",
    "merged_at": "2025-06-12T07:07:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5136"
  },
  {
    "number": 5134,
    "title": "[nvbug/5334370][fix] Fix one model EAGLE3",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T16:28:56Z",
    "closed_at": "2025-06-12T14:28:15Z",
    "merged_at": "2025-06-12T14:28:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5134"
  },
  {
    "number": 5133,
    "title": "[nvbug 5333996 ][fix] Unload XQA cubins early to avoid static lifetime",
    "user": "lowsfer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T15:53:06Z",
    "closed_at": "2025-06-13T07:53:30Z",
    "merged_at": "2025-06-13T07:53:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5133"
  },
  {
    "number": 5132,
    "title": "[feat] Support NVFP4 KV Cache",
    "user": "Tom-Zheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T14:18:46Z",
    "closed_at": "2025-08-04T02:04:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5132"
  },
  {
    "number": 5131,
    "title": "CacheState fix for disabled DP",
    "user": "akhoroshev",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T14:15:56Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5131"
  },
  {
    "number": 5130,
    "title": "Enable trtllm-bench to run LoRA and add basic e2e perf testing capability for LoRA in PyT flow",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T13:33:24Z",
    "closed_at": "2025-06-15T15:54:05Z",
    "merged_at": "2025-06-15T15:54:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5130"
  },
  {
    "number": 5129,
    "title": "refactor: remove decoder request from decoder interface",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T13:00:16Z",
    "closed_at": "2025-06-16T07:12:31Z",
    "merged_at": "2025-06-16T07:12:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5129"
  },
  {
    "number": 5128,
    "title": "[TRTLLM-5835][feat] Optimized Mamba2Mixer prefill",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T12:35:50Z",
    "closed_at": "2025-06-16T13:29:18Z",
    "merged_at": "2025-06-16T13:29:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5128"
  },
  {
    "number": 5126,
    "title": "[TRTLLM-5801][infra] Add more RTX Pro 6000 test stages",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T10:01:01Z",
    "closed_at": "2025-08-22T07:12:03Z",
    "merged_at": "2025-08-22T07:12:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5126"
  },
  {
    "number": 5125,
    "title": "[test] add nvfp4 DeepSeek-V3-Lite-mtp tests",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T09:12:16Z",
    "closed_at": "2025-06-19T01:48:23Z",
    "merged_at": "2025-06-19T01:48:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5125"
  },
  {
    "number": 5124,
    "title": "[test] Update timeout params in QA test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T08:58:05Z",
    "closed_at": "2025-06-13T05:40:03Z",
    "merged_at": "2025-06-13T05:40:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5124"
  },
  {
    "number": 5123,
    "title": "use file lock to avoid port conflict",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T07:53:00Z",
    "closed_at": "2025-06-16T06:15:38Z",
    "merged_at": "2025-06-16T06:15:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5123"
  },
  {
    "number": 5122,
    "title": "feat: Support post_proc for bench",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T07:48:25Z",
    "closed_at": "2025-06-15T05:02:38Z",
    "merged_at": "2025-06-15T05:02:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5122"
  },
  {
    "number": 5121,
    "title": "[fix][test] clear cuda cache before unittests automatically",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T07:42:22Z",
    "closed_at": "2025-06-18T21:36:53Z",
    "merged_at": "2025-06-18T21:36:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5121"
  },
  {
    "number": 5119,
    "title": "[TRTLLM-5278][feat] Add attention dp support to MTP relaxed acceptance",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T07:28:57Z",
    "closed_at": "2025-06-13T00:58:45Z",
    "merged_at": "2025-06-13T00:58:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5119"
  },
  {
    "number": 5118,
    "title": "Adding TRT-LLM Discussion Wechat Group QR code into the readme of mai…",
    "user": "AdamzNV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T07:06:42Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5118"
  },
  {
    "number": 5117,
    "title": "[fix] Fix llama4 min latency",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T06:55:46Z",
    "closed_at": "2025-06-11T07:44:38Z",
    "merged_at": "2025-06-11T07:44:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5117"
  },
  {
    "number": 5116,
    "title": "[fix][test] report individual unittests results to jenkins",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T06:12:08Z",
    "closed_at": "2025-06-12T17:52:09Z",
    "merged_at": "2025-06-12T17:52:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5116"
  },
  {
    "number": 5115,
    "title": "feat: support more parameters in openai worker of scaffolding",
    "user": "ccs96307",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T06:03:54Z",
    "closed_at": "2025-07-04T01:35:34Z",
    "merged_at": "2025-07-04T01:35:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5115"
  },
  {
    "number": 5114,
    "title": "update the free_gpu_mem_fraction for H100 qwen3 qa test",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T05:46:20Z",
    "closed_at": "2025-06-12T06:40:57Z",
    "merged_at": "2025-06-12T06:40:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5114"
  },
  {
    "number": 5113,
    "title": "fix pad attention dp dummy request logic",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T03:38:43Z",
    "closed_at": "2025-06-30T02:40:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5113"
  },
  {
    "number": 5112,
    "title": "chore: bump version to 0.21.0rc2",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T03:01:27Z",
    "closed_at": "2025-06-11T07:08:14Z",
    "merged_at": "2025-06-11T07:08:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5112"
  },
  {
    "number": 5111,
    "title": "Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T02:41:57Z",
    "closed_at": "2025-06-11T03:53:16Z",
    "merged_at": "2025-06-11T03:53:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5111"
  },
  {
    "number": 5110,
    "title": "perf: avoid dynamic import overhead in is_llm_response with duck typing",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T02:32:38Z",
    "closed_at": "2025-06-14T23:45:02Z",
    "merged_at": "2025-06-14T23:45:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5110"
  },
  {
    "number": 5109,
    "title": "chore: gracefully exit disagg process in tests; better startup and logging",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T02:18:15Z",
    "closed_at": "2025-06-13T06:03:55Z",
    "merged_at": "2025-06-13T06:03:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5109"
  },
  {
    "number": 5108,
    "title": "feat: Basic skeleton for Gemma3 VLM",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-11T02:00:32Z",
    "closed_at": "2025-06-13T09:27:04Z",
    "merged_at": "2025-06-13T09:27:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5108"
  },
  {
    "number": 5106,
    "title": "[nvbugs/5331013] fix AutoDeploy for PyTorch 25.05 dependency upgrade",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T23:21:13Z",
    "closed_at": "2025-06-12T05:07:28Z",
    "merged_at": "2025-06-12T05:07:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5106"
  },
  {
    "number": 5105,
    "title": "fix: Updates to yarn implementation",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T22:51:51Z",
    "closed_at": "2025-06-12T12:45:35Z",
    "merged_at": "2025-06-12T12:45:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5105"
  },
  {
    "number": 5104,
    "title": "[TRTLLM-4770][feat] Enhance cpp executor cmake to listen to ENABLE_MU…",
    "user": "WilliamTambellini",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T22:39:48Z",
    "closed_at": "2025-07-11T02:59:45Z",
    "merged_at": "2025-07-11T02:59:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5104"
  },
  {
    "number": 5103,
    "title": "Qixiang feat on netanel 0610",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T22:18:41Z",
    "closed_at": "2025-06-12T23:03:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5103"
  },
  {
    "number": 5102,
    "title": "[chore] 2025-06-10 update allowlist",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T22:11:26Z",
    "closed_at": "2025-06-11T10:02:19Z",
    "merged_at": "2025-06-11T10:02:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5102"
  },
  {
    "number": 5101,
    "title": "CI: Allow run",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T21:14:35Z",
    "closed_at": "2025-06-10T22:03:39Z",
    "merged_at": "2025-06-10T22:03:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5101"
  },
  {
    "number": 5100,
    "title": "feat: Optimize response handling by speeding up the serialization/deserialization",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T20:03:53Z",
    "closed_at": "2025-06-17T13:29:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5100"
  },
  {
    "number": 5098,
    "title": "DeepEP integration -- Build image only -- Do not merge",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T17:37:05Z",
    "closed_at": "2025-06-16T03:40:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5098"
  },
  {
    "number": 5097,
    "title": "[test] Use LLM API for Nemotron-H correctness test",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T16:57:31Z",
    "closed_at": "2025-06-12T06:54:46Z",
    "merged_at": "2025-06-12T06:54:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5097"
  },
  {
    "number": 5096,
    "title": "draft, to CI test switch to internal lib",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T16:02:00Z",
    "closed_at": "2025-06-16T05:25:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5096"
  },
  {
    "number": 5095,
    "title": "Unwaive disaggregated serving accuracy tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T15:19:59Z",
    "closed_at": "2025-06-18T16:41:16Z",
    "merged_at": "2025-06-18T16:41:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5095"
  },
  {
    "number": 5094,
    "title": "[nvbugs/5334098] test: Fix gpt-ib test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T14:37:37Z",
    "closed_at": "2025-06-10T15:24:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5094"
  },
  {
    "number": 5093,
    "title": "refactor: decoder state setup",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T13:41:49Z",
    "closed_at": "2025-06-30T09:09:44Z",
    "merged_at": "2025-06-30T09:09:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5093"
  },
  {
    "number": 5092,
    "title": "chore: fix typo in tests",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T12:48:00Z",
    "closed_at": "2025-06-12T07:11:27Z",
    "merged_at": "2025-06-12T07:11:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5092"
  },
  {
    "number": 5091,
    "title": "refactor: Speculative decoding buffers",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T12:23:17Z",
    "closed_at": "2025-06-14T09:39:32Z",
    "merged_at": "2025-06-14T09:39:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5091"
  },
  {
    "number": 5090,
    "title": "Draft: Revert 4664 and 4689",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T12:00:31Z",
    "closed_at": "2025-06-11T02:53:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5090"
  },
  {
    "number": 5089,
    "title": "[CI] waive failing L0 test",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T11:32:48Z",
    "closed_at": "2025-06-10T12:40:45Z",
    "merged_at": "2025-06-10T12:40:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5089"
  },
  {
    "number": 5088,
    "title": "fix: limit process pool size when prefetching",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T10:47:14Z",
    "closed_at": "2025-06-12T02:52:53Z",
    "merged_at": "2025-06-12T02:52:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5088"
  },
  {
    "number": 5087,
    "title": "[feat]: improve performance of XQA-MLA for sm120",
    "user": "lowsfer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T10:10:59Z",
    "closed_at": "2025-06-18T06:19:23Z",
    "merged_at": "2025-06-18T06:19:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5087"
  },
  {
    "number": 5086,
    "title": "[feat] Support torch compile for attention dp",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T09:50:57Z",
    "closed_at": "2025-07-01T17:48:52Z",
    "merged_at": "2025-07-01T17:48:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5086"
  },
  {
    "number": 5085,
    "title": "test: strictly constraint disaggregated serving llama4 to H200",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T09:37:33Z",
    "closed_at": "2025-06-12T02:13:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5085"
  },
  {
    "number": 5084,
    "title": "[fix] Fix test_attention_mla",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T09:28:06Z",
    "closed_at": "2025-06-10T21:20:12Z",
    "merged_at": "2025-06-10T21:20:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5084"
  },
  {
    "number": 5083,
    "title": "test: add more cases for rtx_pro_6000_se and add option kv_cache_dtype in perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T08:58:13Z",
    "closed_at": "2025-06-13T03:09:15Z",
    "merged_at": "2025-06-13T03:09:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5083"
  },
  {
    "number": 5082,
    "title": "chore: Mass integration of release/0.20",
    "user": "amirkl94",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T08:52:43Z",
    "closed_at": "2025-06-17T11:32:03Z",
    "merged_at": "2025-06-17T11:32:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5082"
  },
  {
    "number": 5081,
    "title": "ci: waive test [NVBUGS/5301492]",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T08:36:14Z",
    "closed_at": "2025-07-21T07:11:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5081"
  },
  {
    "number": 5080,
    "title": "[TRTLLM-5921][feat] Prevent serialization of entire LoRA adapters in each request",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T08:26:42Z",
    "closed_at": "2025-06-26T05:15:06Z",
    "merged_at": "2025-06-26T05:15:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5080"
  },
  {
    "number": 5079,
    "title": "refactor: improve code readability",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T08:22:42Z",
    "closed_at": "2025-07-11T07:19:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5079"
  },
  {
    "number": 5077,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T08:10:41Z",
    "closed_at": "2025-06-10T08:23:50Z",
    "merged_at": "2025-06-10T08:23:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5077"
  },
  {
    "number": 5076,
    "title": "fix: fix cuda graph max batch size for spec decoding cases.",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T07:58:46Z",
    "closed_at": "2025-06-15T06:57:29Z",
    "merged_at": "2025-06-15T06:57:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5076"
  },
  {
    "number": 5075,
    "title": "infra[TRTLLM-5635] remove package stage in CI build",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T07:39:18Z",
    "closed_at": "2025-06-17T15:44:47Z",
    "merged_at": "2025-06-17T15:44:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5075"
  },
  {
    "number": 5074,
    "title": "[fix]: Fall back to HMAC to Avoid IPC Serialization Churn",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T07:24:44Z",
    "closed_at": "2025-06-13T03:37:51Z",
    "merged_at": "2025-06-13T03:37:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5074"
  },
  {
    "number": 5073,
    "title": "[TRTLLM-5786][https://nvbugspro.nvidia.com/bug/5310520][test] Add QA test cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T07:14:52Z",
    "closed_at": "2025-06-17T09:14:02Z",
    "merged_at": "2025-06-17T09:14:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5073"
  },
  {
    "number": 5072,
    "title": "fix: remove duplicate trust_remote_code from serve command",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T07:04:04Z",
    "closed_at": "2025-06-12T14:08:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5072"
  },
  {
    "number": 5071,
    "title": "CI: waive test_ad_build_small_multi",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T06:41:35Z",
    "closed_at": "2025-06-10T06:54:06Z",
    "merged_at": "2025-06-10T06:54:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5071"
  },
  {
    "number": 5070,
    "title": "test: skip disaggregated tests on arm",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T05:57:54Z",
    "closed_at": "2025-06-11T09:00:10Z",
    "merged_at": "2025-06-11T09:00:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5070"
  },
  {
    "number": 5069,
    "title": "chore: Waive CI failure.",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T05:14:25Z",
    "closed_at": "2025-06-10T06:04:11Z",
    "merged_at": "2025-06-10T06:04:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5069"
  },
  {
    "number": 5068,
    "title": "chore: rename IOFormatter to BaseCacheFormatter",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T05:11:11Z",
    "closed_at": "2025-06-12T02:50:14Z",
    "merged_at": "2025-06-12T02:50:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5068"
  },
  {
    "number": 5067,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T05:05:44Z",
    "closed_at": "2025-06-10T07:40:57Z",
    "merged_at": "2025-06-10T07:40:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5067"
  },
  {
    "number": 5066,
    "title": "test(perf): Add remaining Llama-Nemotron perftests (nano, super, ultra) + extras ✨",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T04:08:55Z",
    "closed_at": "2025-06-12T06:19:15Z",
    "merged_at": "2025-06-12T06:19:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5066"
  },
  {
    "number": 5065,
    "title": "[https://nvbugspro.nvidia.com/bug/5332927][fix] Fix the bug in the routing unit test",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T03:55:17Z",
    "closed_at": "2025-06-11T01:44:35Z",
    "merged_at": "2025-06-11T01:44:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5065"
  },
  {
    "number": 5063,
    "title": "Draft: test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T03:29:27Z",
    "closed_at": "2025-06-16T08:12:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5063"
  },
  {
    "number": 5062,
    "title": "Fix:Add decodingcondfig into serialization",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T03:17:10Z",
    "closed_at": "2025-06-30T08:23:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5062"
  },
  {
    "number": 5061,
    "title": "chore: build the wheel using NIXL as the default",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T03:14:00Z",
    "closed_at": "2025-08-13T09:55:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5061"
  },
  {
    "number": 5059,
    "title": "bugfix [AutoDeploy]: Correct usage of pytorch_config in autodeploy integration of trtllm-bench. Other perf improvements",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T02:36:09Z",
    "closed_at": "2025-07-14T20:13:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5059"
  },
  {
    "number": 5058,
    "title": "[https://nvbugs/5277592][fix] fix cuda graph padding for spec decoding (only for 0.20)",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T01:57:25Z",
    "closed_at": "2025-06-10T18:14:14Z",
    "merged_at": "2025-06-10T18:14:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5058"
  },
  {
    "number": 5057,
    "title": "test(perf): Add Llama-3_1-Nemotron-Ultra-253B-v1 perf tests (pyt, fp8)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T00:47:40Z",
    "closed_at": "2025-06-24T19:17:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5057"
  },
  {
    "number": 5056,
    "title": "None: fix OOM because of unnecessary mha workspace",
    "user": "ttyio",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T00:42:33Z",
    "closed_at": "2025-06-12T19:56:05Z",
    "merged_at": "2025-06-12T19:56:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5056"
  },
  {
    "number": 5055,
    "title": "chore: Include prompt_token_ids only for context-only disagg requests",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-10T00:04:03Z",
    "closed_at": "2025-06-12T19:00:08Z",
    "merged_at": "2025-06-12T19:00:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5055"
  },
  {
    "number": 5054,
    "title": "[AutoDeploy] Merge Feature Branch Week 3",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T22:49:01Z",
    "closed_at": "2025-06-10T16:20:43Z",
    "merged_at": "2025-06-10T16:20:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5054"
  },
  {
    "number": 5053,
    "title": "Ll/feat/ad coverage week3",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T21:44:36Z",
    "closed_at": "2025-06-09T21:46:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5053"
  },
  {
    "number": 5052,
    "title": "[TRTLLM-5581][infra] Update Module Owners",
    "user": "poweiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T21:08:26Z",
    "closed_at": "2025-06-12T01:38:42Z",
    "merged_at": "2025-06-12T01:38:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5052"
  },
  {
    "number": 5051,
    "title": "[https://nvbugs/5332927] Waive new tests",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T21:04:22Z",
    "closed_at": "2025-06-09T21:17:54Z",
    "merged_at": "2025-06-09T21:17:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5051"
  },
  {
    "number": 5044,
    "title": "[5310329] fix: Fix warmup phase batch size out of range. (#4912)",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T16:01:28Z",
    "closed_at": "2025-06-09T17:22:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5044"
  },
  {
    "number": 5043,
    "title": "add KVCache BW metrics",
    "user": "BatshevaBlack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T15:31:00Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5043"
  },
  {
    "number": 5042,
    "title": "[fix] Unwaive test_llama_eagle3",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T15:07:55Z",
    "closed_at": "2025-06-10T22:11:08Z",
    "merged_at": "2025-06-10T22:11:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5042"
  },
  {
    "number": 5041,
    "title": "chore: Change cutlass version back to 4.0",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T14:13:14Z",
    "closed_at": "2025-06-09T14:57:06Z",
    "merged_at": "2025-06-09T14:57:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5041"
  },
  {
    "number": 5039,
    "title": "chore: Merge remaining changes from feat/large-ep branch to main",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T12:15:11Z",
    "closed_at": "2025-06-11T05:47:43Z",
    "merged_at": "2025-06-11T05:47:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5039"
  },
  {
    "number": 5037,
    "title": "add feature support matrix for PyTorch backend",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T11:29:14Z",
    "closed_at": "2025-07-01T02:09:55Z",
    "merged_at": "2025-07-01T02:09:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5037"
  },
  {
    "number": 5036,
    "title": "test: add more disaggregated serving tests into QA testlist",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T10:47:58Z",
    "closed_at": "2025-06-10T01:24:54Z",
    "merged_at": "2025-06-10T01:24:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5036"
  },
  {
    "number": 5035,
    "title": "infra: Add timeout and retry for wget in docker image build",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T10:39:16Z",
    "closed_at": "2025-06-11T02:37:13Z",
    "merged_at": "2025-06-11T02:37:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5035"
  },
  {
    "number": 5034,
    "title": "[TRTLLM-4406][feat] LLM sleep & wakeup Part 1: virtual device memory",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T10:07:25Z",
    "closed_at": "2025-08-04T05:51:01Z",
    "merged_at": "2025-08-04T05:51:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5034"
  },
  {
    "number": 5033,
    "title": "[Stress test] Add DeepSeek-R1 stress test",
    "user": "Wanli-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T09:54:06Z",
    "closed_at": "2025-06-16T03:54:31Z",
    "merged_at": "2025-06-16T03:54:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5033"
  },
  {
    "number": 5032,
    "title": "refactor [BREAKING CHANGE]: enhance the llm args pytorch config part 3(torch_compile_config)",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T09:52:26Z",
    "closed_at": "2025-06-14T06:23:13Z",
    "merged_at": "2025-06-14T06:23:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5032"
  },
  {
    "number": 5031,
    "title": "refactor [BREAKING CHANGE]:: remove the redundant use_kv_cache field from PytorchConfig ",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T09:24:19Z",
    "closed_at": "2025-06-13T08:34:25Z",
    "merged_at": "2025-06-13T08:34:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5031"
  },
  {
    "number": 5030,
    "title": "refactor [BREAKING CHANGE]: enhance the llm args pytorch config part 2(moe_config)",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T09:01:20Z",
    "closed_at": "2025-07-15T07:32:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5030"
  },
  {
    "number": 5029,
    "title": "[NVBUG-5304516/5319741]Qwen2.5VL FP8 support",
    "user": "DylanChen-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T08:55:27Z",
    "closed_at": "2025-07-09T15:16:43Z",
    "merged_at": "2025-07-09T15:16:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5029"
  },
  {
    "number": 5028,
    "title": "test: Add fixture to skip tests based on MPI world size",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T08:47:32Z",
    "closed_at": "2025-06-16T03:25:01Z",
    "merged_at": "2025-06-16T03:25:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5028"
  },
  {
    "number": 5027,
    "title": "refactoring: port customized kernels with public cutlass version",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T08:13:43Z",
    "closed_at": "2025-06-13T08:19:31Z",
    "merged_at": "2025-06-13T08:19:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5027"
  },
  {
    "number": 5026,
    "title": "[fix] Fix W4A8 weight loading error in WInt4AFP8FusedMoEMethod",
    "user": "xiaoweiw-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T08:07:16Z",
    "closed_at": "2025-06-10T07:09:07Z",
    "merged_at": "2025-06-10T07:09:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5026"
  },
  {
    "number": 5025,
    "title": "fix cuda driver link issue with driver version less than 12.3",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T07:55:06Z",
    "closed_at": "2025-06-10T07:27:40Z",
    "merged_at": "2025-06-10T07:27:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5025"
  },
  {
    "number": 5024,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T07:43:16Z",
    "closed_at": "2025-06-09T08:03:16Z",
    "merged_at": "2025-06-09T08:03:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5024"
  },
  {
    "number": 5022,
    "title": "fix:https://nvbugs/5298661",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T07:11:51Z",
    "closed_at": "2025-06-12T12:41:44Z",
    "merged_at": "2025-06-12T12:41:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5022"
  },
  {
    "number": 5021,
    "title": "ci: unwaive llmapi launch test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T06:56:56Z",
    "closed_at": "2025-06-10T08:10:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5021"
  },
  {
    "number": 5020,
    "title": "fix #4974: A thread leak issue in scaffolding unittest",
    "user": "ccs96307",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T06:10:22Z",
    "closed_at": "2025-07-14T11:22:03Z",
    "merged_at": "2025-07-14T11:22:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5020"
  },
  {
    "number": 5019,
    "title": "Draft: Add some more R1 and llama tests to L0",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T04:31:37Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5019"
  },
  {
    "number": 5017,
    "title": "[fix] Fix illegal mem access and possible accuracy lose. Cherry-pick …",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T03:23:49Z",
    "closed_at": "2025-06-09T09:50:57Z",
    "merged_at": "2025-06-09T09:50:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5017"
  },
  {
    "number": 5016,
    "title": "tests: fix commit info",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T03:11:18Z",
    "closed_at": "2025-06-09T03:12:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5016"
  },
  {
    "number": 5015,
    "title": "Revert \"[TRTLLM-5692][tests] Add speculative decoding test cases on torch flow\"",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T02:47:05Z",
    "closed_at": "2025-06-09T02:50:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5015"
  },
  {
    "number": 5014,
    "title": "[TRTLLM-5530][BREAKING CHANGE]: enhance the llm args pytorch config part 1(cuda_graph_config)",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T02:25:13Z",
    "closed_at": "2025-06-30T03:05:41Z",
    "merged_at": "2025-06-30T03:05:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5014"
  },
  {
    "number": 5013,
    "title": "feat: Add support for per expert activation scaling factors",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-09T01:40:24Z",
    "closed_at": "2025-06-27T21:10:35Z",
    "merged_at": "2025-06-27T21:10:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5013"
  },
  {
    "number": 5011,
    "title": "feat: Add support for LLGuidance",
    "user": "jellysnack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-08T23:04:56Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5011"
  },
  {
    "number": 5010,
    "title": "Draft: test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-08T12:39:43Z",
    "closed_at": "2025-06-09T01:11:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5010"
  },
  {
    "number": 5009,
    "title": "[Infra] - Update JNLP container config",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-08T09:46:32Z",
    "closed_at": "2025-06-08T10:09:35Z",
    "merged_at": "2025-06-08T10:09:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5009"
  },
  {
    "number": 5008,
    "title": "[Infra] - Update JNLP container config",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-08T03:07:15Z",
    "closed_at": "2025-06-08T08:44:09Z",
    "merged_at": "2025-06-08T08:44:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5008"
  },
  {
    "number": 5007,
    "title": "Fix buffer count",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-08T02:50:45Z",
    "closed_at": "2025-06-09T06:01:14Z",
    "merged_at": "2025-06-09T06:01:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5007"
  },
  {
    "number": 5006,
    "title": "Edits for tech blog 4",
    "user": "jdemouth-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-07T16:09:00Z",
    "closed_at": "2025-06-09T01:38:42Z",
    "merged_at": "2025-06-09T01:38:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5006"
  },
  {
    "number": 5005,
    "title": "Cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/5004",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-07T13:04:14Z",
    "closed_at": "2025-06-08T03:23:56Z",
    "merged_at": "2025-06-08T03:23:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5005"
  },
  {
    "number": 5004,
    "title": "chore:set the flashinfer to 0.2.5.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-07T08:48:33Z",
    "closed_at": "2025-06-07T12:42:10Z",
    "merged_at": "2025-06-07T12:42:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5004"
  },
  {
    "number": 5003,
    "title": "chore:update modelopt to 0.31",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-07T07:25:37Z",
    "closed_at": "2025-06-08T07:55:33Z",
    "merged_at": "2025-06-08T07:55:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5003"
  },
  {
    "number": 5001,
    "title": "[DON'T MERGE] [feat]VSWA support for PyTorch KVCacheManager",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-07T01:33:14Z",
    "closed_at": "2025-06-12T06:38:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5001"
  },
  {
    "number": 5000,
    "title": "[TRTLLM-5195][feat] Multimodal Disagg Support in TRTLLM",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T20:03:30Z",
    "closed_at": "2025-10-29T06:55:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5000"
  },
  {
    "number": 4999,
    "title": "Fix ngram unit test",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T19:42:50Z",
    "closed_at": "2025-08-26T22:00:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4999"
  },
  {
    "number": 4998,
    "title": "draft:[AutoDeploy] test quant moe op with CI",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T17:26:12Z",
    "closed_at": "2025-07-19T21:57:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4998"
  },
  {
    "number": 4997,
    "title": "Cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/4853",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T16:16:11Z",
    "closed_at": "2025-06-10T04:33:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4997"
  },
  {
    "number": 4996,
    "title": "Fix NGram unit test",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T14:42:12Z",
    "closed_at": "2025-06-06T19:32:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4996"
  },
  {
    "number": 4994,
    "title": "UCXX uses only the ucp_feature_tag to avoid certain issues on specific platforms",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T14:12:47Z",
    "closed_at": "2025-06-13T11:14:26Z",
    "merged_at": "2025-06-13T11:14:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4994"
  },
  {
    "number": 4993,
    "title": "fix: Disaggregate serving with attention DP",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T14:09:11Z",
    "closed_at": "2025-07-08T08:15:04Z",
    "merged_at": "2025-07-08T08:15:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4993"
  },
  {
    "number": 4992,
    "title": "ci: waive testcase [NVBUG 5247271]",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T12:51:16Z",
    "closed_at": "2025-06-08T08:47:07Z",
    "merged_at": "2025-06-08T08:47:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4992"
  },
  {
    "number": 4991,
    "title": "ci: unwaive llmapi launch test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T12:47:40Z",
    "closed_at": "2025-06-09T05:25:43Z",
    "merged_at": "2025-06-09T05:25:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4991"
  },
  {
    "number": 4990,
    "title": "doc: Added documentation for enable_trtllm_sampler.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T11:53:02Z",
    "closed_at": "2025-06-12T10:34:15Z",
    "merged_at": "2025-06-12T10:34:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4990"
  },
  {
    "number": 4989,
    "title": "tests: fix some typo and limitation on test cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T09:38:55Z",
    "closed_at": "2025-06-10T02:47:51Z",
    "merged_at": "2025-06-10T02:47:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4989"
  },
  {
    "number": 4988,
    "title": "[TRTLLM-5312] - Add bot run rules for triton tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T09:23:11Z",
    "closed_at": "2025-07-25T02:31:13Z",
    "merged_at": "2025-07-25T02:31:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4988"
  },
  {
    "number": 4986,
    "title": "fix: Fix warmup phase batch size out of range.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T08:44:54Z",
    "closed_at": "2025-06-09T11:19:16Z",
    "merged_at": "2025-06-09T11:19:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4986"
  },
  {
    "number": 4985,
    "title": "infra: update jnlp image",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T08:43:20Z",
    "closed_at": "2025-06-16T02:52:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4985"
  },
  {
    "number": 4983,
    "title": "Disable cyclic kv cache",
    "user": "ming-wei",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T06:59:12Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4983"
  },
  {
    "number": 4982,
    "title": "feat: Allow discontiguous inputs to the group_rms_norm.",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T06:56:33Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4982"
  },
  {
    "number": 4981,
    "title": "[5310329] chore: Unwaive test_e2e.py::test_openai_reasoning.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T06:45:01Z",
    "closed_at": "2025-06-09T06:05:22Z",
    "merged_at": "2025-06-09T06:05:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4981"
  },
  {
    "number": 4980,
    "title": "test: add unit tests for Llama4 min_latency code",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T06:43:51Z",
    "closed_at": "2025-06-10T19:10:26Z",
    "merged_at": "2025-06-10T19:10:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4980"
  },
  {
    "number": 4979,
    "title": "test: add more llama_v3.3_70b cases in perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T06:28:21Z",
    "closed_at": "2025-06-11T07:44:22Z",
    "merged_at": "2025-06-11T07:44:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4979"
  },
  {
    "number": 4978,
    "title": "Mxfp8xmxfp4",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T06:24:45Z",
    "closed_at": "2025-06-10T14:01:38Z",
    "merged_at": "2025-06-10T14:01:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4978"
  },
  {
    "number": 4977,
    "title": "CI: waive test_llm_multi_node_with_postproc",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T05:58:37Z",
    "closed_at": "2025-06-06T06:19:57Z",
    "merged_at": "2025-06-06T06:19:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4977"
  },
  {
    "number": 4976,
    "title": "fix: [nvbugs/5324954, nvbugs/5304229] fix Qwen2-VL video and Qwen2.5-VL image test case",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T04:10:25Z",
    "closed_at": "2025-06-09T07:25:27Z",
    "merged_at": "2025-06-09T07:25:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4976"
  },
  {
    "number": 4975,
    "title": "doc: Minor fixes and clarification",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T03:06:27Z",
    "closed_at": "2025-06-09T06:06:10Z",
    "merged_at": "2025-06-09T06:06:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4975"
  },
  {
    "number": 4973,
    "title": "fix:https://nvbugs/5324248",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T02:22:43Z",
    "closed_at": "2025-06-06T20:14:07Z",
    "merged_at": "2025-06-06T20:14:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4973"
  },
  {
    "number": 4972,
    "title": "fix: build_config in TorchLlmArgs and avoid arbitrary args ",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T02:21:49Z",
    "closed_at": "2025-06-16T00:51:57Z",
    "merged_at": "2025-06-16T00:51:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4972"
  },
  {
    "number": 4971,
    "title": "feat: Add non-streaming support for trtllm serve bench script & fixed prompt and output token length",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T01:59:53Z",
    "closed_at": "2025-06-18T05:37:31Z",
    "merged_at": "2025-06-18T05:37:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4971"
  },
  {
    "number": 4970,
    "title": "Revert \"fix a bug of global cuda graph dummy request\"",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T00:48:08Z",
    "closed_at": "2025-06-06T00:54:46Z",
    "merged_at": "2025-06-06T00:54:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4970"
  },
  {
    "number": 4969,
    "title": "Resubmit #4894",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T00:25:31Z",
    "closed_at": "2025-06-07T20:42:16Z",
    "merged_at": "2025-06-07T20:42:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4969"
  },
  {
    "number": 4968,
    "title": "feat:[AutoDeploy] Support Quantized MoE matcher - 1",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-06T00:23:14Z",
    "closed_at": "2025-06-27T21:25:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4968"
  },
  {
    "number": 4967,
    "title": "doc: refinement based on Julien's feedbacks",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T23:55:15Z",
    "closed_at": "2025-06-06T00:56:14Z",
    "merged_at": "2025-06-06T00:56:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4967"
  },
  {
    "number": 4966,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T23:24:44Z",
    "closed_at": "2025-06-06T02:30:15Z",
    "merged_at": "2025-06-06T02:30:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4966"
  },
  {
    "number": 4965,
    "title": "Reduce the number of layers for TRT-LLM release",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T21:39:54Z",
    "closed_at": "2025-07-08T15:48:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4965"
  },
  {
    "number": 4964,
    "title": "[https://nvbugspro.nvidia.com/bug/5323820] Fix chunking equation for disabled case.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T20:37:44Z",
    "closed_at": "2025-06-06T07:51:10Z",
    "merged_at": "2025-06-06T07:51:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4964"
  },
  {
    "number": 4963,
    "title": "[fix] Fix incorrect get_spec_worker selection logic",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T19:52:54Z",
    "closed_at": "2025-09-04T16:38:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4963"
  },
  {
    "number": 4962,
    "title": "[TRTLLM-6088][doc] Add speculative decoding PyTorch docs",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T17:50:07Z",
    "closed_at": "2025-07-14T20:01:08Z",
    "merged_at": "2025-07-14T20:01:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4962"
  },
  {
    "number": 4961,
    "title": "Fix Llama-3_3-Nemotron-Super-49B-v1 FP8 accuracy threshold configs",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T17:14:39Z",
    "closed_at": "2025-06-12T06:32:04Z",
    "merged_at": "2025-06-12T06:32:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4961"
  },
  {
    "number": 4960,
    "title": "doc: expose Large-scale EP design and implementation tech blog in the main…",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T14:36:06Z",
    "closed_at": "2025-06-05T14:51:25Z",
    "merged_at": "2025-06-05T14:51:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4960"
  },
  {
    "number": 4959,
    "title": "[infra] Redo unwaive unittest/_torch",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T14:17:07Z",
    "closed_at": "2025-06-05T15:14:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4959"
  },
  {
    "number": 4958,
    "title": "blog: Scaling Expert Parallelism in TensorRT-LLM (Part 1: Design and Implementation of Large-scale EP)",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T13:45:52Z",
    "closed_at": "2025-06-05T14:24:05Z",
    "merged_at": "2025-06-05T14:24:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4958"
  },
  {
    "number": 4957,
    "title": "Fix: fix autodeploy",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T12:53:38Z",
    "closed_at": "2025-06-05T13:06:55Z",
    "merged_at": "2025-06-05T13:06:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4957"
  },
  {
    "number": 4956,
    "title": "Doc: Add info about stop words appearing in output",
    "user": "Linda-Stadter",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T12:51:05Z",
    "closed_at": "2025-06-10T20:38:33Z",
    "merged_at": "2025-06-10T20:38:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4956"
  },
  {
    "number": 4955,
    "title": "Add customized renormalized moe routing kernel for moe cutlass backend",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T12:28:55Z",
    "closed_at": "2025-06-09T09:38:50Z",
    "merged_at": "2025-06-09T09:38:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4955"
  },
  {
    "number": 4954,
    "title": "[nvbug 5325284][fix] Increase Nemotron-H warmup request robustness",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T12:19:31Z",
    "closed_at": "2025-06-10T08:09:38Z",
    "merged_at": "2025-06-10T08:09:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4954"
  },
  {
    "number": 4953,
    "title": "Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T10:52:45Z",
    "closed_at": "2025-06-05T11:36:49Z",
    "merged_at": "2025-06-05T11:36:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4953"
  },
  {
    "number": 4952,
    "title": "[fix] Add MPI barrier for proper NVLS multicast team initialization",
    "user": "ikryukov",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T10:04:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4952"
  },
  {
    "number": 4951,
    "title": "ci: [nvbugs/5280806] Unwaive unittests/_torch.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T10:02:59Z",
    "closed_at": "2025-06-09T11:04:11Z",
    "merged_at": "2025-06-09T11:04:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4951"
  },
  {
    "number": 4950,
    "title": "Revert \"[infra] Unwaive unittests/_torch\"",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T09:12:23Z",
    "closed_at": "2025-06-05T09:21:07Z",
    "merged_at": "2025-06-05T09:21:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4950"
  },
  {
    "number": 4949,
    "title": "Revert \"fix: build_config in TorchLlmArgs and avoid invalid args\"",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T09:10:39Z",
    "closed_at": "2025-06-05T09:43:31Z",
    "merged_at": "2025-06-05T09:43:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4949"
  },
  {
    "number": 4948,
    "title": "[WIP] Introduce Flux MoE operator",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T08:54:08Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4948"
  },
  {
    "number": 4946,
    "title": "Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T08:31:01Z",
    "closed_at": "2025-06-05T08:38:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4946"
  },
  {
    "number": 4945,
    "title": "CI: waive test_llm_get_queued_stats",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T08:29:37Z",
    "closed_at": "2025-06-05T08:44:56Z",
    "merged_at": "2025-06-05T08:44:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4945"
  },
  {
    "number": 4944,
    "title": "infra: update jnlp version in container image",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T08:27:07Z",
    "closed_at": "2025-06-05T14:36:11Z",
    "merged_at": "2025-06-05T14:36:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4944"
  },
  {
    "number": 4943,
    "title": "[fix] Fix illegal mem access and possible accuracy lose",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T08:13:13Z",
    "closed_at": "2025-06-08T03:19:42Z",
    "merged_at": "2025-06-08T03:19:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4943"
  },
  {
    "number": 4942,
    "title": "[TRTLLM-5512] - Move part of tests from A100X to A100_80GB_PCIE multi…",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T07:53:22Z",
    "closed_at": "2025-06-16T03:24:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4942"
  },
  {
    "number": 4941,
    "title": "fix: Refactor the first token response in PD (#4692)",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T07:49:50Z",
    "closed_at": "2025-06-16T06:50:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4941"
  },
  {
    "number": 4940,
    "title": "[TRTLLM-5692][tests] Add speculative decoding test cases on torch flow",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T07:48:52Z",
    "closed_at": "2025-06-07T03:18:33Z",
    "merged_at": "2025-06-07T03:18:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4940"
  },
  {
    "number": 4939,
    "title": "infra: [TRTLLM-5873] Use build stage wheels to speed up docker release image build",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T07:47:17Z",
    "closed_at": "2025-07-29T16:54:38Z",
    "merged_at": "2025-07-29T16:54:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4939"
  },
  {
    "number": 4938,
    "title": "Perf: cache tokens in Python side to reduce reading overhead of pybind",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T07:38:58Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4938"
  },
  {
    "number": 4936,
    "title": "Raise shut down error for each request",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T07:10:41Z",
    "closed_at": "2025-07-04T09:58:25Z",
    "merged_at": "2025-07-04T09:58:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4936"
  },
  {
    "number": 4935,
    "title": "fix: Mapping rank boundary check bug",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T06:38:31Z",
    "closed_at": "2025-06-26T23:28:00Z",
    "merged_at": "2025-06-26T23:28:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4935"
  },
  {
    "number": 4934,
    "title": "feat : add PositionEmbeddingType=0 to xqa support",
    "user": "dongjiyingdjy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T06:36:13Z",
    "closed_at": "2025-06-05T13:50:43Z",
    "merged_at": "2025-06-05T13:50:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4934"
  },
  {
    "number": 4933,
    "title": "[TRTLLM-4932] Add Llama-3.1-Nemotron-Nano-8B-v1-FP8 accuracy tests",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T05:44:03Z",
    "closed_at": "2025-06-12T07:06:29Z",
    "merged_at": "2025-06-12T07:06:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4933"
  },
  {
    "number": 4932,
    "title": "infra: downgrade NCCL from 2.26 to 2.25",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T05:36:00Z",
    "closed_at": "2025-06-05T05:36:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4932"
  },
  {
    "number": 4931,
    "title": "Downgrade NCCL version from 2.26.5 to 2.25.1",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T05:28:43Z",
    "closed_at": "2025-06-05T06:03:39Z",
    "merged_at": "2025-06-05T06:03:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4931"
  },
  {
    "number": 4930,
    "title": "fix: [nvbugs/5324229] Fix broken WInt4AFP8FusedMoEMethod since FusedMoE refactor.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T04:22:58Z",
    "closed_at": "2025-06-13T02:21:33Z",
    "merged_at": "2025-06-13T02:21:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4930"
  },
  {
    "number": 4929,
    "title": "Kv cache transfer support duplicate heads",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T03:04:08Z",
    "closed_at": "2025-06-09T06:11:20Z",
    "merged_at": "2025-06-09T06:11:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4929"
  },
  {
    "number": 4928,
    "title": "chore: cleanup GDS Cmake interface",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T03:01:13Z",
    "closed_at": "2025-06-10T09:25:44Z",
    "merged_at": "2025-06-10T09:25:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4928"
  },
  {
    "number": 4927,
    "title": "Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T02:46:47Z",
    "closed_at": "2025-06-05T03:09:02Z",
    "merged_at": "2025-06-05T03:09:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4927"
  },
  {
    "number": 4925,
    "title": "fix:https://nvbugs/5324252",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T02:10:16Z",
    "closed_at": "2025-06-08T17:15:45Z",
    "merged_at": "2025-06-08T17:15:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4925"
  },
  {
    "number": 4924,
    "title": "fix: trtllm-bench --dataset required=True",
    "user": "jasonqinzhou",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T02:06:44Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4924"
  },
  {
    "number": 4923,
    "title": "Coalesce text diffs in streaming requests.",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-05T00:37:49Z",
    "closed_at": "2025-10-03T16:04:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4923"
  },
  {
    "number": 4922,
    "title": "fix: LLM invalid arg in a test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T23:44:14Z",
    "closed_at": "2025-06-05T00:00:32Z",
    "merged_at": "2025-06-05T00:00:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4922"
  },
  {
    "number": 4921,
    "title": "fix: [nvbug 5321627] handle cases when TRT backend return more logits than output tokens  ",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T20:19:33Z",
    "closed_at": "2025-06-05T23:12:42Z",
    "merged_at": "2025-06-05T23:12:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4921"
  },
  {
    "number": 4920,
    "title": "Only pass `fast_build=true` to non-pytorch backend",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T19:08:08Z",
    "closed_at": "2025-06-05T05:30:17Z",
    "merged_at": "2025-06-05T05:30:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4920"
  },
  {
    "number": 4919,
    "title": "[infra] Unwaive unittests/_torch",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T19:02:52Z",
    "closed_at": "2025-06-05T00:49:38Z",
    "merged_at": "2025-06-05T00:49:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4919"
  },
  {
    "number": 4918,
    "title": "chore: Refactor apply_rope.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T17:17:11Z",
    "closed_at": "2025-06-09T08:51:59Z",
    "merged_at": "2025-06-09T08:51:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4918"
  },
  {
    "number": 4916,
    "title": "[Draft] Test Flux MoE operator",
    "user": "lancelly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T13:59:38Z",
    "closed_at": "2025-06-05T08:17:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4916"
  },
  {
    "number": 4915,
    "title": "perf: Removing initializing ptuning buffers to zero",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T13:38:55Z",
    "closed_at": "2025-06-10T01:57:21Z",
    "merged_at": "2025-06-10T01:57:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4915"
  },
  {
    "number": 4914,
    "title": "fix : tmp disable fuse_qk_norm_rope on hopper",
    "user": "dongjiyingdjy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T12:16:19Z",
    "closed_at": "2025-06-05T13:52:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4914"
  },
  {
    "number": 4913,
    "title": "fix: params passed to BuildConfig will propagate to LlmArgs as well",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T11:39:44Z",
    "closed_at": "2025-06-17T11:42:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4913"
  },
  {
    "number": 4912,
    "title": "[5310329] fix: Fix warmup phase batch size out of range.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T11:15:12Z",
    "closed_at": "2025-06-06T04:26:05Z",
    "merged_at": "2025-06-06T04:26:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4912"
  },
  {
    "number": 4911,
    "title": "fix: cache-aware router related test fix",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T10:42:11Z",
    "closed_at": "2025-06-05T05:07:25Z",
    "merged_at": "2025-06-05T05:07:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4911"
  },
  {
    "number": 4908,
    "title": "test: fix potential teardown error",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T09:59:38Z",
    "closed_at": "2025-06-05T02:39:57Z",
    "merged_at": "2025-06-05T02:39:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4908"
  },
  {
    "number": 4907,
    "title": "[TRTLLM-5340] fix: remove the accuracy assert on run_majority_vote_ai…",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T08:50:43Z",
    "closed_at": "2025-06-04T22:40:47Z",
    "merged_at": "2025-06-04T22:40:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4907"
  },
  {
    "number": 4906,
    "title": "feat: Add support for YARN in NemotronNAS models",
    "user": "amirkl94",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T08:41:46Z",
    "closed_at": "2025-06-29T06:45:49Z",
    "merged_at": "2025-06-29T06:45:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4906"
  },
  {
    "number": 4905,
    "title": "[TO-DELETE] Debugging SILMA's reduce_fusion flag",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T08:15:29Z",
    "closed_at": "2025-07-09T08:41:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4905"
  },
  {
    "number": 4904,
    "title": "[nvbug/5195657][fix] fix reset spec buffer and update mMaxAttentionWindowVec logic",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T07:53:17Z",
    "closed_at": "2025-06-14T19:19:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4904"
  },
  {
    "number": 4903,
    "title": "Fix support of system error",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T07:45:41Z",
    "closed_at": "2025-06-27T03:10:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4903"
  },
  {
    "number": 4902,
    "title": "fix: [nvbugs/5312750] Keep embed_tokens for last pp rank if tie_word_embeddings.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T07:30:30Z",
    "closed_at": "2025-06-04T11:49:08Z",
    "merged_at": "2025-06-04T11:49:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4902"
  },
  {
    "number": 4901,
    "title": "tests: [TRTQA-2906] add benchmark serving tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T07:01:17Z",
    "closed_at": "2025-06-05T06:33:04Z",
    "merged_at": "2025-06-05T06:33:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4901"
  },
  {
    "number": 4900,
    "title": "chore: partition LLM class into TorchLLM and TrtLLM",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T06:55:04Z",
    "closed_at": "2025-06-18T06:01:26Z",
    "merged_at": "2025-06-18T06:01:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4900"
  },
  {
    "number": 4899,
    "title": "Add disaggregated unittest",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T06:17:37Z",
    "closed_at": "2025-06-05T11:14:31Z",
    "merged_at": "2025-06-05T11:14:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4899"
  },
  {
    "number": 4898,
    "title": "chore: Mass integration of release/0.20",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T06:16:23Z",
    "closed_at": "2025-06-08T15:26:26Z",
    "merged_at": "2025-06-08T15:26:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4898"
  },
  {
    "number": 4897,
    "title": "fix: Fix broken vanilla moe since FusedMoE refactor.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T06:09:18Z",
    "closed_at": "2025-06-04T19:56:41Z",
    "merged_at": "2025-06-04T19:56:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4897"
  },
  {
    "number": 4896,
    "title": "chore: bump version to 0.21.0rc1",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T05:43:57Z",
    "closed_at": "2025-06-04T06:31:19Z",
    "merged_at": "2025-06-04T06:31:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4896"
  },
  {
    "number": 4895,
    "title": "update fmha_v2",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T05:43:23Z",
    "closed_at": "2025-06-05T14:14:29Z",
    "merged_at": "2025-06-05T14:14:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4895"
  },
  {
    "number": 4894,
    "title": "fix a bug of global cuda graph dummy request",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T04:20:57Z",
    "closed_at": "2025-06-05T11:47:40Z",
    "merged_at": "2025-06-05T11:47:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4894"
  },
  {
    "number": 4893,
    "title": "chore: Refine weight prefetching.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T03:26:38Z",
    "closed_at": "2025-06-09T13:24:17Z",
    "merged_at": "2025-06-09T13:24:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4893"
  },
  {
    "number": 4892,
    "title": "[AutoDeploy] deprecate CI post-merge tests and keep them for local testing",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T03:19:59Z",
    "closed_at": "2025-06-05T00:27:17Z",
    "merged_at": "2025-06-05T00:27:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4892"
  },
  {
    "number": 4891,
    "title": "[AutoDeploy] _AutoDeployLlmArgs as primary config object",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T03:07:35Z",
    "closed_at": "2025-06-05T09:20:56Z",
    "merged_at": "2025-06-05T09:20:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4891"
  },
  {
    "number": 4890,
    "title": "fix: pytorch_backend_config is deprecated in update_llm_args_with_extra_dict.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T02:53:50Z",
    "closed_at": "2025-06-10T10:40:30Z",
    "merged_at": "2025-06-10T10:40:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4890"
  },
  {
    "number": 4889,
    "title": "Fix: max_num_sequences calculation with overlap scheduling into release/0.20",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T02:53:39Z",
    "closed_at": "2025-06-04T14:33:12Z",
    "merged_at": "2025-06-04T14:33:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4889"
  },
  {
    "number": 4886,
    "title": "Fix DeepGEMM NVCC Path",
    "user": "lucifer1004",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T02:40:43Z",
    "closed_at": "2025-06-05T03:55:37Z",
    "merged_at": "2025-06-05T03:55:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4886"
  },
  {
    "number": 4885,
    "title": "[Infra] - Update dependencies with NGC PyTorch 25.05 and TRT 10.11",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T02:38:25Z",
    "closed_at": "2025-06-17T15:48:35Z",
    "merged_at": "2025-06-17T15:48:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4885"
  },
  {
    "number": 4884,
    "title": "enh/test: Add basic e2e perf testing capability for LoRA with PyT Flow",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-04T00:04:07Z",
    "closed_at": "2025-06-24T19:17:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4884"
  },
  {
    "number": 4883,
    "title": "[TRTLLM-5644][infra] Update the community action to more appropriate api",
    "user": "poweiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T23:51:26Z",
    "closed_at": "2025-07-01T21:44:16Z",
    "merged_at": "2025-07-01T21:44:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4883"
  },
  {
    "number": 4882,
    "title": "Fix: draft target README and set exclude_input_in_output to False",
    "user": "eagle705",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T23:47:44Z",
    "closed_at": "2025-06-04T06:45:02Z",
    "merged_at": "2025-06-04T06:45:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4882"
  },
  {
    "number": 4879,
    "title": "[nvbug/5319281][fix] Stop drafting when we hit the draft model's max seq len",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T23:10:38Z",
    "closed_at": "2025-06-13T15:06:36Z",
    "merged_at": "2025-06-13T15:06:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4879"
  },
  {
    "number": 4878,
    "title": "[arch] Make infer_max_seq_len a staticmethod",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T19:46:14Z",
    "closed_at": "2025-06-03T21:05:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4878"
  },
  {
    "number": 4877,
    "title": "[TRTLLM-5518] doc: Adding disaggregated serving section to models doc",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T17:46:37Z",
    "closed_at": "2025-06-09T21:19:03Z",
    "merged_at": "2025-06-09T21:19:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4877"
  },
  {
    "number": 4876,
    "title": "feat: Add support for fp8 rowwise quantization",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T17:35:40Z",
    "closed_at": "2025-06-14T13:37:48Z",
    "merged_at": "2025-06-14T13:37:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4876"
  },
  {
    "number": 4875,
    "title": "ReDrafter support for Qwen",
    "user": "darraghdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T16:29:20Z",
    "closed_at": "2025-06-27T18:33:11Z",
    "merged_at": "2025-06-27T18:33:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4875"
  },
  {
    "number": 4874,
    "title": "ReDrafter support for Qwen",
    "user": "darraghdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T16:14:20Z",
    "closed_at": "2025-06-03T16:20:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4874"
  },
  {
    "number": 4873,
    "title": "chore: Waive examples/test_mistral.py::test_llm_mistral_v1_1gpu.",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T16:08:53Z",
    "closed_at": "2025-06-03T18:45:15Z",
    "merged_at": "2025-06-03T18:45:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4873"
  },
  {
    "number": 4872,
    "title": "[TRTLLM-5589] feat: Integrate TRT-LLM Gen FP8 Batched GEMM with Pytorch workflow kernel autotuner",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T16:04:44Z",
    "closed_at": "2025-06-09T10:02:49Z",
    "merged_at": "2025-06-09T10:02:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4872"
  },
  {
    "number": 4871,
    "title": "chore: Mass integration of release/0.20.",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T14:09:26Z",
    "closed_at": "2025-06-04T06:12:28Z",
    "merged_at": "2025-06-04T06:12:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4871"
  },
  {
    "number": 4870,
    "title": "TRTLLM-5219[feat] W4A8 AWQ Support",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T13:51:57Z",
    "closed_at": "2025-07-14T13:48:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4870"
  },
  {
    "number": 4869,
    "title": "[feat] Optimize KV Cache Reuse for MLA",
    "user": "zhhuang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T13:51:27Z",
    "closed_at": "2025-06-13T03:03:05Z",
    "merged_at": "2025-06-13T03:03:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4869"
  },
  {
    "number": 4868,
    "title": "test: fix rss increasement test case issue",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T12:38:20Z",
    "closed_at": "2025-06-04T02:35:06Z",
    "merged_at": "2025-06-04T02:35:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4868"
  },
  {
    "number": 4867,
    "title": "feat: Add w4a8_mxfp4_fp8 quantization recipe.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T11:28:32Z",
    "closed_at": "2025-06-16T03:30:58Z",
    "merged_at": "2025-06-16T03:30:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4867"
  },
  {
    "number": 4865,
    "title": "refactor: Scheduling based on KV cache state",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T11:02:15Z",
    "closed_at": "2025-06-16T06:14:58Z",
    "merged_at": "2025-06-16T06:14:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4865"
  },
  {
    "number": 4864,
    "title": "fix: Register MoeLoadBalancerConfig to serialization.py",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T10:58:09Z",
    "closed_at": "2025-06-03T11:22:36Z",
    "merged_at": "2025-06-03T11:22:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4864"
  },
  {
    "number": 4863,
    "title": "Fix DeepSeek FP8 TRT-LLM Gen",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T10:27:43Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4863"
  },
  {
    "number": 4862,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T10:23:39Z",
    "closed_at": "2025-06-03T10:37:21Z",
    "merged_at": "2025-06-03T10:37:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4862"
  },
  {
    "number": 4861,
    "title": "Chore: remove duplicate allgather of attention DP path in model engine",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T10:18:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4861"
  },
  {
    "number": 4860,
    "title": "Release/0.20",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T10:01:05Z",
    "closed_at": "2025-06-03T10:01:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4860"
  },
  {
    "number": 4859,
    "title": "[TRTLLM-5630] restore free_gpu_memory_fraction=0.9 in tests",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T10:00:28Z",
    "closed_at": "2025-06-05T09:46:30Z",
    "merged_at": "2025-06-05T09:46:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4859"
  },
  {
    "number": 4858,
    "title": "[feat] Support XQA-based MLA on SM120",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T07:59:06Z",
    "closed_at": "2025-06-06T14:32:49Z",
    "merged_at": "2025-06-06T14:32:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4858"
  },
  {
    "number": 4857,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T07:41:14Z",
    "closed_at": "2025-06-03T07:58:26Z",
    "merged_at": "2025-06-03T07:58:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4857"
  },
  {
    "number": 4856,
    "title": "[feat] Support XQA-based MLA on SM120",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T07:20:34Z",
    "closed_at": "2025-06-03T07:28:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4856"
  },
  {
    "number": 4855,
    "title": "Disable cyclic kv cache chunk",
    "user": "ming-wei",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T07:08:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4855"
  },
  {
    "number": 4854,
    "title": "Draft: Generalize Checkpoint Loading Logic",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T07:00:36Z",
    "closed_at": "2025-06-03T07:01:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4854"
  },
  {
    "number": 4853,
    "title": "fix: fix cuda graph padding for spec decoding",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T06:17:23Z",
    "closed_at": "2025-06-06T14:21:42Z",
    "merged_at": "2025-06-06T14:21:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4853"
  },
  {
    "number": 4852,
    "title": "Remove unnecessary duplicated tests from H100/H200 DGX pre/post-merge",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T06:17:18Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4852"
  },
  {
    "number": 4851,
    "title": "Replace memset with data initialization within kernels",
    "user": "ChristinaZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T05:19:02Z",
    "closed_at": "2025-06-04T00:56:46Z",
    "merged_at": "2025-06-04T00:56:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4851"
  },
  {
    "number": 4850,
    "title": "[Infra] - Better utilize multi-GPU CI resources",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T03:28:33Z",
    "closed_at": "2025-06-03T04:25:20Z",
    "merged_at": "2025-06-03T04:25:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4850"
  },
  {
    "number": 4849,
    "title": "shorten reqs in con:1 cases and add streaming cases, and add l2 perf …",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T02:19:19Z",
    "closed_at": "2025-06-03T04:28:13Z",
    "merged_at": "2025-06-03T04:28:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4849"
  },
  {
    "number": 4848,
    "title": "enh: Enable trtllm-bench to run LoRA PyT flow",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-03T00:53:07Z",
    "closed_at": "2025-06-24T19:16:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4848"
  },
  {
    "number": 4847,
    "title": "[DO NOT MERGE] test PR to test action",
    "user": "poweiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T22:49:17Z",
    "closed_at": "2025-06-10T17:41:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4847"
  },
  {
    "number": 4846,
    "title": "[Doc] Fix readme for disaggregated serving",
    "user": "arekay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T21:11:47Z",
    "closed_at": "2025-06-03T15:45:27Z",
    "merged_at": "2025-06-03T15:45:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4846"
  },
  {
    "number": 4845,
    "title": "[nvbug 5283506] fix: Fix spec decode triton test",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T17:01:07Z",
    "closed_at": "2025-06-09T12:40:18Z",
    "merged_at": "2025-06-09T12:40:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4845"
  },
  {
    "number": 4844,
    "title": "[fix] Fix Llama4 guradwords failures",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T16:41:56Z",
    "closed_at": "2025-06-02T20:43:43Z",
    "merged_at": "2025-06-02T20:43:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4844"
  },
  {
    "number": 4843,
    "title": "[nvbug/5314469][feat] Include the executor's max batch size in CUDA g…",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T15:56:48Z",
    "closed_at": "2025-06-09T12:31:35Z",
    "merged_at": "2025-06-09T12:31:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4843"
  },
  {
    "number": 4842,
    "title": "Add pre-merge Triton backend tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T15:44:27Z",
    "closed_at": "2025-06-03T04:47:58Z",
    "merged_at": "2025-06-03T04:47:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4842"
  },
  {
    "number": 4840,
    "title": "[Infra] - Reduce the default Pytest timeout and test Ubuntu mirrors",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T14:53:28Z",
    "closed_at": "2025-06-03T02:54:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4840"
  },
  {
    "number": 4839,
    "title": "Update code owner list",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T14:10:25Z",
    "closed_at": "2025-06-05T13:35:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4839"
  },
  {
    "number": 4838,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T13:51:17Z",
    "closed_at": "2025-06-03T01:11:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4838"
  },
  {
    "number": 4837,
    "title": "Chore: refine comments of prepare inputs method of model engine",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T12:18:06Z",
    "closed_at": "2025-06-04T04:14:14Z",
    "merged_at": "2025-06-04T04:14:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4837"
  },
  {
    "number": 4836,
    "title": "[TRTLLM-4995][feat] TRTLLM Sampler log probs support",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T12:15:03Z",
    "closed_at": "2025-06-11T06:18:13Z",
    "merged_at": "2025-06-11T06:18:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4836"
  },
  {
    "number": 4835,
    "title": "Fix: NVBug 5302895",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T09:28:57Z",
    "closed_at": "2025-06-04T01:31:40Z",
    "merged_at": "2025-06-04T01:31:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4835"
  },
  {
    "number": 4834,
    "title": "Cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/4536",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T09:19:39Z",
    "closed_at": "2025-06-03T04:29:54Z",
    "merged_at": "2025-06-03T04:29:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4834"
  },
  {
    "number": 4833,
    "title": "Cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/4379",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T08:37:11Z",
    "closed_at": "2025-06-03T04:30:02Z",
    "merged_at": "2025-06-03T04:30:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4833"
  },
  {
    "number": 4832,
    "title": "fix: [https://nvbugspro.nvidia.com/bug/5273945] Unwaive tests for bug-5273945",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T08:13:16Z",
    "closed_at": "2025-06-02T14:01:57Z",
    "merged_at": "2025-06-02T14:01:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4832"
  },
  {
    "number": 4831,
    "title": "test",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T07:49:27Z",
    "closed_at": "2025-06-19T05:57:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4831"
  },
  {
    "number": 4830,
    "title": "fix: [nvbugs/5298600] fix illegal memory access on mrope_position_deltas",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T07:41:08Z",
    "closed_at": "2025-06-03T06:56:50Z",
    "merged_at": "2025-06-03T06:56:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4830"
  },
  {
    "number": 4829,
    "title": "[Infra] - Minor clean-up and test Ubuntu mirrors",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T07:21:27Z",
    "closed_at": "2025-06-02T12:18:20Z",
    "merged_at": "2025-06-02T12:18:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4829"
  },
  {
    "number": 4828,
    "title": "feat: port MakeDecodingBatchInputOutput to python in TRTLLMSampler",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T06:48:36Z",
    "closed_at": "2025-06-09T23:28:35Z",
    "merged_at": "2025-06-09T23:28:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4828"
  },
  {
    "number": 4827,
    "title": "fix: trtllm-bench iter_stats and cuda_graph_batch_sizes error errors.",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T05:27:14Z",
    "closed_at": "2025-06-04T08:36:07Z",
    "merged_at": "2025-06-04T08:36:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4827"
  },
  {
    "number": 4826,
    "title": "chore: memoize weight shuffle index to speed up weight preproc in moe_backend=TRTLLM",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-02T01:56:42Z",
    "closed_at": "2025-06-06T08:13:54Z",
    "merged_at": "2025-06-06T08:13:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4826"
  },
  {
    "number": 4824,
    "title": "[TRTLLM-5502][infra] Add github action to identify if PR is from community",
    "user": "poweiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-01T23:13:31Z",
    "closed_at": "2025-06-02T22:36:35Z",
    "merged_at": "2025-06-02T22:36:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4824"
  },
  {
    "number": 4823,
    "title": "fix: make scale_bmm2_d optional in epilogue too",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-01T13:44:00Z",
    "closed_at": "2025-08-08T06:39:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4823"
  },
  {
    "number": 4822,
    "title": "[TRTLLM-4923][feat] Paged mamba cache",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-01T13:27:13Z",
    "closed_at": "2025-06-04T06:27:08Z",
    "merged_at": "2025-06-04T06:27:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4822"
  },
  {
    "number": 4821,
    "title": "Draft: test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-01T12:38:26Z",
    "closed_at": "2025-06-06T09:27:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4821"
  },
  {
    "number": 4820,
    "title": "'entered copyBlock' format string expects %s, pass string rather than int",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-01T11:45:38Z",
    "closed_at": "2025-06-01T15:54:34Z",
    "merged_at": "2025-06-01T15:54:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4820"
  },
  {
    "number": 4819,
    "title": "[TRTLLM-4987][feat] Support generation logits in TRTLLMSampler",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-01T10:08:15Z",
    "closed_at": "2025-06-09T03:30:01Z",
    "merged_at": "2025-06-09T03:30:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4819"
  },
  {
    "number": 4818,
    "title": "feat: large-scale EP(part 6: Online EP load balancer integration for GB200 nvfp4)",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-01T09:12:27Z",
    "closed_at": "2025-06-08T02:25:19Z",
    "merged_at": "2025-06-08T02:25:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4818"
  },
  {
    "number": 4817,
    "title": "[AutoDeploy] Increased Model Coverage Mass Migration Week 2",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-06-01T01:48:55Z",
    "closed_at": "2025-06-01T06:40:29Z",
    "merged_at": "2025-06-01T06:40:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4817"
  },
  {
    "number": 4814,
    "title": "[Arch] Freeze model_config",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-31T04:44:55Z",
    "closed_at": "2025-06-03T18:51:35Z",
    "merged_at": "2025-06-03T18:51:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4814"
  },
  {
    "number": 4813,
    "title": "feat: enable streaming weight updates",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T23:42:13Z",
    "closed_at": "2025-07-03T23:00:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4813"
  },
  {
    "number": 4812,
    "title": "[enhanchment] Add beam width to low latency.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T23:26:35Z",
    "closed_at": "2025-06-03T09:24:56Z",
    "merged_at": "2025-06-03T09:24:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4812"
  },
  {
    "number": 4810,
    "title": "[fix] Fix llama4 min-latency mode",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T20:36:54Z",
    "closed_at": "2025-06-02T00:50:02Z",
    "merged_at": "2025-06-02T00:50:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4810"
  },
  {
    "number": 4809,
    "title": "[fix] Fix llama 4 long context",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T19:22:31Z",
    "closed_at": "2025-06-03T23:48:08Z",
    "merged_at": "2025-06-03T23:48:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4809"
  },
  {
    "number": 4808,
    "title": "[TRTLLM-4932] Remove moe- related arguments from Llama-3_1-Nemotron-Ultra-253B-v1 CLI accuracy test",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T18:46:47Z",
    "closed_at": "2025-06-02T19:16:29Z",
    "merged_at": "2025-06-02T19:16:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4808"
  },
  {
    "number": 4807,
    "title": "[nvbug/5280806][fix] Fix 2 model spec decode flow",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T18:13:00Z",
    "closed_at": "2025-06-08T11:40:02Z",
    "merged_at": "2025-06-08T11:40:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4807"
  },
  {
    "number": 4806,
    "title": "fix: Fix queued req stats for release/0.20",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T14:34:10Z",
    "closed_at": "2025-06-02T12:32:24Z",
    "merged_at": "2025-06-02T12:32:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4806"
  },
  {
    "number": 4805,
    "title": "chore: simplify serialization check",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T11:54:54Z",
    "closed_at": "2025-06-18T08:44:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4805"
  },
  {
    "number": 4804,
    "title": "[fix] Do not reuse dummy request KVCache",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T11:14:31Z",
    "closed_at": "2025-06-12T07:24:51Z",
    "merged_at": "2025-06-12T07:24:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4804"
  },
  {
    "number": 4803,
    "title": "Expose new tech blog about DSR1 throughput optimization to the main R…",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T11:05:23Z",
    "closed_at": "2025-05-30T12:44:12Z",
    "merged_at": "2025-05-30T12:44:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4803"
  },
  {
    "number": 4802,
    "title": "[TRTLLM-4983] feat: enable overlap scheduler between draft forwards",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T09:01:11Z",
    "closed_at": "2025-06-15T15:09:17Z",
    "merged_at": "2025-06-15T15:09:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4802"
  },
  {
    "number": 4801,
    "title": "test: remove invalid triton integration test cases",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T08:51:38Z",
    "closed_at": "2025-06-03T01:39:23Z",
    "merged_at": "2025-06-03T01:39:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4801"
  },
  {
    "number": 4799,
    "title": "feat: add HyperCLOVAX-SEED-Vision support in refactored way",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T08:04:42Z",
    "closed_at": "2025-06-09T03:04:05Z",
    "merged_at": "2025-06-09T03:04:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4799"
  },
  {
    "number": 4798,
    "title": "fix [nvbug5256044]: bench hang due to llmapi ipc",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T07:31:41Z",
    "closed_at": "2025-06-03T02:10:53Z",
    "merged_at": "2025-06-03T02:10:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4798"
  },
  {
    "number": 4797,
    "title": "test: Waive test_llm_loading_from_ckpt_for_tp2",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T07:30:02Z",
    "closed_at": "2025-05-30T07:43:00Z",
    "merged_at": "2025-05-30T07:43:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4797"
  },
  {
    "number": 4796,
    "title": "test: shorten reqs in con:1 cases and add streaming cases, add l2 perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T07:18:29Z",
    "closed_at": "2025-06-03T02:20:56Z",
    "merged_at": "2025-06-03T02:20:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4796"
  },
  {
    "number": 4795,
    "title": "Waive l0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T07:17:21Z",
    "closed_at": "2025-05-30T07:56:58Z",
    "merged_at": "2025-05-30T07:56:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4795"
  },
  {
    "number": 4794,
    "title": "upgrade cutlass to 4.0",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T07:14:21Z",
    "closed_at": "2025-06-03T01:55:02Z",
    "merged_at": "2025-06-03T01:55:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4794"
  },
  {
    "number": 4792,
    "title": "feat: large-scale EP(part 7: DeepEP integration)",
    "user": "yuantailing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T05:41:36Z",
    "closed_at": "2025-06-14T11:12:39Z",
    "merged_at": "2025-06-14T11:12:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4792"
  },
  {
    "number": 4791,
    "title": "DeepSeek R1 throughut optimization tech blog for Blackwell GPUs",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T05:01:50Z",
    "closed_at": "2025-05-30T10:54:19Z",
    "merged_at": "2025-05-30T10:54:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4791"
  },
  {
    "number": 4790,
    "title": "[Architecture] Refactor FusedMoE",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T05:00:20Z",
    "closed_at": "2025-06-03T06:02:20Z",
    "merged_at": "2025-06-03T06:02:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4790"
  },
  {
    "number": 4786,
    "title": "fix: Skip dummy medusa/eagle tests when WORLD_SIZE env variable is missing",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T03:14:30Z",
    "closed_at": "2025-06-02T09:21:25Z",
    "merged_at": "2025-06-02T09:21:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4786"
  },
  {
    "number": 4785,
    "title": "tests: waive failed case",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T03:12:12Z",
    "closed_at": "2025-05-30T03:24:25Z",
    "merged_at": "2025-05-30T03:24:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4785"
  },
  {
    "number": 4784,
    "title": "fix: remove the accuracy assert on run_majority_vote_aime24.py #5340",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T03:08:37Z",
    "closed_at": "2025-06-03T02:41:03Z",
    "merged_at": "2025-06-03T02:41:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4784"
  },
  {
    "number": 4782,
    "title": "[5206383] chore: Check current CI status",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T01:33:41Z",
    "closed_at": "2025-08-22T02:53:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4782"
  },
  {
    "number": 4781,
    "title": "fix: correct the order of llm request state",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-30T01:21:24Z",
    "closed_at": "2025-06-04T06:45:13Z",
    "merged_at": "2025-06-04T06:45:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4781"
  },
  {
    "number": 4780,
    "title": "Draft: Dup of #4739 to run post-merge pipeline",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T23:35:44Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4780"
  },
  {
    "number": 4779,
    "title": "test: skip test_llm_hf_gemma_quantization_1gpu_vswa on A100",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T22:55:21Z",
    "closed_at": "2025-05-30T07:12:13Z",
    "merged_at": "2025-05-30T07:12:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4779"
  },
  {
    "number": 4778,
    "title": "[feat] Implement model-agnostic one-engine eagle3",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T22:33:33Z",
    "closed_at": "2025-06-13T15:11:41Z",
    "merged_at": "2025-06-13T15:11:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4778"
  },
  {
    "number": 4777,
    "title": "[infra] pull system packages from artifactory",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T21:49:44Z",
    "closed_at": "2025-06-02T19:23:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4777"
  },
  {
    "number": 4775,
    "title": "Maverick: Eagle Repro Guide",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T20:20:35Z",
    "closed_at": "2025-05-29T21:48:53Z",
    "merged_at": "2025-05-29T21:48:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4775"
  },
  {
    "number": 4774,
    "title": "fix: Updates to yarn",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T20:04:04Z",
    "closed_at": "2025-06-12T16:33:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4774"
  },
  {
    "number": 4773,
    "title": "[feat] Enable CUDA graphs for EAGLE3 two model implementation prefill stage",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T19:46:57Z",
    "closed_at": "2025-06-09T17:54:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4773"
  },
  {
    "number": 4772,
    "title": "[fix] Fix Llama 3.3 70b EAGLE",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T19:10:56Z",
    "closed_at": "2025-05-30T14:08:09Z",
    "merged_at": "2025-05-30T14:08:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4772"
  },
  {
    "number": 4771,
    "title": "[feat] Multi-node CI testing support via Slurm",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T18:34:14Z",
    "closed_at": "2025-06-18T17:11:13Z",
    "merged_at": "2025-06-18T17:11:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4771"
  },
  {
    "number": 4769,
    "title": "Added code owners for AutoDeploy",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T14:55:56Z",
    "closed_at": "2025-05-30T01:55:28Z",
    "merged_at": "2025-05-30T01:55:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4769"
  },
  {
    "number": 4768,
    "title": "[DO NOT MERGE] Debug perf",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T13:26:05Z",
    "closed_at": "2025-06-18T10:10:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4768"
  },
  {
    "number": 4767,
    "title": "fix: large-scale EP - EP load balancer with MTP layer and route offset by EP rank",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T13:04:52Z",
    "closed_at": "2025-05-31T16:07:45Z",
    "merged_at": "2025-05-31T16:07:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4767"
  },
  {
    "number": 4766,
    "title": "fix: re-enable tp/pp for quickstart_advanced.py.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T12:03:17Z",
    "closed_at": "2025-05-31T11:13:46Z",
    "merged_at": "2025-05-31T11:13:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4766"
  },
  {
    "number": 4765,
    "title": "feat: add heuristics for checkpoint files prefetching.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T11:55:23Z",
    "closed_at": "2025-06-03T04:10:37Z",
    "merged_at": "2025-06-03T04:10:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4765"
  },
  {
    "number": 4764,
    "title": "infra: upload imageTag info to artifactory and add ngc_staging to save ngc image",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T10:16:53Z",
    "closed_at": "2025-06-12T07:38:47Z",
    "merged_at": "2025-06-12T07:38:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4764"
  },
  {
    "number": 4763,
    "title": "chore: remove request_error ipc in LLM.submit",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T10:02:54Z",
    "closed_at": "2025-06-03T12:55:59Z",
    "merged_at": "2025-06-03T12:55:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4763"
  },
  {
    "number": 4762,
    "title": "fix: refactor and fix mtp vanilla",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T09:35:26Z",
    "closed_at": "2025-06-19T21:23:40Z",
    "merged_at": "2025-06-19T21:23:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4762"
  },
  {
    "number": 4761,
    "title": "Draft: feat: port MakeDecodingBatchInputOutput alg to python",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T08:55:07Z",
    "closed_at": "2025-06-02T06:48:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4761"
  },
  {
    "number": 4760,
    "title": "Upgrade CUTLASS to v3.9.2",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T08:52:58Z",
    "closed_at": "2025-05-30T07:16:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4760"
  },
  {
    "number": 4759,
    "title": "[nvbug 5305210] Resolve nvbug 5305210",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T08:52:53Z",
    "closed_at": "2025-05-31T11:21:06Z",
    "merged_at": "2025-05-31T11:21:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4759"
  },
  {
    "number": 4758,
    "title": "fix: [nvbugs/5310520] disable embed_tokens's TP when DP enabled for llama model.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T08:50:38Z",
    "closed_at": "2025-05-30T10:04:08Z",
    "merged_at": "2025-05-30T10:04:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4758"
  },
  {
    "number": 4757,
    "title": "Refactor test timeout for individual long case",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T07:56:23Z",
    "closed_at": "2025-06-19T05:52:11Z",
    "merged_at": "2025-06-19T05:52:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4757"
  },
  {
    "number": 4756,
    "title": "[TRTLLM-3927] [feat] Finalize + Allreduce + add + rmsnorm fusion",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T06:19:59Z",
    "closed_at": "2025-06-10T11:55:16Z",
    "merged_at": "2025-06-10T11:55:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4756"
  },
  {
    "number": 4755,
    "title": "test: remove perf test l40s/l20 oom test cases and unwaive tests",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T05:23:01Z",
    "closed_at": "2025-05-29T05:58:47Z",
    "merged_at": "2025-05-29T05:58:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4755"
  },
  {
    "number": 4754,
    "title": "tests: Update gb200 test case",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T04:45:51Z",
    "closed_at": "2025-06-04T10:49:20Z",
    "merged_at": "2025-06-04T10:49:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4754"
  },
  {
    "number": 4753,
    "title": "tests: [TRTQA-2905] improve timeout report for qa test cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T04:38:26Z",
    "closed_at": "2025-06-03T04:27:27Z",
    "merged_at": "2025-06-03T04:27:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4753"
  },
  {
    "number": 4752,
    "title": "[Docs] - Add date and commit info (#4448)",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T04:24:49Z",
    "closed_at": "2025-05-30T07:58:49Z",
    "merged_at": "2025-05-30T07:58:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4752"
  },
  {
    "number": 4751,
    "title": "tests: fix 5250460",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T04:06:26Z",
    "closed_at": "2025-05-30T02:13:46Z",
    "merged_at": "2025-05-30T02:13:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4751"
  },
  {
    "number": 4750,
    "title": "feat: Add Mixture of Experts FP8xMXFP4 support",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T03:49:53Z",
    "closed_at": "2025-06-09T05:25:05Z",
    "merged_at": "2025-06-09T05:25:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4750"
  },
  {
    "number": 4749,
    "title": "feat: cache reuse support (selective cache transfer) in mla cache formatter",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T03:31:08Z",
    "closed_at": "2025-06-04T01:56:31Z",
    "merged_at": "2025-06-04T01:56:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4749"
  },
  {
    "number": 4748,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T02:44:22Z",
    "closed_at": "2025-05-29T03:05:27Z",
    "merged_at": "2025-05-29T03:05:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4748"
  },
  {
    "number": 4747,
    "title": "[nvbugs/5302709] fix: Use HF vision tower for llava-next on A100",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T02:11:18Z",
    "closed_at": "2025-05-29T18:27:07Z",
    "merged_at": "2025-05-29T18:27:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4747"
  },
  {
    "number": 4746,
    "title": "Cherry-pick feat/llama4's changes",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T02:10:51Z",
    "closed_at": "2025-06-09T02:14:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4746"
  },
  {
    "number": 4744,
    "title": "[fix] Add back RTX6000Pro post-merge tests",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-29T01:03:04Z",
    "closed_at": "2025-05-29T05:17:34Z",
    "merged_at": "2025-05-29T05:17:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4744"
  },
  {
    "number": 4743,
    "title": "[nvbugs/5297821] Fix llama4 disaggregated serving accuracy tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T22:41:19Z",
    "closed_at": "2025-05-29T19:55:17Z",
    "merged_at": "2025-05-29T19:55:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4743"
  },
  {
    "number": 4742,
    "title": "fix: Set missing env variable WORLD_SIZE temporarily for trainer init",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T21:27:29Z",
    "closed_at": "2025-05-30T23:05:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4742"
  },
  {
    "number": 4741,
    "title": "chore: fix llm_root when LLM_ROOT is not set",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T20:16:08Z",
    "closed_at": "2025-05-30T02:44:34Z",
    "merged_at": "2025-05-30T02:44:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4741"
  },
  {
    "number": 4739,
    "title": "Cherry pick feat/llama4 to main",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T18:09:24Z",
    "closed_at": "2025-05-29T21:28:41Z",
    "merged_at": "2025-05-29T21:28:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4739"
  },
  {
    "number": 4738,
    "title": "chore: improve disagg test failure detection",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T16:29:31Z",
    "closed_at": "2025-06-14T17:28:26Z",
    "merged_at": "2025-06-14T17:28:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4738"
  },
  {
    "number": 4737,
    "title": "[feat] Enable NVFP4 output for TRTLLM attention kernels",
    "user": "Tom-Zheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T14:30:58Z",
    "closed_at": "2025-06-03T02:00:17Z",
    "merged_at": "2025-06-03T02:00:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4737"
  },
  {
    "number": 4735,
    "title": "[nvbugs/5303555] ci: unwaive test_fp8_block_scales_cuda_graph_padding",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T14:10:34Z",
    "closed_at": "2025-06-03T02:40:43Z",
    "merged_at": "2025-06-03T02:40:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4735"
  },
  {
    "number": 4734,
    "title": "fix: iteration logging and typing in PyExecutor",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T14:02:48Z",
    "closed_at": "2025-05-30T09:01:20Z",
    "merged_at": "2025-05-30T09:01:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4734"
  },
  {
    "number": 4733,
    "title": "CI: move post-merge multi GPU test of PyTorch backend to H200",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T11:13:34Z",
    "closed_at": "2025-05-29T03:15:57Z",
    "merged_at": "2025-05-29T03:15:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4733"
  },
  {
    "number": 4732,
    "title": "chore: Mass integration of release/0.20.",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T11:13:26Z",
    "closed_at": "2025-08-03T12:25:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4732"
  },
  {
    "number": 4731,
    "title": "[https://nvbugspro.nvidia.com/bug/5236935][Fix] Fix document of using Draft-Target-Model (DTM) speculative decoding in Triton Server",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T10:05:39Z",
    "closed_at": "2025-05-29T02:38:35Z",
    "merged_at": "2025-05-29T02:38:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4731"
  },
  {
    "number": 4729,
    "title": "[TRTLLM-5516] perf: replicate dummy request for cuda graph padding",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T08:50:38Z",
    "closed_at": "2025-05-30T09:14:23Z",
    "merged_at": "2025-05-30T09:14:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4729"
  },
  {
    "number": 4727,
    "title": "fix[nvbug5298640]: trtllm-llmapi-launch multiple LLM instances",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T07:40:24Z",
    "closed_at": "2025-06-18T22:13:53Z",
    "merged_at": "2025-06-18T22:13:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4727"
  },
  {
    "number": 4726,
    "title": "test: remove large bs as it will oom",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T07:27:58Z",
    "closed_at": "2025-05-29T06:31:57Z",
    "merged_at": "2025-05-29T06:31:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4726"
  },
  {
    "number": 4725,
    "title": "chore: [nvbug_5273941] unwaive test_llm_loading_from_ckpt_for_tp2",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T07:07:05Z",
    "closed_at": "2025-05-28T22:54:32Z",
    "merged_at": "2025-05-28T22:54:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4725"
  },
  {
    "number": 4724,
    "title": "fix: [https://nvbugspro.nvidia.com/bug/5286795] Unwaive tests for bug-5286795.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T07:05:19Z",
    "closed_at": "2025-05-28T16:51:23Z",
    "merged_at": "2025-05-28T16:51:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4724"
  },
  {
    "number": 4723,
    "title": "test CI",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T06:26:04Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4723"
  },
  {
    "number": 4722,
    "title": "tests: [https://nvbugspro.nvidia.com/bug/5289908] run maverick bf16 on blackwell",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T06:24:56Z",
    "closed_at": "2025-05-28T14:05:51Z",
    "merged_at": "2025-05-28T14:05:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4722"
  },
  {
    "number": 4721,
    "title": "[Architecture] Redesign Linear module",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T06:10:41Z",
    "closed_at": "2025-05-29T23:05:46Z",
    "merged_at": "2025-05-29T23:05:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4721"
  },
  {
    "number": 4720,
    "title": "test: remove perf test l40s/l20 oom test cases and unwaive tests",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T05:37:44Z",
    "closed_at": "2025-05-29T04:47:52Z",
    "merged_at": "2025-05-29T04:47:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4720"
  },
  {
    "number": 4719,
    "title": "feat: support packed weights in vanilla moe",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T05:23:38Z",
    "closed_at": "2025-05-28T22:24:24Z",
    "merged_at": "2025-05-28T22:24:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4719"
  },
  {
    "number": 4718,
    "title": "fix:https://nvbugs/5214239",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T05:08:48Z",
    "closed_at": "2025-05-29T01:36:31Z",
    "merged_at": "2025-05-29T01:36:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4718"
  },
  {
    "number": 4717,
    "title": "Fabric Memory for KV Cache Transfer",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T04:53:47Z",
    "closed_at": "2025-05-30T07:50:21Z",
    "merged_at": "2025-05-30T07:50:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4717"
  },
  {
    "number": 4716,
    "title": "chore: rename ExecutorBindingsWorker/Proxy",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T03:25:47Z",
    "closed_at": "2025-05-29T02:32:35Z",
    "merged_at": "2025-05-29T02:32:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4716"
  },
  {
    "number": 4715,
    "title": "Fix rerun step",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T02:25:52Z",
    "closed_at": "2025-05-28T09:01:06Z",
    "merged_at": "2025-05-28T09:01:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4715"
  },
  {
    "number": 4714,
    "title": "[nvbug 5294316] fix: Fix queued request stats",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-28T01:38:21Z",
    "closed_at": "2025-06-03T12:33:08Z",
    "merged_at": "2025-06-03T12:33:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4714"
  },
  {
    "number": 4713,
    "title": "fix: Mistral Small vision encoder with BS>1",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T22:49:30Z",
    "closed_at": "2025-05-28T04:49:28Z",
    "merged_at": "2025-05-28T04:49:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4713"
  },
  {
    "number": 4712,
    "title": "fix: Mistral Small vision encoder with BS>1",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T22:47:35Z",
    "closed_at": "2025-05-28T01:19:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4712"
  },
  {
    "number": 4711,
    "title": "chore: add -f to pkill calls",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T22:01:15Z",
    "closed_at": "2025-05-28T18:54:32Z",
    "merged_at": "2025-05-28T18:54:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4711"
  },
  {
    "number": 4710,
    "title": "Support loading other model's tokenizer for apple models",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T21:55:23Z",
    "closed_at": "2025-05-27T21:57:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4710"
  },
  {
    "number": 4709,
    "title": "[feat] add b200 support via slurm",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T21:46:24Z",
    "closed_at": "2025-05-29T06:49:47Z",
    "merged_at": "2025-05-29T06:49:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4709"
  },
  {
    "number": 4707,
    "title": "Remove disaggregated cuda graph waived test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T20:23:07Z",
    "closed_at": "2025-05-30T23:24:01Z",
    "merged_at": "2025-05-30T23:24:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4707"
  },
  {
    "number": 4706,
    "title": "chore: remove extra paths to find binaries",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T19:03:33Z",
    "closed_at": "2025-05-28T13:51:43Z",
    "merged_at": "2025-05-28T13:51:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4706"
  },
  {
    "number": 4705,
    "title": "doc: Document the docker release image on NGC",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T18:49:49Z",
    "closed_at": "2025-05-28T04:00:53Z",
    "merged_at": "2025-05-28T04:00:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4705"
  },
  {
    "number": 4703,
    "title": "Remove V1 batching tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T16:40:08Z",
    "closed_at": "2025-05-28T21:57:58Z",
    "merged_at": "2025-05-28T21:57:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4703"
  },
  {
    "number": 4702,
    "title": "Update the description for NGC docker images (#4671)",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T16:26:42Z",
    "closed_at": "2025-05-27T16:44:35Z",
    "merged_at": "2025-05-27T16:44:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4702"
  },
  {
    "number": 4700,
    "title": "refactor: Separate DecoderState from GptDecoderBatched",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T12:33:00Z",
    "closed_at": "2025-06-03T07:42:02Z",
    "merged_at": "2025-06-03T07:42:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4700"
  },
  {
    "number": 4699,
    "title": "fix: [nvbugs/5289912][nvbugs/5232406] use thread pool for multi-thread weight loading in fused moe.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T12:21:51Z",
    "closed_at": "2025-05-28T00:13:07Z",
    "merged_at": "2025-05-28T00:13:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4699"
  },
  {
    "number": 4698,
    "title": "fix:https://nvbugs/5305692 update invalid links in doc.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T11:47:28Z",
    "closed_at": "2025-05-28T09:24:56Z",
    "merged_at": "2025-05-28T09:24:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4698"
  },
  {
    "number": 4697,
    "title": "refactor: unique_ptr instead of shared_ptr",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T11:27:09Z",
    "closed_at": "2025-05-29T20:49:35Z",
    "merged_at": "2025-05-29T20:49:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4697"
  },
  {
    "number": 4695,
    "title": "feat: large-scale EP(part 5: Static EP load balancer with offline statistics)",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T10:41:50Z",
    "closed_at": "2025-06-01T17:25:03Z",
    "merged_at": "2025-06-01T17:25:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4695"
  },
  {
    "number": 4694,
    "title": "use cu for fmha_v2",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T09:52:09Z",
    "closed_at": "2025-06-15T10:40:44Z",
    "merged_at": "2025-06-15T10:40:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4694"
  },
  {
    "number": 4693,
    "title": "[https://nvbugspro.nvidia.com/bug/5300080] Fix the bug of setting attention_chunk_size and enable chunked-attention in the generation-phase by default",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T09:40:08Z",
    "closed_at": "2025-06-03T23:02:58Z",
    "merged_at": "2025-06-03T23:02:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4693"
  },
  {
    "number": 4692,
    "title": "Refactor the first token response in PD",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T09:30:42Z",
    "closed_at": "2025-06-04T01:11:23Z",
    "merged_at": "2025-06-04T01:11:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4692"
  },
  {
    "number": 4691,
    "title": "[TRTLLM-5326] - Fix test coverage report generation",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T08:23:45Z",
    "closed_at": "2025-05-27T10:24:30Z",
    "merged_at": "2025-05-27T10:24:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4691"
  },
  {
    "number": 4690,
    "title": "fix: handle OOMs during KV cache estimation",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T07:54:08Z",
    "closed_at": "2025-06-05T08:02:26Z",
    "merged_at": "2025-06-05T08:02:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4690"
  },
  {
    "number": 4689,
    "title": "Chore: fuse _merge_requests method into _fetch_new_requests method",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T07:48:43Z",
    "closed_at": "2025-05-29T09:47:44Z",
    "merged_at": "2025-05-29T09:47:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4689"
  },
  {
    "number": 4688,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T07:19:42Z",
    "closed_at": "2025-05-28T14:04:35Z",
    "merged_at": "2025-05-28T14:04:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4688"
  },
  {
    "number": 4687,
    "title": "Dev/no venv",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T06:41:19Z",
    "closed_at": "2025-05-27T06:48:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4687"
  },
  {
    "number": 4686,
    "title": "Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T06:40:07Z",
    "closed_at": "2025-05-27T07:07:02Z",
    "merged_at": "2025-05-27T07:07:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4686"
  },
  {
    "number": 4685,
    "title": "tests: fix 5273697",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T06:34:49Z",
    "closed_at": "2025-06-05T02:39:21Z",
    "merged_at": "2025-06-05T02:39:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4685"
  },
  {
    "number": 4683,
    "title": "[AutoDeploy] Increased Model Coverage Mass Migration Week 2",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T05:52:28Z",
    "closed_at": "2025-05-27T18:32:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4683"
  },
  {
    "number": 4682,
    "title": "feat: Add vanilla MOE.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T05:51:54Z",
    "closed_at": "2025-05-28T02:44:14Z",
    "merged_at": "2025-05-28T02:44:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4682"
  },
  {
    "number": 4681,
    "title": "refactor: extract and reuse filter_weights.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T05:40:13Z",
    "closed_at": "2025-05-27T11:48:01Z",
    "merged_at": "2025-05-27T11:48:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4681"
  },
  {
    "number": 4680,
    "title": "[Test] - Correct waive the Slurm test stage",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T04:48:23Z",
    "closed_at": "2025-05-27T05:34:50Z",
    "merged_at": "2025-05-27T05:34:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4680"
  },
  {
    "number": 4679,
    "title": "[fix] add back rtx6000pro tests",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T04:28:23Z",
    "closed_at": "2025-05-29T01:22:53Z",
    "merged_at": "2025-05-29T01:22:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4679"
  },
  {
    "number": 4678,
    "title": "Fix: hang on disagg when MNNVL two-shot AllReduce is enabled",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T03:45:31Z",
    "closed_at": "2025-05-28T05:03:54Z",
    "merged_at": "2025-05-28T05:03:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4678"
  },
  {
    "number": 4677,
    "title": "[Test] - Correct waive the Slurm test stage",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T03:26:35Z",
    "closed_at": "2025-05-27T04:35:43Z",
    "merged_at": "2025-05-27T04:35:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4677"
  },
  {
    "number": 4676,
    "title": "[https://nvbugs/5303634] skip evaluating empty batch_input_ids in summarize.py",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T02:52:38Z",
    "closed_at": "2025-06-02T08:16:05Z",
    "merged_at": "2025-06-02T08:16:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4676"
  },
  {
    "number": 4675,
    "title": "[NVBUG-5291971] JIT path for XQA",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-27T02:15:50Z",
    "closed_at": "2025-06-02T14:25:00Z",
    "merged_at": "2025-06-02T14:25:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4675"
  },
  {
    "number": 4673,
    "title": "[TRTLLM-4958][feat] improve multimodal workflow with Vision + LLM inflight batching",
    "user": "symphonylyh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T21:43:18Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4673"
  },
  {
    "number": 4672,
    "title": "[Test] - Waive RTX Pro 6000 Slurm testing",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T17:01:38Z",
    "closed_at": "2025-05-26T17:17:22Z",
    "merged_at": "2025-05-26T17:17:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4672"
  },
  {
    "number": 4671,
    "title": "Update the description for NGC docker images",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T16:38:16Z",
    "closed_at": "2025-05-27T02:57:39Z",
    "merged_at": "2025-05-27T02:57:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4671"
  },
  {
    "number": 4670,
    "title": "fix: Fix AutoTuner warmup request generating.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T15:10:02Z",
    "closed_at": "2025-05-28T16:25:58Z",
    "merged_at": "2025-05-28T16:25:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4670"
  },
  {
    "number": 4667,
    "title": "Solve underallocation in VSWA+/VGQA ",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T13:38:11Z",
    "closed_at": "2025-06-12T04:12:46Z",
    "merged_at": "2025-06-12T04:12:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4667"
  },
  {
    "number": 4665,
    "title": "Add more logs and checks in test cases [verify on main]",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T12:44:10Z",
    "closed_at": "2025-05-27T13:38:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4665"
  },
  {
    "number": 4664,
    "title": "Chore: only pad one dummy request for attention dp scenario",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T10:12:55Z",
    "closed_at": "2025-05-27T06:56:23Z",
    "merged_at": "2025-05-27T06:56:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4664"
  },
  {
    "number": 4663,
    "title": "Add files into scan ignoreList",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T10:11:09Z",
    "closed_at": "2025-05-26T11:43:05Z",
    "merged_at": "2025-05-26T11:43:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4663"
  },
  {
    "number": 4662,
    "title": "[NVBUG 5301980] Fix fp4 gemm padding.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T09:41:22Z",
    "closed_at": "2025-05-27T03:30:54Z",
    "merged_at": "2025-05-27T03:30:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4662"
  },
  {
    "number": 4661,
    "title": "fix fmha v2 tests",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T09:36:21Z",
    "closed_at": "2025-05-27T01:47:01Z",
    "merged_at": "2025-05-27T01:47:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4661"
  },
  {
    "number": 4660,
    "title": "fix: [nvbug5300494] Use runtime total gpu memory to calculate kv cache memory and log more memory information",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T08:51:06Z",
    "closed_at": "2025-05-28T02:00:20Z",
    "merged_at": "2025-05-28T02:00:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4660"
  },
  {
    "number": 4659,
    "title": "fix: fmha_v2 compilation",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T08:44:42Z",
    "closed_at": "2025-05-27T09:39:39Z",
    "merged_at": "2025-05-27T09:39:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4659"
  },
  {
    "number": 4658,
    "title": "[https://nvbugs/5294983][fix] unwaive TestDeepSeekV3Lite::test_fp8_block_scales_4gpus",
    "user": "zhhuang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T08:24:53Z",
    "closed_at": "2025-06-09T03:54:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4658"
  },
  {
    "number": 4657,
    "title": "[5289904] chore: Unwaive test for Qwen model.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T08:01:39Z",
    "closed_at": "2025-06-09T06:06:59Z",
    "merged_at": "2025-06-09T06:06:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4657"
  },
  {
    "number": 4656,
    "title": "infra: [TRTLLM-5250] Add sanity check stage for ngc-release images (Build wheels for devel image)",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T07:52:28Z",
    "closed_at": "2025-07-21T08:06:43Z",
    "merged_at": "2025-07-21T08:06:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4656"
  },
  {
    "number": 4655,
    "title": "[fix] Fix SamplingParams check on n and best_of",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T07:37:54Z",
    "closed_at": "2025-06-01T01:11:55Z",
    "merged_at": "2025-06-01T01:11:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4655"
  },
  {
    "number": 4654,
    "title": "[TRTLLM-5327] - Fix guardwords scan step",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T06:40:11Z",
    "closed_at": "2025-05-26T07:42:51Z",
    "merged_at": "2025-05-26T07:42:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4654"
  },
  {
    "number": 4652,
    "title": "User/nvpohanh/pull out to min latency py",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T05:08:23Z",
    "closed_at": "2025-05-27T08:53:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4652"
  },
  {
    "number": 4651,
    "title": "feat: chunked prefill for MLA (Blackwell)",
    "user": "jmydurant",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T04:13:03Z",
    "closed_at": "2025-06-26T01:01:01Z",
    "merged_at": "2025-06-26T01:01:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4651"
  },
  {
    "number": 4650,
    "title": "fix nvbug 5302895",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T04:03:00Z",
    "closed_at": "2025-06-04T02:09:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4650"
  },
  {
    "number": 4649,
    "title": "Chore: introduce RequestQueueItem class instead of using tuple",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T04:02:06Z",
    "closed_at": "2025-05-26T09:34:54Z",
    "merged_at": "2025-05-26T09:34:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4649"
  },
  {
    "number": 4648,
    "title": "Fix handle cancel request for attentionDP",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T03:09:39Z",
    "closed_at": "2025-05-28T03:04:02Z",
    "merged_at": "2025-05-28T03:04:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4648"
  },
  {
    "number": 4647,
    "title": "[fix] Unwaive torch compile tests",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T03:04:57Z",
    "closed_at": "2025-06-09T05:51:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4647"
  },
  {
    "number": 4646,
    "title": "fix disagg config params",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T02:43:50Z",
    "closed_at": "2025-05-26T15:28:52Z",
    "merged_at": "2025-05-26T15:28:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4646"
  },
  {
    "number": 4645,
    "title": "Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T02:30:38Z",
    "closed_at": "2025-05-26T03:05:01Z",
    "merged_at": "2025-05-26T03:05:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4645"
  },
  {
    "number": 4644,
    "title": "tests: waive and unwaive QA test cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T02:21:00Z",
    "closed_at": "2025-05-27T07:19:46Z",
    "merged_at": "2025-05-27T07:19:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4644"
  },
  {
    "number": 4643,
    "title": "feat: update DeepSeek FP8 TRT-LLM Gen cubins",
    "user": "nekorobov",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T00:04:28Z",
    "closed_at": "2025-06-03T21:07:54Z",
    "merged_at": "2025-06-03T21:07:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4643"
  },
  {
    "number": 4642,
    "title": "Add missing serialization classes",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-25T22:13:14Z",
    "closed_at": "2025-05-28T08:40:24Z",
    "merged_at": "2025-05-28T08:40:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4642"
  },
  {
    "number": 4640,
    "title": "fix: Update approved list to fix pipeline tests after rebasing",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-25T19:07:48Z",
    "closed_at": "2025-05-26T00:38:08Z",
    "merged_at": "2025-05-26T00:38:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4640"
  },
  {
    "number": 4639,
    "title": "[#3334][feat] Support of CPU Inference for Scaffolding via PyTorch",
    "user": "amemov",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-25T14:36:56Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4639"
  },
  {
    "number": 4638,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-25T12:14:59Z",
    "closed_at": "2025-05-27T10:08:00Z",
    "merged_at": "2025-05-27T10:07:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4638"
  },
  {
    "number": 4637,
    "title": "Temp",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-25T09:11:19Z",
    "closed_at": "2025-05-26T13:12:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4637"
  },
  {
    "number": 4636,
    "title": "Update main README.md with the LLaMA4 perf news",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-25T07:29:20Z",
    "closed_at": "2025-05-25T08:57:49Z",
    "merged_at": "2025-05-25T08:57:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4636"
  },
  {
    "number": 4635,
    "title": "Use backend to replace macro to control enablement of MNNVL all reduce",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-25T01:09:41Z",
    "closed_at": "2025-06-12T03:22:50Z",
    "merged_at": "2025-06-12T03:22:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4635"
  },
  {
    "number": 4634,
    "title": "[#4633][doc] Fixed typo in scaffolding README.md",
    "user": "amemov",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-24T21:41:28Z",
    "closed_at": "2025-05-25T01:04:12Z",
    "merged_at": "2025-05-25T01:04:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4634"
  },
  {
    "number": 4632,
    "title": "chore: Change the type annotations of input_ids and position_ids to int32.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-24T17:04:24Z",
    "closed_at": "2025-06-07T08:10:47Z",
    "merged_at": "2025-06-07T08:10:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4632"
  },
  {
    "number": 4631,
    "title": "fix: Mistral Small vision encoder with BS>1",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T23:01:06Z",
    "closed_at": "2025-05-28T01:23:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4631"
  },
  {
    "number": 4630,
    "title": "[TRTLLM-4971]: Use safe deserialization in ParallelConfig",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T20:46:32Z",
    "closed_at": "2025-06-27T01:58:42Z",
    "merged_at": "2025-06-27T01:58:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4630"
  },
  {
    "number": 4629,
    "title": "[nvbugs/5300689] Fix V1 static batching tests for Triton backend",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T19:05:14Z",
    "closed_at": "2025-09-08T16:25:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4629"
  },
  {
    "number": 4628,
    "title": "Fix RoPE args extraction",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T17:57:07Z",
    "closed_at": "2025-05-23T17:57:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4628"
  },
  {
    "number": 4627,
    "title": "[CI] Waive known errors with test TestDeepSeekV3Lite::test_fp8_block_scales_4gpus",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T17:08:16Z",
    "closed_at": "2025-05-23T17:33:44Z",
    "merged_at": "2025-05-23T17:33:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4627"
  },
  {
    "number": 4626,
    "title": "Fix invalid testcase name",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T16:27:38Z",
    "closed_at": "2025-05-23T16:40:00Z",
    "merged_at": "2025-05-23T16:40:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4626"
  },
  {
    "number": 4624,
    "title": "[https://nvbugs/5295389][fix]fix moe fp4 on sm120",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T16:13:01Z",
    "closed_at": "2025-05-29T16:50:47Z",
    "merged_at": "2025-05-29T16:50:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4624"
  },
  {
    "number": 4623,
    "title": "[TRTLLM-1658][feat] Enable multiple response in trtllm-serve for TRT backend",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T14:40:26Z",
    "closed_at": "2025-05-28T03:36:45Z",
    "merged_at": "2025-05-28T03:36:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4623"
  },
  {
    "number": 4622,
    "title": "[nvbugs/5301636] ci: skip test test_trtllm_bench_llmapi_launch",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T14:20:18Z",
    "closed_at": "2025-06-26T12:56:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4622"
  },
  {
    "number": 4621,
    "title": "[nvbugs/5274894] fix: Sort requests for functional correctness and performance (adapted from #4608)",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T13:24:42Z",
    "closed_at": "2025-05-26T09:10:55Z",
    "merged_at": "2025-05-26T09:10:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4621"
  },
  {
    "number": 4620,
    "title": "Update CODEOWNERS for PyTorch backend - runtime component",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T12:29:21Z",
    "closed_at": "2025-05-23T12:40:45Z",
    "merged_at": "2025-05-23T12:40:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4620"
  },
  {
    "number": 4619,
    "title": "Update internal cutlass kernels commit id",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T11:58:37Z",
    "closed_at": "2025-05-23T12:07:42Z",
    "merged_at": "2025-05-23T12:07:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4619"
  },
  {
    "number": 4618,
    "title": "Reorgnize code structure in FusedMoe",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T11:55:37Z",
    "closed_at": "2025-05-27T13:42:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4618"
  },
  {
    "number": 4617,
    "title": "[nvbugs/5301492] ci: waive test_workers_kv_cache_aware_router",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T11:50:40Z",
    "closed_at": "2025-05-23T12:14:29Z",
    "merged_at": "2025-05-23T12:14:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4617"
  },
  {
    "number": 4616,
    "title": "ci: waive testcase [NVBUG 5297821]",
    "user": "stnie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T10:00:12Z",
    "closed_at": "2025-05-23T12:54:43Z",
    "merged_at": "2025-05-23T12:54:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4616"
  },
  {
    "number": 4615,
    "title": "feat: large-scale EP(part 4: Static EP load balancer integration)",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T09:57:30Z",
    "closed_at": "2025-05-26T10:25:11Z",
    "merged_at": "2025-05-26T10:25:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4615"
  },
  {
    "number": 4614,
    "title": "Chore: refine shutdown signal of PyExecutor",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T09:50:25Z",
    "closed_at": "2025-05-26T03:14:55Z",
    "merged_at": "2025-05-26T03:14:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4614"
  },
  {
    "number": 4613,
    "title": "fix: test trtllm-bench mgmn",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T09:31:21Z",
    "closed_at": "2025-05-29T06:43:48Z",
    "merged_at": "2025-05-29T06:43:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4613"
  },
  {
    "number": 4612,
    "title": "Update fmha v2 and switch to cu",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T09:16:55Z",
    "closed_at": "2025-05-28T04:15:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4612"
  },
  {
    "number": 4611,
    "title": "feat: Integration of Fused QKNorm+RoPE.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T08:59:14Z",
    "closed_at": "2025-05-28T03:20:46Z",
    "merged_at": "2025-05-28T03:20:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4611"
  },
  {
    "number": 4610,
    "title": "Update the GH main page to expose tech blogs ",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T08:51:27Z",
    "closed_at": "2025-05-23T09:03:56Z",
    "merged_at": "2025-05-23T09:03:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4610"
  },
  {
    "number": 4609,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T07:22:31Z",
    "closed_at": "2025-05-23T07:54:12Z",
    "merged_at": "2025-05-23T07:54:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4609"
  },
  {
    "number": 4608,
    "title": "[nvbugs/5274894] fix: Sort requests for functional correctness and performance",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T07:19:40Z",
    "closed_at": "2025-05-23T13:08:55Z",
    "merged_at": "2025-05-23T13:08:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4608"
  },
  {
    "number": 4607,
    "title": "chore: sort llm request state enums in chronological order",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T07:15:43Z",
    "closed_at": "2025-05-26T05:47:01Z",
    "merged_at": "2025-05-26T05:47:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4607"
  },
  {
    "number": 4606,
    "title": "fix: datatype check in the cache transmission",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T06:53:03Z",
    "closed_at": "2025-05-24T00:36:18Z",
    "merged_at": "2025-05-24T00:36:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4606"
  },
  {
    "number": 4605,
    "title": "[test] Unwaive testcases",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T06:42:48Z",
    "closed_at": "2025-06-16T01:57:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4605"
  },
  {
    "number": 4604,
    "title": "[fix] Incorrect mocker argument for a CLI accuracy test in Llama-3.3-70B-Instruct",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T06:35:14Z",
    "closed_at": "2025-05-23T19:18:37Z",
    "merged_at": "2025-05-23T19:18:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4604"
  },
  {
    "number": 4603,
    "title": "chore [BREAKING CHANGE]: Flatten PyTorchConfig knobs into TorchLlmArgs",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T06:02:47Z",
    "closed_at": "2025-05-28T10:43:05Z",
    "merged_at": "2025-05-28T10:43:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4603"
  },
  {
    "number": 4602,
    "title": "[TRTLLM-5327] - Add scan stage",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T04:55:04Z",
    "closed_at": "2025-05-25T00:55:08Z",
    "merged_at": "2025-05-25T00:55:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4602"
  },
  {
    "number": 4600,
    "title": "fix: build_config in TorchLlmArgs and avoid invalid args",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T03:45:10Z",
    "closed_at": "2025-06-04T05:17:29Z",
    "merged_at": "2025-06-04T05:17:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4600"
  },
  {
    "number": 4598,
    "title": "test: fix for perf sanity test and skip fp8 deepseek blackwell cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T02:12:51Z",
    "closed_at": "2025-05-23T03:13:15Z",
    "merged_at": "2025-05-23T03:13:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4598"
  },
  {
    "number": 4597,
    "title": "fix: random fail of cache router test",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-23T01:54:06Z",
    "closed_at": "2025-05-30T08:28:19Z",
    "merged_at": "2025-05-30T08:28:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4597"
  },
  {
    "number": 4596,
    "title": "[JIRA-5226219][fix] Fix Bug in KV cache manager",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T23:33:32Z",
    "closed_at": "2025-05-30T05:03:21Z",
    "merged_at": "2025-05-30T05:03:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4596"
  },
  {
    "number": 4594,
    "title": "[TRTLLM-4647][fix] Fix the no fusion allreduce hanging",
    "user": "timlee0212",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T22:22:18Z",
    "closed_at": "2025-06-05T01:26:13Z",
    "merged_at": "2025-06-05T01:26:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4594"
  },
  {
    "number": 4592,
    "title": "[feat] Support RULER + chunked prefill in lm-eval-harness",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T22:08:05Z",
    "closed_at": "2025-05-29T20:14:01Z",
    "merged_at": "2025-05-29T20:14:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4592"
  },
  {
    "number": 4591,
    "title": "Add missing rcca folder",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T21:11:42Z",
    "closed_at": "2025-05-23T19:28:10Z",
    "merged_at": "2025-05-23T19:28:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4591"
  },
  {
    "number": 4590,
    "title": "[cherry-pick] test(perf): Add Llama-3_1-Nemotron-Ultra-253B-v1 perf tests (cpp) (#4446)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T20:10:20Z",
    "closed_at": "2025-05-28T06:17:24Z",
    "merged_at": "2025-05-28T06:17:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4590"
  },
  {
    "number": 4589,
    "title": "[cherry-pick] test(perf): Add remaining `Phi-4-mini-instruct` perf tests (#4443)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T19:51:30Z",
    "closed_at": "2025-05-28T06:17:19Z",
    "merged_at": "2025-05-28T06:17:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4589"
  },
  {
    "number": 4588,
    "title": "[cherry-pick] test(perf): Pt.2 Add Llama-3_3-Nemotron-Super-49B-v1 integration-perf-tests (cpp) (#4499)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T19:46:44Z",
    "closed_at": "2025-05-28T07:48:02Z",
    "merged_at": "2025-05-28T07:48:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4588"
  },
  {
    "number": 4587,
    "title": "Patch torch.ops.linear.simple to always gets bias arg in export pipeline",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T18:59:55Z",
    "closed_at": "2025-05-22T19:00:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4587"
  },
  {
    "number": 4583,
    "title": "None: Make add_special_tokens to false for completions API",
    "user": "Pernekhan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T17:54:47Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4583"
  },
  {
    "number": 4582,
    "title": "LLama4: small fixes",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T16:44:35Z",
    "closed_at": "2025-08-21T01:27:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4582"
  },
  {
    "number": 4581,
    "title": "chore: introduce KvCacheCreator",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T16:32:44Z",
    "closed_at": "2025-06-04T09:03:17Z",
    "merged_at": "2025-06-04T09:03:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4581"
  },
  {
    "number": 4580,
    "title": "[doc]: add mtp tech blog",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T15:11:50Z",
    "closed_at": "2025-05-23T05:54:22Z",
    "merged_at": "2025-05-23T05:54:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4580"
  },
  {
    "number": 4578,
    "title": "[feat] support sharegpt downloading in benchmark_serving",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T13:39:11Z",
    "closed_at": "2025-05-30T09:27:54Z",
    "merged_at": "2025-05-30T09:27:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4578"
  },
  {
    "number": 4577,
    "title": "Mass-integration 0.20 to main",
    "user": "amirkl94",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T12:38:46Z",
    "closed_at": "2025-05-28T08:25:34Z",
    "merged_at": "2025-05-28T08:25:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4577"
  },
  {
    "number": 4576,
    "title": "[nvbugs/5274894] fix: Moving finished context requests to generation",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T12:17:38Z",
    "closed_at": "2025-05-22T15:49:40Z",
    "merged_at": "2025-05-22T15:49:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4576"
  },
  {
    "number": 4575,
    "title": "[Fix][Qwen3] fix bug of qwen3 fp4 workflow with EP",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T11:49:53Z",
    "closed_at": "2025-05-23T05:34:05Z",
    "merged_at": "2025-05-23T05:34:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4575"
  },
  {
    "number": 4574,
    "title": "feat: estimate GPU mem. usage w/ minimal KV cache",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T11:49:36Z",
    "closed_at": "2025-05-30T08:40:45Z",
    "merged_at": "2025-05-30T08:40:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4574"
  },
  {
    "number": 4573,
    "title": "fix[nvbug-5295425]: [TRTLLM-5385] fix race condition in MoeLoadBalancer",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T11:33:42Z",
    "closed_at": "2025-05-23T01:24:24Z",
    "merged_at": "2025-05-23T01:24:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4573"
  },
  {
    "number": 4572,
    "title": "Add CODEOWNERS for PyTorch backend runtime component",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T11:32:13Z",
    "closed_at": "2025-05-23T08:44:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4572"
  },
  {
    "number": 4571,
    "title": "Chore: clean up _gather_dp_requests_num method of PyExecutor",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T10:50:09Z",
    "closed_at": "2025-05-23T00:37:40Z",
    "merged_at": "2025-05-23T00:37:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4571"
  },
  {
    "number": 4570,
    "title": "[TRTLLM-4413][infra] Add additional nightly build of wheel package with local version besides ordinary one",
    "user": "zhanga5",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T10:01:57Z",
    "closed_at": "2025-09-02T01:06:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4570"
  },
  {
    "number": 4569,
    "title": "[TRTLLM-5000][feat] NGrams V2",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T09:57:42Z",
    "closed_at": "2025-06-27T15:00:18Z",
    "merged_at": "2025-06-27T15:00:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4569"
  },
  {
    "number": 4568,
    "title": "[TRTLLM-5326] - Fix test coverage report generation",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T09:38:26Z",
    "closed_at": "2025-05-27T08:20:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4568"
  },
  {
    "number": 4567,
    "title": "chroe:clean useless flag",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T09:15:11Z",
    "closed_at": "2025-05-22T23:05:15Z",
    "merged_at": "2025-05-22T23:05:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4567"
  },
  {
    "number": 4566,
    "title": "chore: fix bug of llama lora test",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T08:56:26Z",
    "closed_at": "2025-05-23T06:06:40Z",
    "merged_at": "2025-05-23T06:06:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4566"
  },
  {
    "number": 4565,
    "title": "fix sequence data race",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T08:21:14Z",
    "closed_at": "2025-05-22T15:13:49Z",
    "merged_at": "2025-05-22T15:13:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4565"
  },
  {
    "number": 4564,
    "title": "Feat: add sliding-window-attention generation-phase kernels on Blackwell",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T08:08:40Z",
    "closed_at": "2025-05-26T01:06:34Z",
    "merged_at": "2025-05-26T01:06:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4564"
  },
  {
    "number": 4563,
    "title": "test: waive hanging cases for perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T07:43:04Z",
    "closed_at": "2025-05-22T13:09:12Z",
    "merged_at": "2025-05-22T13:09:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4563"
  },
  {
    "number": 4562,
    "title": "test: waive hanging cases for perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T07:41:34Z",
    "closed_at": "2025-05-22T07:50:05Z",
    "merged_at": "2025-05-22T07:50:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4562"
  },
  {
    "number": 4561,
    "title": "fix: fix dsr1 min lat cga ar rate drop(0.2)",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T07:21:07Z",
    "closed_at": "2025-05-27T13:59:58Z",
    "merged_at": "2025-05-27T13:59:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4561"
  },
  {
    "number": 4560,
    "title": "Feat/ds r1 min latency opt round3, add router gemm, fused a gemm, PDL ",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T07:08:11Z",
    "closed_at": "2025-06-14T09:36:22Z",
    "merged_at": "2025-06-14T09:36:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4560"
  },
  {
    "number": 4559,
    "title": "Fix snake case format",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T06:53:13Z",
    "closed_at": "2025-05-25T09:57:17Z",
    "merged_at": "2025-05-25T09:57:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4559"
  },
  {
    "number": 4558,
    "title": "[TRTLLM-3456] Speculation: Draft Target in new FW",
    "user": "IzzyPutterman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T06:12:30Z",
    "closed_at": "2025-06-16T18:26:09Z",
    "merged_at": "2025-06-16T18:26:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4558"
  },
  {
    "number": 4557,
    "title": "Cherry-pick feat/llama4's updates",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T05:30:39Z",
    "closed_at": "2025-05-26T01:26:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4557"
  },
  {
    "number": 4556,
    "title": "cache_transceiver_config",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T05:23:53Z",
    "closed_at": "2025-05-22T05:59:52Z",
    "merged_at": "2025-05-22T05:59:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4556"
  },
  {
    "number": 4555,
    "title": "fix: Fix moe_ep_groups/moe_cluster_groups in Mapping.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T04:11:28Z",
    "closed_at": "2025-05-23T02:41:49Z",
    "merged_at": "2025-05-23T02:41:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4555"
  },
  {
    "number": 4554,
    "title": "Feat/ds r1 min latency opt round3, add router gemm, fused a gemm, PDL ",
    "user": "yunruis",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T04:07:44Z",
    "closed_at": "2025-05-22T06:00:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4554"
  },
  {
    "number": 4553,
    "title": "[https://nvbugs/5297775] fix: Correct memory guard for large MOE tests to account for TP space",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T04:03:18Z",
    "closed_at": "2025-05-23T02:57:49Z",
    "merged_at": "2025-05-23T02:57:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4553"
  },
  {
    "number": 4552,
    "title": "[Infra]Remove some old keyword",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T03:56:56Z",
    "closed_at": "2025-05-31T05:50:45Z",
    "merged_at": "2025-05-31T05:50:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4552"
  },
  {
    "number": 4551,
    "title": "Feat/staging 05 21",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T03:43:40Z",
    "closed_at": "2025-05-22T03:44:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4551"
  },
  {
    "number": 4550,
    "title": "fix: llmapi-launch add add trtllm-bench test with engine building (#4…",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-22T00:43:32Z",
    "closed_at": "2025-06-01T00:38:02Z",
    "merged_at": "2025-06-01T00:38:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4550"
  },
  {
    "number": 4549,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T23:03:58Z",
    "closed_at": "2025-05-22T09:18:54Z",
    "merged_at": "2025-05-22T09:18:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4549"
  },
  {
    "number": 4548,
    "title": "[TR[TLLM-4618][feat] Add remaining NVFP4 Nemotron Super 49B test on RTX6000 Pro (SM120)",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T20:03:57Z",
    "closed_at": "2025-05-23T17:42:33Z",
    "merged_at": "2025-05-23T17:42:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4548"
  },
  {
    "number": 4546,
    "title": "Add two MTP disaggregated test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T18:34:41Z",
    "closed_at": "2025-06-13T04:17:46Z",
    "merged_at": "2025-06-13T04:17:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4546"
  },
  {
    "number": 4545,
    "title": "[https://nvbugs/5289907][fix] Restore per-channel pre-quant",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T15:36:18Z",
    "closed_at": "2025-05-23T11:46:53Z",
    "merged_at": "2025-05-23T11:46:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4545"
  },
  {
    "number": 4544,
    "title": "Add tritonrelease container",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T14:29:06Z",
    "closed_at": "2025-05-21T17:48:30Z",
    "merged_at": "2025-05-21T17:48:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4544"
  },
  {
    "number": 4543,
    "title": "chore: update transformers version to 4.52.3",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T13:23:42Z",
    "closed_at": "2025-06-10T04:24:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4543"
  },
  {
    "number": 4542,
    "title": "chore: update transformers version to 4.53.0",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T13:16:12Z",
    "closed_at": "2025-07-09T07:03:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4542"
  },
  {
    "number": 4541,
    "title": "fix: Move cv2 import to load_video function",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T13:13:04Z",
    "closed_at": "2025-05-22T15:56:10Z",
    "merged_at": "2025-05-22T15:56:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4541"
  },
  {
    "number": 4540,
    "title": "chore:  clean-up for header file.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T13:11:50Z",
    "closed_at": "2025-05-23T02:08:14Z",
    "merged_at": "2025-05-23T02:08:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4540"
  },
  {
    "number": 4539,
    "title": "fix: [TRTLLM-325]WAR against security vulnerabilities in Python packages",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T12:54:48Z",
    "closed_at": "2025-05-22T00:33:21Z",
    "merged_at": "2025-05-22T00:33:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4539"
  },
  {
    "number": 4538,
    "title": "[TRTLLM-4987][feat] Support context logits in TRTLLMSampler",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T12:50:34Z",
    "closed_at": "2025-05-31T19:32:43Z",
    "merged_at": "2025-05-31T19:32:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4538"
  },
  {
    "number": 4537,
    "title": "chore: Add all_reduce.py benchmark script to test",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T12:20:02Z",
    "closed_at": "2025-05-22T02:13:27Z",
    "merged_at": "2025-05-22T02:13:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4537"
  },
  {
    "number": 4536,
    "title": "[https://nvbugs/5271281][fix] fix a pd+mtp accuracy issue",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T11:55:03Z",
    "closed_at": "2025-06-03T02:03:35Z",
    "merged_at": "2025-06-03T02:03:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4536"
  },
  {
    "number": 4535,
    "title": "[TRTLLM-5070][feat] Support FP8 KV Cache Reuse for MLA",
    "user": "zhhuang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T11:36:49Z",
    "closed_at": "2025-05-23T11:47:50Z",
    "merged_at": "2025-05-23T11:47:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4535"
  },
  {
    "number": 4534,
    "title": "fix: rename some terms",
    "user": "lowsfer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T11:10:32Z",
    "closed_at": "2025-05-23T15:23:50Z",
    "merged_at": "2025-05-23T15:23:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4534"
  },
  {
    "number": 4533,
    "title": "[DON'T MERGE] NGram V2 draft",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T10:59:33Z",
    "closed_at": "2025-06-09T12:59:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4533"
  },
  {
    "number": 4532,
    "title": "fix: max_num_sequences calculation with overlap scheduling",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T10:05:53Z",
    "closed_at": "2025-06-03T07:31:22Z",
    "merged_at": "2025-06-03T07:31:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4532"
  },
  {
    "number": 4531,
    "title": "chore: clean ucx and nixl mirror.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T09:16:17Z",
    "closed_at": "2025-05-21T11:45:21Z",
    "merged_at": "2025-05-21T11:45:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4531"
  },
  {
    "number": 4530,
    "title": "Qwen3 supports TRTLLM FP4 MoE backend",
    "user": "rosenrodt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T08:42:58Z",
    "closed_at": "2025-05-23T10:31:09Z",
    "merged_at": "2025-05-23T10:31:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4530"
  },
  {
    "number": 4529,
    "title": "fix[nvbug/5286515]: trtllm-llmapi-launch on single node single gpu ",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T08:28:06Z",
    "closed_at": "2025-05-27T07:19:05Z",
    "merged_at": "2025-05-27T07:19:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4529"
  },
  {
    "number": 4528,
    "title": "test: add failed case in waive list and fix some test script issue for perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T07:55:43Z",
    "closed_at": "2025-05-21T08:36:32Z",
    "merged_at": "2025-05-21T08:36:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4528"
  },
  {
    "number": 4527,
    "title": "test: add failed case in waive list and fix some test script issue for perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T07:45:49Z",
    "closed_at": "2025-05-21T08:37:26Z",
    "merged_at": "2025-05-21T08:37:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4527"
  },
  {
    "number": 4526,
    "title": "chore: minor refactoring and code clean-up",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T07:38:57Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4526"
  },
  {
    "number": 4525,
    "title": "feat: better build_wheel.py venv handling",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T07:16:02Z",
    "closed_at": "2025-05-27T07:17:14Z",
    "merged_at": "2025-05-27T07:17:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4525"
  },
  {
    "number": 4524,
    "title": "[5180961] chore: Unwaive test for Qwen model.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T06:49:11Z",
    "closed_at": "2025-05-23T05:28:08Z",
    "merged_at": "2025-05-23T05:28:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4524"
  },
  {
    "number": 4523,
    "title": "[nvbugs5214239] - Unwaive test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T06:14:39Z",
    "closed_at": "2025-06-18T10:36:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4523"
  },
  {
    "number": 4522,
    "title": "test: conditional disagg and cache aware balancing for deepseek v3",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T05:45:27Z",
    "closed_at": "2025-06-11T01:44:29Z",
    "merged_at": "2025-06-11T01:44:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4522"
  },
  {
    "number": 4521,
    "title": "fix: Constrain tornado and setuptools",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T05:18:42Z",
    "closed_at": "2025-05-21T13:23:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4521"
  },
  {
    "number": 4520,
    "title": "CI: waive test_fp8_block_scales_4gpus of deepseek v3 lite",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T05:06:39Z",
    "closed_at": "2025-05-21T05:19:44Z",
    "merged_at": "2025-05-21T05:19:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4520"
  },
  {
    "number": 4519,
    "title": "[5234029][5226211] chore: Unwaive multimodal tests for Qwen model.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T05:00:08Z",
    "closed_at": "2025-05-23T00:04:56Z",
    "merged_at": "2025-05-23T00:04:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4519"
  },
  {
    "number": 4518,
    "title": "User/nvpohanh/cherry pick llama4 change",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T04:57:31Z",
    "closed_at": "2025-05-22T05:29:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4518"
  },
  {
    "number": 4517,
    "title": "Cherry pick https://github.com/NVIDIA/TensorRT-LLM/pull/4447",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T04:30:10Z",
    "closed_at": "2025-05-21T05:30:21Z",
    "merged_at": "2025-05-21T05:30:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4517"
  },
  {
    "number": 4515,
    "title": "[https://nvbugspro.nvidia.com/bug/5181262] [test] Unwaive Mistral Nemo test",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T03:33:33Z",
    "closed_at": "2025-05-23T02:14:01Z",
    "merged_at": "2025-05-23T02:14:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4515"
  },
  {
    "number": 4514,
    "title": "feat: Skip sampler for intermediate pp stages.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T03:24:01Z",
    "closed_at": "2025-05-26T02:08:51Z",
    "merged_at": "2025-05-26T02:08:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4514"
  },
  {
    "number": 4512,
    "title": "Cherry pick #4508",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T03:13:38Z",
    "closed_at": "2025-05-21T03:25:37Z",
    "merged_at": "2025-05-21T03:25:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4512"
  },
  {
    "number": 4511,
    "title": "[fix] Fix Llama4 allgather error due to None tensor",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T02:59:06Z",
    "closed_at": "2025-05-24T11:12:12Z",
    "merged_at": "2025-05-24T11:12:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4511"
  },
  {
    "number": 4510,
    "title": "test: rcca https://nvbugs/5223130",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T02:55:26Z",
    "closed_at": "2025-05-27T01:59:47Z",
    "merged_at": "2025-05-27T01:59:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4510"
  },
  {
    "number": 4508,
    "title": "Chore: waive torch compile test cases of deepseek v3 lite",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T02:22:01Z",
    "closed_at": "2025-05-21T02:43:31Z",
    "merged_at": "2025-05-21T02:43:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4508"
  },
  {
    "number": 4507,
    "title": "fix: Fix trtllm sampler beam width bug",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T02:08:04Z",
    "closed_at": "2025-05-21T06:21:39Z",
    "merged_at": "2025-05-21T06:21:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4507"
  },
  {
    "number": 4506,
    "title": "[TRTLLM-5053] Refactoring and Unifying the Multimodal input preparation",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-21T01:17:16Z",
    "closed_at": "2025-06-03T19:02:08Z",
    "merged_at": "2025-06-03T19:02:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4506"
  },
  {
    "number": 4503,
    "title": "fix: TRT-LLM Gen dtype declaration ",
    "user": "nekorobov",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T22:27:23Z",
    "closed_at": "2025-05-21T21:56:38Z",
    "merged_at": "2025-05-21T21:56:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4503"
  },
  {
    "number": 4501,
    "title": "[Do not land] Temp changes for testing chunked attention",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T21:22:27Z",
    "closed_at": "2025-05-20T21:44:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4501"
  },
  {
    "number": 4500,
    "title": "[None][feat] Add NCCL Symmetric Integration for All Reduce",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T20:48:32Z",
    "closed_at": "2025-08-08T00:28:14Z",
    "merged_at": "2025-08-08T00:28:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4500"
  },
  {
    "number": 4499,
    "title": "test(perf):  Pt.2 Add `Llama-3_3-Nemotron-Super-49B-v1` integration-perf-tests (cpp)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T16:57:56Z",
    "closed_at": "2025-05-21T17:46:48Z",
    "merged_at": "2025-05-21T17:46:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4499"
  },
  {
    "number": 4498,
    "title": "fix: Handle additional model outputs based on pipeline parallel rank",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T16:45:26Z",
    "closed_at": "2025-05-26T07:04:40Z",
    "merged_at": "2025-05-26T07:04:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4498"
  },
  {
    "number": 4497,
    "title": "feat: forward exceptions to Python and catch OOMs",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T16:12:34Z",
    "closed_at": "2025-05-28T09:58:11Z",
    "merged_at": "2025-05-28T09:58:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4497"
  },
  {
    "number": 4496,
    "title": "Clean: fmha codes",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T15:13:50Z",
    "closed_at": "2025-05-21T03:45:48Z",
    "merged_at": "2025-05-21T03:45:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4496"
  },
  {
    "number": 4495,
    "title": "feat: large-scale EP(part 3: refactor - FusedMoe for redundant expert)",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T15:08:42Z",
    "closed_at": "2025-05-21T09:17:50Z",
    "merged_at": "2025-05-21T09:17:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4495"
  },
  {
    "number": 4494,
    "title": "[TRTLLM-4783][feat] Mamba2 kernel updates for Nemotron-H",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T14:35:18Z",
    "closed_at": "2025-06-01T10:56:44Z",
    "merged_at": "2025-06-01T10:56:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4494"
  },
  {
    "number": 4493,
    "title": "fix: update usage of TRTLLMSampler.beam_width",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T14:19:25Z",
    "closed_at": "2025-05-20T14:58:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4493"
  },
  {
    "number": 4492,
    "title": "fix: Remove duplicate tokenization in generation server",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T13:44:58Z",
    "closed_at": "2025-05-26T08:43:07Z",
    "merged_at": "2025-05-26T08:43:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4492"
  },
  {
    "number": 4490,
    "title": "fix: replace the image links in the blog",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T12:30:30Z",
    "closed_at": "2025-05-20T14:39:59Z",
    "merged_at": "2025-05-20T14:39:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4490"
  },
  {
    "number": 4489,
    "title": "fix: replace the image links in the blog",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T12:29:48Z",
    "closed_at": "2025-05-20T14:39:26Z",
    "merged_at": "2025-05-20T14:39:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4489"
  },
  {
    "number": 4487,
    "title": "[5141290][5273694][5260696] fix: Fix mrope argument missing issue in the summary tasks for Qwen model.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T10:38:05Z",
    "closed_at": "2025-05-20T10:38:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4487"
  },
  {
    "number": 4486,
    "title": "test: NIXL single process test",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T10:14:31Z",
    "closed_at": "2025-05-21T02:41:46Z",
    "merged_at": "2025-05-21T02:41:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4486"
  },
  {
    "number": 4485,
    "title": "chore: Remove unused script",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T09:49:33Z",
    "closed_at": "2025-05-21T05:46:40Z",
    "merged_at": "2025-05-21T05:46:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4485"
  },
  {
    "number": 4483,
    "title": "infra: Add qwen3 235B tests into QA",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T08:26:59Z",
    "closed_at": "2025-05-20T09:37:09Z",
    "merged_at": "2025-05-20T09:37:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4483"
  },
  {
    "number": 4482,
    "title": "perf: Add fused q_norm/k_norm/RoPE for Qwen3.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T08:14:13Z",
    "closed_at": "2025-05-23T07:31:04Z",
    "merged_at": "2025-05-23T07:31:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4482"
  },
  {
    "number": 4481,
    "title": "[feat] support fp8 blockscale gemm on sm89",
    "user": "CarstyYou",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T08:11:14Z",
    "closed_at": "2025-05-23T02:39:11Z",
    "merged_at": "2025-05-23T02:39:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4481"
  },
  {
    "number": 4480,
    "title": "[TEST] Pytest skip for Llama4 tests fix",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T07:47:10Z",
    "closed_at": "2025-05-28T10:33:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4480"
  },
  {
    "number": 4479,
    "title": "tests: update api change from decoder to sampler in test",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T07:45:20Z",
    "closed_at": "2025-05-21T06:22:18Z",
    "merged_at": "2025-05-21T06:22:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4479"
  },
  {
    "number": 4478,
    "title": "tests: add qwene fp4 tests into QA test list & update sanity test list",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T07:36:12Z",
    "closed_at": "2025-05-21T08:52:02Z",
    "merged_at": "2025-05-21T08:52:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4478"
  },
  {
    "number": 4477,
    "title": "tests: update trtllm_decoder api change in test case",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T07:33:04Z",
    "closed_at": "2025-05-20T07:44:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4477"
  },
  {
    "number": 4476,
    "title": "unwaive some disagg test",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T07:19:30Z",
    "closed_at": "2025-05-21T03:45:12Z",
    "merged_at": "2025-05-21T03:45:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4476"
  },
  {
    "number": 4475,
    "title": "Update fmha v2 repo",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T07:08:36Z",
    "closed_at": "2025-06-04T02:29:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4475"
  },
  {
    "number": 4474,
    "title": "feat: support prefill-first scheduler",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T06:44:19Z",
    "closed_at": "2025-06-27T02:46:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4474"
  },
  {
    "number": 4473,
    "title": "fix: Fix TRTLLMSampler beam width bug.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T06:19:29Z",
    "closed_at": "2025-05-20T16:00:14Z",
    "merged_at": "2025-05-20T16:00:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4473"
  },
  {
    "number": 4472,
    "title": "[Infra]Add timestamper for pytest",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T05:15:32Z",
    "closed_at": "2025-05-28T10:32:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4472"
  },
  {
    "number": 4471,
    "title": "chore: Deprecate autopp.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T05:10:34Z",
    "closed_at": "2025-05-21T05:50:11Z",
    "merged_at": "2025-05-21T05:50:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4471"
  },
  {
    "number": 4470,
    "title": "[CI] Waive failing DeepSeekV3 tests",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T05:09:01Z",
    "closed_at": "2025-05-21T05:06:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4470"
  },
  {
    "number": 4469,
    "title": "chore: bump version to 0.20.0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T04:23:22Z",
    "closed_at": "2025-05-20T07:27:29Z",
    "merged_at": "2025-05-20T07:27:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4469"
  },
  {
    "number": 4468,
    "title": "[AutoDeploy] Increased Model Coverage Mass Migration Week 1",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T04:01:46Z",
    "closed_at": "2025-05-27T08:43:15Z",
    "merged_at": "2025-05-27T08:43:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4468"
  },
  {
    "number": 4467,
    "title": "[feat] Piecewise cuda graph support for MLA",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T03:29:20Z",
    "closed_at": "2025-06-17T10:58:39Z",
    "merged_at": "2025-06-17T10:58:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4467"
  },
  {
    "number": 4466,
    "title": "feat: Enhance AutoTuner inference path and code readability",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T02:46:11Z",
    "closed_at": "2025-06-04T02:53:11Z",
    "merged_at": "2025-06-04T02:53:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4466"
  },
  {
    "number": 4465,
    "title": "chore: bump version to 0.21.0rc0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T02:42:17Z",
    "closed_at": "2025-05-20T04:19:50Z",
    "merged_at": "2025-05-20T04:19:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4465"
  },
  {
    "number": 4464,
    "title": "draft: KV Cache GPUDirect Storage",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T02:38:18Z",
    "closed_at": "2025-06-02T19:51:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4464"
  },
  {
    "number": 4463,
    "title": "Test Release 0.20 CI",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T02:27:54Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4463"
  },
  {
    "number": 4462,
    "title": "Adapt Sentence Transformer model to BertForSeqClassification",
    "user": "caronzh03",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-20T00:01:10Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4462"
  },
  {
    "number": 4457,
    "title": "feat: add dataset support for benchmark_core_model with LLMAPI",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T19:54:19Z",
    "closed_at": "2025-05-22T02:18:44Z",
    "merged_at": "2025-05-22T02:18:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4457"
  },
  {
    "number": 4456,
    "title": "Build Triton for arm",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T19:18:02Z",
    "closed_at": "2025-05-20T17:52:33Z",
    "merged_at": "2025-05-20T17:52:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4456"
  },
  {
    "number": 4455,
    "title": "Add tritonrelease container",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T17:53:58Z",
    "closed_at": "2025-05-22T03:47:50Z",
    "merged_at": "2025-05-22T03:47:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4455"
  },
  {
    "number": 4454,
    "title": "[Infra] - Multi-GPU testing support with Slurm",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T17:44:14Z",
    "closed_at": "2025-05-26T11:44:19Z",
    "merged_at": "2025-05-26T11:44:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4454"
  },
  {
    "number": 4453,
    "title": "chore: remove extra PYTHONPATH",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T17:32:46Z",
    "closed_at": "2025-05-22T00:38:01Z",
    "merged_at": "2025-05-22T00:38:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4453"
  },
  {
    "number": 4452,
    "title": "refactor: CreateNewDecoderRequests",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T16:44:36Z",
    "closed_at": "2025-05-23T14:54:38Z",
    "merged_at": "2025-05-23T14:54:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4452"
  },
  {
    "number": 4451,
    "title": "test: Split test_simple into mpi_utils and cache transceiver tests for DGX",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T16:15:50Z",
    "closed_at": "2025-05-21T20:26:21Z",
    "merged_at": "2025-05-21T20:26:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4451"
  },
  {
    "number": 4450,
    "title": "refactor: Update decoder buffer and logits management",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T15:30:30Z",
    "closed_at": "2025-06-18T00:10:33Z",
    "merged_at": "2025-06-18T00:10:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4450"
  },
  {
    "number": 4449,
    "title": "chore: Clean up cpp runtime",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T14:27:04Z",
    "closed_at": "2025-05-28T14:32:59Z",
    "merged_at": "2025-05-28T14:32:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4449"
  },
  {
    "number": 4448,
    "title": "[Docs] - Add date and commit info",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T14:21:42Z",
    "closed_at": "2025-05-20T10:53:45Z",
    "merged_at": "2025-05-20T10:53:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4448"
  },
  {
    "number": 4447,
    "title": "fix: skip weights defined in create_weights for pp.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T14:15:58Z",
    "closed_at": "2025-05-21T02:13:21Z",
    "merged_at": "2025-05-21T02:13:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4447"
  },
  {
    "number": 4446,
    "title": "test(perf): Add `Llama-3_1-Nemotron-Ultra-253B-v1` perf tests (cpp)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T13:35:15Z",
    "closed_at": "2025-05-22T20:07:34Z",
    "merged_at": "2025-05-22T20:07:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4446"
  },
  {
    "number": 4445,
    "title": "fix: temp disable the problem test",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T13:30:35Z",
    "closed_at": "2025-05-19T13:54:32Z",
    "merged_at": "2025-05-19T13:54:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4445"
  },
  {
    "number": 4444,
    "title": "[TRTLLM-5085][fix] Nemotron H correctness test",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T13:13:29Z",
    "closed_at": "2025-05-20T09:55:25Z",
    "merged_at": "2025-05-20T09:55:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4444"
  },
  {
    "number": 4443,
    "title": "test(perf): Add remaining `Phi-4-mini-instruct` perf tests",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T12:56:14Z",
    "closed_at": "2025-05-21T01:26:12Z",
    "merged_at": "2025-05-21T01:26:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4443"
  },
  {
    "number": 4440,
    "title": "Downgrade the logger level for fallback tactic warning.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T10:05:07Z",
    "closed_at": "2025-05-19T10:26:54Z",
    "merged_at": "2025-05-19T10:26:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4440"
  },
  {
    "number": 4439,
    "title": "[Docs] - Clean up legacy switcher.json files",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T09:55:24Z",
    "closed_at": "2025-05-19T14:52:17Z",
    "merged_at": "2025-05-19T14:52:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4439"
  },
  {
    "number": 4438,
    "title": "Chore: clean up _merge_dummy_request method of PyExecutor",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T08:51:16Z",
    "closed_at": "2025-05-22T10:19:07Z",
    "merged_at": "2025-05-22T10:19:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4438"
  },
  {
    "number": 4437,
    "title": "Draft: CI tests",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T08:16:05Z",
    "closed_at": "2025-09-02T00:54:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4437"
  },
  {
    "number": 4436,
    "title": "Feature flux",
    "user": "forrestl111",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T07:43:17Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4436"
  },
  {
    "number": 4435,
    "title": "Chore: clean up _merge_requests method of PyExecutor",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T07:38:43Z",
    "closed_at": "2025-05-22T09:51:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4435"
  },
  {
    "number": 4434,
    "title": "[Docs] - Reapply #4220",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T07:28:21Z",
    "closed_at": "2025-05-19T14:52:37Z",
    "merged_at": "2025-05-19T14:52:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4434"
  },
  {
    "number": 4433,
    "title": "fix: wrong argument name `enable_overlap_scheduler`",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T06:46:03Z",
    "closed_at": "2025-05-19T07:02:22Z",
    "merged_at": "2025-05-19T07:02:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4433"
  },
  {
    "number": 4432,
    "title": "[5141290][5273694][5260696] fix: Fix mrope argument missing issue in the summary tasks for Qwen model.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T06:37:42Z",
    "closed_at": "2025-05-22T09:46:00Z",
    "merged_at": "2025-05-22T09:46:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4432"
  },
  {
    "number": 4431,
    "title": "https://nvbugspro.nvidia.com/bug/5285965 fix: DeepSeekR1 H200 accuracy issue",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T05:39:50Z",
    "closed_at": "2025-07-25T03:29:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4431"
  },
  {
    "number": 4430,
    "title": "Feat: add deep_gemm swapab Kernel",
    "user": "ruoqianguo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T05:32:34Z",
    "closed_at": "2025-05-21T02:48:44Z",
    "merged_at": "2025-05-21T02:48:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4430"
  },
  {
    "number": 4429,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T05:07:50Z",
    "closed_at": "2025-05-20T01:43:55Z",
    "merged_at": "2025-05-20T01:43:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4429"
  },
  {
    "number": 4428,
    "title": "fix[nvbug/5286515]: trtllm-llmapi-launch on single node single gpu",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T03:15:25Z",
    "closed_at": "2025-05-20T12:16:15Z",
    "merged_at": "2025-05-20T12:16:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4428"
  },
  {
    "number": 4427,
    "title": "draft: [Auto Deploy] Deepseek support for MLA attention with compressed kv",
    "user": "sugunav14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T03:08:40Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4427"
  },
  {
    "number": 4426,
    "title": "[Infra] - Always push the release images in the post-merge job",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T02:51:50Z",
    "closed_at": "2025-05-19T03:05:43Z",
    "merged_at": "2025-05-19T03:05:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4426"
  },
  {
    "number": 4425,
    "title": "Update \"Roadmap\" link under README.md to the issues with Roadmap label",
    "user": "AdamzNV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T02:47:05Z",
    "closed_at": "2025-05-19T07:50:41Z",
    "merged_at": "2025-05-19T07:50:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4425"
  },
  {
    "number": 4424,
    "title": "[nvbug/5028235][fix]pytest bindings tokens logtis comparison.",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T02:29:25Z",
    "closed_at": "2025-05-23T12:41:00Z",
    "merged_at": "2025-05-23T12:41:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4424"
  },
  {
    "number": 4423,
    "title": "[DON'T MERGER] Sharing only: Add quickstart dataset",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-19T02:08:13Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4423"
  },
  {
    "number": 4422,
    "title": "refine doc",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-18T22:05:32Z",
    "closed_at": "2025-05-18T22:06:22Z",
    "merged_at": "2025-05-18T22:06:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4422"
  },
  {
    "number": 4421,
    "title": "Refine doc",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-18T22:02:56Z",
    "closed_at": "2025-05-18T22:03:05Z",
    "merged_at": "2025-05-18T22:03:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4421"
  },
  {
    "number": 4420,
    "title": "Refine doc",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-18T21:05:03Z",
    "closed_at": "2025-05-18T21:05:25Z",
    "merged_at": "2025-05-18T21:05:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4420"
  },
  {
    "number": 4419,
    "title": "[Infra][Docs] - Some clean-up for the CI pipeline and docs",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-18T15:45:22Z",
    "closed_at": "2025-05-18T16:07:46Z",
    "merged_at": "2025-05-18T16:07:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4419"
  },
  {
    "number": 4418,
    "title": "fix gemma2 27b fp8",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-18T15:06:53Z",
    "closed_at": "2025-05-26T13:12:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4418"
  },
  {
    "number": 4417,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-18T12:14:40Z",
    "closed_at": "2025-05-21T01:13:26Z",
    "merged_at": "2025-05-21T01:13:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4417"
  },
  {
    "number": 4416,
    "title": "[None][feat] Multi-block mode for Hopper spec dec XQA kernel",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-18T08:18:53Z",
    "closed_at": "2025-08-03T21:31:34Z",
    "merged_at": "2025-08-03T21:31:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4416"
  },
  {
    "number": 4415,
    "title": "[TRTLLM-4932] Add CLI accuracy tests for Phi-4-mini-instruct",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-18T06:55:14Z",
    "closed_at": "2025-05-22T01:56:49Z",
    "merged_at": "2025-05-22T01:56:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4415"
  },
  {
    "number": 4413,
    "title": "[Fix][Deepseek] Fix bugs in TestDeepSeekR1",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-18T00:02:04Z",
    "closed_at": "2025-05-24T01:52:58Z",
    "merged_at": "2025-05-24T01:52:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4413"
  },
  {
    "number": 4411,
    "title": "fix: remove unused parameter in apply_chat_template",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-17T09:21:01Z",
    "closed_at": "2025-05-17T15:01:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4411"
  },
  {
    "number": 4410,
    "title": "Scaffoldingllm supports MCP",
    "user": "wu1du2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-17T04:06:58Z",
    "closed_at": "2025-05-23T01:54:50Z",
    "merged_at": "2025-05-23T01:54:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4410"
  },
  {
    "number": 4409,
    "title": "test: Waive tests for nvbugs/5286795.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-17T02:06:51Z",
    "closed_at": "2025-05-17T11:41:05Z",
    "merged_at": "2025-05-17T11:41:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4409"
  },
  {
    "number": 4408,
    "title": "Removing the outdated argument",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T23:44:43Z",
    "closed_at": "2025-05-18T07:52:15Z",
    "merged_at": "2025-05-18T07:52:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4408"
  },
  {
    "number": 4407,
    "title": "test(perf): Extend the Llama-Nemotron-Nano-8B perf-integration-tests (pyt) ",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T23:33:55Z",
    "closed_at": "2025-05-23T00:44:38Z",
    "merged_at": "2025-05-23T00:44:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4407"
  },
  {
    "number": 4405,
    "title": "Add pytorch backend team",
    "user": "kevinch-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T20:24:07Z",
    "closed_at": "2025-05-21T13:10:35Z",
    "merged_at": "2025-05-21T13:10:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4405"
  },
  {
    "number": 4402,
    "title": "[nvbug/5285881][fix] Fix chunked prefill + overlap scheduler",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T18:57:29Z",
    "closed_at": "2025-05-22T20:38:22Z",
    "merged_at": "2025-05-22T20:38:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4402"
  },
  {
    "number": 4401,
    "title": "feature: make trtllmsampler new_tokens format the universal format",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T16:10:45Z",
    "closed_at": "2025-06-23T17:38:38Z",
    "merged_at": "2025-06-23T17:38:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4401"
  },
  {
    "number": 4400,
    "title": "doc: [TRTLLM-325]Integrate the NGC image in Makefile automation and document",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T15:00:31Z",
    "closed_at": "2025-05-20T06:45:02Z",
    "merged_at": "2025-05-20T06:45:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4400"
  },
  {
    "number": 4399,
    "title": "fix: [nvbugs/5287097] Align PP layer distribution between pytorch and TRT flow.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T13:49:37Z",
    "closed_at": "2025-05-19T21:25:36Z",
    "merged_at": "2025-05-19T21:25:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4399"
  },
  {
    "number": 4398,
    "title": "refactor: DisaggExecutorTest",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T12:29:52Z",
    "closed_at": "2025-05-21T10:01:45Z",
    "merged_at": "2025-05-21T10:01:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4398"
  },
  {
    "number": 4397,
    "title": "[CI] waive accuracy/test_cli_flow.py::TestTinyLlama1_1BChat::test_pp4",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T12:01:14Z",
    "closed_at": "2025-05-16T12:18:07Z",
    "merged_at": "2025-05-16T12:18:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4397"
  },
  {
    "number": 4396,
    "title": "fix: Remove real size allocation",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T11:07:57Z",
    "closed_at": "2025-05-18T11:13:22Z",
    "merged_at": "2025-05-18T11:13:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4396"
  },
  {
    "number": 4394,
    "title": "Feat: add chunked-attention kernels on Blackwell",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T10:34:02Z",
    "closed_at": "2025-05-21T02:16:46Z",
    "merged_at": "2025-05-21T02:16:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4394"
  },
  {
    "number": 4393,
    "title": "Fix test_fused_moe_w4afp8",
    "user": "StudyingShao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T09:03:21Z",
    "closed_at": "2025-05-16T09:21:34Z",
    "merged_at": "2025-05-16T09:21:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4393"
  },
  {
    "number": 4391,
    "title": "tests: add llama 3.3 70b 2 nodes tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T07:33:32Z",
    "closed_at": "2025-05-21T04:42:47Z",
    "merged_at": "2025-05-21T04:42:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4391"
  },
  {
    "number": 4390,
    "title": "[TRTLLM-5082] - Add a bot run option for detailed logs",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T07:25:40Z",
    "closed_at": "2025-06-11T07:44:57Z",
    "merged_at": "2025-06-11T07:44:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4390"
  },
  {
    "number": 4389,
    "title": "[TRTLLM-4932] Add LLM API accuracy tests for Llama-4-Maverick-17B-128E-Instruct - FP8 variant",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T07:01:54Z",
    "closed_at": "2025-09-07T05:46:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4389"
  },
  {
    "number": 4388,
    "title": "[TRTLLM-4932] Add CLI accuracy tests for Llama-4-Scout-17B-16E-Instruct",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T05:45:04Z",
    "closed_at": "2025-05-21T03:33:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4388"
  },
  {
    "number": 4387,
    "title": "fix: Fix chat template kwargs bug.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T05:37:26Z",
    "closed_at": "2025-05-16T15:07:47Z",
    "merged_at": "2025-05-16T15:07:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4387"
  },
  {
    "number": 4386,
    "title": "doc： DS r1 min latency blog",
    "user": "Kefeng-Duan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T05:32:35Z",
    "closed_at": "2025-05-16T12:20:29Z",
    "merged_at": "2025-05-16T12:20:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4386"
  },
  {
    "number": 4385,
    "title": "[TEST]Test timeout new opt for both pytest commands",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T05:22:02Z",
    "closed_at": "2025-05-18T05:43:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4385"
  },
  {
    "number": 4384,
    "title": "feat: large-scale EP(part 2: MoE Load Balancer - core utilities) ",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T04:23:16Z",
    "closed_at": "2025-05-20T09:53:49Z",
    "merged_at": "2025-05-20T09:53:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4384"
  },
  {
    "number": 4383,
    "title": "feat: add support for florence2 ",
    "user": "ducviet00",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T03:04:54Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4383"
  },
  {
    "number": 4381,
    "title": "update README version",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T02:12:57Z",
    "closed_at": "2025-05-16T02:36:39Z",
    "merged_at": "2025-05-16T02:36:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4381"
  },
  {
    "number": 4380,
    "title": "[CI] waive test_chunked_prefill test cases",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T01:54:04Z",
    "closed_at": "2025-05-16T02:27:20Z",
    "merged_at": "2025-05-16T02:27:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4380"
  },
  {
    "number": 4379,
    "title": "fix: fix accuracy and illegal memory access issues when using mtp + attention dp",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T01:35:41Z",
    "closed_at": "2025-06-01T16:35:53Z",
    "merged_at": "2025-06-01T16:35:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4379"
  },
  {
    "number": 4378,
    "title": "[CI] update multi-gpu test triggering file list",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T00:41:04Z",
    "closed_at": "2025-05-16T01:05:44Z",
    "merged_at": "2025-05-16T01:05:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4378"
  },
  {
    "number": 4376,
    "title": "Remove vila test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-16T00:04:20Z",
    "closed_at": "2025-05-19T01:02:39Z",
    "merged_at": "2025-05-19T01:02:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4376"
  },
  {
    "number": 4375,
    "title": "[TRTLLM-4932] Add CLI accuracy tests for Llama-3_3-Nemotron-Super-49B-v1 and LLM API FP8 variant",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T23:37:15Z",
    "closed_at": "2025-05-23T19:17:23Z",
    "merged_at": "2025-05-23T19:17:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4375"
  },
  {
    "number": 4374,
    "title": "[Deepseek] Add accuracy test references for fp8 kvcache",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T23:22:22Z",
    "closed_at": "2025-05-17T03:23:00Z",
    "merged_at": "2025-05-17T03:23:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4374"
  },
  {
    "number": 4373,
    "title": "[AutoDeploy] fix: proper process group clean up",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T22:44:20Z",
    "closed_at": "2025-05-16T16:35:25Z",
    "merged_at": "2025-05-16T16:35:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4373"
  },
  {
    "number": 4372,
    "title": "[AutoDeploy] configurable cache resize",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T22:40:15Z",
    "closed_at": "2025-05-16T14:07:10Z",
    "merged_at": "2025-05-16T14:07:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4372"
  },
  {
    "number": 4371,
    "title": "[AutoDeploy] HF factory improvements",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T22:38:01Z",
    "closed_at": "2025-05-20T03:13:43Z",
    "merged_at": "2025-05-20T03:13:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4371"
  },
  {
    "number": 4370,
    "title": "[AutoDeploy] eager pattern matcher new pattern",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T22:34:08Z",
    "closed_at": "2025-05-16T16:35:44Z",
    "merged_at": "2025-05-16T16:35:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4370"
  },
  {
    "number": 4369,
    "title": "[AutoDeploy] more robust handling of attention interface and input nodes",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T22:30:51Z",
    "closed_at": "2025-05-23T03:23:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4369"
  },
  {
    "number": 4368,
    "title": "[AutoDeploy] bmm sharder",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T22:28:09Z",
    "closed_at": "2025-05-20T04:54:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4368"
  },
  {
    "number": 4365,
    "title": "[AutoDeploy] fix: disable overlap scheduler until supported",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T18:54:00Z",
    "closed_at": "2025-05-15T23:19:31Z",
    "merged_at": "2025-05-15T23:19:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4365"
  },
  {
    "number": 4363,
    "title": "[TRTLLM-4618][feat] Add Nemotron Super 49B FP8 test on RTX6000 Pro (SM120)",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T17:46:37Z",
    "closed_at": "2025-05-19T01:30:24Z",
    "merged_at": "2025-05-19T01:30:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4363"
  },
  {
    "number": 4362,
    "title": "[TRTLLM-4932] Add CLI accuracy tests for Llama-3.3-70B-Instruct and LLM API BF16 variant",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T17:42:36Z",
    "closed_at": "2025-05-20T01:48:14Z",
    "merged_at": "2025-05-20T01:48:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4362"
  },
  {
    "number": 4361,
    "title": "feat:[AutoDeploy] Support Quantized MoE matcher",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T16:37:07Z",
    "closed_at": "2025-07-11T21:55:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4361"
  },
  {
    "number": 4360,
    "title": "docs: update the introduction for scaffolding",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T15:35:30Z",
    "closed_at": "2025-05-21T06:54:01Z",
    "merged_at": "2025-05-21T06:54:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4360"
  },
  {
    "number": 4358,
    "title": "feat: Add pp support for hybrid attn/mamba model",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T13:01:08Z",
    "closed_at": "2025-05-19T06:47:46Z",
    "merged_at": "2025-05-19T06:47:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4358"
  },
  {
    "number": 4357,
    "title": "tests: add qa test mentioned in docs",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T11:57:13Z",
    "closed_at": "2025-05-19T02:06:52Z",
    "merged_at": "2025-05-19T02:06:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4357"
  },
  {
    "number": 4356,
    "title": "chore: prevent using default stream",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T10:21:48Z",
    "closed_at": "2025-07-18T22:38:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4356"
  },
  {
    "number": 4355,
    "title": "Revert \"[test] add qa test mentioned in docs\"",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T10:16:30Z",
    "closed_at": "2025-05-15T10:47:30Z",
    "merged_at": "2025-05-15T10:47:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4355"
  },
  {
    "number": 4354,
    "title": "doc: Add docstring for Attention and MLA module.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T08:59:36Z",
    "closed_at": "2025-05-16T01:37:04Z",
    "merged_at": "2025-05-16T01:37:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4354"
  },
  {
    "number": 4353,
    "title": "fix: support TensorRT 10.11+ in FindTensorRT.cmake",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T08:37:14Z",
    "closed_at": "2025-05-16T06:04:56Z",
    "merged_at": "2025-05-16T06:04:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4353"
  },
  {
    "number": 4352,
    "title": "chore: improve log-level setting UX",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T08:22:13Z",
    "closed_at": "2025-05-16T08:47:45Z",
    "merged_at": "2025-05-16T08:47:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4352"
  },
  {
    "number": 4351,
    "title": "[fix] Fixed incorrect mixed precision MoE conversion",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T08:22:11Z",
    "closed_at": "2025-05-16T05:43:42Z",
    "merged_at": "2025-05-16T05:43:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4351"
  },
  {
    "number": 4350,
    "title": "[fix] test_no_kv_cache_reuse for overlap_scheduler",
    "user": "zhhuang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T08:16:48Z",
    "closed_at": "2025-05-15T08:43:54Z",
    "merged_at": "2025-05-15T08:43:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4350"
  },
  {
    "number": 4349,
    "title": "feat: add health_generate route to openai serving (Cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/3856)",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T06:59:31Z",
    "closed_at": "2025-05-22T03:46:06Z",
    "merged_at": "2025-05-22T03:46:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4349"
  },
  {
    "number": 4348,
    "title": "Fix bias shape in weightOnlyGroupwiseQuantMatmulPlugin for TRT workflow",
    "user": "StudyingShao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T06:01:12Z",
    "closed_at": "2025-05-16T02:02:31Z",
    "merged_at": "2025-05-16T02:02:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4348"
  },
  {
    "number": 4347,
    "title": "tests: Add test cases for rcca cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T05:06:26Z",
    "closed_at": "2025-05-19T04:06:43Z",
    "merged_at": "2025-05-19T04:06:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4347"
  },
  {
    "number": 4346,
    "title": "[test] Reorganize TestDeepSeekR1::test_nvfp4_8gpus",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T04:35:51Z",
    "closed_at": "2025-05-15T05:09:13Z",
    "merged_at": "2025-05-15T05:09:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4346"
  },
  {
    "number": 4345,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T03:14:08Z",
    "closed_at": "2025-05-16T05:47:43Z",
    "merged_at": "2025-05-16T05:47:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4345"
  },
  {
    "number": 4344,
    "title": "feat: Low Precision Allreduce for PCIe based GPU ",
    "user": "kanghui0204",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T02:52:28Z",
    "closed_at": "2025-05-19T22:53:47Z",
    "merged_at": "2025-05-19T22:53:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4344"
  },
  {
    "number": 4343,
    "title": "Enable trtllm decoder by default in PytorchConfig for OpenAI server",
    "user": "vegaluisjose",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T02:20:50Z",
    "closed_at": "2025-05-20T17:48:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4343"
  },
  {
    "number": 4342,
    "title": "Draft: test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T02:19:38Z",
    "closed_at": "2025-05-15T03:05:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4342"
  },
  {
    "number": 4341,
    "title": "[TRTLLM-4886][infra]Try another timeout opt to exit test thread directly instead of gracefully",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T01:55:29Z",
    "closed_at": "2025-05-16T09:56:41Z",
    "merged_at": "2025-05-16T09:56:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4341"
  },
  {
    "number": 4340,
    "title": "Revert \"feat: Low Precision Allreduce for PCIe based GPU\"",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T01:31:32Z",
    "closed_at": "2025-05-15T01:52:39Z",
    "merged_at": "2025-05-15T01:52:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4340"
  },
  {
    "number": 4339,
    "title": "fix: update checks that broke medusa tests when use_py_session=True",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T01:25:24Z",
    "closed_at": "2025-05-15T22:47:28Z",
    "merged_at": "2025-05-15T22:47:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4339"
  },
  {
    "number": 4338,
    "title": "[doc] Add tensorrtllm_backend serving documentation in the Deepseek-V3 README",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T01:10:34Z",
    "closed_at": "2025-05-15T01:31:29Z",
    "merged_at": "2025-05-15T01:31:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4338"
  },
  {
    "number": 4337,
    "title": "[TRTLLM-4638] feat(scaffolding): update Reward Controller to PRM specific controller with step split",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T00:41:58Z",
    "closed_at": "2025-05-19T09:53:41Z",
    "merged_at": "2025-05-19T09:53:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4337"
  },
  {
    "number": 4336,
    "title": "Add llama4 disagg accuracy tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T00:40:49Z",
    "closed_at": "2025-05-19T13:55:08Z",
    "merged_at": "2025-05-19T13:55:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4336"
  },
  {
    "number": 4335,
    "title": "[TRTLLM-4618][feat] Fix cutlass MoE GEMM fallback failure on FP8 + add e2e test for Mixtral 8x7B FP8 on RTX6000 Pro (SM120) ",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T00:15:06Z",
    "closed_at": "2025-05-19T15:56:21Z",
    "merged_at": "2025-05-19T15:56:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4335"
  },
  {
    "number": 4334,
    "title": "[Infra] - Terminate the Slurm job if node does not come online in 2 hours",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T00:06:13Z",
    "closed_at": "2025-05-17T16:17:35Z",
    "merged_at": "2025-05-17T16:17:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4334"
  },
  {
    "number": 4332,
    "title": "Change the method to calculate kv memory size in tests",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T22:51:57Z",
    "closed_at": "2025-05-16T07:35:41Z",
    "merged_at": "2025-05-16T07:35:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4332"
  },
  {
    "number": 4331,
    "title": "[Draft] [TRTLLM-5227]: Remove HMAC code ",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T21:48:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4331"
  },
  {
    "number": 4330,
    "title": "[feat] Integrate Hopper chunked attention kernels",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T21:18:55Z",
    "closed_at": "2025-05-22T21:10:58Z",
    "merged_at": "2025-05-22T21:10:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4330"
  },
  {
    "number": 4326,
    "title": "[fix] Remove stale cublas heuristics",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T19:41:01Z",
    "closed_at": "2025-05-15T00:35:52Z",
    "merged_at": "2025-05-15T00:35:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4326"
  },
  {
    "number": 4316,
    "title": "Terminate slurm job when node does't come online in 2 hours",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T19:13:31Z",
    "closed_at": "2025-05-14T20:53:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4316"
  },
  {
    "number": 4304,
    "title": "Add allreduce and rmsnorm fusion for qwen3",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T15:25:14Z",
    "closed_at": "2025-05-15T08:22:12Z",
    "merged_at": "2025-05-15T08:22:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4304"
  },
  {
    "number": 4303,
    "title": "[perf] Reduce the workspace size of FP4 activation scales for MoE",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T15:01:37Z",
    "closed_at": "2025-05-30T01:03:52Z",
    "merged_at": "2025-05-30T01:03:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4303"
  },
  {
    "number": 4302,
    "title": "feat: Add Python implementation of GenerateRequestOptions",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T12:06:05Z",
    "closed_at": "2025-06-24T11:22:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4302"
  },
  {
    "number": 4301,
    "title": "fix: fix the speculative decoding accuracy issue when using cuda graph padded requests (dummy requests) and enabling overlap scheduler",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T11:52:42Z",
    "closed_at": "2025-05-23T06:08:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4301"
  },
  {
    "number": 4300,
    "title": "[https://nvbugs/5277113][fix]genai-perf API change stress test",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T11:17:31Z",
    "closed_at": "2025-05-15T06:12:34Z",
    "merged_at": "2025-05-15T06:12:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4300"
  },
  {
    "number": 4299,
    "title": "fix: improve PyExecutor resource allocations",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T10:43:58Z",
    "closed_at": "2025-05-16T15:28:11Z",
    "merged_at": "2025-05-16T15:28:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4299"
  },
  {
    "number": 4298,
    "title": "[CI] add some sanity check test cases for PyTorch backend",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T10:01:20Z",
    "closed_at": "2025-07-03T06:56:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4298"
  },
  {
    "number": 4297,
    "title": "chore: reduce code duplication",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T08:59:58Z",
    "closed_at": "2025-05-15T08:25:38Z",
    "merged_at": "2025-05-15T08:25:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4297"
  },
  {
    "number": 4296,
    "title": "test: add llama_v4_scout_instruct and llama_v4_maverick_instruct into perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T08:28:47Z",
    "closed_at": "2025-06-09T09:50:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4296"
  },
  {
    "number": 4295,
    "title": "[Infra] Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T08:02:26Z",
    "closed_at": "2025-05-14T08:38:33Z",
    "merged_at": "2025-05-14T08:38:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4295"
  },
  {
    "number": 4294,
    "title": "infra: [TRTLLM-5247][TRTLLM-5248][TRTLLM-5249] Refactor docker build image groovy and support NGC images",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T07:58:27Z",
    "closed_at": "2025-05-29T03:23:30Z",
    "merged_at": "2025-05-29T03:23:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4294"
  },
  {
    "number": 4293,
    "title": "Draft: Enable TRTLLMDecoder by default.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T07:48:57Z",
    "closed_at": "2025-06-24T11:22:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4293"
  },
  {
    "number": 4292,
    "title": "Check test names in waive list",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T07:18:15Z",
    "closed_at": "2025-06-01T06:39:30Z",
    "merged_at": "2025-06-01T06:39:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4292"
  },
  {
    "number": 4291,
    "title": "[Feat] add chunked-attention kernels on Hopper (for llama4)",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T07:10:27Z",
    "closed_at": "2025-05-19T16:57:11Z",
    "merged_at": "2025-05-19T16:57:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4291"
  },
  {
    "number": 4290,
    "title": "[TRTLLM-5273]feat/Use full attention mask if Llama3 is used as encoder and fix EarlyStopDecoder unsqueeze bug",
    "user": "nvrohanv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T06:00:25Z",
    "closed_at": "2025-05-20T17:15:37Z",
    "merged_at": "2025-05-20T17:15:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4290"
  },
  {
    "number": 4289,
    "title": "feat: change eagle one model flag from an env var to a config in Eagle3Config",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T05:51:15Z",
    "closed_at": "2025-05-16T03:14:23Z",
    "merged_at": "2025-05-16T03:14:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4289"
  },
  {
    "number": 4288,
    "title": "feat: Refactor Llama4 min-latency kernels and custom torch ops",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T03:54:24Z",
    "closed_at": "2025-05-21T00:01:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4288"
  },
  {
    "number": 4287,
    "title": "fix: gh-pages switcher",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T03:30:28Z",
    "closed_at": "2025-05-14T04:34:05Z",
    "merged_at": "2025-05-14T04:34:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4287"
  },
  {
    "number": 4286,
    "title": "feat: support benchmark on scaffolding (#3328)",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T03:08:53Z",
    "closed_at": "2025-05-16T04:28:49Z",
    "merged_at": "2025-05-16T04:28:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4286"
  },
  {
    "number": 4285,
    "title": "test: remove enable_overlap_schedule in pytorch config and set enable_chunked prefill to be true for isl>2048 cases",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T02:54:56Z",
    "closed_at": "2025-05-21T06:26:57Z",
    "merged_at": "2025-05-21T06:26:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4285"
  },
  {
    "number": 4284,
    "title": "Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T02:42:58Z",
    "closed_at": "2025-05-14T03:12:52Z",
    "merged_at": "2025-05-14T03:12:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4284"
  },
  {
    "number": 4283,
    "title": "test: FIX test_ptp_quickstart_advanced_deepseek_v3_2nodes_8gpus",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T02:36:07Z",
    "closed_at": "2025-05-15T07:57:44Z",
    "merged_at": "2025-05-15T07:57:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4283"
  },
  {
    "number": 4282,
    "title": "test: update test filter in perf test yml file to select cases by gpu name and add cases for RTX 6000 pro",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T02:25:20Z",
    "closed_at": "2025-05-20T02:58:05Z",
    "merged_at": "2025-05-20T02:58:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4282"
  },
  {
    "number": 4281,
    "title": "feat: [AutoDeploy] DeepseekV3 e2e support with sdpa attention",
    "user": "sugunav14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T02:18:05Z",
    "closed_at": "2025-06-05T23:18:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4281"
  },
  {
    "number": 4280,
    "title": "feat: TRT-LLM Gen integration for BMM and MoE refactoring",
    "user": "nekorobov",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T02:08:28Z",
    "closed_at": "2025-05-16T11:31:53Z",
    "merged_at": "2025-05-16T11:31:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4280"
  },
  {
    "number": 4279,
    "title": "[Draft] test: Streamline and optimize QA test lists",
    "user": "kxdc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-14T00:31:46Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4279"
  },
  {
    "number": 4278,
    "title": "[chore] update CI allowlist 2025-05-13",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T22:37:55Z",
    "closed_at": "2025-05-14T02:41:57Z",
    "merged_at": "2025-05-14T02:41:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4278"
  },
  {
    "number": 4276,
    "title": "Waive disagg kv cache load balancer test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T20:57:34Z",
    "closed_at": "2025-05-13T22:03:25Z",
    "merged_at": "2025-05-13T22:03:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4276"
  },
  {
    "number": 4274,
    "title": "[Draft] [TRTLLM-4971]: Use safe deserialization in ParallelConfig",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T20:30:08Z",
    "closed_at": "2025-05-23T20:48:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4274"
  },
  {
    "number": 4273,
    "title": "[TRTLLM-5188] fix: [AutoDeploy] unwaive AD build test",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T16:26:35Z",
    "closed_at": "2025-05-14T02:40:12Z",
    "merged_at": "2025-05-14T02:40:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4273"
  },
  {
    "number": 4272,
    "title": "feat: [AutoDeploy] DSV3 mla attn ref op",
    "user": "sugunav14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T16:26:33Z",
    "closed_at": "2025-05-14T17:58:21Z",
    "merged_at": "2025-05-14T17:58:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4272"
  },
  {
    "number": 4271,
    "title": "[TRTLLM-5188] fix: [AutoDeploy] Unwaive test_ad_build_small.py",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T16:25:02Z",
    "closed_at": "2025-05-13T16:26:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4271"
  },
  {
    "number": 4270,
    "title": "[fix] add missing () in branch condition",
    "user": "ixlmar",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T15:19:34Z",
    "closed_at": "2025-05-14T07:16:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4270"
  },
  {
    "number": 4269,
    "title": "[Infra] Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T14:50:50Z",
    "closed_at": "2025-05-13T15:06:17Z",
    "merged_at": "2025-05-13T15:06:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4269"
  },
  {
    "number": 4268,
    "title": "[Infra] Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T14:27:51Z",
    "closed_at": "2025-05-13T14:43:17Z",
    "merged_at": "2025-05-13T14:43:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4268"
  },
  {
    "number": 4267,
    "title": "test(perf): Add `Phi-4-mini-instruct` to perf tests",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T14:22:56Z",
    "closed_at": "2025-05-15T13:27:04Z",
    "merged_at": "2025-05-15T13:27:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4267"
  },
  {
    "number": 4266,
    "title": "CI: add fp8/fp4 ci on Qwen3-30B-A3B",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T12:46:32Z",
    "closed_at": "2025-05-14T06:38:04Z",
    "merged_at": "2025-05-14T06:38:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4266"
  },
  {
    "number": 4265,
    "title": "[https://nvbugspro.nvidia.com/bug/5243740][fix] deduce default max_tokens for trtllm-serve",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T11:56:08Z",
    "closed_at": "2025-05-18T16:34:41Z",
    "merged_at": "2025-05-18T16:34:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4265"
  },
  {
    "number": 4264,
    "title": "fix: XQA is not enabled when history_length < kMinHistoryTokensPerBlock.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T11:32:35Z",
    "closed_at": "2025-06-11T01:38:10Z",
    "merged_at": "2025-06-11T01:38:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4264"
  },
  {
    "number": 4263,
    "title": "Test: Improve model re-use in C++ DGX tests for CI stability",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T11:16:41Z",
    "closed_at": "2025-05-19T13:20:22Z",
    "merged_at": "2025-05-19T13:20:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4263"
  },
  {
    "number": 4262,
    "title": "Waive stress test.",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T11:02:35Z",
    "closed_at": "2025-05-13T13:01:57Z",
    "merged_at": "2025-05-13T13:01:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4262"
  },
  {
    "number": 4261,
    "title": "chore: bump version to 0.20.0rc3",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T10:14:33Z",
    "closed_at": "2025-05-14T02:15:25Z",
    "merged_at": "2025-05-14T02:15:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4261"
  },
  {
    "number": 4260,
    "title": "[doc] fix typo",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T09:32:59Z",
    "closed_at": "2025-06-06T05:17:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4260"
  },
  {
    "number": 4258,
    "title": "test: Add UT for moe trtllmgen",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T09:25:00Z",
    "closed_at": "2025-05-14T07:22:59Z",
    "merged_at": "2025-05-14T07:22:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4258"
  },
  {
    "number": 4257,
    "title": "test: add kv cache aware test cases to qa test list",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T09:17:50Z",
    "closed_at": "2025-05-16T04:47:02Z",
    "merged_at": "2025-05-16T04:47:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4257"
  },
  {
    "number": 4255,
    "title": "chore: Mass Integration 0.19",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T08:26:13Z",
    "closed_at": "2025-05-16T08:53:26Z",
    "merged_at": "2025-05-16T08:53:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4255"
  },
  {
    "number": 4254,
    "title": "test: add qa test list for rtx5090 and rtx_pro_6000",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T08:10:39Z",
    "closed_at": "2025-05-15T09:57:31Z",
    "merged_at": "2025-05-15T09:57:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4254"
  },
  {
    "number": 4252,
    "title": "[https://nvbugs/5220763] [test] Unwaive Mixtral FP8 TP2 test",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T07:20:25Z",
    "closed_at": "2025-05-13T07:55:33Z",
    "merged_at": "2025-05-13T07:55:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4252"
  },
  {
    "number": 4251,
    "title": "[AutoDeploy]simple example of nvtx marker insertion",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T07:06:10Z",
    "closed_at": "2025-10-09T02:46:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4251"
  },
  {
    "number": 4250,
    "title": "feat: [TRTLLM 4571] Support dynamic per-tensor FP8",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T05:49:29Z",
    "closed_at": "2025-05-16T05:33:59Z",
    "merged_at": "2025-05-16T05:33:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4250"
  },
  {
    "number": 4249,
    "title": "fix: add max_batch_size to the cuda graph batch sizes",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T05:20:27Z",
    "closed_at": "2025-05-16T04:59:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4249"
  },
  {
    "number": 4248,
    "title": "[test] add qa test mentioned in docs",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T05:13:40Z",
    "closed_at": "2025-05-15T05:37:11Z",
    "merged_at": "2025-05-15T05:37:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4248"
  },
  {
    "number": 4247,
    "title": "fix: correct batchsize dimension hint for tasks in enc_dec models",
    "user": "ducviet00",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T02:58:29Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4247"
  },
  {
    "number": 4246,
    "title": "doc: update qwen3 document",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T02:38:14Z",
    "closed_at": "2025-05-13T03:05:47Z",
    "merged_at": "2025-05-13T03:05:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4246"
  },
  {
    "number": 4244,
    "title": "refactor: use x is None instead of x == None.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T02:14:37Z",
    "closed_at": "2025-05-15T12:00:04Z",
    "merged_at": "2025-05-15T12:00:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4244"
  },
  {
    "number": 4243,
    "title": "[feat][TRTLLM-5018] Dis serving python runtime trt backend",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-13T01:59:24Z",
    "closed_at": "2025-05-23T02:01:07Z",
    "merged_at": "2025-05-23T02:01:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4243"
  },
  {
    "number": 4242,
    "title": "[TRTLLM-4932] Add QA accuracy tests for NIM-prioritized models",
    "user": "moraxu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T20:11:08Z",
    "closed_at": "2025-05-24T11:17:21Z",
    "merged_at": "2025-05-24T11:17:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4242"
  },
  {
    "number": 4240,
    "title": "[AutoDeploy]feat: Add an AutoDeploy compile backend that only calls torch.compile",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T19:20:58Z",
    "closed_at": "2025-05-16T00:38:16Z",
    "merged_at": "2025-05-16T00:38:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4240"
  },
  {
    "number": 4239,
    "title": "fix: reshape token_ids for lp in torch backend",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T19:02:44Z",
    "closed_at": "2025-05-13T00:43:48Z",
    "merged_at": "2025-05-13T00:43:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4239"
  },
  {
    "number": 4237,
    "title": "tests: fix llama4 tests",
    "user": "milesial",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T15:54:42Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4237"
  },
  {
    "number": 4236,
    "title": "Draft: add NVLM_D support",
    "user": "mwawrzos",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T15:47:01Z",
    "closed_at": "2025-07-08T12:45:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4236"
  },
  {
    "number": 4235,
    "title": "[Infra] - Update the upstream PyTorch dependency to 2.7.0",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T15:29:11Z",
    "closed_at": "2025-05-14T14:28:13Z",
    "merged_at": "2025-05-14T14:28:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4235"
  },
  {
    "number": 4234,
    "title": "Revert \"Add initial list of CODEOWNERS (#4105)\"",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T14:52:23Z",
    "closed_at": "2025-05-12T14:53:49Z",
    "merged_at": "2025-05-12T14:53:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4234"
  },
  {
    "number": 4233,
    "title": "fix: Upgrade h11 to 0.16.0",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T12:16:23Z",
    "closed_at": "2025-05-13T11:53:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4233"
  },
  {
    "number": 4232,
    "title": "feat: W4A16 GEMM",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T10:35:01Z",
    "closed_at": "2025-07-01T07:36:06Z",
    "merged_at": "2025-07-01T07:36:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4232"
  },
  {
    "number": 4231,
    "title": "infra: [TRTLLM-5072] Add SBSA release images",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T10:13:39Z",
    "closed_at": "2025-05-17T16:00:07Z",
    "merged_at": "2025-05-17T16:00:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4231"
  },
  {
    "number": 4230,
    "title": "test: fix for perf test script issue",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T10:13:37Z",
    "closed_at": "2025-05-13T02:29:20Z",
    "merged_at": "2025-05-13T02:29:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4230"
  },
  {
    "number": 4229,
    "title": "fix: Eagle decoding in TRT flow",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T09:36:34Z",
    "closed_at": "2025-05-14T14:10:52Z",
    "merged_at": "2025-05-14T14:10:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4229"
  },
  {
    "number": 4228,
    "title": "doc: remove version list in code branch",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T08:43:21Z",
    "closed_at": "2025-05-12T08:58:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4228"
  },
  {
    "number": 4227,
    "title": "fix: Reset planned states to avoid memory leak in TrtllmAttentionWrapper",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T08:22:24Z",
    "closed_at": "2025-05-12T15:25:55Z",
    "merged_at": "2025-05-12T15:25:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4227"
  },
  {
    "number": 4226,
    "title": "fix potential issues in allreduce fusion kernel and ut",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T08:21:34Z",
    "closed_at": "2025-05-19T09:38:30Z",
    "merged_at": "2025-05-19T09:38:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4226"
  },
  {
    "number": 4224,
    "title": "[perf] Manually free some tensors in TrtllmAttention to reduce memory usage",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T07:59:39Z",
    "closed_at": "2025-05-12T15:24:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4224"
  },
  {
    "number": 4223,
    "title": "[TRTLLM-5000][feat] NGram for test",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T07:25:29Z",
    "closed_at": "2025-05-20T08:26:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4223"
  },
  {
    "number": 4222,
    "title": "[https://nvbugs/5214229] [fix] Unwaive lm_head quantization case",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T07:04:52Z",
    "closed_at": "2025-05-12T12:23:07Z",
    "merged_at": "2025-05-12T12:23:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4222"
  },
  {
    "number": 4221,
    "title": "chore: Update CODEOWNERS",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T06:51:28Z",
    "closed_at": "2025-05-12T06:55:20Z",
    "merged_at": "2025-05-12T06:55:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4221"
  },
  {
    "number": 4220,
    "title": "doc: update switcher.json config",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T06:38:46Z",
    "closed_at": "2025-05-12T12:40:56Z",
    "merged_at": "2025-05-12T12:40:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4220"
  },
  {
    "number": 4216,
    "title": "Adding two-shot allreduce kernel and mnnvl multicasting buffer",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T04:19:49Z",
    "closed_at": "2025-05-21T19:42:37Z",
    "merged_at": "2025-05-21T19:42:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4216"
  },
  {
    "number": 4215,
    "title": "tests: PyTorch multimodal using keyword match",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T03:53:34Z",
    "closed_at": "2025-05-14T09:18:44Z",
    "merged_at": "2025-05-14T09:18:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4215"
  },
  {
    "number": 4214,
    "title": "opt: the perormance for dist-agg streaming generation",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T03:40:42Z",
    "closed_at": "2025-05-31T09:40:33Z",
    "merged_at": "2025-05-31T09:40:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4214"
  },
  {
    "number": 4213,
    "title": "fix: Fix input_scale no attribute issue in BF16 mode",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T02:21:44Z",
    "closed_at": "2025-05-14T00:57:14Z",
    "merged_at": "2025-05-14T00:57:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4213"
  },
  {
    "number": 4212,
    "title": "[Infra] Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T02:06:48Z",
    "closed_at": "2025-05-12T03:37:49Z",
    "merged_at": "2025-05-12T03:37:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4212"
  },
  {
    "number": 4211,
    "title": "[https://nvbugspro.nvidia.com/bug/5270564][test] skip per-hopper for llama4",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T01:52:25Z",
    "closed_at": "2025-05-12T07:27:16Z",
    "merged_at": "2025-05-12T07:27:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4211"
  },
  {
    "number": 4210,
    "title": "[CI] update pytorch only file list",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-12T01:48:07Z",
    "closed_at": "2025-05-12T02:06:26Z",
    "merged_at": "2025-05-12T02:06:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4210"
  },
  {
    "number": 4209,
    "title": "doc:update linux installation md.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-11T17:10:19Z",
    "closed_at": "2025-07-17T09:37:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4209"
  },
  {
    "number": 4208,
    "title": "fix: fix qwen3 rope to use xqa",
    "user": "dongjiyingdjy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-11T14:38:17Z",
    "closed_at": "2025-05-14T06:55:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4208"
  },
  {
    "number": 4207,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-11T12:14:16Z",
    "closed_at": "2025-05-14T09:59:06Z",
    "merged_at": "2025-05-14T09:59:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4207"
  },
  {
    "number": 4206,
    "title": "[CI] waive two multi-gpu test cases",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-11T11:38:14Z",
    "closed_at": "2025-05-12T00:04:48Z",
    "merged_at": "2025-05-12T00:04:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4206"
  },
  {
    "number": 4205,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-11T11:14:00Z",
    "closed_at": "2025-05-13T02:22:42Z",
    "merged_at": "2025-05-13T02:22:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4205"
  },
  {
    "number": 4204,
    "title": "[doc] fix: disaggreggated examples",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-11T03:01:23Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4204"
  },
  {
    "number": 4203,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-10T19:54:34Z",
    "closed_at": "2025-05-13T02:08:03Z",
    "merged_at": "2025-05-13T02:08:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4203"
  },
  {
    "number": 4202,
    "title": "[draft] Refactor quant in linear",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-10T13:34:54Z",
    "closed_at": "2025-05-27T13:39:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4202"
  },
  {
    "number": 4201,
    "title": "Integrate trtllm-gen kernels for QKVGemm, FC13+swiGLU, and FC2 for Llama4",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-10T06:42:23Z",
    "closed_at": "2025-05-12T00:52:17Z",
    "merged_at": "2025-05-12T00:52:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4201"
  },
  {
    "number": 4200,
    "title": "chore: PR to fix the formatting errors",
    "user": "mayani-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T23:22:49Z",
    "closed_at": "2025-05-09T23:31:37Z",
    "merged_at": "2025-05-09T23:31:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4200"
  },
  {
    "number": 4199,
    "title": "[TRTLLM-5188] fix: [AutoDeploy] update output shape of prepare_fused_mha_metadata_fake",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T22:51:04Z",
    "closed_at": "2025-05-12T15:11:40Z",
    "merged_at": "2025-05-12T15:11:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4199"
  },
  {
    "number": 4198,
    "title": "Added tests for Llama3.1-70B-BF16 on SM120",
    "user": "farazkh80",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T22:14:42Z",
    "closed_at": "2025-05-14T15:57:49Z",
    "merged_at": "2025-05-14T15:57:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4198"
  },
  {
    "number": 4195,
    "title": "Extend the Llama-Nemotron-Nano-8B perf-integration-tests (cpp)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T17:02:22Z",
    "closed_at": "2025-05-17T14:46:21Z",
    "merged_at": "2025-05-17T14:46:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4195"
  },
  {
    "number": 4194,
    "title": "Test main images CI result",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T16:42:24Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4194"
  },
  {
    "number": 4193,
    "title": "Update 0.19",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T14:36:22Z",
    "closed_at": "2025-05-09T14:47:22Z",
    "merged_at": "2025-05-09T14:47:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4193"
  },
  {
    "number": 4191,
    "title": "infra: [TRTLLM-325] Prepare for NGC release - multiplatform build",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T12:40:32Z",
    "closed_at": "2025-05-12T07:38:45Z",
    "merged_at": "2025-05-12T07:38:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4191"
  },
  {
    "number": 4190,
    "title": "fix: Revert NIXL and ETCD from the main image",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T10:46:32Z",
    "closed_at": "2025-05-14T07:37:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4190"
  },
  {
    "number": 4189,
    "title": "Cherry-pick commits from feat/llama4 to main",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T10:42:00Z",
    "closed_at": "2025-05-30T01:14:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4189"
  },
  {
    "number": 4188,
    "title": "[bug/5247505] fix: CP accuracy on Blackwell",
    "user": "DylanChen-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T09:54:28Z",
    "closed_at": "2025-05-14T09:40:50Z",
    "merged_at": "2025-05-14T09:40:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4188"
  },
  {
    "number": 4187,
    "title": "test: Remove CNN Dailymail tasks in favor of GSM8K",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T09:53:55Z",
    "closed_at": "2025-05-10T01:02:07Z",
    "merged_at": "2025-05-10T01:02:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4187"
  },
  {
    "number": 4186,
    "title": "test: amend regex match for perf throughput",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T09:19:51Z",
    "closed_at": "2025-05-09T09:33:26Z",
    "merged_at": "2025-05-09T09:33:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4186"
  },
  {
    "number": 4185,
    "title": "infra: open source fmha v2 kernels",
    "user": "qsang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T08:57:01Z",
    "closed_at": "2025-05-15T02:56:35Z",
    "merged_at": "2025-05-15T02:56:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4185"
  },
  {
    "number": 4184,
    "title": "fix: library path of nixl",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T08:13:27Z",
    "closed_at": "2025-05-09T08:31:55Z",
    "merged_at": "2025-05-09T08:31:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4184"
  },
  {
    "number": 4183,
    "title": "feat: Support Mistral Small 3.1 24B VLM in TRT workflow",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T08:09:35Z",
    "closed_at": "2025-05-13T19:47:22Z",
    "merged_at": "2025-05-13T19:47:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4183"
  },
  {
    "number": 4182,
    "title": "feat: Prefetch safetensors files before loading them",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T08:07:36Z",
    "closed_at": "2025-05-09T08:07:52Z",
    "merged_at": "2025-05-09T08:07:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4182"
  },
  {
    "number": 4181,
    "title": "fix NVBUG 5247845   ^gdr_copy",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T07:44:43Z",
    "closed_at": "2025-05-21T07:38:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4181"
  },
  {
    "number": 4180,
    "title": "add changes for fp8, nemotron-nas, API",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T07:38:34Z",
    "closed_at": "2025-05-18T15:27:25Z",
    "merged_at": "2025-05-18T15:27:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4180"
  },
  {
    "number": 4179,
    "title": "feat: Improve perf of AllGather-Top1 after LMHead",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T07:37:39Z",
    "closed_at": "2025-05-09T08:09:17Z",
    "merged_at": "2025-05-09T08:09:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4179"
  },
  {
    "number": 4178,
    "title": "Guard FC13+SwiGLU kernel use correctly for TP4",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T07:22:24Z",
    "closed_at": "2025-05-09T07:48:20Z",
    "merged_at": "2025-05-09T07:48:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4178"
  },
  {
    "number": 4176,
    "title": "test: amend default pytorch extra-llm-api-config.yml in perf test",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T07:01:16Z",
    "closed_at": "2025-05-09T08:46:49Z",
    "merged_at": "2025-05-09T08:46:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4176"
  },
  {
    "number": 4175,
    "title": "[TRTQA-2802][fix]: add --host for mgmn serve examples script",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T06:26:58Z",
    "closed_at": "2025-05-12T05:28:43Z",
    "merged_at": "2025-05-12T05:28:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4175"
  },
  {
    "number": 4174,
    "title": "Breaking change: perf: Enable scheduling overlap by default",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T05:02:40Z",
    "closed_at": "2025-05-15T06:27:37Z",
    "merged_at": "2025-05-15T06:27:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4174"
  },
  {
    "number": 4173,
    "title": "chore: Deprecate evaltool",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T04:53:34Z",
    "closed_at": "2025-05-09T12:31:54Z",
    "merged_at": "2025-05-09T12:31:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4173"
  },
  {
    "number": 4172,
    "title": "fix: Fix import of AllReduceFusionOp.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T04:49:38Z",
    "closed_at": "2025-05-09T05:04:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4172"
  },
  {
    "number": 4171,
    "title": "chore: Remove deprecated Python runtime benchmark",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T04:25:17Z",
    "closed_at": "2025-05-14T10:41:05Z",
    "merged_at": "2025-05-14T10:41:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4171"
  },
  {
    "number": 4170,
    "title": "exp: pull/4114",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T04:08:21Z",
    "closed_at": "2025-05-15T03:38:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4170"
  },
  {
    "number": 4169,
    "title": "chore: Fix pipeline break caused by previous PR (#4081) rebase + pipeline reuse",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T03:59:10Z",
    "closed_at": "2025-05-09T04:51:03Z",
    "merged_at": "2025-05-09T04:51:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4169"
  },
  {
    "number": 4168,
    "title": "fix: fix the gather_ids in eagle3 single-model implementation",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-09T01:58:41Z",
    "closed_at": "2025-05-09T02:33:41Z",
    "merged_at": "2025-05-09T02:33:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4168"
  },
  {
    "number": 4167,
    "title": "fix: draft target README and assertion for logits-based acceptance",
    "user": "mayani-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T22:27:57Z",
    "closed_at": "2025-05-09T23:08:47Z",
    "merged_at": "2025-05-09T23:08:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4167"
  },
  {
    "number": 4166,
    "title": "doc: Release V0.19 Perf Overview Update",
    "user": "zbpatel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T22:07:26Z",
    "closed_at": "2025-05-09T10:02:35Z",
    "merged_at": "2025-05-09T10:02:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4166"
  },
  {
    "number": 4165,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T21:16:41Z",
    "closed_at": "2025-05-09T08:56:30Z",
    "merged_at": "2025-05-09T08:56:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4165"
  },
  {
    "number": 4164,
    "title": "[feat] Allow overriding cli args with yaml file in trtllm-serve",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T18:42:48Z",
    "closed_at": "2025-05-09T01:19:06Z",
    "merged_at": "2025-05-09T01:19:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4164"
  },
  {
    "number": 4163,
    "title": "[feat] [AutoDeploy] Llama-4 Support",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T18:33:54Z",
    "closed_at": "2025-05-23T03:43:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4163"
  },
  {
    "number": 4162,
    "title": "[fix] [AutoDeploy] flashinfer usage on H100",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T18:24:14Z",
    "closed_at": "2025-05-08T22:00:58Z",
    "merged_at": "2025-05-08T22:00:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4162"
  },
  {
    "number": 4161,
    "title": "[TRTLLM-5054][fix] Removing repeated loading of input processor",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T17:07:26Z",
    "closed_at": "2025-05-16T00:04:58Z",
    "merged_at": "2025-05-16T00:04:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4161"
  },
  {
    "number": 4160,
    "title": "fix: bump xgrammar",
    "user": "milesial",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T15:49:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4160"
  },
  {
    "number": 4159,
    "title": "[nvbugs/5268808][fix] Fix the potential out-of-range-access issue of allreduce workspace.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T14:23:15Z",
    "closed_at": "2025-05-13T09:17:26Z",
    "merged_at": "2025-05-13T09:17:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4159"
  },
  {
    "number": 4158,
    "title": "Add test case for kv memory estimation",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T14:08:44Z",
    "closed_at": "2025-05-14T10:39:25Z",
    "merged_at": "2025-05-14T10:39:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4158"
  },
  {
    "number": 4157,
    "title": "fix: alltoall padding for chunked MoE",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T14:00:28Z",
    "closed_at": "2025-05-09T01:01:36Z",
    "merged_at": "2025-05-09T01:01:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4157"
  },
  {
    "number": 4156,
    "title": "[TRTLLM-5050][feat] Enable per-request stats with PyT backend ",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T13:21:26Z",
    "closed_at": "2025-05-13T01:35:15Z",
    "merged_at": "2025-05-13T01:35:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4156"
  },
  {
    "number": 4155,
    "title": "Feat: support exporting softmax statistics and update the kernel-selection heuristic",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T12:47:14Z",
    "closed_at": "2025-05-12T07:31:46Z",
    "merged_at": "2025-05-12T07:31:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4155"
  },
  {
    "number": 4153,
    "title": "remove cache_transceiver_prealloc_size",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T10:02:05Z",
    "closed_at": "2025-05-12T03:53:53Z",
    "merged_at": "2025-05-12T03:53:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4153"
  },
  {
    "number": 4152,
    "title": "infra: [TRTLLM-4392] Move SBSA build stage to Blossom",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T09:41:48Z",
    "closed_at": "2025-05-15T09:06:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4152"
  },
  {
    "number": 4151,
    "title": "[TRTLLM-4911] feat(scaffolding): make sampling_params only setable by controller",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T09:34:09Z",
    "closed_at": "2025-05-12T07:29:09Z",
    "merged_at": "2025-05-12T07:29:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4151"
  },
  {
    "number": 4150,
    "title": "chore:update modelopt to 0.29",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T09:32:26Z",
    "closed_at": "2025-05-12T02:32:19Z",
    "merged_at": "2025-05-12T02:32:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4150"
  },
  {
    "number": 4149,
    "title": "Ensure PDL is enabled for fc13 swiglu",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T08:30:03Z",
    "closed_at": "2025-05-09T00:02:56Z",
    "merged_at": "2025-05-09T00:02:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4149"
  },
  {
    "number": 4148,
    "title": "[Infra] Waive L0 flaky test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T07:58:16Z",
    "closed_at": "2025-05-08T09:23:45Z",
    "merged_at": "2025-05-08T09:23:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4148"
  },
  {
    "number": 4147,
    "title": "fix/ replace sanity test for nemotron h with a correctness test",
    "user": "omera-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T07:12:49Z",
    "closed_at": "2025-05-19T15:22:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4147"
  },
  {
    "number": 4146,
    "title": "perf: Fuse gemm setup function for SM90/SM100 MOE plugin path",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T05:40:55Z",
    "closed_at": "2025-05-21T02:00:36Z",
    "merged_at": "2025-05-21T02:00:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4146"
  },
  {
    "number": 4145,
    "title": "[TRTLLM-5007][feat] Add multimodal hashing support (image hashing)",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T05:10:25Z",
    "closed_at": "2025-06-09T17:59:57Z",
    "merged_at": "2025-06-09T17:59:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4145"
  },
  {
    "number": 4144,
    "title": "Draft: Integrate group_rms_norm with DeepSeek modeling for min-latency",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T05:09:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4144"
  },
  {
    "number": 4143,
    "title": "Fix TP8 for NVFP4 kv dupilcation.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T04:43:57Z",
    "closed_at": "2025-05-08T09:30:02Z",
    "merged_at": "2025-05-08T09:30:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4143"
  },
  {
    "number": 4142,
    "title": "enh: Enable option in trtllm-bench build subcommand to avoid loading weights",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T03:59:09Z",
    "closed_at": "2025-05-15T19:50:54Z",
    "merged_at": "2025-05-15T19:50:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4142"
  },
  {
    "number": 4141,
    "title": "[TRTLLM-5147][Qwen3] fix: fix bug of attention dp on qwen3_moe model",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T03:42:22Z",
    "closed_at": "2025-05-09T01:29:39Z",
    "merged_at": "2025-05-09T01:29:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4141"
  },
  {
    "number": 4140,
    "title": "feat: Prefetch safetensors files before loading them",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T03:26:23Z",
    "closed_at": "2025-05-13T05:35:31Z",
    "merged_at": "2025-05-13T05:35:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4140"
  },
  {
    "number": 4139,
    "title": "test: add llama_3.2_1B model and fix for test lora script issue",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T03:19:46Z",
    "closed_at": "2025-05-12T06:52:00Z",
    "merged_at": "2025-05-12T06:52:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4139"
  },
  {
    "number": 4138,
    "title": "chore: remove data stage in serve example on slurm",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T03:04:22Z",
    "closed_at": "2025-05-08T03:18:57Z",
    "merged_at": "2025-05-08T03:18:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4138"
  },
  {
    "number": 4137,
    "title": "Cherry-pick: Use multi-threading to load MoE expert weights",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T02:42:58Z",
    "closed_at": "2025-05-09T09:29:24Z",
    "merged_at": "2025-05-09T09:29:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4137"
  },
  {
    "number": 4136,
    "title": "test: Waive test_llm cases",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-08T00:47:26Z",
    "closed_at": "2025-05-08T03:53:17Z",
    "merged_at": "2025-05-08T03:53:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4136"
  },
  {
    "number": 4135,
    "title": "fix: Fix MOE benchmark to rotate buffers to prevent L2 cache reuse",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T23:00:40Z",
    "closed_at": "2025-07-15T01:40:43Z",
    "merged_at": "2025-07-15T01:40:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4135"
  },
  {
    "number": 4134,
    "title": "feat: Add disagg accuracy testing for DeepSeek V3 Lite",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T19:50:47Z",
    "closed_at": "2025-06-04T19:35:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4134"
  },
  {
    "number": 4133,
    "title": "[TRTLLM-5233][feat]: Add chunking to PyT heuristic for trtllm-bench.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T18:46:25Z",
    "closed_at": "2025-05-13T13:47:06Z",
    "merged_at": "2025-05-13T13:47:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4133"
  },
  {
    "number": 4132,
    "title": "[feat] Enable chunked context for flashinfer",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T18:35:32Z",
    "closed_at": "2025-05-15T02:59:39Z",
    "merged_at": "2025-05-15T02:59:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4132"
  },
  {
    "number": 4131,
    "title": "infra: WAR for Argument list too long of globalVars[CACHED_CHANGED_FILE_LIST]",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T18:11:06Z",
    "closed_at": "2025-05-08T06:36:18Z",
    "merged_at": "2025-05-08T06:36:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4131"
  },
  {
    "number": 4130,
    "title": "fix: change pp broadcast pattern for LPs",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T17:18:26Z",
    "closed_at": "2025-05-09T03:07:13Z",
    "merged_at": "2025-05-09T03:07:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4130"
  },
  {
    "number": 4129,
    "title": "docs:add torch flow supported model list.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T16:56:36Z",
    "closed_at": "2025-05-08T02:25:58Z",
    "merged_at": "2025-05-08T02:25:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4129"
  },
  {
    "number": 4128,
    "title": "test(perf): Add some `Llama-3_3-Nemotron-Super-49B-v1` integration-perf-tests (TRT flow, trtllm-bench)",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T16:39:26Z",
    "closed_at": "2025-05-19T19:00:48Z",
    "merged_at": "2025-05-19T19:00:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4128"
  },
  {
    "number": 4126,
    "title": "[fix] Fix relaxed acceptance to support enabling it in context phase",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T13:13:56Z",
    "closed_at": "2025-05-09T06:11:15Z",
    "merged_at": "2025-05-09T06:11:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4126"
  },
  {
    "number": 4125,
    "title": "Agent interface impl for NIXL",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T13:09:12Z",
    "closed_at": "2025-05-22T01:09:41Z",
    "merged_at": "2025-05-22T01:09:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4125"
  },
  {
    "number": 4124,
    "title": "test: Waive disagg accuracy test",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T12:24:08Z",
    "closed_at": "2025-05-08T05:39:08Z",
    "merged_at": "2025-05-08T05:39:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4124"
  },
  {
    "number": 4123,
    "title": "[TRTLLM-3330][feat] Support DeepSeek-R1 W4A8 on Hopper",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T12:20:21Z",
    "closed_at": "2025-05-14T07:48:07Z",
    "merged_at": "2025-05-14T07:48:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4123"
  },
  {
    "number": 4122,
    "title": "[Infra] - Update code ownership rules for public APIs",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T12:10:11Z",
    "closed_at": "2025-05-08T03:04:32Z",
    "merged_at": "2025-05-08T03:04:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4122"
  },
  {
    "number": 4121,
    "title": "API Breaking Change + Readability: \"decoder\"->\"sampler\"",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T10:54:42Z",
    "closed_at": "2025-05-16T15:52:26Z",
    "merged_at": "2025-05-16T15:52:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4121"
  },
  {
    "number": 4120,
    "title": "docs:update 0.19 doc.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T10:47:06Z",
    "closed_at": "2025-05-07T11:49:30Z",
    "merged_at": "2025-05-07T11:49:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4120"
  },
  {
    "number": 4119,
    "title": "[fix] trtllm-gen mla kernel warnings",
    "user": "zhhuang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T10:15:39Z",
    "closed_at": "2025-05-09T12:21:28Z",
    "merged_at": "2025-05-09T12:21:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4119"
  },
  {
    "number": 4117,
    "title": "Feat: support MTP for fmha_v2 based MLA kernels.",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T09:03:36Z",
    "closed_at": "2025-05-16T09:50:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4117"
  },
  {
    "number": 4116,
    "title": "fix: cleanup process tree for disaggregated test",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T07:56:43Z",
    "closed_at": "2025-05-21T03:01:14Z",
    "merged_at": "2025-05-21T03:01:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4116"
  },
  {
    "number": 4114,
    "title": "infra: Down the gcc toolset version from 13 to 11",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T07:30:31Z",
    "closed_at": "2025-05-15T03:08:51Z",
    "merged_at": "2025-05-15T03:08:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4114"
  },
  {
    "number": 4113,
    "title": "tests: https://nvbugs/5219534 remove failed tests from test list",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T05:20:26Z",
    "closed_at": "2025-05-12T06:13:40Z",
    "merged_at": "2025-05-12T06:13:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4113"
  },
  {
    "number": 4112,
    "title": "fix: Fix incorrect conversion of Gen TPS/user",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T05:13:13Z",
    "closed_at": "2025-05-08T22:34:53Z",
    "merged_at": "2025-05-08T22:34:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4112"
  },
  {
    "number": 4111,
    "title": "Draft: chore: Make GEMM config enums human readable for better logging",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T05:05:14Z",
    "closed_at": "2025-08-11T23:40:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4111"
  },
  {
    "number": 4110,
    "title": "fix: better method to help torch find nvtx3",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T03:25:48Z",
    "closed_at": "2025-05-15T08:42:30Z",
    "merged_at": "2025-05-15T08:42:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4110"
  },
  {
    "number": 4109,
    "title": "[Infra] - Update code ownership rules",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T02:00:39Z",
    "closed_at": "2025-05-07T05:35:27Z",
    "merged_at": "2025-05-07T05:35:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4109"
  },
  {
    "number": 4108,
    "title": "doc: update release notes",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T01:44:15Z",
    "closed_at": "2025-05-07T06:01:16Z",
    "merged_at": "2025-05-07T06:01:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4108"
  },
  {
    "number": 4107,
    "title": "[TRTLLM-5057][fix] Adding option to specify a set of token ids for multimodal tokens",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-07T00:45:05Z",
    "closed_at": "2025-05-07T04:15:41Z",
    "merged_at": "2025-05-07T04:15:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4107"
  },
  {
    "number": 4106,
    "title": "Fix Pipeline Parallelism in Llama4",
    "user": "v-shobhit",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T22:11:15Z",
    "closed_at": "2025-05-13T05:54:37Z",
    "merged_at": "2025-05-13T05:54:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4106"
  },
  {
    "number": 4105,
    "title": "Add initial list of CODEOWNERS",
    "user": "kevinch-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T21:47:56Z",
    "closed_at": "2025-05-09T23:16:48Z",
    "merged_at": "2025-05-09T23:16:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4105"
  },
  {
    "number": 4104,
    "title": "[nvbug/5262268][fix] Fix trtllm-bench for llama 4",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T21:44:11Z",
    "closed_at": "2025-05-09T04:27:58Z",
    "merged_at": "2025-05-09T04:27:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4104"
  },
  {
    "number": 4103,
    "title": "Test",
    "user": "nv-yilinf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T20:29:37Z",
    "closed_at": "2025-05-06T20:34:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4103"
  },
  {
    "number": 4102,
    "title": "refactor: Copy sequence lengths once in decoder setup",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T17:41:49Z",
    "closed_at": "2025-05-16T14:03:56Z",
    "merged_at": "2025-05-16T14:03:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4102"
  },
  {
    "number": 4101,
    "title": "[https://nvbugspro.nvidia.com/bug/5238626] illegal memory address when running llama 4 with cuda graph enabled",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T16:52:52Z",
    "closed_at": "2025-05-13T06:58:55Z",
    "merged_at": "2025-05-13T06:58:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4101"
  },
  {
    "number": 4100,
    "title": "doc: TRTLLM-4797 Update perf-analysis.md",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T15:19:36Z",
    "closed_at": "2025-05-08T09:24:46Z",
    "merged_at": "2025-05-08T09:24:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4100"
  },
  {
    "number": 4098,
    "title": "doc: Update version switcher",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T14:44:48Z",
    "closed_at": "2025-05-06T15:50:03Z",
    "merged_at": "2025-05-06T15:50:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4098"
  },
  {
    "number": 4097,
    "title": "enh: Update docker Makefile to use only the visible GPUs of machine",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T14:38:27Z",
    "closed_at": "2025-05-07T10:01:35Z",
    "merged_at": "2025-05-07T10:01:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4097"
  },
  {
    "number": 4096,
    "title": "refactor: Unify request order in TRT and PyTorch workflow",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T14:36:32Z",
    "closed_at": "2025-05-20T16:49:27Z",
    "merged_at": "2025-05-20T16:49:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4096"
  },
  {
    "number": 4095,
    "title": "[https://nvbugspro.nvidia.com/bug/5260676]test: skip fp8 quantization case for pre-ada",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T14:27:43Z",
    "closed_at": "2025-05-09T05:30:16Z",
    "merged_at": "2025-05-09T05:30:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4095"
  },
  {
    "number": 4094,
    "title": "fix: llmapi-launch add add trtllm-bench test with engine building",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T14:15:11Z",
    "closed_at": "2025-05-07T12:22:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4094"
  },
  {
    "number": 4093,
    "title": "[Qwen3] chore: fix bug of fused_moe on tp > 1",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T14:10:13Z",
    "closed_at": "2025-05-07T03:06:38Z",
    "merged_at": "2025-05-07T03:06:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4093"
  },
  {
    "number": 4092,
    "title": "[TRTLLM-5171] chore: Remove GptSession/V1 from TRT workflow",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T14:09:45Z",
    "closed_at": "2025-05-14T21:10:04Z",
    "merged_at": "2025-05-14T21:10:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4092"
  },
  {
    "number": 4091,
    "title": "fix [nvbug/5220766]: llmapi-launch add add trtllm-bench test with engine building",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T14:04:22Z",
    "closed_at": "2025-05-21T02:18:01Z",
    "merged_at": "2025-05-21T02:18:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4091"
  },
  {
    "number": 4090,
    "title": "[TRTLLM-5081] [test] Align parametrize_with_ids to the pytest behavior",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T13:32:59Z",
    "closed_at": "2025-05-12T23:41:52Z",
    "merged_at": "2025-05-12T23:41:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4090"
  },
  {
    "number": 4089,
    "title": "[#4085][fix] Fix `apply_per_channel_scale` for extremely large input sequence length.",
    "user": "StudyingShao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T12:14:48Z",
    "closed_at": "2025-05-09T03:57:02Z",
    "merged_at": "2025-05-09T03:57:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4089"
  },
  {
    "number": 4087,
    "title": "chore: misc static analysis fixes and generating xqa source header at config time",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T11:42:04Z",
    "closed_at": "2025-08-08T06:43:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4087"
  },
  {
    "number": 4086,
    "title": "Cherry-pick trtllm-gen from feat/llama4 to main",
    "user": "chenfeiz0326",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T10:32:30Z",
    "closed_at": "2025-05-08T21:13:02Z",
    "merged_at": "2025-05-08T21:13:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4086"
  },
  {
    "number": 4084,
    "title": "[fix] Fix add_dummy_requests for spec decoding cases",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T09:47:37Z",
    "closed_at": "2025-05-09T08:52:52Z",
    "merged_at": "2025-05-09T08:52:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4084"
  },
  {
    "number": 4083,
    "title": "test: add qwen3 and disaggregated serving accuracy tests to qa test list",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T09:46:00Z",
    "closed_at": "2025-05-09T03:03:03Z",
    "merged_at": "2025-05-09T03:03:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4083"
  },
  {
    "number": 4082,
    "title": "Integrate trtllm-gen kernel for the QKV gemm in llama4",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T09:34:16Z",
    "closed_at": "2025-05-28T18:43:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4082"
  },
  {
    "number": 4081,
    "title": "chore: Clean up the legacy DeepseekAllreudceFusionOp.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T09:22:50Z",
    "closed_at": "2025-05-09T02:20:41Z",
    "merged_at": "2025-05-09T02:20:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4081"
  },
  {
    "number": 4080,
    "title": "feat: Fallback to NCCL for various patterns when input size is large.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T08:12:06Z",
    "closed_at": "2025-05-08T18:13:13Z",
    "merged_at": "2025-05-08T18:13:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4080"
  },
  {
    "number": 4079,
    "title": "fix: Enable test case disabled by nvbug 5245262",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T06:19:04Z",
    "closed_at": "2025-05-07T12:28:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4079"
  },
  {
    "number": 4078,
    "title": "refactor: Allow models to override apply_qk_norm.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T05:47:24Z",
    "closed_at": "2025-05-12T11:38:25Z",
    "merged_at": "2025-05-12T11:38:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4078"
  },
  {
    "number": 4077,
    "title": "[https://nvbugspro.nvidia.com/bug/5244006, https://nvbugspro.nvidia.com/bug/5240350][test] Unwaive guided decoding tests",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T03:45:22Z",
    "closed_at": "2025-06-13T15:48:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4077"
  },
  {
    "number": 4076,
    "title": "Reduce computation for the top layer in chunked context.",
    "user": "ming-wei",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T03:17:07Z",
    "closed_at": "2025-09-18T02:46:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4076"
  },
  {
    "number": 4075,
    "title": "[Don't merge][Draft]Just reproduce hang",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T03:07:53Z",
    "closed_at": "2025-05-19T03:25:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4075"
  },
  {
    "number": 4074,
    "title": "[fix] Loosen the thresholds of test_attention_mla",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T00:59:36Z",
    "closed_at": "2025-05-06T03:31:09Z",
    "merged_at": "2025-05-06T03:31:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4074"
  },
  {
    "number": 4073,
    "title": "doc: update qwen3 document",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-06T00:27:31Z",
    "closed_at": "2025-05-06T00:42:51Z",
    "merged_at": "2025-05-06T00:42:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4073"
  },
  {
    "number": 4070,
    "title": "fix: Update log query regex in perf integration test to match trtllm-bench reporting",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T20:21:56Z",
    "closed_at": "2025-05-08T04:03:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4070"
  },
  {
    "number": 4069,
    "title": "[fix][nvbug/5244009] Fix llama 4 test lists/scout accuracy issue",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T19:26:53Z",
    "closed_at": "2025-05-09T14:45:14Z",
    "merged_at": "2025-05-09T14:45:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4069"
  },
  {
    "number": 4068,
    "title": "fix: Set `trust_remote_code=True` when verifying config.json load",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T18:42:01Z",
    "closed_at": "2025-05-07T17:22:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4068"
  },
  {
    "number": 4067,
    "title": "feat: Reduce branch overhead in groupRMSNorm kernels",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T18:29:24Z",
    "closed_at": "2025-05-07T16:55:27Z",
    "merged_at": "2025-05-07T16:55:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4067"
  },
  {
    "number": 4066,
    "title": "feat: Support the Structural Tag in guided decoding",
    "user": "Ubospica",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T17:47:48Z",
    "closed_at": "2025-05-12T09:24:51Z",
    "merged_at": "2025-05-12T09:24:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4066"
  },
  {
    "number": 4065,
    "title": "[feat/] enable attention DP in Llama4 maverick model - part 1",
    "user": "zihaok",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T17:03:51Z",
    "closed_at": "2025-05-07T21:06:41Z",
    "merged_at": "2025-05-07T21:06:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4065"
  },
  {
    "number": 4064,
    "title": "feat:[AutoDeploy] utilize torch._inductor.pattern_matcher to write pattern matcher",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T16:55:25Z",
    "closed_at": "2025-06-04T23:52:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4064"
  },
  {
    "number": 4063,
    "title": "[feat] trtllmGen MoE routing: added support for top groups and top K bounds",
    "user": "MatthiasKohl",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T16:08:57Z",
    "closed_at": "2025-06-12T22:00:02Z",
    "merged_at": "2025-06-12T22:00:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4063"
  },
  {
    "number": 4062,
    "title": "[Test]: Clean up stale waives",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T14:00:39Z",
    "closed_at": "2025-05-05T14:13:14Z",
    "merged_at": "2025-05-05T14:13:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4062"
  },
  {
    "number": 4061,
    "title": "Refactor: Lookahead TRT workflow",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T13:29:24Z",
    "closed_at": "2025-11-03T23:42:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4061"
  },
  {
    "number": 4060,
    "title": "[nvbugs/5247414][fix] Optimize the AutoTuner cache access code to reduce host code overhead.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T12:50:37Z",
    "closed_at": "2025-05-06T02:57:42Z",
    "merged_at": "2025-05-06T02:57:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4060"
  },
  {
    "number": 4059,
    "title": "[Test]: Waive unsupported tests",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T12:39:27Z",
    "closed_at": "2025-05-05T12:51:50Z",
    "merged_at": "2025-05-05T12:51:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4059"
  },
  {
    "number": 4058,
    "title": "Fix: fix bug of qwen3 moe",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T12:00:19Z",
    "closed_at": "2025-05-06T00:20:16Z",
    "merged_at": "2025-05-06T00:20:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4058"
  },
  {
    "number": 4057,
    "title": "feat: adopt new logprob definition in PyTorch flow",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T08:50:26Z",
    "closed_at": "2025-05-08T12:16:41Z",
    "merged_at": "2025-05-08T12:16:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4057"
  },
  {
    "number": 4056,
    "title": "feat: run mmlu and summarize without engine_dir.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T08:48:56Z",
    "closed_at": "2025-05-05T11:35:08Z",
    "merged_at": "2025-05-05T11:35:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4056"
  },
  {
    "number": 4055,
    "title": "[https://nvbugs/5257681] fix: draft/target probs shape",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T08:47:08Z",
    "closed_at": "2025-05-06T07:56:44Z",
    "merged_at": "2025-05-06T07:56:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4055"
  },
  {
    "number": 4054,
    "title": "[feat]: Allow for a settable end-of-sequence/padding token in max throughput benchmark. (#3776)",
    "user": "nvpohanh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T08:44:24Z",
    "closed_at": "2025-05-05T08:45:02Z",
    "merged_at": "2025-05-05T08:45:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4054"
  },
  {
    "number": 4053,
    "title": "[TRTQA-2861][test]: add nemotron and llama4 cases into qa test",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T07:16:00Z",
    "closed_at": "2025-05-08T10:10:41Z",
    "merged_at": "2025-05-08T10:10:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4053"
  },
  {
    "number": 4052,
    "title": "Leverage trtllm-gen kernel for FC+SwiGLU",
    "user": "eopXD",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T04:42:16Z",
    "closed_at": "2025-05-05T04:53:40Z",
    "merged_at": "2025-05-05T04:53:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4052"
  },
  {
    "number": 4051,
    "title": "[Infra] Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T04:35:01Z",
    "closed_at": "2025-05-05T04:53:22Z",
    "merged_at": "2025-05-05T04:53:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4051"
  },
  {
    "number": 4049,
    "title": "[Infra][TRTLLM-4374] Upgrade TRT 10.10.0 GA, CUDA 12.9 GA and DLFW 25.04",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T04:03:41Z",
    "closed_at": "2025-05-13T06:59:12Z",
    "merged_at": "2025-05-13T06:59:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4049"
  },
  {
    "number": 4048,
    "title": "fix waive",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-05T01:51:00Z",
    "closed_at": "2025-05-05T05:17:30Z",
    "merged_at": "2025-05-05T05:17:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4048"
  },
  {
    "number": 4047,
    "title": "feat: Add heuristic for GroupRMSNorm kernel selection.",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-04T23:58:55Z",
    "closed_at": "2025-05-13T00:52:53Z",
    "merged_at": "2025-05-13T00:52:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4047"
  },
  {
    "number": 4046,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-04T12:14:33Z",
    "closed_at": "2025-05-08T10:04:44Z",
    "merged_at": "2025-05-08T10:04:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4046"
  },
  {
    "number": 4042,
    "title": "fix: [nvbug/5247414] Fix the perf regression caused by insufficient cache warmup.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-03T10:04:17Z",
    "closed_at": "2025-05-03T16:46:23Z",
    "merged_at": "2025-05-03T16:46:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4042"
  },
  {
    "number": 4041,
    "title": "fix: [nvbug/5247414] Fix the perf regression cause by insufficient cache warmup.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-03T06:46:37Z",
    "closed_at": "2025-05-03T10:04:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4041"
  },
  {
    "number": 4040,
    "title": "fix: apply rope twice in Qwen3.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-03T05:35:17Z",
    "closed_at": "2025-05-05T07:12:46Z",
    "merged_at": "2025-05-05T07:12:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4040"
  },
  {
    "number": 4039,
    "title": "chore: cleanup llmapi for 1.0",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T23:36:18Z",
    "closed_at": "2025-05-06T16:50:17Z",
    "merged_at": "2025-05-06T16:50:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4039"
  },
  {
    "number": 4038,
    "title": "[Deepseek] Add fp8 kvcache test",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T22:52:11Z",
    "closed_at": "2025-05-04T22:06:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4038"
  },
  {
    "number": 4036,
    "title": "test: Add disaggregated serving accuracy tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T18:51:29Z",
    "closed_at": "2025-05-05T15:56:59Z",
    "merged_at": "2025-05-05T15:56:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4036"
  },
  {
    "number": 4035,
    "title": "test: Test OOB access issue in penaltyKernel for endId=-1",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T17:12:00Z",
    "closed_at": "2025-05-05T17:24:29Z",
    "merged_at": "2025-05-05T17:24:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4035"
  },
  {
    "number": 4034,
    "title": "feat: [TRTLLM-5623][nvbugs/5261055][nvbugs/5170160] non-invasive pipeline parallelism",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T16:33:39Z",
    "closed_at": "2025-05-15T20:16:54Z",
    "merged_at": "2025-05-15T20:16:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4034"
  },
  {
    "number": 4032,
    "title": "[nvbug/5248986][fix] Skip debugCheckSemaphores in stream capture mode",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T14:44:24Z",
    "closed_at": "2025-05-05T17:24:10Z",
    "merged_at": "2025-05-05T17:24:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4032"
  },
  {
    "number": 4031,
    "title": "[DRAFT] setting attention prior",
    "user": "vklimkov-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T12:24:09Z",
    "closed_at": "2025-07-17T17:47:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4031"
  },
  {
    "number": 4030,
    "title": "[DRAFT] Introducing multi-vocab token sampling for audio generation",
    "user": "vklimkov-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T11:03:32Z",
    "closed_at": "2025-06-05T20:15:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4030"
  },
  {
    "number": 4029,
    "title": "fix: instantiate decoder early in pytorch",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T10:36:42Z",
    "closed_at": "2025-05-05T08:31:53Z",
    "merged_at": "2025-05-05T08:31:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4029"
  },
  {
    "number": 4028,
    "title": "feat:enable kvcache to be reused during request generation",
    "user": "narutolhy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T10:16:28Z",
    "closed_at": "2025-07-10T13:18:02Z",
    "merged_at": "2025-07-10T13:18:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4028"
  },
  {
    "number": 4027,
    "title": "Refactor: Restructure C++ tests for better modularisation of non-shared code",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T09:46:16Z",
    "closed_at": "2025-05-09T18:16:51Z",
    "merged_at": "2025-05-09T18:16:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4027"
  },
  {
    "number": 4026,
    "title": "fix: Properly get decoding mode according to same logic as cpp.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T09:20:03Z",
    "closed_at": "2025-05-06T13:53:17Z",
    "merged_at": "2025-05-06T13:53:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4026"
  },
  {
    "number": 4025,
    "title": "[fix] [nvbug/5252057] Fix kv cache reuse on PyTorch multimodal",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T08:01:02Z",
    "closed_at": "2025-05-02T17:53:06Z",
    "merged_at": "2025-05-02T17:53:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4025"
  },
  {
    "number": 4024,
    "title": "[AutoDeploy][perf] Further optimize flashinfer backend in AutoDeploy",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T05:57:37Z",
    "closed_at": "2025-05-06T02:46:36Z",
    "merged_at": "2025-05-06T02:46:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4024"
  },
  {
    "number": 4022,
    "title": "model/infra: add ci and doc for qwen3",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T03:28:36Z",
    "closed_at": "2025-05-02T06:13:39Z",
    "merged_at": "2025-05-02T06:13:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4022"
  },
  {
    "number": 4021,
    "title": "infra/doc: Add document and CI for qwen3",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T03:23:48Z",
    "closed_at": "2025-05-02T03:27:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4021"
  },
  {
    "number": 4020,
    "title": "feat: Enable AutoDeploy to llm-eval example",
    "user": "meenchen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T03:19:46Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4020"
  },
  {
    "number": 4019,
    "title": "feat: Add Slurm support and enable RTX Pro 6000 testing pipeline in CI",
    "user": "yuanjingx87",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-02T00:59:13Z",
    "closed_at": "2025-05-08T07:15:36Z",
    "merged_at": "2025-05-08T07:15:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4019"
  },
  {
    "number": 4018,
    "title": "TorchLLM: Pass local dir to processor creation",
    "user": "milesial",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T22:44:04Z",
    "closed_at": "2025-05-06T19:25:04Z",
    "merged_at": "2025-05-06T19:25:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4018"
  },
  {
    "number": 4017,
    "title": "Draft: Support long context LLama 4 (flashinfer backend)",
    "user": "vanshilshah97",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T22:26:02Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4017"
  },
  {
    "number": 4016,
    "title": "[Deepseek] Refactor Deepseek Decoder layer",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T22:24:39Z",
    "closed_at": "2025-05-07T17:43:11Z",
    "merged_at": "2025-05-07T17:43:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4016"
  },
  {
    "number": 4015,
    "title": "[Draft][AutoDeploy] Split prefill and decode in AD's flashinfer backend",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T22:09:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4015"
  },
  {
    "number": 4014,
    "title": "[fix] keep using system python for dev install",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T21:48:46Z",
    "closed_at": "2025-05-03T05:38:47Z",
    "merged_at": "2025-05-03T05:38:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4014"
  },
  {
    "number": 4013,
    "title": "[fix] support llama + eagle head checkpoint conversion",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T17:21:42Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4013"
  },
  {
    "number": 4011,
    "title": "bench: TRTLLM-4936 Port benchmark_serving.py",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T14:44:26Z",
    "closed_at": "2025-05-07T01:45:15Z",
    "merged_at": "2025-05-07T01:45:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4011"
  },
  {
    "number": 4010,
    "title": "model: support Qwen3",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T11:42:01Z",
    "closed_at": "2025-05-01T15:12:42Z",
    "merged_at": "2025-05-01T15:12:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4010"
  },
  {
    "number": 4009,
    "title": "fix: Fallback to NCCL for various patterns when input size is large.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T10:16:07Z",
    "closed_at": "2025-05-01T22:17:16Z",
    "merged_at": "2025-05-01T22:17:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4009"
  },
  {
    "number": 4008,
    "title": "Truncate image embeddings for Qwen2.5-7B-instruct model",
    "user": "mayani-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T08:11:01Z",
    "closed_at": "2025-05-02T18:32:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4008"
  },
  {
    "number": 4007,
    "title": "[AutoDeploy] Make all ranks agree on kv-cache size",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T04:44:01Z",
    "closed_at": "2025-05-01T20:07:29Z",
    "merged_at": "2025-05-01T20:07:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4007"
  },
  {
    "number": 4006,
    "title": "Doc: Fix H200 DeepSeek R1 perf doc",
    "user": "jiahanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T02:33:06Z",
    "closed_at": "2025-05-02T01:01:08Z",
    "merged_at": "2025-05-02T01:01:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4006"
  },
  {
    "number": 4005,
    "title": "Put the computation of Q and K norm (in attn) into a single CUDA stream, and get a 5% - 8% throughput improvement on Qwen3 4B and Qwen3 - moe 30B - A3B.",
    "user": "shaonvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T00:51:13Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4005"
  },
  {
    "number": 4004,
    "title": "[https://nvbugspro.nvidia.com/bug/5247762][fix] Add more valid outputs for multimodal accuracy test",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-01T00:21:15Z",
    "closed_at": "2025-05-01T00:47:56Z",
    "merged_at": "2025-05-01T00:47:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4004"
  },
  {
    "number": 4002,
    "title": "[DO NOT MERGE]experiments: set `self.max_position_embedding=None` in Attention module",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T22:32:41Z",
    "closed_at": "2025-05-05T23:27:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4002"
  },
  {
    "number": 4001,
    "title": "[Deepseek][fix] Fix Deepseek MTP with moe_backend=TRTLLM",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T22:04:57Z",
    "closed_at": "2025-05-02T06:47:23Z",
    "merged_at": "2025-05-02T06:47:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4001"
  },
  {
    "number": 3999,
    "title": "feat: Support Gemma3-1b-it in Pytorch workflow",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T20:41:40Z",
    "closed_at": "2025-05-14T06:02:45Z",
    "merged_at": "2025-05-14T06:02:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3999"
  },
  {
    "number": 3998,
    "title": "[fix] Fix llama4 + eagle3",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T20:07:35Z",
    "closed_at": "2025-05-08T23:20:28Z",
    "merged_at": "2025-05-08T23:20:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3998"
  },
  {
    "number": 3997,
    "title": "waive test_tinyllama_guided_decoding",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T19:26:29Z",
    "closed_at": "2025-04-30T19:46:33Z",
    "merged_at": "2025-04-30T19:46:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3997"
  },
  {
    "number": 3996,
    "title": "fix: [nvbug/5251968] Fix NVLink version decoding.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T18:34:06Z",
    "closed_at": "2025-05-06T05:56:51Z",
    "merged_at": "2025-05-06T05:56:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3996"
  },
  {
    "number": 3994,
    "title": "Llama4 processor fixes",
    "user": "milesial",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T16:38:55Z",
    "closed_at": "2025-05-01T04:45:54Z",
    "merged_at": "2025-05-01T04:45:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3994"
  },
  {
    "number": 3993,
    "title": "chore:update .gitignore for doc building task.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T15:37:40Z",
    "closed_at": "2025-05-07T09:45:20Z",
    "merged_at": "2025-05-07T09:45:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3993"
  },
  {
    "number": 3992,
    "title": "chore: enhance the cmake experience by ignoring the additional semicolon",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T15:21:05Z",
    "closed_at": "2025-05-08T10:43:37Z",
    "merged_at": "2025-05-08T10:43:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3992"
  },
  {
    "number": 3991,
    "title": "fix[nvbug5245262]: skip add new slot if request has slot 0",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T15:16:25Z",
    "closed_at": "2025-05-06T05:46:39Z",
    "merged_at": "2025-05-06T05:46:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3991"
  },
  {
    "number": 3990,
    "title": "chore: reduce size of the docker images",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T15:13:10Z",
    "closed_at": "2025-05-09T11:31:29Z",
    "merged_at": "2025-05-09T11:31:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3990"
  },
  {
    "number": 3989,
    "title": "fix:https://nvbugs/5246733",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T14:36:34Z",
    "closed_at": "2025-05-01T14:52:32Z",
    "merged_at": "2025-05-01T14:52:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3989"
  },
  {
    "number": 3988,
    "title": "fix: [nvbug/5241627] Fix AllReduce kernel hang issue when both tp and pp are enabled.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T12:05:49Z",
    "closed_at": "2025-05-05T03:33:25Z",
    "merged_at": "2025-05-05T03:33:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3988"
  },
  {
    "number": 3986,
    "title": "docs:update 0.19 docs",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T11:09:47Z",
    "closed_at": "2025-04-30T11:25:26Z",
    "merged_at": "2025-04-30T11:25:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3986"
  },
  {
    "number": 3985,
    "title": "[TRTLLM-3925, https://nvbugs/5245262] [fix] Normalize LLM.generate API",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T10:56:58Z",
    "closed_at": "2025-05-07T03:06:23Z",
    "merged_at": "2025-05-07T03:06:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3985"
  },
  {
    "number": 3984,
    "title": "fix: Correctly sizes seqslotmanager considering pp.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T10:51:11Z",
    "closed_at": "2025-05-02T18:06:33Z",
    "merged_at": "2025-05-02T18:06:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3984"
  },
  {
    "number": 3983,
    "title": "feat: support to trace executor loop.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T10:11:47Z",
    "closed_at": "2025-05-05T02:26:34Z",
    "merged_at": "2025-05-05T02:26:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3983"
  },
  {
    "number": 3981,
    "title": "infra: Add NIXL into the Dockerfile",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T08:36:56Z",
    "closed_at": "2025-05-08T12:38:05Z",
    "merged_at": "2025-05-08T12:38:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3981"
  },
  {
    "number": 3980,
    "title": "refactor: Move ModelSpec to core library",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T08:00:13Z",
    "closed_at": "2025-05-03T17:39:09Z",
    "merged_at": "2025-05-03T17:39:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3980"
  },
  {
    "number": 3979,
    "title": "Feat: Variable-Beam-Width-Search (VBWS) part4",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T07:57:24Z",
    "closed_at": "2025-05-12T20:32:30Z",
    "merged_at": "2025-05-12T20:32:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3979"
  },
  {
    "number": 3978,
    "title": "[fix] Enable pp tests",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T07:41:21Z",
    "closed_at": "2025-05-14T02:51:20Z",
    "merged_at": "2025-05-14T02:51:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3978"
  },
  {
    "number": 3976,
    "title": "doc: Update 0.19.0 release notes",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T05:50:09Z",
    "closed_at": "2025-05-06T00:59:07Z",
    "merged_at": "2025-05-06T00:59:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3976"
  },
  {
    "number": 3975,
    "title": "[https://nvbugspro.nvidia.com/bug/5247148][fix] Attention DP with overlap scheduler",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T05:48:33Z",
    "closed_at": "2025-05-01T02:49:46Z",
    "merged_at": "2025-05-01T02:49:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3975"
  },
  {
    "number": 3974,
    "title": "feat: conditional disaggregation in disagg server",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T05:34:07Z",
    "closed_at": "2025-05-21T01:57:46Z",
    "merged_at": "2025-05-21T01:57:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3974"
  },
  {
    "number": 3973,
    "title": "chore: update internal_cutlass_kernels.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T05:25:12Z",
    "closed_at": "2025-04-30T14:13:17Z",
    "merged_at": "2025-04-30T14:13:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3973"
  },
  {
    "number": 3972,
    "title": "fix[nvbug-5228840]: Remove test cases of feature not supported anymore",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T02:30:32Z",
    "closed_at": "2025-05-22T03:18:58Z",
    "merged_at": "2025-05-22T03:18:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3972"
  },
  {
    "number": 3971,
    "title": "chore: update multi-gpu trigger file list",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-30T00:16:48Z",
    "closed_at": "2025-04-30T01:15:26Z",
    "merged_at": "2025-04-30T01:15:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3971"
  },
  {
    "number": 3970,
    "title": "fix: Add attention workspace memory check",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T23:15:51Z",
    "closed_at": "2025-05-01T06:51:09Z",
    "merged_at": "2025-05-01T06:51:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3970"
  },
  {
    "number": 3969,
    "title": "Chore: 2025-04-29 CI allowlist update",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T21:39:43Z",
    "closed_at": "2025-05-05T02:05:04Z",
    "merged_at": "2025-05-05T02:05:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3969"
  },
  {
    "number": 3968,
    "title": "[TRTLLM-4623][fix] sync internal cutlass kernel changes",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T20:32:55Z",
    "closed_at": "2025-04-30T00:57:29Z",
    "merged_at": "2025-04-30T00:57:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3968"
  },
  {
    "number": 3967,
    "title": "[fix] Eagle-2 LLMAPI pybind argument fix.",
    "user": "jhaotingc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T20:17:21Z",
    "closed_at": "2025-05-29T19:23:26Z",
    "merged_at": "2025-05-29T19:23:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3967"
  },
  {
    "number": 3957,
    "title": "[fix] Pad requests to maximum draft length in spec decode",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T19:18:39Z",
    "closed_at": "2025-04-30T15:02:21Z",
    "merged_at": "2025-04-30T15:02:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3957"
  },
  {
    "number": 3956,
    "title": "Re-position PyTorch arch as v1.0",
    "user": "laikhtewari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T17:27:27Z",
    "closed_at": "2025-06-18T05:11:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3956"
  },
  {
    "number": 3954,
    "title": "chore: remove release branch codeowners from main",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T16:55:00Z",
    "closed_at": "2025-04-30T03:59:43Z",
    "merged_at": "2025-04-30T03:59:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3954"
  },
  {
    "number": 3953,
    "title": "align decoder state with trtllm decoder",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T14:50:32Z",
    "closed_at": "2025-05-11T15:28:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3953"
  },
  {
    "number": 3952,
    "title": "[https://nvbugs/5123103][fix] Fix torch compile for DeepSeekV3",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T11:40:21Z",
    "closed_at": "2025-05-19T14:12:26Z",
    "merged_at": "2025-05-19T14:12:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3952"
  },
  {
    "number": 3951,
    "title": "[https://nvbugs/5238105] fix: ModelRunnerCpp num_return_sequences",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T10:39:09Z",
    "closed_at": "2025-06-06T10:31:12Z",
    "merged_at": "2025-06-06T10:31:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3951"
  },
  {
    "number": 3950,
    "title": "test: Add fp8kv to DS-v3-lite integration tests.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T10:24:00Z",
    "closed_at": "2025-05-09T05:35:05Z",
    "merged_at": "2025-05-09T05:35:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3950"
  },
  {
    "number": 3949,
    "title": "chore: bump version to 0.20.0rc2",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T10:21:50Z",
    "closed_at": "2025-04-30T03:44:43Z",
    "merged_at": "2025-04-30T03:44:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3949"
  },
  {
    "number": 3948,
    "title": "infra: Fix pipeline step error in post merge",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T10:10:52Z",
    "closed_at": "2025-05-09T07:26:15Z",
    "merged_at": "2025-05-09T07:26:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3948"
  },
  {
    "number": 3947,
    "title": "chore: skip pipeline parallelism test of pytorch flow",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T09:39:58Z",
    "closed_at": "2025-04-29T17:00:16Z",
    "merged_at": "2025-04-29T17:00:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3947"
  },
  {
    "number": 3946,
    "title": "[TRTLLM-4480][doc] Documentation for new accuracy test suite and trtllm-eval",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T09:20:05Z",
    "closed_at": "2025-05-08T11:35:23Z",
    "merged_at": "2025-05-08T11:35:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3946"
  },
  {
    "number": 3945,
    "title": "fix: Move all casters to customCasters.",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T08:58:43Z",
    "closed_at": "2025-05-02T11:08:28Z",
    "merged_at": "2025-05-02T11:08:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3945"
  },
  {
    "number": 3943,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T08:53:17Z",
    "closed_at": "2025-05-01T15:43:11Z",
    "merged_at": "2025-05-01T15:43:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3943"
  },
  {
    "number": 3942,
    "title": "fix cache transfer buffer",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T08:21:01Z",
    "closed_at": "2025-05-07T01:49:45Z",
    "merged_at": "2025-05-07T01:49:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3942"
  },
  {
    "number": 3941,
    "title": "debug multi-gpu ci",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T08:16:00Z",
    "closed_at": "2025-04-29T09:39:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3941"
  },
  {
    "number": 3940,
    "title": "Debug multi-gpu CI",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T08:08:12Z",
    "closed_at": "2025-04-29T08:09:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3940"
  },
  {
    "number": 3939,
    "title": "docs:fix https://nvbugs/5244616 by removing new invalid links.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T08:07:18Z",
    "closed_at": "2025-04-29T08:49:17Z",
    "merged_at": "2025-04-29T08:49:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3939"
  },
  {
    "number": 3938,
    "title": "Revert \"fix: Update num_of_ctx_tokens in iteration stats\"",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T08:03:29Z",
    "closed_at": "2025-04-29T08:07:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3938"
  },
  {
    "number": 3936,
    "title": "[TRTLLM-5000][feat] Pytorch implementation of ngram drafter",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T07:27:28Z",
    "closed_at": "2025-05-21T02:40:00Z",
    "merged_at": "2025-05-21T02:40:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3936"
  },
  {
    "number": 3935,
    "title": "chore: Remove duplicated get_sm_version.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T07:22:31Z",
    "closed_at": "2025-04-30T03:43:54Z",
    "merged_at": "2025-04-30T03:43:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3935"
  },
  {
    "number": 3934,
    "title": "feat: NIXL interface integration",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T06:08:33Z",
    "closed_at": "2025-05-19T10:18:23Z",
    "merged_at": "2025-05-19T10:18:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3934"
  },
  {
    "number": 3933,
    "title": "chore: revert PR 3751",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T05:50:13Z",
    "closed_at": "2025-04-29T23:44:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3933"
  },
  {
    "number": 3932,
    "title": "Updating the multimodal models README to add steps for running phi-4-multimodal instruct",
    "user": "mayani-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T05:32:28Z",
    "closed_at": "2025-05-09T22:42:58Z",
    "merged_at": "2025-05-09T22:42:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3932"
  },
  {
    "number": 3931,
    "title": "[https://nvbugspro.nvidia.com/bug/5239633][test] skip fp8 gemm for pre-hopper",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T05:24:30Z",
    "closed_at": "2025-04-30T04:22:19Z",
    "merged_at": "2025-04-30T04:22:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3931"
  },
  {
    "number": 3930,
    "title": "chore: change log level of some text from info to debug",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T05:20:52Z",
    "closed_at": "2025-04-29T05:38:35Z",
    "merged_at": "2025-04-29T05:38:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3930"
  },
  {
    "number": 3929,
    "title": "[NVBUG 5247699]Fix mixtral fp4 llmapi bug.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T05:05:21Z",
    "closed_at": "2025-04-30T06:56:58Z",
    "merged_at": "2025-04-30T06:56:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3929"
  },
  {
    "number": 3928,
    "title": "fix: revert https://github.com/NVIDIA/TensorRT-LLM/pull/3858",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T03:06:00Z",
    "closed_at": "2025-04-29T03:26:14Z",
    "merged_at": "2025-04-29T03:26:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3928"
  },
  {
    "number": 3927,
    "title": "[CI] increase H100 CI nodes for PyTorch only pipelines",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T02:44:57Z",
    "closed_at": "2025-04-29T02:58:43Z",
    "merged_at": "2025-04-29T02:58:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3927"
  },
  {
    "number": 3926,
    "title": "Cherry pick https://github.com/NVIDIA/TensorRT-LLM/pull/3906/",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T01:46:50Z",
    "closed_at": "2025-04-29T01:55:13Z",
    "merged_at": "2025-04-29T01:55:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3926"
  },
  {
    "number": 3925,
    "title": "unwaive disagg tests",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-29T01:32:01Z",
    "closed_at": "2025-04-30T08:44:01Z",
    "merged_at": "2025-04-30T08:44:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3925"
  },
  {
    "number": 3924,
    "title": "Retrofit sentence bert into BertForSeqClassification",
    "user": "caronzh03",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T22:43:44Z",
    "closed_at": "2025-05-20T00:28:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3924"
  },
  {
    "number": 3923,
    "title": "[TRTLLM-4883][fix]: Update output speed calculation.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T22:01:32Z",
    "closed_at": "2025-04-29T03:04:13Z",
    "merged_at": "2025-04-29T03:04:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3923"
  },
  {
    "number": 3922,
    "title": "feat:[AutoDeploy] E2E build example for llama4 VLM",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T21:14:29Z",
    "closed_at": "2025-07-02T23:29:34Z",
    "merged_at": "2025-07-02T23:29:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3922"
  },
  {
    "number": 3921,
    "title": "waive test_attention_no_cache",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T20:12:11Z",
    "closed_at": "2025-04-28T20:57:01Z",
    "merged_at": "2025-04-28T20:57:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3921"
  },
  {
    "number": 3920,
    "title": "Llama4: Fix PP",
    "user": "v-shobhit",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T19:27:13Z",
    "closed_at": "2025-05-06T22:10:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3920"
  },
  {
    "number": 3919,
    "title": "fix(test): remove random context seq lengths and set random seed",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T17:54:04Z",
    "closed_at": "2025-04-29T02:08:05Z",
    "merged_at": "2025-04-29T02:08:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3919"
  },
  {
    "number": 3918,
    "title": "docs:fix https://nvbugs/5244616 by removing invalid links.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T15:39:35Z",
    "closed_at": "2025-04-28T23:58:52Z",
    "merged_at": "2025-04-28T23:58:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3918"
  },
  {
    "number": 3917,
    "title": "feat: parallel q_b_proj and concat",
    "user": "hello-11",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T12:32:27Z",
    "closed_at": "2025-04-29T14:07:06Z",
    "merged_at": "2025-04-29T14:07:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3917"
  },
  {
    "number": 3916,
    "title": "fix: get head_dim from model’s config.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T11:46:23Z",
    "closed_at": "2025-04-29T15:04:30Z",
    "merged_at": "2025-04-29T15:04:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3916"
  },
  {
    "number": 3914,
    "title": "chore: add num_scheduled_requests into print_log",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T10:02:41Z",
    "closed_at": "2025-04-29T03:22:22Z",
    "merged_at": "2025-04-29T03:22:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3914"
  },
  {
    "number": 3913,
    "title": "test: skip tests on b200",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T09:39:55Z",
    "closed_at": "2025-05-09T06:51:56Z",
    "merged_at": "2025-05-09T06:51:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3913"
  },
  {
    "number": 3912,
    "title": "fix: Fixing minor typo in allreduce kernel selection",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T09:27:22Z",
    "closed_at": "2025-04-28T15:06:50Z",
    "merged_at": "2025-04-28T15:06:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3912"
  },
  {
    "number": 3911,
    "title": "fix[nvbug-5228840]: Add debug log memory infomation for memory allocation error",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T09:21:31Z",
    "closed_at": "2025-04-29T02:53:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3911"
  },
  {
    "number": 3910,
    "title": "Add docs about DeepSeek-R1 long context support.",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T08:11:37Z",
    "closed_at": "2025-04-28T10:33:06Z",
    "merged_at": "2025-04-28T10:33:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3910"
  },
  {
    "number": 3909,
    "title": "fix:https://nvbugs/5234033 enable starcoder trt-flow with transforme…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T08:03:20Z",
    "closed_at": "2025-05-15T03:16:46Z",
    "merged_at": "2025-05-15T03:16:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3909"
  },
  {
    "number": 3908,
    "title": "docs: Add KV Cache Management documentation",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T08:01:56Z",
    "closed_at": "2025-05-21T06:39:29Z",
    "merged_at": "2025-05-21T06:39:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3908"
  },
  {
    "number": 3907,
    "title": "[fix] optimize cudaMemGetInfo for TllmGenFmhaRunner",
    "user": "zhhuang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T07:51:09Z",
    "closed_at": "2025-04-29T06:17:07Z",
    "merged_at": "2025-04-29T06:17:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3907"
  },
  {
    "number": 3906,
    "title": "[https://nvbugs/5247300] fix(requirements): fix neither 'setup.py' nor 'pyproject.toml' found",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T07:40:37Z",
    "closed_at": "2025-04-28T10:35:20Z",
    "merged_at": "2025-04-28T10:35:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3906"
  },
  {
    "number": 3905,
    "title": "[TRTLLM-4535][infra]: Add marker TIMEOUT for test level",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T07:08:06Z",
    "closed_at": "2025-05-26T06:30:40Z",
    "merged_at": "2025-05-26T06:30:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3905"
  },
  {
    "number": 3904,
    "title": "[Infra] Revert #3759",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T06:49:55Z",
    "closed_at": "2025-04-28T07:42:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3904"
  },
  {
    "number": 3903,
    "title": "Fix the link of doc",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T06:31:41Z",
    "closed_at": "2025-04-28T06:41:40Z",
    "merged_at": "2025-04-28T06:41:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3903"
  },
  {
    "number": 3902,
    "title": "[feat] Support HyperCLOVAX-SEED-Text language part",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T04:35:57Z",
    "closed_at": "2025-05-12T08:05:14Z",
    "merged_at": "2025-05-12T08:05:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3902"
  },
  {
    "number": 3901,
    "title": "fix: fix bug of fused_moe torch.cuda.Stream() additional memory cost on release 0.19",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T03:32:28Z",
    "closed_at": "2025-04-30T04:25:27Z",
    "merged_at": "2025-04-30T04:25:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3901"
  },
  {
    "number": 3900,
    "title": "Draft: Infra: Test timeout method is thread",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T03:09:27Z",
    "closed_at": "2025-05-19T03:26:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3900"
  },
  {
    "number": 3899,
    "title": "feat: fix erros on scaffolding README",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T03:03:50Z",
    "closed_at": "2025-04-29T02:15:06Z",
    "merged_at": "2025-04-29T02:15:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3899"
  },
  {
    "number": 3898,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T03:01:39Z",
    "closed_at": "2025-04-28T03:04:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3898"
  },
  {
    "number": 3897,
    "title": "ci: waive test_ptp_quickstart_multimodal[qwen2.5-vl-7b-instruct-Qwen2.5-VL-7B-Instruct-image]",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T02:52:15Z",
    "closed_at": "2025-04-28T05:12:53Z",
    "merged_at": "2025-04-28T05:12:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3897"
  },
  {
    "number": 3896,
    "title": "chore: remove DummyKvCacheManager.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T02:42:34Z",
    "closed_at": "2025-04-29T01:59:37Z",
    "merged_at": "2025-04-29T01:59:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3896"
  },
  {
    "number": 3895,
    "title": "[Infra] Waive l0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T02:21:59Z",
    "closed_at": "2025-04-28T03:00:39Z",
    "merged_at": "2025-04-28T03:00:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3895"
  },
  {
    "number": 3894,
    "title": "Test: waive intermittent test hang",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-28T00:41:47Z",
    "closed_at": "2025-04-28T00:53:21Z",
    "merged_at": "2025-04-28T00:53:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3894"
  },
  {
    "number": 3893,
    "title": "refactor: Introduce MpiTag enumeration and update MPI function signatures",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T17:55:14Z",
    "closed_at": "2025-05-04T11:24:29Z",
    "merged_at": "2025-05-04T11:24:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3893"
  },
  {
    "number": 3892,
    "title": "fix: request termination in pipeline parallelism",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T16:11:26Z",
    "closed_at": "2025-05-05T13:51:42Z",
    "merged_at": "2025-05-05T13:51:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3892"
  },
  {
    "number": 3891,
    "title": "[feat] support ModelOpt NemotronH FP8 quantized checkpoints in TRTLLM pytorch flow",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T13:01:47Z",
    "closed_at": "2025-04-29T15:51:43Z",
    "merged_at": "2025-04-29T15:51:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3891"
  },
  {
    "number": 3890,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T12:13:54Z",
    "closed_at": "2025-04-30T03:07:48Z",
    "merged_at": "2025-04-30T03:07:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3890"
  },
  {
    "number": 3888,
    "title": "[https://nvbugspro.nvidia.com/bug/5246419][fix] Align default setting & remove unnecessary check for chat and completion",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T09:24:04Z",
    "closed_at": "2025-05-07T06:42:53Z",
    "merged_at": "2025-05-07T06:42:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3888"
  },
  {
    "number": 3887,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T08:58:44Z",
    "closed_at": "2025-04-28T06:29:35Z",
    "merged_at": "2025-04-28T06:29:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3887"
  },
  {
    "number": 3886,
    "title": "ci: waive precession test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T07:37:34Z",
    "closed_at": "2025-04-27T08:47:19Z",
    "merged_at": "2025-04-27T08:47:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3886"
  },
  {
    "number": 3885,
    "title": "feat: support multi lora adapters and TP",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T04:46:46Z",
    "closed_at": "2025-05-08T15:45:45Z",
    "merged_at": "2025-05-08T15:45:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3885"
  },
  {
    "number": 3884,
    "title": "fix: remote mpi session abort",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T02:25:27Z",
    "closed_at": "2025-04-29T10:17:31Z",
    "merged_at": "2025-04-29T10:17:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3884"
  },
  {
    "number": 3883,
    "title": "feat: Llama 4 chunk attention",
    "user": "vanshilshah97",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-27T00:33:35Z",
    "closed_at": "2025-05-06T22:06:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3883"
  },
  {
    "number": 3882,
    "title": "refactor: (part1) Add contraints doc for fusedMoe module.",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-26T15:20:32Z",
    "closed_at": "2025-04-29T14:23:04Z",
    "merged_at": "2025-04-29T14:23:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3882"
  },
  {
    "number": 3881,
    "title": "feat: integrate modelopt hf export into our quantization script",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-26T09:09:28Z",
    "closed_at": "2025-08-08T06:42:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3881"
  },
  {
    "number": 3880,
    "title": "pick Boyang's change to fix multi streams",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-26T02:39:24Z",
    "closed_at": "2025-04-28T04:04:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3880"
  },
  {
    "number": 3878,
    "title": "fix: Merge PP overlap and non-overlap executor loop",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-26T01:40:34Z",
    "closed_at": "2025-05-13T22:04:37Z",
    "merged_at": "2025-05-13T22:04:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3878"
  },
  {
    "number": 3877,
    "title": "fix: [https://nvbugspro.nvidia.com/bug/5242406][fix] Fix fp8 kvcache support",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-26T00:20:34Z",
    "closed_at": "2025-04-29T02:31:10Z",
    "merged_at": "2025-04-29T02:31:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3877"
  },
  {
    "number": 3876,
    "title": "feat: Re-enable Llama4 fusion and add AllReduce CUDA Graph Fix",
    "user": "zihaok",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T20:39:29Z",
    "closed_at": "2025-04-29T22:26:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3876"
  },
  {
    "number": 3875,
    "title": "perf: [TRTLLM-4717][perf] Set CUDA graph max batch size and padding in throughput benchmark.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T20:23:31Z",
    "closed_at": "2025-05-09T15:20:53Z",
    "merged_at": "2025-05-09T15:20:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3875"
  },
  {
    "number": 3873,
    "title": "chore: update pytorch only change file list",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T19:50:59Z",
    "closed_at": "2025-04-25T20:03:52Z",
    "merged_at": "2025-04-25T20:03:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3873"
  },
  {
    "number": 3872,
    "title": "feat: Add `py_state` member in `LlmRequest` class",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T18:45:42Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3872"
  },
  {
    "number": 3871,
    "title": "TRTLLM-4875 feat: Add version switcher to doc",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T18:05:30Z",
    "closed_at": "2025-04-25T23:18:49Z",
    "merged_at": "2025-04-25T23:18:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3871"
  },
  {
    "number": 3870,
    "title": "doc: support main page redirection",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T17:48:02Z",
    "closed_at": "2025-04-25T17:50:51Z",
    "merged_at": "2025-04-25T17:50:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3870"
  },
  {
    "number": 3869,
    "title": "fix: [nvbugs/5066257] serialization improvments",
    "user": "coldwaterq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T16:47:14Z",
    "closed_at": "2025-05-23T05:06:30Z",
    "merged_at": "2025-05-23T05:06:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3869"
  },
  {
    "number": 3868,
    "title": "Test: Split C++ unit tests for CI granularity",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T16:36:57Z",
    "closed_at": "2025-04-25T20:30:59Z",
    "merged_at": "2025-04-25T20:30:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3868"
  },
  {
    "number": 3867,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T15:12:12Z",
    "closed_at": "2025-04-28T06:32:50Z",
    "merged_at": "2025-04-28T06:32:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3867"
  },
  {
    "number": 3866,
    "title": "feat: refactoring dataset generation and adding tests",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T12:22:18Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3866"
  },
  {
    "number": 3865,
    "title": "feat: add relaxed acceptance for DS",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T09:03:05Z",
    "closed_at": "2025-05-01T13:50:37Z",
    "merged_at": "2025-05-01T13:50:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3865"
  },
  {
    "number": 3864,
    "title": "tests: skip writing prepare_dataset output to logs, and add llama_v3.1_8b_fp8, llama_v3.3_70b_fp8, llama_v3.1_405b_fp4 models",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T08:44:25Z",
    "closed_at": "2025-05-07T05:56:36Z",
    "merged_at": "2025-05-07T05:56:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3864"
  },
  {
    "number": 3863,
    "title": "fix: Fix FMHA-based MLA in the generation phase and add MLA unit test",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T08:14:57Z",
    "closed_at": "2025-04-29T01:09:43Z",
    "merged_at": "2025-04-29T01:09:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3863"
  },
  {
    "number": 3862,
    "title": "fix: [https://nvbugspro.nvidia.com/bug/5243482] If FlashMLA is used, the existence of FMHA based MLA kernels should not be checked.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T07:58:43Z",
    "closed_at": "2025-04-30T06:27:39Z",
    "merged_at": "2025-04-30T06:27:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3862"
  },
  {
    "number": 3861,
    "title": "Add multi-version documents",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T07:51:19Z",
    "closed_at": "2025-04-25T17:35:44Z",
    "merged_at": "2025-04-25T17:35:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3861"
  },
  {
    "number": 3860,
    "title": "fix: fix bug of deepseek gropu_size setting",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T07:34:03Z",
    "closed_at": "2025-04-27T01:10:37Z",
    "merged_at": "2025-04-27T01:10:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3860"
  },
  {
    "number": 3859,
    "title": "infra: [TRTLLM-4475][TRTLLM-4565] Add pipeline hierarchy and basic info in the Jenkins job page",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T07:31:03Z",
    "closed_at": "2025-05-06T08:40:00Z",
    "merged_at": "2025-05-06T08:40:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3859"
  },
  {
    "number": 3858,
    "title": "fix: [nvbug/5234873] Detect pmix and raise error when mpirun is not used.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T07:14:57Z",
    "closed_at": "2025-04-26T13:49:42Z",
    "merged_at": "2025-04-26T13:49:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3858"
  },
  {
    "number": 3856,
    "title": "feat: add health_generate route to openai serving",
    "user": "dsingal0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T07:11:05Z",
    "closed_at": "2025-05-20T03:25:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3856"
  },
  {
    "number": 3855,
    "title": "feat: Add multimodal embedding field in LlmRequest",
    "user": "katec846",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T06:45:51Z",
    "closed_at": "2025-05-01T04:23:31Z",
    "merged_at": "2025-05-01T04:23:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3855"
  },
  {
    "number": 3854,
    "title": "feat: enable PP on Llama4",
    "user": "v-shobhit",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T06:44:01Z",
    "closed_at": "2025-05-01T20:36:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3854"
  },
  {
    "number": 3853,
    "title": "[infra] Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T06:33:36Z",
    "closed_at": "2025-04-25T09:32:22Z",
    "merged_at": "2025-04-25T09:32:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3853"
  },
  {
    "number": 3852,
    "title": "fix: add warmup flag into py_executor to prevent enable profiler during wa…",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T06:02:16Z",
    "closed_at": "2025-04-27T11:22:42Z",
    "merged_at": "2025-04-27T11:22:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3852"
  },
  {
    "number": 3851,
    "title": "feat: Low Precision Allreduce for PCIe based GPU",
    "user": "kanghui0204",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T04:16:55Z",
    "closed_at": "2025-05-14T08:45:45Z",
    "merged_at": "2025-05-14T08:45:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3851"
  },
  {
    "number": 3850,
    "title": "chore: [DEMONSTRATION ONLY] 1st Mass integration of release/0.19",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T01:58:20Z",
    "closed_at": "2025-05-06T02:57:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3850"
  },
  {
    "number": 3849,
    "title": "feat: AutoDeploy fp8 quantization support for bmm",
    "user": "meenchen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T00:14:56Z",
    "closed_at": "2025-06-30T16:36:34Z",
    "merged_at": "2025-06-30T16:36:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3849"
  },
  {
    "number": 3848,
    "title": "[chore] Add Llama 4 Maverick to quickstart README",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T23:57:06Z",
    "closed_at": "2025-04-25T17:04:25Z",
    "merged_at": "2025-04-25T17:04:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3848"
  },
  {
    "number": 3847,
    "title": "fix:[AutoDeploy] Patch for torch load_state_dict()",
    "user": "sugunav14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T23:25:42Z",
    "closed_at": "2025-04-25T18:03:48Z",
    "merged_at": "2025-04-25T18:03:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3847"
  },
  {
    "number": 3846,
    "title": "TRTLLM-4875 feat: Add version switcher to doc",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T18:31:30Z",
    "closed_at": "2025-04-25T21:42:47Z",
    "merged_at": "2025-04-25T21:42:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3846"
  },
  {
    "number": 3845,
    "title": "feat: Mistral-Large-2 support in the Pytorch workflow",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T18:07:36Z",
    "closed_at": "2025-04-30T12:12:40Z",
    "merged_at": "2025-04-30T12:12:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3845"
  },
  {
    "number": 3844,
    "title": "temp",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T16:43:50Z",
    "closed_at": "2025-05-26T13:13:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3844"
  },
  {
    "number": 3843,
    "title": "feat: added support for Mistral models in Pytorch workflow and HF quantization script",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T15:48:31Z",
    "closed_at": "2025-04-26T13:39:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3843"
  },
  {
    "number": 3841,
    "title": "chore: Mass integration of release/0.19 into main",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T12:14:54Z",
    "closed_at": "2025-04-29T08:57:22Z",
    "merged_at": "2025-04-29T08:57:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3841"
  },
  {
    "number": 3840,
    "title": "[TRTLLM-2795] feat: Add yarn support for other models in trt-flow",
    "user": "uchihatmtkinu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T10:58:58Z",
    "closed_at": "2025-05-15T03:03:58Z",
    "merged_at": "2025-05-15T03:03:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3840"
  },
  {
    "number": 3839,
    "title": "[https://nvbugspro.nvidia.com/bug/5241495][fix] CUDA Graph padding with overlap scheduler",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T10:26:52Z",
    "closed_at": "2025-04-25T08:56:48Z",
    "merged_at": "2025-04-25T08:56:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3839"
  },
  {
    "number": 3838,
    "title": "fix: fix lora case failure",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T10:00:00Z",
    "closed_at": "2025-04-24T14:29:09Z",
    "merged_at": "2025-04-24T14:29:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3838"
  },
  {
    "number": 3837,
    "title": "feat: Enable overlap scheduling test on trtllm_decoder",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T09:37:26Z",
    "closed_at": "2025-04-25T10:40:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3837"
  },
  {
    "number": 3836,
    "title": "fix: trtllm-serve hang in stress test and ds v3 stress parameter update",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T08:53:14Z",
    "closed_at": "2025-05-06T08:52:30Z",
    "merged_at": "2025-05-06T08:52:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3836"
  },
  {
    "number": 3835,
    "title": "[TRTLLM-4786] infra: add scaffolding paths to pytorch only files",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T08:51:32Z",
    "closed_at": "2025-04-28T05:49:27Z",
    "merged_at": "2025-04-28T05:49:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3835"
  },
  {
    "number": 3834,
    "title": "chore: bump version to 0.20.0rc1",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T06:15:18Z",
    "closed_at": "2025-04-24T09:43:38Z",
    "merged_at": "2025-04-24T09:43:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3834"
  },
  {
    "number": 3833,
    "title": "chore: cleanup perf_evaluator code",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T06:04:43Z",
    "closed_at": "2025-05-19T05:21:37Z",
    "merged_at": "2025-05-19T05:21:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3833"
  },
  {
    "number": 3832,
    "title": "Fix: Revert commit 25f9669",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T05:44:16Z",
    "closed_at": "2025-04-24T06:03:21Z",
    "merged_at": "2025-04-24T06:03:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3832"
  },
  {
    "number": 3831,
    "title": "feat: add kv cache aware router",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T05:10:07Z",
    "closed_at": "2025-05-12T11:23:57Z",
    "merged_at": "2025-05-12T11:23:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3831"
  },
  {
    "number": 3830,
    "title": "[https://nvbugspro.nvidia.com/bug/5241590] Fix test_llama_lora",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T04:32:42Z",
    "closed_at": "2025-04-24T16:20:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3830"
  },
  {
    "number": 3829,
    "title": "refactor: Clean up allreduce module for Deepseek V3 model",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T04:23:02Z",
    "closed_at": "2025-04-30T23:56:36Z",
    "merged_at": "2025-04-30T23:56:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3829"
  },
  {
    "number": 3828,
    "title": "refactor: skip autotune and torch compile in second warmup",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T03:02:54Z",
    "closed_at": "2025-04-28T08:53:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3828"
  },
  {
    "number": 3827,
    "title": "[https://nvbugs/5154414][fix] Balanced layer to PP rank assignment",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T02:19:43Z",
    "closed_at": "2025-05-16T16:29:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3827"
  },
  {
    "number": 3826,
    "title": "[infra] Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T01:53:48Z",
    "closed_at": "2025-04-24T02:11:22Z",
    "merged_at": "2025-04-24T02:11:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3826"
  },
  {
    "number": 3825,
    "title": "fix: trtllm-bench build trt engine on slurm",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T01:11:58Z",
    "closed_at": "2025-04-27T14:26:23Z",
    "merged_at": "2025-04-27T14:26:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3825"
  },
  {
    "number": 3824,
    "title": "feat: support task collection for to collect information (#3328)",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T01:07:34Z",
    "closed_at": "2025-05-09T09:09:02Z",
    "merged_at": "2025-05-09T09:09:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3824"
  },
  {
    "number": 3823,
    "title": "chore: Partition LlmArgs into TorchLlmArgs and TrtLlmArgs",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-24T00:22:20Z",
    "closed_at": "2025-05-22T01:40:57Z",
    "merged_at": "2025-05-22T01:40:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3823"
  },
  {
    "number": 3822,
    "title": "test(perf): Add Llama-3.1-Nemotron-Nano-8B-v1 to QA Perf Tests",
    "user": "venkywonka",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T23:26:17Z",
    "closed_at": "2025-05-07T00:17:56Z",
    "merged_at": "2025-05-07T00:17:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3822"
  },
  {
    "number": 3821,
    "title": "[infra] Improve llama4 parallelism test coverage",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T23:09:48Z",
    "closed_at": "2025-05-02T20:15:05Z",
    "merged_at": "2025-05-02T20:15:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3821"
  },
  {
    "number": 3819,
    "title": "fix: [Deepseek] Pass hidden_states_fp4 to shared_experts",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T21:43:51Z",
    "closed_at": "2025-04-24T20:12:14Z",
    "merged_at": "2025-04-24T20:12:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3819"
  },
  {
    "number": 3818,
    "title": "chore: fix some invalid paths of contrib models",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T21:21:33Z",
    "closed_at": "2025-04-23T21:36:17Z",
    "merged_at": "2025-04-23T21:36:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3818"
  },
  {
    "number": 3817,
    "title": "[feat] Add relaxed sampling for spec decode",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T19:54:58Z",
    "closed_at": "2025-05-06T14:56:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3817"
  },
  {
    "number": 3815,
    "title": "[https://nvbugs/5178445][fix] Skip blackwell tests for sm120",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T18:36:03Z",
    "closed_at": "2025-04-29T16:53:36Z",
    "merged_at": "2025-04-29T16:53:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3815"
  },
  {
    "number": 3814,
    "title": "doc: fix path after examples migration",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T18:02:23Z",
    "closed_at": "2025-04-23T18:36:47Z",
    "merged_at": "2025-04-23T18:36:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3814"
  },
  {
    "number": 3811,
    "title": "chore: increase A30 for cpp test",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T15:08:59Z",
    "closed_at": "2025-04-24T23:34:39Z",
    "merged_at": "2025-04-24T23:34:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3811"
  },
  {
    "number": 3810,
    "title": "chore: waive some unit tests",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T13:45:57Z",
    "closed_at": "2025-04-23T13:53:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3810"
  },
  {
    "number": 3809,
    "title": "[https://nvbugspro.nvidia.com/bug/5238602][fix] Package lm_eval configuration files",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T13:25:47Z",
    "closed_at": "2025-04-24T01:57:01Z",
    "merged_at": "2025-04-24T01:57:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3809"
  },
  {
    "number": 3808,
    "title": "Infra: Move some functions to trtllm_utils",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T12:04:05Z",
    "closed_at": "2025-09-02T00:55:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3808"
  },
  {
    "number": 3807,
    "title": "[TRTLLM-4638\t][feat] add best of n support with reward model in scaffolding",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T11:47:26Z",
    "closed_at": "2025-04-28T09:15:34Z",
    "merged_at": "2025-04-28T09:15:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3807"
  },
  {
    "number": 3805,
    "title": "[https://nvbugspro.nvidia.com/bug/5238599][fix] Normalize example path in accuracy tests",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T09:48:47Z",
    "closed_at": "2025-04-24T02:09:59Z",
    "merged_at": "2025-04-24T02:09:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3805"
  },
  {
    "number": 3804,
    "title": "[TRTLLM-3105][feat] Add Piecewise CUDA Graph Support",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T09:27:21Z",
    "closed_at": "2025-05-09T03:04:01Z",
    "merged_at": "2025-05-09T03:04:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3804"
  },
  {
    "number": 3803,
    "title": "chore: refactor llmapi e2e tests",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T09:26:25Z",
    "closed_at": "2025-05-04T23:37:24Z",
    "merged_at": "2025-05-04T23:37:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3803"
  },
  {
    "number": 3802,
    "title": "[TRTLLM-4763][test] Accuracy test improvement (Part 3.6): Deprecate mmlu_llmapi.py",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T08:48:08Z",
    "closed_at": "2025-04-23T15:05:13Z",
    "merged_at": "2025-04-23T15:05:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3802"
  },
  {
    "number": 3800,
    "title": "doc: add know issues to deepseek doc",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T07:46:43Z",
    "closed_at": "2025-04-23T09:21:50Z",
    "merged_at": "2025-04-23T09:21:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3800"
  },
  {
    "number": 3799,
    "title": "fix: Fix/fused moe 0.19",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T07:40:09Z",
    "closed_at": "2025-04-24T14:25:42Z",
    "merged_at": "2025-04-24T14:25:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3799"
  },
  {
    "number": 3798,
    "title": "cacheTransceiver buffer manager",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T07:33:14Z",
    "closed_at": "2025-04-27T03:48:15Z",
    "merged_at": "2025-04-27T03:48:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3798"
  },
  {
    "number": 3797,
    "title": "[doc] Better document for Draft-Target-Model (DTM) speculative decoding",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T07:23:55Z",
    "closed_at": "2025-04-24T03:41:36Z",
    "merged_at": "2025-04-24T03:41:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3797"
  },
  {
    "number": 3796,
    "title": "[feat] Add torch.compile multi-stream support",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T07:14:57Z",
    "closed_at": "2025-07-09T02:42:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3796"
  },
  {
    "number": 3795,
    "title": "chore: make IPC port bounding atomic for slurm",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T07:03:20Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3795"
  },
  {
    "number": 3794,
    "title": "Infra: Remove empty junit xml",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T06:57:32Z",
    "closed_at": "2025-04-27T01:46:18Z",
    "merged_at": "2025-04-27T01:46:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3794"
  },
  {
    "number": 3793,
    "title": "fix: Reduce memory usage in fused moe op associated with AutoTuning and fix moe fallback issue.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T06:52:16Z",
    "closed_at": "2025-04-24T02:14:26Z",
    "merged_at": "2025-04-24T02:14:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3793"
  },
  {
    "number": 3792,
    "title": "fix: Set default prompts and media for multimodal quickstart example",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T06:28:20Z",
    "closed_at": "2025-04-24T05:02:28Z",
    "merged_at": "2025-04-24T05:02:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3792"
  },
  {
    "number": 3791,
    "title": "feat: add Pytorch support of Vision Encoder for multimodal models",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T05:45:57Z",
    "closed_at": "2025-05-02T21:13:48Z",
    "merged_at": "2025-05-02T21:13:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3791"
  },
  {
    "number": 3790,
    "title": "perf: Optimise MOE prologue to use fused setup function",
    "user": "djns99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T05:31:14Z",
    "closed_at": "2025-04-30T03:44:48Z",
    "merged_at": "2025-04-30T03:44:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3790"
  },
  {
    "number": 3788,
    "title": "add passing E2E LoRA flow",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T03:56:39Z",
    "closed_at": "2025-04-23T15:38:09Z",
    "merged_at": "2025-04-23T15:38:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3788"
  },
  {
    "number": 3787,
    "title": "fix: remove the unnecessary metadata changes in mtp",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T03:32:27Z",
    "closed_at": "2025-04-23T08:01:28Z",
    "merged_at": "2025-04-23T08:01:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3787"
  },
  {
    "number": 3786,
    "title": "fix: change the seq_lens sync copy to an async one",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T03:14:31Z",
    "closed_at": "2025-04-29T15:56:50Z",
    "merged_at": "2025-04-29T15:56:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3786"
  },
  {
    "number": 3785,
    "title": "fix: Update num_of_ctx_tokens in iteration stats",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T03:08:38Z",
    "closed_at": "2025-04-27T02:24:48Z",
    "merged_at": "2025-04-27T02:24:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3785"
  },
  {
    "number": 3784,
    "title": "[infra] Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T03:05:37Z",
    "closed_at": "2025-04-23T03:22:11Z",
    "merged_at": "2025-04-23T03:22:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3784"
  },
  {
    "number": 3783,
    "title": "fix: Reduce memory usage in fused moe op associated with AutoTuning.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T03:01:49Z",
    "closed_at": "2025-04-23T06:55:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3783"
  },
  {
    "number": 3781,
    "title": "chore: Fix KV cache block reuse flag name in quickstart_advanced",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-23T00:49:26Z",
    "closed_at": "2025-04-23T22:02:47Z",
    "merged_at": "2025-04-23T22:02:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3781"
  },
  {
    "number": 3780,
    "title": "chore: reorganize some unit tests of PyTorch",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T23:48:59Z",
    "closed_at": "2025-04-23T18:19:10Z",
    "merged_at": "2025-04-23T18:19:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3780"
  },
  {
    "number": 3779,
    "title": "chore: Make llama4 MoE use maybe_execute_in_parallel",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T23:46:41Z",
    "closed_at": "2025-04-28T14:58:03Z",
    "merged_at": "2025-04-28T14:58:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3779"
  },
  {
    "number": 3778,
    "title": "fix: Limit llama4 context length to 8k",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T21:50:02Z",
    "closed_at": "2025-04-23T15:55:10Z",
    "merged_at": "2025-04-23T15:55:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3778"
  },
  {
    "number": 3777,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T21:10:57Z",
    "closed_at": "2025-04-24T01:36:06Z",
    "merged_at": "2025-04-24T01:36:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3777"
  },
  {
    "number": 3776,
    "title": "[feat]: Allow for a settable end-of-sequence/padding token in max throughput benchmark.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T20:29:30Z",
    "closed_at": "2025-05-01T01:42:46Z",
    "merged_at": "2025-05-01T01:42:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3776"
  },
  {
    "number": 3775,
    "title": "Adding chunk attention for LLama4",
    "user": "vanshilshah97",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T18:55:21Z",
    "closed_at": "2025-05-06T22:05:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3775"
  },
  {
    "number": 3774,
    "title": "Fixing the metric fmeasure access",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T18:52:53Z",
    "closed_at": "2025-04-22T21:10:06Z",
    "merged_at": "2025-04-22T21:10:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3774"
  },
  {
    "number": 3772,
    "title": "Add pre-download of checkpoint before benchmark.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T13:26:09Z",
    "closed_at": "2025-04-25T01:02:54Z",
    "merged_at": "2025-04-25T01:02:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3772"
  },
  {
    "number": 3771,
    "title": "test: Add DeepSeek-V3-Lite GSM8K tests",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T13:15:28Z",
    "closed_at": "2025-04-23T08:54:49Z",
    "merged_at": "2025-04-23T08:54:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3771"
  },
  {
    "number": 3770,
    "title": "TRTLLM-4624 feat: Add nvfp4 gemm and moe support for SM120",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T11:06:15Z",
    "closed_at": "2025-04-29T15:19:12Z",
    "merged_at": "2025-04-29T15:19:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3770"
  },
  {
    "number": 3769,
    "title": "Nixl infra",
    "user": "zackyoray",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T10:02:40Z",
    "closed_at": "2025-06-03T05:32:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3769"
  },
  {
    "number": 3768,
    "title": "chore: Move opencv-python import inside load_video() function",
    "user": "AlessioNetti",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T09:35:52Z",
    "closed_at": "2025-04-22T19:48:36Z",
    "merged_at": "2025-04-22T19:48:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3768"
  },
  {
    "number": 3767,
    "title": "test: waive gemma on L20",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T08:52:43Z",
    "closed_at": "2025-04-22T09:52:49Z",
    "merged_at": "2025-04-22T09:52:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3767"
  },
  {
    "number": 3766,
    "title": "test: waive gemma on L20",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T08:51:56Z",
    "closed_at": "2025-04-22T10:48:07Z",
    "merged_at": "2025-04-22T10:48:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3766"
  },
  {
    "number": 3765,
    "title": "Add log_level for disaggregated_mpi_worker",
    "user": "qiaoxj07",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T08:41:37Z",
    "closed_at": "2025-04-22T16:14:47Z",
    "merged_at": "2025-04-22T16:14:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3765"
  },
  {
    "number": 3764,
    "title": "fix bug of create cuda stream as default parameter which will be init…",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T08:37:42Z",
    "closed_at": "2025-04-28T00:16:03Z",
    "merged_at": "2025-04-28T00:16:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3764"
  },
  {
    "number": 3763,
    "title": "feat: add CGA reduction fmha kernels on Blackwell.",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T08:18:04Z",
    "closed_at": "2025-04-29T02:43:55Z",
    "merged_at": "2025-04-29T02:43:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3763"
  },
  {
    "number": 3762,
    "title": "infra: open source XQA kernels",
    "user": "ming-wei",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T08:01:34Z",
    "closed_at": "2025-04-30T10:05:16Z",
    "merged_at": "2025-04-30T10:05:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3762"
  },
  {
    "number": 3761,
    "title": "Fix: nvbugs/5234210 ModelOpt Mixtral AWQ OOM (cherry-picking to release/0.19)",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T07:56:50Z",
    "closed_at": "2025-04-23T19:17:48Z",
    "merged_at": "2025-04-23T19:17:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3761"
  },
  {
    "number": 3760,
    "title": "chore: add pull request template",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T07:23:44Z",
    "closed_at": "2025-04-23T02:21:31Z",
    "merged_at": "2025-04-23T02:21:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3760"
  },
  {
    "number": 3759,
    "title": "infra: install Triton in the base image",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T07:05:24Z",
    "closed_at": "2025-04-27T23:36:31Z",
    "merged_at": "2025-04-27T23:36:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3759"
  },
  {
    "number": 3758,
    "title": "https://nvbugs/5141291: Fix convert.py script for Qwen model.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T06:36:05Z",
    "closed_at": "2025-04-22T12:18:04Z",
    "merged_at": "2025-04-22T12:18:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3758"
  },
  {
    "number": 3757,
    "title": "feat: enhance trtllm serve multimodal",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T05:58:05Z",
    "closed_at": "2025-05-15T23:16:32Z",
    "merged_at": "2025-05-15T23:16:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3757"
  },
  {
    "number": 3756,
    "title": "chore: Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T05:38:45Z",
    "closed_at": "2025-04-22T06:26:19Z",
    "merged_at": "2025-04-22T06:26:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3756"
  },
  {
    "number": 3755,
    "title": "https://nvbugs/5141291: Fix convert.py script for Qwen model.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T05:11:43Z",
    "closed_at": "2025-04-22T06:29:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3755"
  },
  {
    "number": 3754,
    "title": "fix: fnmatch usage in modeling_utils.py (https://nvbugspro.nvidia.com/bug/5234567)",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T04:14:26Z",
    "closed_at": "2025-04-22T05:13:54Z",
    "merged_at": "2025-04-22T05:13:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3754"
  },
  {
    "number": 3752,
    "title": "feat: add QMMA-based MLA kernels",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T03:22:16Z",
    "closed_at": "2025-04-23T02:18:20Z",
    "merged_at": "2025-04-23T02:18:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3752"
  },
  {
    "number": 3751,
    "title": "chore: remove useless allgather",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T03:01:47Z",
    "closed_at": "2025-04-22T13:26:22Z",
    "merged_at": "2025-04-22T13:26:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3751"
  },
  {
    "number": 3750,
    "title": "Adding local paths to the datasets to make them loadable in offline mode",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T02:41:40Z",
    "closed_at": "2025-04-24T19:51:03Z",
    "merged_at": "2025-04-24T19:51:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3750"
  },
  {
    "number": 3749,
    "title": "doc:add torch examples link into torch backend documentation",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-22T00:17:35Z",
    "closed_at": "2025-04-22T05:24:10Z",
    "merged_at": "2025-04-22T05:24:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3749"
  },
  {
    "number": 3747,
    "title": "fix: Fix CUDA graphs when tp_size > 1",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T22:29:12Z",
    "closed_at": "2025-04-22T02:12:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3747"
  },
  {
    "number": 3746,
    "title": "Update README.md",
    "user": "ncomly-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T22:15:49Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3746"
  },
  {
    "number": 3745,
    "title": "move pytorch tests of LLM API into separate test files",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T21:33:28Z",
    "closed_at": "2025-04-22T21:36:59Z",
    "merged_at": "2025-04-22T21:36:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3745"
  },
  {
    "number": 3744,
    "title": "infra: add conan",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T21:30:32Z",
    "closed_at": "2025-04-30T18:53:15Z",
    "merged_at": "2025-04-30T18:53:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3744"
  },
  {
    "number": 3743,
    "title": "feat: Add head size 72 support for QKV Preprocessing kernel",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T20:54:30Z",
    "closed_at": "2025-04-25T18:07:41Z",
    "merged_at": "2025-04-25T18:07:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3743"
  },
  {
    "number": 3742,
    "title": "Draft: feat: Sm120 fp4 gemm",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T20:10:40Z",
    "closed_at": "2025-04-30T15:26:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3742"
  },
  {
    "number": 3741,
    "title": "datasets API change : datasets.load_metric => evaluate.load",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T17:09:26Z",
    "closed_at": "2025-04-22T00:23:48Z",
    "merged_at": "2025-04-22T00:23:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3741"
  },
  {
    "number": 3740,
    "title": "Revert \"Report number of context tokens in one iteration (#3691)\"",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T17:07:14Z",
    "closed_at": "2025-04-21T17:21:44Z",
    "merged_at": "2025-04-21T17:21:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3740"
  },
  {
    "number": 3739,
    "title": "Revert \"Report number of context tokens in one iteration\"",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T16:50:59Z",
    "closed_at": "2025-04-21T17:05:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3739"
  },
  {
    "number": 3738,
    "title": "feat: Add integration of etcd",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T15:08:29Z",
    "closed_at": "2025-06-03T12:01:44Z",
    "merged_at": "2025-06-03T12:01:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3738"
  },
  {
    "number": 3737,
    "title": "fix: use cuKernelGetAttribute for wider CUDA compatibility",
    "user": "lucifer1004",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T14:54:07Z",
    "closed_at": "2025-04-26T08:17:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3737"
  },
  {
    "number": 3735,
    "title": "Fix TLLM_CHECK(isLeaf()); assertion",
    "user": "akhoroshev",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T13:42:40Z",
    "closed_at": "2025-06-05T19:44:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3735"
  },
  {
    "number": 3732,
    "title": "chore: Cleanup deprecated APIs from LLM-API (part 1/2)",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T13:10:37Z",
    "closed_at": "2025-05-07T05:20:26Z",
    "merged_at": "2025-05-07T05:20:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3732"
  },
  {
    "number": 3731,
    "title": "feat(part 2): Enhance the integrated robustness of scaffolding with _init__.py #3305",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T13:03:10Z",
    "closed_at": "2025-04-24T10:47:04Z",
    "merged_at": "2025-04-24T10:47:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3731"
  },
  {
    "number": 3730,
    "title": "feat/add latency support for trtllm bench",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T12:50:37Z",
    "closed_at": "2025-07-15T20:13:49Z",
    "merged_at": "2025-07-15T20:13:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3730"
  },
  {
    "number": 3729,
    "title": "Draft: Test OOM",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T10:42:03Z",
    "closed_at": "2025-05-19T03:25:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3729"
  },
  {
    "number": 3728,
    "title": "fix: make IPC port bounding atomic to avoid conflict",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T09:51:49Z",
    "closed_at": "2025-04-23T07:20:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3728"
  },
  {
    "number": 3727,
    "title": "fix: load_metric error in summarize tests",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T08:47:54Z",
    "closed_at": "2025-04-22T01:16:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3727"
  },
  {
    "number": 3726,
    "title": "fix: nvbugs/5234029 fix Qwen2.5-VL image test",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T08:25:22Z",
    "closed_at": "2025-04-23T06:09:40Z",
    "merged_at": "2025-04-23T06:09:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3726"
  },
  {
    "number": 3722,
    "title": "fix: nvbug/5093459 cherry-pick from main (c35d2a7)",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T07:11:47Z",
    "closed_at": "2025-04-22T05:59:24Z",
    "merged_at": "2025-04-22T05:59:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3722"
  },
  {
    "number": 3721,
    "title": "ci: unwaive multi-node test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T06:50:10Z",
    "closed_at": "2025-04-21T23:56:30Z",
    "merged_at": "2025-04-21T23:56:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3721"
  },
  {
    "number": 3720,
    "title": "nvbugs/5232406  skip disagg deepseek test if sm!=90",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T06:48:43Z",
    "closed_at": "2025-04-23T06:47:45Z",
    "merged_at": "2025-04-23T06:47:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3720"
  },
  {
    "number": 3719,
    "title": "test: skip_pre_ada for fp8 test cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T06:34:03Z",
    "closed_at": "2025-04-25T01:32:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3719"
  },
  {
    "number": 3718,
    "title": "test: [nvbug: 5234494] skip_pre_ada for fp8 cases",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T06:23:55Z",
    "closed_at": "2025-04-23T08:54:40Z",
    "merged_at": "2025-04-23T08:54:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3718"
  },
  {
    "number": 3717,
    "title": "doc: Update doc for Deepseek min latency",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T04:24:20Z",
    "closed_at": "2025-04-22T15:08:00Z",
    "merged_at": "2025-04-22T15:08:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3717"
  },
  {
    "number": 3716,
    "title": "test: add rcca tests 4753548",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T03:32:02Z",
    "closed_at": "2025-04-23T05:40:12Z",
    "merged_at": "2025-04-23T05:40:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3716"
  },
  {
    "number": 3715,
    "title": "ci: unwaive multi_node test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T03:29:25Z",
    "closed_at": "2025-04-21T13:26:07Z",
    "merged_at": "2025-04-21T13:26:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3715"
  },
  {
    "number": 3714,
    "title": "Fix: nvbugs/5232457 ModelOpt Mixtral AWQ OOM",
    "user": "Barry-Delaney",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T03:27:56Z",
    "closed_at": "2025-04-21T11:14:14Z",
    "merged_at": "2025-04-21T11:14:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3714"
  },
  {
    "number": 3713,
    "title": "fix: update test_user_buffers_mm_add_prologue atol (#3711)",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T03:01:11Z",
    "closed_at": "2025-04-21T03:24:20Z",
    "merged_at": "2025-04-21T03:24:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3713"
  },
  {
    "number": 3712,
    "title": "refact: use pybind block key and hasher in disagg worker test",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T02:52:20Z",
    "closed_at": "2025-04-21T10:50:58Z",
    "merged_at": "2025-04-21T10:50:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3712"
  },
  {
    "number": 3711,
    "title": "fix: update test_user_buffers_mm_add_prologue atol",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T02:44:12Z",
    "closed_at": "2025-04-21T02:56:30Z",
    "merged_at": "2025-04-21T02:56:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3711"
  },
  {
    "number": 3710,
    "title": "test: skip failed cases on B200",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T02:40:02Z",
    "closed_at": "2025-04-23T08:19:40Z",
    "merged_at": "2025-04-23T08:19:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3710"
  },
  {
    "number": 3709,
    "title": "Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T02:29:17Z",
    "closed_at": "2025-04-21T03:24:01Z",
    "merged_at": "2025-04-21T03:24:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3709"
  },
  {
    "number": 3708,
    "title": "[None][fix] Fix failed to rename bug in torch profiler",
    "user": "Fridge003",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-20T21:35:22Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3708"
  },
  {
    "number": 3707,
    "title": "Fix double link to fp8_blockscale_gemm_src",
    "user": "WilliamTambellini",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-20T17:54:50Z",
    "closed_at": "2025-04-23T02:16:07Z",
    "merged_at": "2025-04-23T02:16:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3707"
  },
  {
    "number": 3697,
    "title": "DO NOT MERGE: Dry run some tests",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-19T18:38:58Z",
    "closed_at": "2025-05-14T22:39:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3697"
  },
  {
    "number": 3696,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-19T17:49:16Z",
    "closed_at": "2025-04-23T06:18:57Z",
    "merged_at": "2025-04-23T06:18:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3696"
  },
  {
    "number": 3695,
    "title": "fix:  Refactor Deepseek tp_size calculation",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-19T06:17:16Z",
    "closed_at": "2025-04-20T06:55:20Z",
    "merged_at": "2025-04-20T06:55:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3695"
  },
  {
    "number": 3694,
    "title": "DRAFT: integrate sm120 fusedmoe",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-19T00:45:15Z",
    "closed_at": "2025-04-22T11:04:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3694"
  },
  {
    "number": 3693,
    "title": "doc: Update DeepSeek perf docs",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T23:49:32Z",
    "closed_at": "2025-04-19T05:11:01Z",
    "merged_at": "2025-04-19T05:11:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3693"
  },
  {
    "number": 3692,
    "title": "Fix create_weights in attention",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T23:38:11Z",
    "closed_at": "2025-04-23T23:30:01Z",
    "merged_at": "2025-04-23T23:30:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3692"
  },
  {
    "number": 3691,
    "title": "Report number of context tokens in one iteration",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T23:29:56Z",
    "closed_at": "2025-04-21T05:45:28Z",
    "merged_at": "2025-04-21T05:45:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3691"
  },
  {
    "number": 3689,
    "title": "fix: Fix disaggregated load balance test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T23:17:56Z",
    "closed_at": "2025-04-19T02:40:41Z",
    "merged_at": "2025-04-19T02:40:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3689"
  },
  {
    "number": 3688,
    "title": "fix: https://nvbugs/5177565 PP4 race condition",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T22:42:58Z",
    "closed_at": "2025-04-23T02:53:55Z",
    "merged_at": "2025-04-23T02:53:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3688"
  },
  {
    "number": 3687,
    "title": "chore: Waive disaggregated load balance",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T22:14:24Z",
    "closed_at": "2025-04-18T23:04:33Z",
    "merged_at": "2025-04-18T23:04:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3687"
  },
  {
    "number": 3686,
    "title": "[fix] Fix a few issues with EAGLE3 in PyTorch backend",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T20:02:13Z",
    "closed_at": "2025-04-28T18:34:22Z",
    "merged_at": "2025-04-28T18:34:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3686"
  },
  {
    "number": 3685,
    "title": "test:sync waives.txt from main branch by disabling test_perf/gpt_350m…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T17:01:21Z",
    "closed_at": "2025-04-18T17:22:39Z",
    "merged_at": "2025-04-18T17:22:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3685"
  },
  {
    "number": 3683,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T16:28:19Z",
    "closed_at": "2025-04-22T03:09:41Z",
    "merged_at": "2025-04-22T03:09:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3683"
  },
  {
    "number": 3682,
    "title": "Draft: NGRAM drafter for speculative decoding",
    "user": "thorjohnsen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T13:45:39Z",
    "closed_at": "2025-04-30T16:29:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3682"
  },
  {
    "number": 3681,
    "title": "Fix/executor bugs",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T12:31:41Z",
    "closed_at": "2025-04-21T23:23:28Z",
    "merged_at": "2025-04-21T23:23:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3681"
  },
  {
    "number": 3680,
    "title": "chore: update FMHA cubin files",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T11:53:30Z",
    "closed_at": "2025-04-21T07:04:05Z",
    "merged_at": "2025-04-21T07:04:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3680"
  },
  {
    "number": 3679,
    "title": "fix: Support TLLM_OVERRIDE_LAYER_NUM for llama4.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T10:21:42Z",
    "closed_at": "2025-04-21T04:28:57Z",
    "merged_at": "2025-04-21T04:28:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3679"
  },
  {
    "number": 3678,
    "title": "fix: Remove ParallelConfig.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T10:12:31Z",
    "closed_at": "2025-04-21T06:14:09Z",
    "merged_at": "2025-04-21T06:14:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3678"
  },
  {
    "number": 3677,
    "title": "feat: Add NIXL support",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T10:11:03Z",
    "closed_at": "2025-06-03T05:31:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3677"
  },
  {
    "number": 3676,
    "title": "Update ds v3 parameters in stress test.",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T06:14:57Z",
    "closed_at": "2025-04-22T10:04:17Z",
    "merged_at": "2025-04-22T10:04:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3676"
  },
  {
    "number": 3675,
    "title": "fix: FP8 kv accuracy",
    "user": "DylanChen-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T05:12:19Z",
    "closed_at": "2025-04-21T07:59:32Z",
    "merged_at": "2025-04-21T07:59:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3675"
  },
  {
    "number": 3674,
    "title": "chore : Split more tests out of gpt tests (#3524)",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T05:03:29Z",
    "closed_at": "2025-04-22T03:19:41Z",
    "merged_at": "2025-04-22T03:19:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3674"
  },
  {
    "number": 3673,
    "title": "fix: nvbugs/5231298: pytorch allreduce issue",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T04:52:17Z",
    "closed_at": "2025-04-18T07:50:41Z",
    "merged_at": "2025-04-18T07:50:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3673"
  },
  {
    "number": 3672,
    "title": "test:update waives.txt for nvbug 5219532",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T03:42:07Z",
    "closed_at": "2025-04-19T10:57:40Z",
    "merged_at": "2025-04-19T10:57:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3672"
  },
  {
    "number": 3671,
    "title": "test:restore fp8 kv cache testing for L0",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T03:35:21Z",
    "closed_at": "2025-04-19T04:28:36Z",
    "merged_at": "2025-04-19T04:28:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3671"
  },
  {
    "number": 3670,
    "title": "test: Validate FP8 and LoRA for Gemma3",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T01:58:03Z",
    "closed_at": "2025-05-14T00:28:02Z",
    "merged_at": "2025-05-14T00:28:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3670"
  },
  {
    "number": 3669,
    "title": "Remove dummy forward path",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T00:30:04Z",
    "closed_at": "2025-04-18T08:17:51Z",
    "merged_at": "2025-04-18T08:17:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3669"
  },
  {
    "number": 3668,
    "title": "feat: [AutoDeploy] unfusing attention",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T23:12:03Z",
    "closed_at": "2025-05-02T01:06:50Z",
    "merged_at": "2025-05-02T01:06:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3668"
  },
  {
    "number": 3667,
    "title": "chore: enable test_ptp_quickstart_advanced_mixed_precision back",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T22:15:17Z",
    "closed_at": "2025-04-18T12:06:24Z",
    "merged_at": "2025-04-18T12:06:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3667"
  },
  {
    "number": 3665,
    "title": "chore: update multi gpu trigger file list",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T17:34:26Z",
    "closed_at": "2025-04-17T18:19:02Z",
    "merged_at": "2025-04-17T18:19:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3665"
  },
  {
    "number": 3664,
    "title": "chore: waive test_llm_multi_node",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T17:26:53Z",
    "closed_at": "2025-04-17T17:59:17Z",
    "merged_at": "2025-04-17T17:59:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3664"
  },
  {
    "number": 3663,
    "title": "Added NemotronH to PyTorch supported models",
    "user": "vegaluisjose",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T16:36:16Z",
    "closed_at": "2025-04-24T17:41:33Z",
    "merged_at": "2025-04-24T17:41:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3663"
  },
  {
    "number": 3662,
    "title": "test:update waives.txt for nvbug 5219532",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T16:15:58Z",
    "closed_at": "2025-04-18T03:15:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3662"
  },
  {
    "number": 3661,
    "title": "Support RingAttention in the BertAttention plugin and the DiT model",
    "user": "forrestl111",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T15:38:38Z",
    "closed_at": "2025-05-09T00:06:54Z",
    "merged_at": "2025-05-09T00:06:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3661"
  },
  {
    "number": 3660,
    "title": "fix sage attention headsize check error in bertAttentionPlugin.cpp",
    "user": "Jackch-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T09:58:54Z",
    "closed_at": "2025-04-18T01:28:04Z",
    "merged_at": "2025-04-18T01:28:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3660"
  },
  {
    "number": 3659,
    "title": "feat: Introduce feature properties for attention backend.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T09:14:35Z",
    "closed_at": "2025-04-19T04:37:28Z",
    "merged_at": "2025-04-19T04:37:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3659"
  },
  {
    "number": 3658,
    "title": "feat: support add internal cutlass kernels as subproject",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T09:10:02Z",
    "closed_at": "2025-05-06T03:35:07Z",
    "merged_at": "2025-05-06T03:35:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3658"
  },
  {
    "number": 3657,
    "title": "test: change default max_batch_size to 512 in test config and dump config.json to log",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T09:05:56Z",
    "closed_at": "2025-04-22T06:51:45Z",
    "merged_at": "2025-04-22T06:51:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3657"
  },
  {
    "number": 3656,
    "title": "test: change default max_batch_size to 512 in test config and dump config.json to log",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T08:45:46Z",
    "closed_at": "2025-04-22T06:53:21Z",
    "merged_at": "2025-04-22T06:53:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3656"
  },
  {
    "number": 3655,
    "title": "doc/draft-target-model-V2",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T08:44:19Z",
    "closed_at": "2025-04-24T00:57:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3655"
  },
  {
    "number": 3652,
    "title": "fix: Fix fused_moe cache fallback issue.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T06:45:16Z",
    "closed_at": "2025-04-17T15:17:04Z",
    "merged_at": "2025-04-17T15:17:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3652"
  },
  {
    "number": 3651,
    "title": "Waive L0 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T06:40:07Z",
    "closed_at": "2025-04-17T07:13:56Z",
    "merged_at": "2025-04-17T07:13:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3651"
  },
  {
    "number": 3650,
    "title": "Cherry-pick: update fp8 doc (#3647)",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T05:28:53Z",
    "closed_at": "2025-04-17T05:37:09Z",
    "merged_at": "2025-04-17T05:37:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3650"
  },
  {
    "number": 3649,
    "title": "fix: hmac in remote mpi session",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T05:23:28Z",
    "closed_at": "2025-04-18T09:47:52Z",
    "merged_at": "2025-04-18T09:47:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3649"
  },
  {
    "number": 3648,
    "title": "Add running E2E LoRA flow",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T05:07:11Z",
    "closed_at": "2025-04-23T03:19:41Z",
    "merged_at": "2025-04-23T03:19:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3648"
  },
  {
    "number": 3647,
    "title": "update DeepSeek fp8 doc",
    "user": "litaotju",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T05:07:06Z",
    "closed_at": "2025-04-17T05:16:07Z",
    "merged_at": "2025-04-17T05:16:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3647"
  },
  {
    "number": 3646,
    "title": "Fix rotary_emb param in NemotronH attention",
    "user": "vegaluisjose",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T03:22:42Z",
    "closed_at": "2025-04-17T04:03:08Z",
    "merged_at": "2025-04-17T04:03:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3646"
  },
  {
    "number": 3645,
    "title": "waive various tests on release/0.19",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T03:14:52Z",
    "closed_at": "2025-04-17T06:18:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3645"
  },
  {
    "number": 3644,
    "title": "test: remove benchmark test list on main branch",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T02:46:29Z",
    "closed_at": "2025-04-17T08:23:42Z",
    "merged_at": "2025-04-17T08:23:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3644"
  },
  {
    "number": 3643,
    "title": "test: remove qa benchmark test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T02:41:53Z",
    "closed_at": "2025-04-23T05:30:09Z",
    "merged_at": "2025-04-23T05:30:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3643"
  },
  {
    "number": 3642,
    "title": "fix: Pick waives from main",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T01:47:29Z",
    "closed_at": "2025-04-17T02:41:56Z",
    "merged_at": "2025-04-17T02:41:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3642"
  },
  {
    "number": 3641,
    "title": "feat: Add smart router for moe module",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T00:52:45Z",
    "closed_at": "2025-04-23T04:22:00Z",
    "merged_at": "2025-04-23T04:22:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3641"
  },
  {
    "number": 3640,
    "title": "Clean up modeling_deepseek.py",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-17T00:18:10Z",
    "closed_at": "2025-04-19T00:54:33Z",
    "merged_at": "2025-04-19T00:54:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3640"
  },
  {
    "number": 3639,
    "title": "feat: Upgrade cutlass 3.9",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T23:46:19Z",
    "closed_at": "2025-04-30T15:27:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3639"
  },
  {
    "number": 3638,
    "title": "feat: [AutoDeploy] update rope matcher with minor variants (Deepseek)",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T23:30:14Z",
    "closed_at": "2025-05-16T13:55:33Z",
    "merged_at": "2025-05-16T13:55:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3638"
  },
  {
    "number": 3637,
    "title": "waive test_fp8_scaled_mm",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T21:51:34Z",
    "closed_at": "2025-04-16T22:07:30Z",
    "merged_at": "2025-04-16T22:07:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3637"
  },
  {
    "number": 3635,
    "title": "Fix: cherry-pick hmac encryption from main branch",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T18:55:33Z",
    "closed_at": "2025-04-21T05:08:03Z",
    "merged_at": "2025-04-21T05:08:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3635"
  },
  {
    "number": 3634,
    "title": "infra: Add PR approval protection for the release branch",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T16:29:01Z",
    "closed_at": "2025-04-18T01:16:59Z",
    "merged_at": "2025-04-18T01:16:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3634"
  },
  {
    "number": 3633,
    "title": "fix: read and set head_size and head_dim when convert Eagle checkpoint",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T15:29:09Z",
    "closed_at": "2025-04-17T02:55:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3633"
  },
  {
    "number": 3632,
    "title": "Update Nemotron Super and Ultra in Supported Models and add an example",
    "user": "Naveassaf",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T14:51:59Z",
    "closed_at": "2025-04-20T13:14:34Z",
    "merged_at": "2025-04-20T13:14:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3632"
  },
  {
    "number": 3631,
    "title": "fix: 5197419 and removed unused runtime kernels",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T14:22:21Z",
    "closed_at": "2025-04-23T16:04:52Z",
    "merged_at": "2025-04-23T16:04:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3631"
  },
  {
    "number": 3630,
    "title": "Fix: nvbugs/5222698 variable not defined",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T13:59:48Z",
    "closed_at": "2025-04-18T13:41:21Z",
    "merged_at": "2025-04-18T13:41:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3630"
  },
  {
    "number": 3628,
    "title": "waive test_llm_multi_node_with_postproc",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T12:28:47Z",
    "closed_at": "2025-04-16T12:49:40Z",
    "merged_at": "2025-04-16T12:49:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3628"
  },
  {
    "number": 3627,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T12:15:12Z",
    "closed_at": "2025-04-17T06:45:41Z",
    "merged_at": "2025-04-17T06:45:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3627"
  },
  {
    "number": 3626,
    "title": "chore: Use ellipsis as default value to detect whether residual argument is provided",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T11:19:48Z",
    "closed_at": "2025-04-17T04:31:59Z",
    "merged_at": "2025-04-17T04:31:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3626"
  },
  {
    "number": 3625,
    "title": "[TRTLLM-3429] feat: Overlap scheduling in C++ runtime",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T09:57:06Z",
    "closed_at": "2025-05-06T13:06:47Z",
    "merged_at": "2025-05-06T13:06:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3625"
  },
  {
    "number": 3624,
    "title": "Update GitHub pages to v0.18.2",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T09:57:05Z",
    "closed_at": "2025-04-16T10:17:04Z",
    "merged_at": "2025-04-16T10:17:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3624"
  },
  {
    "number": 3622,
    "title": "Fix [NVBUG 5219533 5220758] Fix script options for engines.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T09:19:37Z",
    "closed_at": "2025-04-21T10:59:18Z",
    "merged_at": "2025-04-21T10:59:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3622"
  },
  {
    "number": 3621,
    "title": "Fix: [NVBUG 5201530]Fix merge dummy request when DP + Overlap + Cuda graph.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T09:14:46Z",
    "closed_at": "2025-04-21T06:54:24Z",
    "merged_at": "2025-04-21T06:54:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3621"
  },
  {
    "number": 3619,
    "title": "tests: change qa perf test to trtllm-bench",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T08:38:37Z",
    "closed_at": "2025-04-17T05:58:39Z",
    "merged_at": "2025-04-17T05:58:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3619"
  },
  {
    "number": 3618,
    "title": "test: add INTEGRATION_TEST env var to speed up integration test",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T08:37:27Z",
    "closed_at": "2025-05-08T02:44:50Z",
    "merged_at": "2025-05-08T02:44:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3618"
  },
  {
    "number": 3616,
    "title": "chore: Add etcd docker dependency",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T07:38:38Z",
    "closed_at": "2025-05-26T02:43:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3616"
  },
  {
    "number": 3615,
    "title": "Updating the run.py to make the draft target model run with the LLaMa 3 1B/8B",
    "user": "mayani-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T07:09:20Z",
    "closed_at": "2025-04-18T17:01:50Z",
    "merged_at": "2025-04-18T17:01:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3615"
  },
  {
    "number": 3614,
    "title": "infra: Update user list",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T06:52:57Z",
    "closed_at": "2025-04-16T07:13:30Z",
    "merged_at": "2025-04-16T07:13:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3614"
  },
  {
    "number": 3613,
    "title": "disable ib for ucx test",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T06:50:43Z",
    "closed_at": "2025-04-16T22:43:58Z",
    "merged_at": "2025-04-16T22:43:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3613"
  },
  {
    "number": 3611,
    "title": "TensorRT-LLM v0.18.2 release",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T06:32:12Z",
    "closed_at": "2025-04-16T06:42:50Z",
    "merged_at": "2025-04-16T06:42:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3611"
  },
  {
    "number": 3610,
    "title": "feat: Support unfused rope in MLA.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T06:28:36Z",
    "closed_at": "2025-04-17T08:50:50Z",
    "merged_at": "2025-04-17T08:50:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3610"
  },
  {
    "number": 3609,
    "title": "feat: Add support for smaller hidden_dim in AR fusion kernel",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T06:27:47Z",
    "closed_at": "2025-04-17T04:00:33Z",
    "merged_at": "2025-04-17T04:00:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3609"
  },
  {
    "number": 3608,
    "title": "test: add test cases for 0.19 release",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T06:25:03Z",
    "closed_at": "2025-04-16T08:19:06Z",
    "merged_at": "2025-04-16T08:19:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3608"
  },
  {
    "number": 3606,
    "title": "test: add test cases for release 0.19",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T06:21:08Z",
    "closed_at": "2025-04-16T06:22:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3606"
  },
  {
    "number": 3605,
    "title": "feat: Add Dynasor-CoT in scaffolding examples",
    "user": "GindaChen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T06:20:47Z",
    "closed_at": "2025-04-17T07:11:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3605"
  },
  {
    "number": 3604,
    "title": "test: add nemotron-ultra torch flow test case for release 0.19",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T06:11:21Z",
    "closed_at": "2025-04-16T06:11:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3604"
  },
  {
    "number": 3603,
    "title": "chore: waive test_llm_phi_quantization_1gpu",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T05:22:16Z",
    "closed_at": "2025-04-16T05:33:46Z",
    "merged_at": "2025-04-16T05:33:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3603"
  },
  {
    "number": 3602,
    "title": "test: add kv cache event tests for disagg workers ",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T05:05:28Z",
    "closed_at": "2025-04-18T10:30:19Z",
    "merged_at": "2025-04-18T10:30:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3602"
  },
  {
    "number": 3600,
    "title": "fix: fix cublas_scaled_mm",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T04:41:05Z",
    "closed_at": "2025-04-21T06:51:42Z",
    "merged_at": "2025-04-21T06:51:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3600"
  },
  {
    "number": 3599,
    "title": "test: add multinode test case for deepseek-v3",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T04:11:39Z",
    "closed_at": "2025-04-28T02:16:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3599"
  },
  {
    "number": 3598,
    "title": "chore: bump version to 0.19.0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T03:52:52Z",
    "closed_at": "2025-04-16T04:15:19Z",
    "merged_at": "2025-04-16T04:15:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3598"
  },
  {
    "number": 3597,
    "title": "feat: add etcd dependency and interface",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T03:37:13Z",
    "closed_at": "2025-05-26T02:43:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3597"
  },
  {
    "number": 3596,
    "title": "test: add quickstart test for nemotron-ultra",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T03:10:54Z",
    "closed_at": "2025-04-17T03:16:41Z",
    "merged_at": "2025-04-17T03:16:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3596"
  },
  {
    "number": 3595,
    "title": "test:restore fp8 kv cache testing for L0",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T03:09:57Z",
    "closed_at": "2025-04-18T03:17:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3595"
  },
  {
    "number": 3593,
    "title": "test: Get Eagle tests working",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T02:42:03Z",
    "closed_at": "2025-04-19T16:50:57Z",
    "merged_at": "2025-04-19T16:50:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3593"
  },
  {
    "number": 3592,
    "title": "ci: waive test_llm_multi_node_pytorch",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T02:37:08Z",
    "closed_at": "2025-04-16T02:49:07Z",
    "merged_at": "2025-04-16T02:49:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3592"
  },
  {
    "number": 3590,
    "title": "feat: trtllm-serve multimodal support",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T00:57:19Z",
    "closed_at": "2025-04-18T21:01:29Z",
    "merged_at": "2025-04-18T21:01:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3590"
  },
  {
    "number": 3589,
    "title": "feat: [AutoDeploy] generalizing cudagraph to multiple dynamic inputs",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-16T00:50:29Z",
    "closed_at": "2025-04-22T19:38:55Z",
    "merged_at": "2025-04-22T19:38:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3589"
  },
  {
    "number": 3586,
    "title": "fix: check for config architectures in model config",
    "user": "vanshilshah97",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T17:57:05Z",
    "closed_at": "2025-04-16T06:23:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3586"
  },
  {
    "number": 3585,
    "title": "fix: nvbugs/5075538: fix cross attention mask when decoder input len > 1",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T16:57:01Z",
    "closed_at": "2025-04-16T00:31:33Z",
    "merged_at": "2025-04-16T00:31:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3585"
  },
  {
    "number": 3584,
    "title": "feat: Disaggregated router class",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T16:10:55Z",
    "closed_at": "2025-04-18T16:34:12Z",
    "merged_at": "2025-04-18T16:34:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3584"
  },
  {
    "number": 3582,
    "title": "feat: Integrate GPUDirect Storage (GDS) into Executor API",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T15:48:19Z",
    "closed_at": "2025-04-18T07:59:21Z",
    "merged_at": "2025-04-18T07:59:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3582"
  },
  {
    "number": 3581,
    "title": "infra: fix some command start with enter and space will not trigger a…",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T13:27:00Z",
    "closed_at": "2025-04-16T02:15:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3581"
  },
  {
    "number": 3579,
    "title": "fix : release torch-managed memory as soon as it's not needed",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T10:55:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3579"
  },
  {
    "number": 3578,
    "title": "infra: [TRTLLM-4051] Support only run some backend type test",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T10:34:40Z",
    "closed_at": "2025-05-07T07:34:17Z",
    "merged_at": "2025-05-07T07:34:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3578"
  },
  {
    "number": 3577,
    "title": "doc: Minor fixes for documents",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T10:08:07Z",
    "closed_at": "2025-04-15T23:47:19Z",
    "merged_at": "2025-04-15T23:47:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3577"
  },
  {
    "number": 3575,
    "title": "fix: add SM90 guard for FP8 Blockscale GEMM",
    "user": "lucifer1004",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T09:57:56Z",
    "closed_at": "2025-04-16T06:44:37Z",
    "merged_at": "2025-04-16T06:44:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3575"
  },
  {
    "number": 3574,
    "title": "fix: Remove unnecessary max call",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T09:49:16Z",
    "closed_at": "2025-04-22T02:33:51Z",
    "merged_at": "2025-04-22T02:33:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3574"
  },
  {
    "number": 3573,
    "title": "Revert \"infra: move nvrtc_wrapper to conan (#3282)\"",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T09:41:34Z",
    "closed_at": "2025-04-15T14:45:13Z",
    "merged_at": "2025-04-15T14:45:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3573"
  },
  {
    "number": 3572,
    "title": "chore: Add comments to modifications that fix TP size of DeepSeek-V3/R1 when using more than 16 GPUs",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T09:05:08Z",
    "closed_at": "2025-04-16T04:51:42Z",
    "merged_at": "2025-04-16T04:51:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3572"
  },
  {
    "number": 3571,
    "title": "feat: support kv cache reuse for MLA",
    "user": "zhhuang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T08:39:13Z",
    "closed_at": "2025-05-15T07:22:21Z",
    "merged_at": "2025-05-15T07:22:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3571"
  },
  {
    "number": 3570,
    "title": "fix: amend word_size in trtllm-llmapi-launch script",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T08:05:54Z",
    "closed_at": "2025-04-15T08:50:22Z",
    "merged_at": "2025-04-15T08:50:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3570"
  },
  {
    "number": 3569,
    "title": "Enable 4 multi-gpu test cases for deepseek",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T07:15:47Z",
    "closed_at": "2025-04-15T14:01:53Z",
    "merged_at": "2025-04-15T14:01:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3569"
  },
  {
    "number": 3567,
    "title": "fix: FP8 quantized lm_head (NvBug 5214229)",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T05:57:19Z",
    "closed_at": "2025-04-17T07:18:34Z",
    "merged_at": "2025-04-17T07:18:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3567"
  },
  {
    "number": 3566,
    "title": "docs:update llm api examples and customizations sections' links.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T05:43:45Z",
    "closed_at": "2025-04-15T05:55:22Z",
    "merged_at": "2025-04-15T05:55:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3566"
  },
  {
    "number": 3565,
    "title": "Chore: Remove test_fp4_quantize_gemm_torch_profiling test.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T05:41:09Z",
    "closed_at": "2025-04-15T06:17:52Z",
    "merged_at": "2025-04-15T06:17:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3565"
  },
  {
    "number": 3563,
    "title": "fix: amend trtllm-bench command in the test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T04:38:44Z",
    "closed_at": "2025-04-16T03:04:09Z",
    "merged_at": "2025-04-16T03:04:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3563"
  },
  {
    "number": 3562,
    "title": "fix: LLM API _hf_model_dir for non-cached case",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T03:19:45Z",
    "closed_at": "2025-04-16T02:39:34Z",
    "merged_at": "2025-04-16T02:39:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3562"
  },
  {
    "number": 3561,
    "title": "chore: bump version to 0.20.0rc0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T03:14:56Z",
    "closed_at": "2025-04-16T03:41:21Z",
    "merged_at": "2025-04-16T03:41:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3561"
  },
  {
    "number": 3560,
    "title": "Test action",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T02:59:57Z",
    "closed_at": "2025-04-15T03:02:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3560"
  },
  {
    "number": 3559,
    "title": "ci: unwaive test",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T02:58:08Z",
    "closed_at": "2025-04-15T11:42:07Z",
    "merged_at": "2025-04-15T11:42:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3559"
  },
  {
    "number": 3558,
    "title": "chore: add assertion for devices to avoid underlying errors",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T02:55:58Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3558"
  },
  {
    "number": 3557,
    "title": "chore: Modifications that should have been included but were mistakenly overwritten in PR #3467",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T02:53:03Z",
    "closed_at": "2025-04-15T06:08:08Z",
    "merged_at": "2025-04-15T06:08:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3557"
  },
  {
    "number": 3556,
    "title": "Test release/0.19 CI",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T02:28:10Z",
    "closed_at": "2025-04-30T04:18:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3556"
  },
  {
    "number": 3555,
    "title": "move the reset models into `examples/models/core` directory",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T02:24:04Z",
    "closed_at": "2025-04-20T03:49:00Z",
    "merged_at": "2025-04-20T03:49:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3555"
  },
  {
    "number": 3553,
    "title": "Clean up linear.py, mlp.py, gated_mlp.py",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T01:50:54Z",
    "closed_at": "2025-04-16T19:21:45Z",
    "merged_at": "2025-04-16T19:21:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3553"
  },
  {
    "number": 3552,
    "title": "test: Unwaive test for nvbug_5150466 ",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T00:38:06Z",
    "closed_at": "2025-04-18T07:15:59Z",
    "merged_at": "2025-04-18T07:15:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3552"
  },
  {
    "number": 3551,
    "title": "fix:Fix test_fp4_quantize_gemm_torch",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T23:52:36Z",
    "closed_at": "2025-04-15T06:58:32Z",
    "merged_at": "2025-04-15T06:58:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3551"
  },
  {
    "number": 3550,
    "title": "fix: remove extraneous local docker user command",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T23:25:57Z",
    "closed_at": "2025-04-15T05:20:34Z",
    "merged_at": "2025-04-15T05:20:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3550"
  },
  {
    "number": 3549,
    "title": "Move Triton backend to TRT-LLM main",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T22:50:26Z",
    "closed_at": "2025-05-15T23:15:24Z",
    "merged_at": "2025-05-15T23:15:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3549"
  },
  {
    "number": 3547,
    "title": "feat: [AutoDeploy] Llama-4 support",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T19:53:11Z",
    "closed_at": "2025-05-12T18:53:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3547"
  },
  {
    "number": 3545,
    "title": "refactor: decoder buffers communication",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T17:35:01Z",
    "closed_at": "2025-04-21T20:01:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3545"
  },
  {
    "number": 3544,
    "title": "test: Fix breaking Phi3 multimodal tests",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T17:22:42Z",
    "closed_at": "2025-04-15T00:02:34Z",
    "merged_at": "2025-04-15T00:02:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3544"
  },
  {
    "number": 3542,
    "title": "test: Add MTP + overlap + Attention DP disaggregated test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T15:51:40Z",
    "closed_at": "2025-04-14T23:46:03Z",
    "merged_at": "2025-04-14T23:46:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3542"
  },
  {
    "number": 3541,
    "title": "feat: Adding FP8 BMM from Codegen",
    "user": "evezhier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T15:34:33Z",
    "closed_at": "2025-04-16T08:37:16Z",
    "merged_at": "2025-04-16T08:37:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3541"
  },
  {
    "number": 3538,
    "title": "Draft: test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T11:08:20Z",
    "closed_at": "2025-04-15T09:05:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3538"
  },
  {
    "number": 3537,
    "title": "chore: Clean up cpp runtime",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T10:48:50Z",
    "closed_at": "2025-04-15T08:06:14Z",
    "merged_at": "2025-04-15T08:06:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3537"
  },
  {
    "number": 3535,
    "title": "chore: bump version to 0.19.0rc0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T09:45:53Z",
    "closed_at": "2025-04-14T10:11:23Z",
    "merged_at": "2025-04-14T10:11:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3535"
  },
  {
    "number": 3534,
    "title": "test: fix test name",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T09:07:45Z",
    "closed_at": "2025-04-14T09:09:51Z",
    "merged_at": "2025-04-14T09:09:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3534"
  },
  {
    "number": 3533,
    "title": "infra: Add test stages for sm120",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T09:02:04Z",
    "closed_at": "2025-04-22T17:26:13Z",
    "merged_at": "2025-04-22T17:26:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3533"
  },
  {
    "number": 3531,
    "title": "CI: Performance regression tests update",
    "user": "amirkl94",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T08:39:56Z",
    "closed_at": "2025-06-01T06:47:55Z",
    "merged_at": "2025-06-01T06:47:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3531"
  },
  {
    "number": 3530,
    "title": "refactor: Split llama4 model from llama model.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T08:25:44Z",
    "closed_at": "2025-04-15T05:41:06Z",
    "merged_at": "2025-04-15T05:41:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3530"
  },
  {
    "number": 3528,
    "title": "test: add deepseek v3 & r1 cases",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T08:13:58Z",
    "closed_at": "2025-04-28T15:37:26Z",
    "merged_at": "2025-04-28T15:37:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3528"
  },
  {
    "number": 3527,
    "title": "feat: Add stream generation task scaffolding examples",
    "user": "narutolhy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T07:18:17Z",
    "closed_at": "2025-04-16T03:33:56Z",
    "merged_at": "2025-04-16T03:33:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3527"
  },
  {
    "number": 3525,
    "title": "fix: Intercept the error of multi-ranks bound to a single device",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T06:55:57Z",
    "closed_at": "2025-04-23T07:50:18Z",
    "merged_at": "2025-04-23T07:50:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3525"
  },
  {
    "number": 3524,
    "title": "chore : Split more tests out of gpt tests",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T06:41:15Z",
    "closed_at": "2025-04-18T04:04:57Z",
    "merged_at": "2025-04-18T04:04:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3524"
  },
  {
    "number": 3523,
    "title": "fix: remove one duplicated line of code",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T06:24:27Z",
    "closed_at": "2025-04-14T06:52:46Z",
    "merged_at": "2025-04-14T06:52:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3523"
  },
  {
    "number": 3520,
    "title": "test: Add llama 4 to ci",
    "user": "dongfengy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T05:00:41Z",
    "closed_at": "2025-04-18T03:25:52Z",
    "merged_at": "2025-04-18T03:25:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3520"
  },
  {
    "number": 3519,
    "title": "doc: Add example section: \"Example: Multi-node benchmark on GB200\"",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T04:54:52Z",
    "closed_at": "2025-04-14T08:45:55Z",
    "merged_at": "2025-04-14T08:45:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3519"
  },
  {
    "number": 3518,
    "title": "test: add llmapi test cases into qa test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T04:45:45Z",
    "closed_at": "2025-04-14T08:41:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3518"
  },
  {
    "number": 3517,
    "title": "feat: Support cos_sin_cache in all cases.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T04:17:51Z",
    "closed_at": "2025-04-16T05:48:44Z",
    "merged_at": "2025-04-16T05:48:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3517"
  },
  {
    "number": 3516,
    "title": "disagg perf tune doc",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T04:00:08Z",
    "closed_at": "2025-04-14T08:05:07Z",
    "merged_at": "2025-04-14T08:05:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3516"
  },
  {
    "number": 3514,
    "title": "chore: bump version to 0.19.0rc0",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T03:50:05Z",
    "closed_at": "2025-04-14T09:32:31Z",
    "merged_at": "2025-04-14T09:32:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3514"
  },
  {
    "number": 3513,
    "title": "infra: add more codespell ignore words, prevent ans->and",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T03:45:01Z",
    "closed_at": "2025-04-14T09:00:31Z",
    "merged_at": "2025-04-14T09:00:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3513"
  },
  {
    "number": 3511,
    "title": "chore: move all distributed related codes into _torch.distributed directory",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T02:29:35Z",
    "closed_at": "2025-04-15T00:39:18Z",
    "merged_at": "2025-04-15T00:39:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3511"
  },
  {
    "number": 3510,
    "title": "test: [CI] remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T02:10:18Z",
    "closed_at": "2025-04-14T07:24:48Z",
    "merged_at": "2025-04-14T07:24:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3510"
  },
  {
    "number": 3509,
    "title": "chore: reduce num layers in attention test",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T01:49:11Z",
    "closed_at": "2025-04-14T04:43:59Z",
    "merged_at": "2025-04-14T04:43:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3509"
  },
  {
    "number": 3508,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-14T01:17:01Z",
    "closed_at": "2025-04-14T01:32:02Z",
    "merged_at": "2025-04-14T01:32:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3508"
  },
  {
    "number": 3506,
    "title": "refactor: Introduce DecoderOutputBuffers per batch",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-13T15:44:46Z",
    "closed_at": "2025-04-22T04:25:53Z",
    "merged_at": "2025-04-22T04:25:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3506"
  },
  {
    "number": 3505,
    "title": "chore: Clean up cpp runtime",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-13T15:40:45Z",
    "closed_at": "2025-04-14T10:00:03Z",
    "merged_at": "2025-04-14T10:00:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3505"
  },
  {
    "number": 3504,
    "title": "feat: large-scale EP(part 1: Add MNNVL MoE A2A support)",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-13T13:04:32Z",
    "closed_at": "2025-04-25T09:29:08Z",
    "merged_at": "2025-04-25T09:29:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3504"
  },
  {
    "number": 3503,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-13T11:17:06Z",
    "closed_at": "2025-04-15T08:53:53Z",
    "merged_at": "2025-04-15T08:53:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3503"
  },
  {
    "number": 3502,
    "title": "refactor: Remove enforced sorted order of batch slots",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-13T11:08:42Z",
    "closed_at": "2025-07-14T15:23:03Z",
    "merged_at": "2025-07-14T15:23:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3502"
  },
  {
    "number": 3501,
    "title": "feat: Add Dynasor-CoT in scaffolding examples",
    "user": "Fsanic",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-13T10:12:42Z",
    "closed_at": "2025-04-18T07:48:02Z",
    "merged_at": "2025-04-18T07:48:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3501"
  },
  {
    "number": 3498,
    "title": "test: [CI] Add failed cases into waives.txt",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-12T18:25:11Z",
    "closed_at": "2025-04-15T06:33:49Z",
    "merged_at": "2025-04-15T06:33:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3498"
  },
  {
    "number": 3497,
    "title": "fix: add kv memory size per token of draft model to calculate max number of tokens of kv cache",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-12T11:35:43Z",
    "closed_at": "2025-04-13T15:02:14Z",
    "merged_at": "2025-04-13T15:02:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3497"
  },
  {
    "number": 3496,
    "title": "refactor: Remove _pp_forward.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-12T09:31:41Z",
    "closed_at": "2025-04-14T01:49:45Z",
    "merged_at": "2025-04-14T01:49:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3496"
  },
  {
    "number": 3494,
    "title": "fix: Correct reporting of text dtype for Llama 4.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-12T06:46:47Z",
    "closed_at": "2025-04-17T16:07:50Z",
    "merged_at": "2025-04-17T16:07:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3494"
  },
  {
    "number": 3493,
    "title": "chore: remove useless max_num_tokens member in PyTorchConfig",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-12T01:34:35Z",
    "closed_at": "2025-04-12T13:09:59Z",
    "merged_at": "2025-04-12T13:09:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3493"
  },
  {
    "number": 3492,
    "title": "fix: llama4: add an option `apply_router_weight_on_input` for in FusedMoE",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T19:18:03Z",
    "closed_at": "2025-04-14T18:56:42Z",
    "merged_at": "2025-04-14T18:56:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3492"
  },
  {
    "number": 3491,
    "title": "fix: llama4: address couple of issues in llama4 attention module",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T19:03:06Z",
    "closed_at": "2025-04-18T01:55:00Z",
    "merged_at": "2025-04-18T01:55:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3491"
  },
  {
    "number": 3490,
    "title": "feat: adding multimodal (only image for now) support in trtllm-bench",
    "user": "rakib-hasan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T18:46:23Z",
    "closed_at": "2025-04-17T23:06:17Z",
    "merged_at": "2025-04-17T23:06:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3490"
  },
  {
    "number": 3489,
    "title": "fix: Allow context_and_generation request type in disagg overlap",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T18:19:15Z",
    "closed_at": "2025-04-11T23:15:02Z",
    "merged_at": "2025-04-11T23:15:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3489"
  },
  {
    "number": 3488,
    "title": "doc: Update instructions to enable FP8 MLA for Deepseek.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T16:44:46Z",
    "closed_at": "2025-04-15T05:12:34Z",
    "merged_at": "2025-04-15T05:12:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3488"
  },
  {
    "number": 3487,
    "title": "fix: fix max_seq_len in executor_config",
    "user": "dongjiyingdjy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T14:36:25Z",
    "closed_at": "2025-04-14T07:13:30Z",
    "merged_at": "2025-04-14T07:13:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3487"
  },
  {
    "number": 3485,
    "title": "fix: don't perform memory estimation for star_attention",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T13:10:14Z",
    "closed_at": "2025-04-12T03:34:47Z",
    "merged_at": "2025-04-12T03:34:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3485"
  },
  {
    "number": 3484,
    "title": "fix: install RTC headers with linking when using --linking_install_binary",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T11:19:18Z",
    "closed_at": "2025-04-14T13:22:04Z",
    "merged_at": "2025-04-14T13:22:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3484"
  },
  {
    "number": 3483,
    "title": "test [TRTLLM-4477,TRTLLM-4481]: Accuracy test improvement (Part 3.5): Support GSM8K and GPQA",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T09:47:48Z",
    "closed_at": "2025-04-21T23:38:16Z",
    "merged_at": "2025-04-21T23:38:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3483"
  },
  {
    "number": 3482,
    "title": "fix: Incorrect update of executor_config.max_seq_len",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T08:19:22Z",
    "closed_at": "2025-04-11T11:48:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3482"
  },
  {
    "number": 3481,
    "title": "fix: build engine for TRT path in slurm",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T07:39:34Z",
    "closed_at": "2025-04-24T01:12:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3481"
  },
  {
    "number": 3479,
    "title": "refactor: Clean up CMakeLists.txt",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T07:33:17Z",
    "closed_at": "2025-04-18T06:39:30Z",
    "merged_at": "2025-04-18T06:39:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3479"
  },
  {
    "number": 3478,
    "title": "infra: [TRTLLM-4417]Support auto trigger special test stage for special file change",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T07:25:05Z",
    "closed_at": "2025-04-23T12:32:19Z",
    "merged_at": "2025-04-23T12:32:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3478"
  },
  {
    "number": 3477,
    "title": "fix:update the default excluded_modules value for fp8rowwise recipe.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T06:53:24Z",
    "closed_at": "2025-04-12T08:00:22Z",
    "merged_at": "2025-04-12T08:00:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3477"
  },
  {
    "number": 3476,
    "title": "chore: add llama mlp_bias reading from hf_config",
    "user": "WhatGhost",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T06:42:30Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3476"
  },
  {
    "number": 3475,
    "title": "test: Unwaive Llama 3.1 with torch compile test",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T06:13:17Z",
    "closed_at": "2025-04-22T02:41:56Z",
    "merged_at": "2025-04-22T02:41:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3475"
  },
  {
    "number": 3474,
    "title": "fix: disable the kv cache reuse for prompt tuning test",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T06:10:34Z",
    "closed_at": "2025-04-14T06:35:47Z",
    "merged_at": "2025-04-14T06:35:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3474"
  },
  {
    "number": 3473,
    "title": "test: waive hanging test rather than waive all multi_gpu tests",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T06:01:44Z",
    "closed_at": "2025-04-11T13:28:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3473"
  },
  {
    "number": 3472,
    "title": "Waive failure post-merge tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T05:57:26Z",
    "closed_at": "2025-04-11T08:23:08Z",
    "merged_at": "2025-04-11T08:23:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3472"
  },
  {
    "number": 3471,
    "title": "test: Waive torch compile tests",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T05:26:08Z",
    "closed_at": "2025-04-11T05:38:06Z",
    "merged_at": "2025-04-11T05:38:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3471"
  },
  {
    "number": 3470,
    "title": "feat: Support torch profiler",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T04:17:50Z",
    "closed_at": "2025-04-14T14:06:07Z",
    "merged_at": "2025-04-14T14:06:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3470"
  },
  {
    "number": 3468,
    "title": "test: Automatically clean checkpoints and engines",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T03:42:57Z",
    "closed_at": "2025-04-12T01:56:29Z",
    "merged_at": "2025-04-12T01:56:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3468"
  },
  {
    "number": 3467,
    "title": "chore: Log memory sizes of weights and activations separately",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T03:09:10Z",
    "closed_at": "2025-04-15T01:48:36Z",
    "merged_at": "2025-04-15T01:48:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3467"
  },
  {
    "number": 3466,
    "title": "perf: Optimize quantization kernels used in DeepSeek on Hopper",
    "user": "jiahanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T02:52:56Z",
    "closed_at": "2025-04-15T09:49:58Z",
    "merged_at": "2025-04-15T09:49:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3466"
  },
  {
    "number": 3465,
    "title": "infra: User/zhanruis/0408 support auto trigger",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T02:04:44Z",
    "closed_at": "2025-04-11T07:16:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3465"
  },
  {
    "number": 3464,
    "title": "waive unittest/_torch/multi_gpu",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T01:46:38Z",
    "closed_at": "2025-04-11T01:59:16Z",
    "merged_at": "2025-04-11T01:59:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3464"
  },
  {
    "number": 3463,
    "title": "chore: remove several useless header files.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T01:39:56Z",
    "closed_at": "2025-04-18T03:15:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3463"
  },
  {
    "number": 3462,
    "title": "fix: switch ZMQ from file socket to tcp socket in RemoteMpiCommSession",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T01:19:47Z",
    "closed_at": "2025-04-13T01:15:56Z",
    "merged_at": "2025-04-13T01:15:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3462"
  },
  {
    "number": 3461,
    "title": "waive a test case of llama 3.1 with torch compile",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T01:03:27Z",
    "closed_at": "2025-04-11T01:15:19Z",
    "merged_at": "2025-04-11T01:15:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3461"
  },
  {
    "number": 3460,
    "title": "feat: Add fp8 bmm quantization support for AutoDeploy for llama4",
    "user": "meenchen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-11T00:55:43Z",
    "closed_at": "2025-04-15T19:09:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3460"
  },
  {
    "number": 3459,
    "title": "feat: [Deepseek] Redesign multi-stream API",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T23:50:51Z",
    "closed_at": "2025-04-12T06:00:26Z",
    "merged_at": "2025-04-12T06:00:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3459"
  },
  {
    "number": 3458,
    "title": "doc: Update perf-benchmarking doc on GPU configuration for consistent benchmarking.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T16:20:40Z",
    "closed_at": "2025-04-11T15:21:28Z",
    "merged_at": "2025-04-11T15:21:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3458"
  },
  {
    "number": 3457,
    "title": "test:add fp8_kv_cache functionality test case.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T15:47:11Z",
    "closed_at": "2025-04-15T01:16:46Z",
    "merged_at": "2025-04-15T01:16:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3457"
  },
  {
    "number": 3456,
    "title": "fix: Eagle decoding",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T15:31:58Z",
    "closed_at": "2025-04-11T14:06:39Z",
    "merged_at": "2025-04-11T14:06:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3456"
  },
  {
    "number": 3455,
    "title": "feat/loraOp ",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T15:21:23Z",
    "closed_at": "2025-04-17T04:48:27Z",
    "merged_at": "2025-04-17T04:48:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3455"
  },
  {
    "number": 3454,
    "title": "test: Add DeepSeek-V3-Lite PP=4 cases",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T15:15:18Z",
    "closed_at": "2025-04-11T16:09:13Z",
    "merged_at": "2025-04-11T16:09:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3454"
  },
  {
    "number": 3453,
    "title": "feat: draft/Lora op",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T15:02:38Z",
    "closed_at": "2025-06-09T07:20:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3453"
  },
  {
    "number": 3452,
    "title": "chore: waive some test cases of test_llm_multi_gpu.py",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T13:45:36Z",
    "closed_at": "2025-04-10T14:02:36Z",
    "merged_at": "2025-04-10T14:02:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3452"
  },
  {
    "number": 3451,
    "title": "chore: add dgx_h200 tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T13:36:28Z",
    "closed_at": "2025-04-14T03:20:55Z",
    "merged_at": "2025-04-14T03:20:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3451"
  },
  {
    "number": 3450,
    "title": "chore: Unify Python NVTX call",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T12:21:31Z",
    "closed_at": "2025-04-15T15:25:37Z",
    "merged_at": "2025-04-15T15:25:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3450"
  },
  {
    "number": 3449,
    "title": "fix: Fix PP for llama.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T11:42:24Z",
    "closed_at": "2025-04-12T09:20:28Z",
    "merged_at": "2025-04-12T09:20:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3449"
  },
  {
    "number": 3448,
    "title": "fix: nvbugs/5187237: fix deterministic mode crash",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T10:56:28Z",
    "closed_at": "2025-04-17T04:01:58Z",
    "merged_at": "2025-04-17T04:01:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3448"
  },
  {
    "number": 3447,
    "title": "chore: Rename nvsmall to nemotron nas",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T10:13:17Z",
    "closed_at": "2025-04-10T15:16:52Z",
    "merged_at": "2025-04-10T15:16:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3447"
  },
  {
    "number": 3446,
    "title": "feat: Update fmha kernels to support Eagle3 on blackwell.",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T09:46:26Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3446"
  },
  {
    "number": 3445,
    "title": "Feat: Variable-Beam-Width-Search (VBWS) part4",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T09:25:06Z",
    "closed_at": "2025-04-28T06:40:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3445"
  },
  {
    "number": 3443,
    "title": "infra: Support auto trigger test stage for special file change.",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T09:02:17Z",
    "closed_at": "2025-04-11T07:23:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3443"
  },
  {
    "number": 3442,
    "title": "Waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T08:48:17Z",
    "closed_at": "2025-04-10T09:43:45Z",
    "merged_at": "2025-04-10T09:43:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3442"
  },
  {
    "number": 3441,
    "title": "[TRTLLM-4001] chore: clean the waive.txt",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T07:32:12Z",
    "closed_at": "2025-04-10T08:20:09Z",
    "merged_at": "2025-04-10T08:20:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3441"
  },
  {
    "number": 3440,
    "title": "test: move mistral / mixtral test cases in QA test list into the new accuracy test suite",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T07:10:46Z",
    "closed_at": "2025-05-09T05:32:02Z",
    "merged_at": "2025-05-09T05:32:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3440"
  },
  {
    "number": 3439,
    "title": "perf: Eliminate the need for attention DP padding when possible",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T07:03:35Z",
    "closed_at": "2025-05-17T05:30:56Z",
    "merged_at": "2025-05-17T05:30:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3439"
  },
  {
    "number": 3438,
    "title": "feat: Add group_rms_norm kernel to normalize multiple inputs in a single operator.",
    "user": "SimengLiu-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T06:22:22Z",
    "closed_at": "2025-05-02T05:25:30Z",
    "merged_at": "2025-05-02T05:25:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3438"
  },
  {
    "number": 3437,
    "title": "infra: Update some test description which is out of date",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T06:07:40Z",
    "closed_at": "2025-04-10T09:29:31Z",
    "merged_at": "2025-04-10T09:29:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3437"
  },
  {
    "number": 3436,
    "title": "feat: replace MLA context (192x128 packed) fmha from ampere-style to hopper-style",
    "user": "zhou-yuxin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T05:47:34Z",
    "closed_at": "2025-08-18T21:01:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3436"
  },
  {
    "number": 3435,
    "title": "fix: Fix the issues related to fused moe path.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T04:44:14Z",
    "closed_at": "2025-04-11T13:41:15Z",
    "merged_at": "2025-04-11T13:41:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3435"
  },
  {
    "number": 3434,
    "title": "infra: always trigger multi gpu test to protect llama and deepseek",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T03:29:33Z",
    "closed_at": "2025-04-11T05:19:23Z",
    "merged_at": "2025-04-11T05:19:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3434"
  },
  {
    "number": 3433,
    "title": "waive llama3.1 8B test cases with pipeline parallelism",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T02:48:50Z",
    "closed_at": "2025-04-10T03:07:59Z",
    "merged_at": "2025-04-10T03:07:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3433"
  },
  {
    "number": 3432,
    "title": "chore: code cleanup for error logging and SharedMemory in proxy.py",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-10T01:42:43Z",
    "closed_at": "2025-04-10T13:57:07Z",
    "merged_at": "2025-04-10T13:57:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3432"
  },
  {
    "number": 3431,
    "title": "Do not merge, internal",
    "user": "milesial",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T22:56:28Z",
    "closed_at": "2025-04-16T19:29:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3431"
  },
  {
    "number": 3430,
    "title": "feat: Nemotron-H model support",
    "user": "vegaluisjose",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T22:27:45Z",
    "closed_at": "2025-04-16T21:05:56Z",
    "merged_at": "2025-04-16T21:05:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3430"
  },
  {
    "number": 3429,
    "title": "chore: unify pp_layers helpers",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T21:46:05Z",
    "closed_at": "2025-04-14T20:49:17Z",
    "merged_at": "2025-04-14T20:49:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3429"
  },
  {
    "number": 3428,
    "title": "chore: update allowlist",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T19:37:10Z",
    "closed_at": "2025-04-09T22:41:41Z",
    "merged_at": "2025-04-09T22:41:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3428"
  },
  {
    "number": 3427,
    "title": "fix: Fixing issue with first gen token being returned twice in streaming",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T18:58:48Z",
    "closed_at": "2025-04-14T02:45:10Z",
    "merged_at": "2025-04-14T02:45:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3427"
  },
  {
    "number": 3425,
    "title": "Feat: Prompt-Lookup-Decoding (PLD) with IFB",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T16:08:04Z",
    "closed_at": "2025-04-28T07:26:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3425"
  },
  {
    "number": 3424,
    "title": "feat: Replace cyclic kv cache with sliding kv cache",
    "user": "tomeras91",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T15:51:43Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3424"
  },
  {
    "number": 3423,
    "title": "feat: trtllm-gen fp4 GEMM for pytorch workflow",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T14:34:54Z",
    "closed_at": "2025-04-10T18:28:08Z",
    "merged_at": "2025-04-10T18:28:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3423"
  },
  {
    "number": 3422,
    "title": "fix: Fix pipeline parallelism for Llama-style models",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T14:28:54Z",
    "closed_at": "2025-04-10T14:55:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3422"
  },
  {
    "number": 3421,
    "title": "chore: Mass integration of release/0.18",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T14:14:09Z",
    "closed_at": "2025-04-16T02:03:29Z",
    "merged_at": "2025-04-16T02:03:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3421"
  },
  {
    "number": 3420,
    "title": "fix: updating ucxx, which appears to avoid occasional segfaults when profiling",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T13:19:07Z",
    "closed_at": "2025-04-10T11:48:20Z",
    "merged_at": "2025-04-10T11:48:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3420"
  },
  {
    "number": 3419,
    "title": "feat: output-format arg for synthetic data generation script and improved support for Mistral models in the Pytorch workflow",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T12:51:49Z",
    "closed_at": "2025-04-25T12:23:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3419"
  },
  {
    "number": 3417,
    "title": "feat: Support TLLM_OVERRIDE_LAYER_NUM and TLLM_TRACE_MODEL_FORWARD for debugging",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T11:30:27Z",
    "closed_at": "2025-04-10T05:18:31Z",
    "merged_at": "2025-04-10T05:18:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3417"
  },
  {
    "number": 3416,
    "title": "feat: Make scaffolding Controller more generic #3408",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T10:53:49Z",
    "closed_at": "2025-04-12T13:35:39Z",
    "merged_at": "2025-04-12T13:35:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3416"
  },
  {
    "number": 3415,
    "title": "chore: disable some env for disagg defaultly",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T10:47:18Z",
    "closed_at": "2025-04-14T02:08:11Z",
    "merged_at": "2025-04-14T02:08:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3415"
  },
  {
    "number": 3414,
    "title": "chore: support getting the latest iteration status",
    "user": "pansicheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T10:40:55Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3414"
  },
  {
    "number": 3413,
    "title": "fix: fix partialMatch",
    "user": "pansicheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T10:06:44Z",
    "closed_at": "2025-04-11T08:42:53Z",
    "merged_at": "2025-04-11T08:42:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3413"
  },
  {
    "number": 3412,
    "title": "chore : Split GptExecutor tests out of gpt tests to reduce single tes…",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T09:58:43Z",
    "closed_at": "2025-04-10T01:08:15Z",
    "merged_at": "2025-04-10T01:08:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3412"
  },
  {
    "number": 3411,
    "title": "fix: remove DeepGEMM line info",
    "user": "lucifer1004",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T09:50:04Z",
    "closed_at": "2025-04-09T10:01:03Z",
    "merged_at": "2025-04-09T10:01:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3411"
  },
  {
    "number": 3410,
    "title": "test: torch-flow conditional disagg test",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T09:48:29Z",
    "closed_at": "2025-04-15T02:54:15Z",
    "merged_at": "2025-04-15T02:54:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3410"
  },
  {
    "number": 3407,
    "title": "doc: genai-perf benchmark & slurm multi-node for trtllm-serve doc",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T06:55:48Z",
    "closed_at": "2025-04-15T16:11:58Z",
    "merged_at": "2025-04-15T16:11:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3407"
  },
  {
    "number": 3406,
    "title": "fix: Fix disagg MTP with overlap",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T06:52:01Z",
    "closed_at": "2025-04-12T04:27:25Z",
    "merged_at": "2025-04-12T04:27:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3406"
  },
  {
    "number": 3405,
    "title": "chore: Update the GitHub pages build script",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T06:46:59Z",
    "closed_at": "2025-04-09T11:58:38Z",
    "merged_at": "2025-04-09T11:58:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3405"
  },
  {
    "number": 3404,
    "title": "test: add torch flow test case in qa test list",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T06:38:02Z",
    "closed_at": "2025-04-11T08:57:41Z",
    "merged_at": "2025-04-11T08:57:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3404"
  },
  {
    "number": 3403,
    "title": "update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T06:13:22Z",
    "closed_at": "2025-04-09T06:14:17Z",
    "merged_at": "2025-04-09T06:14:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3403"
  },
  {
    "number": 3402,
    "title": "chore: make LLM-API slurm examples executable (#3358)",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T05:49:55Z",
    "closed_at": "2025-04-13T13:42:45Z",
    "merged_at": "2025-04-13T13:42:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3402"
  },
  {
    "number": 3400,
    "title": "update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T04:55:09Z",
    "closed_at": "2025-04-09T05:16:40Z",
    "merged_at": "2025-04-09T05:16:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3400"
  },
  {
    "number": 3399,
    "title": "vibe coded bmm sharder",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T04:53:52Z",
    "closed_at": "2025-04-10T18:35:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3399"
  },
  {
    "number": 3398,
    "title": "feat: Add tool_call support for serving that can handle agentic framework function/tool call",
    "user": "sarattha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T04:35:01Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3398"
  },
  {
    "number": 3397,
    "title": "fix: mllama e2e pytorch flow fix",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T04:27:36Z",
    "closed_at": "2025-04-11T09:33:16Z",
    "merged_at": "2025-04-11T09:33:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3397"
  },
  {
    "number": 3396,
    "title": "Update gh-pages",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T04:23:41Z",
    "closed_at": "2025-04-09T04:24:57Z",
    "merged_at": "2025-04-09T04:24:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3396"
  },
  {
    "number": 3395,
    "title": "test: disable attention DP tests for single GPU",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T03:55:29Z",
    "closed_at": "2025-04-10T17:38:18Z",
    "merged_at": "2025-04-10T17:38:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3395"
  },
  {
    "number": 3394,
    "title": "Revert \"Update gh-pages\"",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T03:11:49Z",
    "closed_at": "2025-04-09T03:13:20Z",
    "merged_at": "2025-04-09T03:13:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3394"
  },
  {
    "number": 3393,
    "title": "Update gh-pages",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T03:07:10Z",
    "closed_at": "2025-04-09T03:09:19Z",
    "merged_at": "2025-04-09T03:09:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3393"
  },
  {
    "number": 3392,
    "title": "test: fix memory leak of tests",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T02:25:24Z",
    "closed_at": "2025-04-10T06:31:41Z",
    "merged_at": "2025-04-10T06:31:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3392"
  },
  {
    "number": 3391,
    "title": "chore: fix wheel version <= 0.45.1",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T01:33:00Z",
    "closed_at": "2025-04-09T04:31:56Z",
    "merged_at": "2025-04-09T04:31:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3391"
  },
  {
    "number": 3389,
    "title": "TensorRT-LLM v0.18.1 release",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T01:01:17Z",
    "closed_at": "2025-04-09T01:06:37Z",
    "merged_at": "2025-04-09T01:06:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3389"
  },
  {
    "number": 3388,
    "title": "feat: Support Top-K logprobs and prompt_logprobs in LLMAPI",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T00:21:05Z",
    "closed_at": "2025-05-01T16:47:14Z",
    "merged_at": "2025-05-01T16:47:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3388"
  },
  {
    "number": 3387,
    "title": "feat: [Deepseek] Add trtllm-gen MOE FP4 MOE backend",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T23:56:35Z",
    "closed_at": "2025-04-21T02:01:34Z",
    "merged_at": "2025-04-21T02:01:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3387"
  },
  {
    "number": 3386,
    "title": "fix: llama4 EP weights loading issue",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T23:42:09Z",
    "closed_at": "2025-04-14T18:58:30Z",
    "merged_at": "2025-04-14T18:58:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3386"
  },
  {
    "number": 3385,
    "title": "Fix failing DSV3 unit tests",
    "user": "sugunav14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T23:26:11Z",
    "closed_at": "2025-04-09T03:57:05Z",
    "merged_at": "2025-04-09T03:57:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3385"
  },
  {
    "number": 3384,
    "title": "fix: Use hmac authentication for pickle encryption",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T22:28:51Z",
    "closed_at": "2025-04-16T16:40:13Z",
    "merged_at": "2025-04-16T16:40:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3384"
  },
  {
    "number": 3383,
    "title": "feat: llama4 multimodal input processor",
    "user": "milesial",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T22:20:53Z",
    "closed_at": "2025-04-25T23:47:14Z",
    "merged_at": "2025-04-25T23:47:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3383"
  },
  {
    "number": 3382,
    "title": "feat: support llama4 nope layers; support FP8 checkpoint loading;",
    "user": "nvzhihanj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T21:31:23Z",
    "closed_at": "2025-04-10T17:16:44Z",
    "merged_at": "2025-04-10T17:16:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3382"
  },
  {
    "number": 3381,
    "title": "fix: Add nested aliases for Llama 4",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T21:11:11Z",
    "closed_at": "2025-04-10T02:18:53Z",
    "merged_at": "2025-04-10T02:18:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3381"
  },
  {
    "number": 3380,
    "title": "feat: Offloading Multimodal embedding table to CPU in Chunked Prefill Mode",
    "user": "katec846",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T20:59:41Z",
    "closed_at": "2025-04-21T06:31:02Z",
    "merged_at": "2025-04-21T06:31:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3380"
  },
  {
    "number": 3379,
    "title": "chore: update allowlist",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T19:27:26Z",
    "closed_at": "2025-04-09T03:10:42Z",
    "merged_at": "2025-04-09T03:10:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3379"
  },
  {
    "number": 3378,
    "title": "feat: Cache sin cos in model instead of global LRU cache.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T18:53:22Z",
    "closed_at": "2025-04-14T03:19:10Z",
    "merged_at": "2025-04-14T03:19:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3378"
  },
  {
    "number": 3375,
    "title": "Fix: Beam Search Diversity",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T15:19:38Z",
    "closed_at": "2025-04-11T03:58:59Z",
    "merged_at": "2025-04-11T03:58:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3375"
  },
  {
    "number": 3374,
    "title": "feat: Allow individual gatherContext for each additional output",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T14:27:33Z",
    "closed_at": "2025-04-12T09:00:36Z",
    "merged_at": "2025-04-12T09:00:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3374"
  },
  {
    "number": 3373,
    "title": "Add and Remove server dynamically in PD",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T12:59:58Z",
    "closed_at": "2025-04-17T00:54:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3373"
  },
  {
    "number": 3372,
    "title": "Feat/ Integrate peftCacheManager in PyExecutor creation",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T12:51:23Z",
    "closed_at": "2025-04-15T07:14:43Z",
    "merged_at": "2025-04-15T07:14:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3372"
  },
  {
    "number": 3371,
    "title": "feat: Add NVFP4 UB pattern optimization pass in torch compile",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T11:39:34Z",
    "closed_at": "2025-04-11T13:25:30Z",
    "merged_at": "2025-04-11T13:25:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3371"
  },
  {
    "number": 3370,
    "title": "refactor: remove ParallelConfig in tensorrt_llm._torch.distributed module",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T11:23:36Z",
    "closed_at": "2025-04-11T22:34:21Z",
    "merged_at": "2025-04-11T22:34:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3370"
  },
  {
    "number": 3369,
    "title": "feat: add qwen2 moe to torch flow; fix wrong imported KvCacheConfig in gpqa…",
    "user": "wm2012011492",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T10:59:00Z",
    "closed_at": "2025-04-10T14:45:57Z",
    "merged_at": "2025-04-10T14:45:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3369"
  },
  {
    "number": 3366,
    "title": "Doc: update steps of using Draft-Target-Model (DTM) in the documents.",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T08:58:49Z",
    "closed_at": "2025-04-09T09:35:02Z",
    "merged_at": "2025-04-09T09:35:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3366"
  },
  {
    "number": 3365,
    "title": "infra: [TRTLLM-4450] Support more files for pytorch only mode",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T08:42:16Z",
    "closed_at": "2025-04-08T17:39:06Z",
    "merged_at": "2025-04-08T17:39:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3365"
  },
  {
    "number": 3364,
    "title": "test: add cuda visible device constraint for phi_1gpu test",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T08:28:11Z",
    "closed_at": "2025-04-11T09:14:52Z",
    "merged_at": "2025-04-11T09:14:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3364"
  },
  {
    "number": 3363,
    "title": "test: add llama3.2 ptp test case",
    "user": "StanleySun639",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T08:22:46Z",
    "closed_at": "2025-04-21T07:15:46Z",
    "merged_at": "2025-04-21T07:15:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3363"
  },
  {
    "number": 3362,
    "title": "chore : fix estimated kv cache memory when cuda graph is enabled",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T07:56:48Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3362"
  },
  {
    "number": 3361,
    "title": "fix [NVBUG 5208255] Fix missing bias add for FP4Linear.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T07:33:09Z",
    "closed_at": "2025-04-09T01:17:54Z",
    "merged_at": "2025-04-09T01:17:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3361"
  },
  {
    "number": 3360,
    "title": "chore: bump version to 0.19.0.dev2025041500",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T06:53:32Z",
    "closed_at": "2025-04-08T12:45:27Z",
    "merged_at": "2025-04-08T12:45:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3360"
  },
  {
    "number": 3359,
    "title": "test getFailSignaturesList",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T06:16:47Z",
    "closed_at": "2025-04-08T08:18:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3359"
  },
  {
    "number": 3358,
    "title": "chore: clean up trtllm-llmapi-launch logs",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T05:56:58Z",
    "closed_at": "2025-04-08T08:01:00Z",
    "merged_at": "2025-04-08T08:01:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3358"
  },
  {
    "number": 3356,
    "title": "Waive a mamba unittest",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T05:34:09Z",
    "closed_at": "2025-04-08T05:36:35Z",
    "merged_at": "2025-04-08T05:36:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3356"
  },
  {
    "number": 3355,
    "title": "test: TRTQA-2824 add test for https://nvbugs/4753548",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T05:02:40Z",
    "closed_at": "2025-04-18T06:09:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3355"
  },
  {
    "number": 3354,
    "title": "feat: add deepseek-r1 reasoning parser to trtllm-serve",
    "user": "pansicheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T04:48:59Z",
    "closed_at": "2025-05-06T00:13:04Z",
    "merged_at": "2025-05-06T00:13:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3354"
  },
  {
    "number": 3353,
    "title": "disagg test single h100",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T04:12:59Z",
    "closed_at": "2025-04-08T09:45:35Z",
    "merged_at": "2025-04-08T09:45:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3353"
  },
  {
    "number": 3351,
    "title": "fix: revert extra cmake var",
    "user": "lucifer1004",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T01:46:04Z",
    "closed_at": "2025-04-08T03:57:16Z",
    "merged_at": "2025-04-08T03:57:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3351"
  },
  {
    "number": 3350,
    "title": "test: Accuracy test improvement (Part 3.4): Move LLaMA tests",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T23:49:59Z",
    "closed_at": "2025-04-08T07:07:58Z",
    "merged_at": "2025-04-08T07:07:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3350"
  },
  {
    "number": 3346,
    "title": "feat: Add numNodes to ParallelConfig",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T20:26:54Z",
    "closed_at": "2025-04-13T11:55:05Z",
    "merged_at": "2025-04-13T11:55:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3346"
  },
  {
    "number": 3343,
    "title": "feat: register ENABLE_MULTI_DEVICE and ENABLE_UCX as CMake options",
    "user": "WilliamTambellini",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T18:30:49Z",
    "closed_at": "2025-04-14T02:30:23Z",
    "merged_at": "2025-04-14T02:30:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3343"
  },
  {
    "number": 3342,
    "title": "chore: Adding DS V3-lite tests with overlap + cuda graph",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T16:07:58Z",
    "closed_at": "2025-04-08T13:36:12Z",
    "merged_at": "2025-04-08T13:36:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3342"
  },
  {
    "number": 3341,
    "title": "feat: Enable DeepGEMM by default",
    "user": "lucifer1004",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T15:29:44Z",
    "closed_at": "2025-04-08T05:58:57Z",
    "merged_at": "2025-04-08T05:58:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3341"
  },
  {
    "number": 3339,
    "title": "chore: Unwaive DS + overlap disagg test",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T14:33:20Z",
    "closed_at": "2025-04-12T17:33:38Z",
    "merged_at": "2025-04-12T17:33:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3339"
  },
  {
    "number": 3338,
    "title": "Feat: Variable-Beam-Width-Search (VBWS) part3",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T13:52:04Z",
    "closed_at": "2025-04-08T15:51:29Z",
    "merged_at": "2025-04-08T15:51:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3338"
  },
  {
    "number": 3336,
    "title": "feat: Add Llama 4 Vision",
    "user": "RomaA2000",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T13:11:54Z",
    "closed_at": "2025-04-11T17:28:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3336"
  },
  {
    "number": 3326,
    "title": "fix: Fix p-tuning test bug",
    "user": "amirkl94",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T12:37:01Z",
    "closed_at": "2025-04-08T09:14:00Z",
    "merged_at": "2025-04-08T09:14:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3326"
  },
  {
    "number": 3323,
    "title": "doc: update readme for disaggregated",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T09:56:54Z",
    "closed_at": "2025-04-07T13:29:16Z",
    "merged_at": "2025-04-07T13:29:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3323"
  },
  {
    "number": 3322,
    "title": "perf: Optimize overhead due to get_mla_metadata before each FlashMLA kernel.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T09:16:43Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3322"
  },
  {
    "number": 3321,
    "title": "fix: Proper error bubbling for PyExecutor",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T09:05:55Z",
    "closed_at": "2025-04-15T06:49:47Z",
    "merged_at": "2025-04-15T06:49:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3321"
  },
  {
    "number": 3320,
    "title": "chore: exchange connection id with tagSend/tagRecv",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T08:50:41Z",
    "closed_at": "2025-04-14T01:30:34Z",
    "merged_at": "2025-04-14T01:30:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3320"
  },
  {
    "number": 3319,
    "title": "fix: add tp=2 ci test for vision encoder",
    "user": "MinaHuai",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T08:50:18Z",
    "closed_at": "2025-04-08T04:46:08Z",
    "merged_at": "2025-04-08T04:46:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3319"
  },
  {
    "number": 3318,
    "title": "Draft Feat/support lora in torch llm flow",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T08:35:50Z",
    "closed_at": "2025-04-07T08:36:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3318"
  },
  {
    "number": 3317,
    "title": "test: test debug hook",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T07:56:30Z",
    "closed_at": "2025-09-09T03:28:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3317"
  },
  {
    "number": 3316,
    "title": "test: fix conflicting test names",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T07:54:50Z",
    "closed_at": "2025-04-07T12:10:02Z",
    "merged_at": "2025-04-07T12:10:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3316"
  },
  {
    "number": 3315,
    "title": "fix: fix conflicting test names",
    "user": "yweng0828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T07:46:37Z",
    "closed_at": "2025-04-07T07:52:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3315"
  },
  {
    "number": 3314,
    "title": "infra: Fix bot help error when \" in bot command",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T07:25:25Z",
    "closed_at": "2025-04-08T10:16:06Z",
    "merged_at": "2025-04-08T10:16:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3314"
  },
  {
    "number": 3313,
    "title": "infra: Remove the WAR for test items incompletely",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T03:28:18Z",
    "closed_at": "2025-05-04T03:31:59Z",
    "merged_at": "2025-05-04T03:31:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3313"
  },
  {
    "number": 3312,
    "title": "feat: Enhance the integrated robustness of scaffolding with __init__.…",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T03:09:09Z",
    "closed_at": "2025-04-09T13:13:49Z",
    "merged_at": "2025-04-09T13:13:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3312"
  },
  {
    "number": 3310,
    "title": "chore: waive a timeout multi-GPU test case",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-06T23:49:13Z",
    "closed_at": "2025-04-07T06:04:54Z",
    "merged_at": "2025-04-07T06:04:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3310"
  },
  {
    "number": 3309,
    "title": "test: Waive non-Llama Eagle tests",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-06T20:38:11Z",
    "closed_at": "2025-04-07T01:25:41Z",
    "merged_at": "2025-04-07T01:25:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3309"
  },
  {
    "number": 3308,
    "title": "chore: update internal cutlass library base #2981 and #3165.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-06T15:43:41Z",
    "closed_at": "2025-04-07T05:53:03Z",
    "merged_at": "2025-04-07T05:53:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3308"
  },
  {
    "number": 3307,
    "title": "refactor: decoder buffers",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-06T14:45:28Z",
    "closed_at": "2025-04-12T09:41:24Z",
    "merged_at": "2025-04-12T09:41:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3307"
  },
  {
    "number": 3306,
    "title": "feat: user can specify UCX ip interface",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-06T13:00:03Z",
    "closed_at": "2025-04-07T00:44:34Z",
    "merged_at": "2025-04-07T00:44:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3306"
  },
  {
    "number": 3304,
    "title": "feat: Draft: add scheduler module to disaggregated serving",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-06T08:09:17Z",
    "closed_at": "2025-04-16T17:44:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3304"
  },
  {
    "number": 3303,
    "title": "feat: use cudaMalloc to allocate kvCache",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-06T05:34:20Z",
    "closed_at": "2025-04-08T02:59:14Z",
    "merged_at": "2025-04-08T02:59:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3303"
  },
  {
    "number": 3302,
    "title": "feat: Add Llama 4",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-05T21:46:24Z",
    "closed_at": "2025-04-08T19:35:22Z",
    "merged_at": "2025-04-08T19:35:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3302"
  },
  {
    "number": 3301,
    "title": "L4 added to readme",
    "user": "ncomly-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-05T19:28:48Z",
    "closed_at": "2025-04-06T11:09:28Z",
    "merged_at": "2025-04-06T11:09:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3301"
  },
  {
    "number": 3300,
    "title": "refactor: batch slot management in decoder classes",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-05T12:53:05Z",
    "closed_at": "2025-04-12T21:05:14Z",
    "merged_at": "2025-04-12T21:05:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3300"
  },
  {
    "number": 3299,
    "title": "fix: fix attentionDP padding request type",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-05T12:24:50Z",
    "closed_at": "2025-04-07T05:28:21Z",
    "merged_at": "2025-04-07T05:28:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3299"
  },
  {
    "number": 3298,
    "title": "infra: Add step to generate new duration file",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-05T12:20:19Z",
    "closed_at": "2025-04-18T04:56:32Z",
    "merged_at": "2025-04-18T04:56:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3298"
  },
  {
    "number": 3297,
    "title": "fix: fix the py_decoding_iter update in decoder",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-05T09:01:52Z",
    "closed_at": "2025-04-07T03:18:33Z",
    "merged_at": "2025-04-07T03:18:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3297"
  },
  {
    "number": 3296,
    "title": "feat: Add support for Phi-4-MM",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-05T02:25:27Z",
    "closed_at": "2025-04-14T06:24:11Z",
    "merged_at": "2025-04-14T06:24:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3296"
  },
  {
    "number": 3295,
    "title": "test: Add single gpu disaggregated tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-05T02:04:04Z",
    "closed_at": "2025-04-09T01:34:45Z",
    "merged_at": "2025-04-09T01:34:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3295"
  },
  {
    "number": 3294,
    "title": "[None][feat] CUTLASS MoE FC2+Finalize fusion",
    "user": "sklevtsov-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T22:26:23Z",
    "closed_at": "2025-08-12T07:56:48Z",
    "merged_at": "2025-08-12T07:56:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3294"
  },
  {
    "number": 3293,
    "title": "Update speculative-decoding.md",
    "user": "taras-sereda",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T21:47:16Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3293"
  },
  {
    "number": 3292,
    "title": "Support autodeploy in trtllm lm_eval example",
    "user": "cjluo-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T20:43:19Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3292"
  },
  {
    "number": 3291,
    "title": "chore: remove usernames from comments",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T19:14:40Z",
    "closed_at": "2025-04-05T05:44:28Z",
    "merged_at": "2025-04-05T05:44:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3291"
  },
  {
    "number": 3288,
    "title": "test: fix disagg overlap dp test",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T07:39:18Z",
    "closed_at": "2025-04-09T01:59:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3288"
  },
  {
    "number": 3286,
    "title": "chore: [MLA] Deallocate tensors after use",
    "user": "hlu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T04:20:11Z",
    "closed_at": "2025-04-10T04:36:07Z",
    "merged_at": "2025-04-10T04:36:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3286"
  },
  {
    "number": 3285,
    "title": "fix deepseek multi gpu tests timeout",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T04:09:06Z",
    "closed_at": "2025-04-04T08:19:03Z",
    "merged_at": "2025-04-04T08:19:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3285"
  },
  {
    "number": 3284,
    "title": "[Draft]: Use pydantic json serialization for ZMQ socket IPC communication",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T01:49:22Z",
    "closed_at": "2025-04-24T23:21:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3284"
  },
  {
    "number": 3283,
    "title": "feat:[AutoDeploy] Update MoE pattern matcher to drop expert selection logic",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T00:25:48Z",
    "closed_at": "2025-05-15T05:53:10Z",
    "merged_at": "2025-05-15T05:53:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3283"
  },
  {
    "number": 3282,
    "title": "infra: move nvrtc_wrapper to conan",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T22:35:06Z",
    "closed_at": "2025-04-14T21:31:02Z",
    "merged_at": "2025-04-14T21:31:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3282"
  },
  {
    "number": 3281,
    "title": "feat: [TRTLLM-3510] DeepseekV3 support in AutoDeploy",
    "user": "sugunav14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T19:24:45Z",
    "closed_at": "2025-04-08T13:47:57Z",
    "merged_at": "2025-04-08T13:47:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3281"
  },
  {
    "number": 3279,
    "title": "chore: update internal_cutlass version.txt to d03df7b27",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T17:48:50Z",
    "closed_at": "2025-04-04T07:50:04Z",
    "merged_at": "2025-04-04T07:50:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3279"
  },
  {
    "number": 3278,
    "title": "fix: redrafter sampling",
    "user": "1ytic",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T17:43:43Z",
    "closed_at": "2025-04-07T23:49:32Z",
    "merged_at": "2025-04-07T23:49:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3278"
  },
  {
    "number": 3276,
    "title": "fix: #3137 speculative decoding and multimodal input support",
    "user": "maxilevi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T15:19:51Z",
    "closed_at": "2025-04-09T15:40:20Z",
    "merged_at": "2025-04-09T15:40:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3276"
  },
  {
    "number": 3275,
    "title": "Draft: Test bot",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T13:51:36Z",
    "closed_at": "2025-04-03T13:52:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3275"
  },
  {
    "number": 3272,
    "title": "doc: fix deepseek-v3 mtp cmd",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T11:45:01Z",
    "closed_at": "2025-04-03T13:12:18Z",
    "merged_at": "2025-04-03T13:12:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3272"
  },
  {
    "number": 3271,
    "title": "chore: Refine attention backend interface.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T10:59:24Z",
    "closed_at": "2025-04-08T18:34:53Z",
    "merged_at": "2025-04-08T18:34:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3271"
  },
  {
    "number": 3270,
    "title": "fix: Add thread leak check and fix thread/memory leak issues",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T10:53:05Z",
    "closed_at": "2025-04-08T11:03:18Z",
    "merged_at": "2025-04-08T11:03:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3270"
  },
  {
    "number": 3269,
    "title": "feat: Support speculative decoding with Hopper XQA",
    "user": "lowsfer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T10:31:17Z",
    "closed_at": "2025-04-07T09:14:35Z",
    "merged_at": "2025-04-07T09:14:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3269"
  },
  {
    "number": 3268,
    "title": "infra: Fix bot check error when triggered by pull request",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T10:06:39Z",
    "closed_at": "2025-04-03T13:47:05Z",
    "merged_at": "2025-04-03T13:47:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3268"
  },
  {
    "number": 3267,
    "title": "infra: Support backend type filter",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T10:03:17Z",
    "closed_at": "2025-04-16T02:16:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3267"
  },
  {
    "number": 3266,
    "title": "feat: Add tool_call support with json_schema for LLM",
    "user": "sarattha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T09:25:01Z",
    "closed_at": "2025-04-09T04:06:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3266"
  },
  {
    "number": 3265,
    "title": "Add support tool_call with json_schema object via OpenAI API",
    "user": "sarattha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T09:09:17Z",
    "closed_at": "2025-04-03T09:16:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3265"
  },
  {
    "number": 3264,
    "title": "[Infra][TRTLLM-3929] Rerun failure tests",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T08:13:50Z",
    "closed_at": "2025-05-27T08:13:24Z",
    "merged_at": "2025-05-27T08:13:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3264"
  },
  {
    "number": 3262,
    "title": "doc: Add serving section for DS V3 document",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T07:52:23Z",
    "closed_at": "2025-04-03T13:57:48Z",
    "merged_at": "2025-04-03T13:57:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3262"
  },
  {
    "number": 3261,
    "title": "fix:  Fix minor issues in test_autotuner.py",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T07:29:50Z",
    "closed_at": "2025-04-03T10:24:09Z",
    "merged_at": "2025-04-03T10:24:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3261"
  },
  {
    "number": 3260,
    "title": "test: Accuracy test improvement (Part 3.3): Move DeepSeek tests",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T06:51:03Z",
    "closed_at": "2025-04-07T23:19:05Z",
    "merged_at": "2025-04-07T23:19:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3260"
  },
  {
    "number": 3258,
    "title": "feat: [NVBUG 5200562] AWQ support Modelopt ckpts.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T06:04:33Z",
    "closed_at": "2025-04-04T00:10:36Z",
    "merged_at": "2025-04-04T00:10:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3258"
  },
  {
    "number": 3257,
    "title": "feat: Introduce UB allocator for pytorch flow",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T05:35:12Z",
    "closed_at": "2025-04-08T10:39:50Z",
    "merged_at": "2025-04-08T10:39:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3257"
  },
  {
    "number": 3256,
    "title": "test: remove tests from qa test lists",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T04:46:05Z",
    "closed_at": "2025-04-03T08:06:40Z",
    "merged_at": "2025-04-03T08:06:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3256"
  },
  {
    "number": 3255,
    "title": "test: Waive deepseek test on tp4/ep4 + mtp_nextn=2 + attention_dp",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T03:44:26Z",
    "closed_at": "2025-04-03T04:35:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3255"
  },
  {
    "number": 3254,
    "title": "Fix: Fix warning for fused_moe.py",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T03:31:52Z",
    "closed_at": "2025-04-03T05:30:23Z",
    "merged_at": "2025-04-03T05:30:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3254"
  },
  {
    "number": 3252,
    "title": "chore:update modelopt to 0.27",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T02:43:16Z",
    "closed_at": "2025-04-10T15:48:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3252"
  },
  {
    "number": 3251,
    "title": "perf: Enable post MLP/MOE fusion for non-boundary PP layer",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T02:37:34Z",
    "closed_at": "2025-08-14T01:21:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3251"
  },
  {
    "number": 3250,
    "title": "feat: add trtllm stress test",
    "user": "dominicshanshan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T02:36:31Z",
    "closed_at": "2025-04-13T02:24:25Z",
    "merged_at": "2025-04-13T02:24:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3250"
  },
  {
    "number": 3249,
    "title": "set test timeout threshold to 5400 second",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-03T01:56:20Z",
    "closed_at": "2025-04-03T02:14:00Z",
    "merged_at": "2025-04-03T02:14:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3249"
  },
  {
    "number": 3248,
    "title": "feat: Add FP8 support for SM 120",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T21:05:34Z",
    "closed_at": "2025-04-14T23:05:42Z",
    "merged_at": "2025-04-14T23:05:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3248"
  },
  {
    "number": 3247,
    "title": "feat: Support gemma-3-1b-it",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T18:30:33Z",
    "closed_at": "2025-04-10T04:34:59Z",
    "merged_at": "2025-04-10T04:34:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3247"
  },
  {
    "number": 3246,
    "title": "chore: Reenabling test_llm get_stats_async test, which was fixed by another change to test_llm.py",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T16:44:46Z",
    "closed_at": "2025-04-03T03:57:32Z",
    "merged_at": "2025-04-03T03:57:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3246"
  },
  {
    "number": 3245,
    "title": "feat: Enable serialization and deserialization of AutoTuner cache on disk.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T16:35:22Z",
    "closed_at": "2025-05-20T10:46:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3245"
  },
  {
    "number": 3244,
    "title": "chore: Raise error for PP + MTP",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T15:57:22Z",
    "closed_at": "2025-04-02T20:45:31Z",
    "merged_at": "2025-04-02T20:45:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3244"
  },
  {
    "number": 3243,
    "title": "feat: Add option to run disaggregated serving without ctx servers,…",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T15:14:56Z",
    "closed_at": "2025-04-08T01:56:04Z",
    "merged_at": "2025-04-08T01:56:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3243"
  },
  {
    "number": 3242,
    "title": "doc: Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T14:01:55Z",
    "closed_at": "2025-04-02T14:12:52Z",
    "merged_at": "2025-04-02T14:12:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3242"
  },
  {
    "number": 3241,
    "title": "fix: fix return double first token",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T13:51:49Z",
    "closed_at": "2025-04-10T04:33:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3241"
  },
  {
    "number": 3240,
    "title": "fix: fix the acceptance rate of pytorch workflow in trtllm-bench",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T13:16:07Z",
    "closed_at": "2025-04-03T07:12:25Z",
    "merged_at": "2025-04-03T07:12:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3240"
  },
  {
    "number": 3239,
    "title": "feat: use NVRTC for DeepGEMM JIT compilation",
    "user": "lucifer1004",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T11:08:05Z",
    "closed_at": "2025-04-07T12:29:24Z",
    "merged_at": "2025-04-07T12:29:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3239"
  },
  {
    "number": 3238,
    "title": "Fix torch nvsmall through pyexecutor and fix its TP support",
    "user": "amitz-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T11:01:16Z",
    "closed_at": "2025-04-07T08:53:18Z",
    "merged_at": "2025-04-07T08:53:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3238"
  },
  {
    "number": 3234,
    "title": "refactor: collect executor and decoder states into dataclass",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T09:16:11Z",
    "closed_at": "2025-04-15T08:31:46Z",
    "merged_at": "2025-04-15T08:31:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3234"
  },
  {
    "number": 3232,
    "title": "doc: Add DeepSeek-R1 perf doc",
    "user": "Kefeng-Duan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T08:41:23Z",
    "closed_at": "2025-04-10T09:00:50Z",
    "merged_at": "2025-04-10T09:00:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3232"
  },
  {
    "number": 3231,
    "title": "TensorRT-LLM v0.18 release",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T08:18:03Z",
    "closed_at": "2025-04-02T09:01:16Z",
    "merged_at": "2025-04-02T09:01:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3231"
  },
  {
    "number": 3230,
    "title": "add best perf practice on DSR1",
    "user": "Kefeng-Duan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T08:09:19Z",
    "closed_at": "2025-04-02T08:42:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3230"
  },
  {
    "number": 3229,
    "title": "infra: [TRTLLM-4370] Fix the build error when build GH200 image",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T07:52:17Z",
    "closed_at": "2025-04-03T09:33:51Z",
    "merged_at": "2025-04-03T09:33:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3229"
  },
  {
    "number": 3228,
    "title": "fix: remove test relies on timing",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T07:48:37Z",
    "closed_at": "2025-04-02T10:38:38Z",
    "merged_at": "2025-04-02T10:38:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3228"
  },
  {
    "number": 3227,
    "title": "Draft: Only for test",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T07:32:15Z",
    "closed_at": "2025-04-03T07:28:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3227"
  },
  {
    "number": 3226,
    "title": "fix: runtime error in test_deepseek_allreduce.py",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T07:30:17Z",
    "closed_at": "2025-04-08T01:19:48Z",
    "merged_at": "2025-04-08T01:19:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3226"
  },
  {
    "number": 3225,
    "title": "fix deepseek failure with pipeline parallelism",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T07:29:33Z",
    "closed_at": "2025-04-02T14:56:39Z",
    "merged_at": "2025-04-02T14:56:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3225"
  },
  {
    "number": 3224,
    "title": "doc: add a directory for scaffolding contributors",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T07:23:14Z",
    "closed_at": "2025-04-02T08:08:01Z",
    "merged_at": "2025-04-02T08:08:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3224"
  },
  {
    "number": 3222,
    "title": "chore: Remove gen_cuda_headers_for_xqa.py",
    "user": "ming-wei",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T07:11:59Z",
    "closed_at": "2025-04-02T23:13:22Z",
    "merged_at": "2025-04-02T23:13:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3222"
  },
  {
    "number": 3221,
    "title": "feat: Return logits in PyTorch flow",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T06:44:15Z",
    "closed_at": "2025-04-24T23:56:04Z",
    "merged_at": "2025-04-24T23:56:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3221"
  },
  {
    "number": 3220,
    "title": "fix: GPT-Next convert failure",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T06:01:43Z",
    "closed_at": "2025-04-02T09:14:40Z",
    "merged_at": "2025-04-02T09:14:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3220"
  },
  {
    "number": 3219,
    "title": "test: Accuracy test improvement (Part 3.2): Move Qwen tests (NvBug 5135332)",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T05:40:46Z",
    "closed_at": "2025-04-02T09:29:58Z",
    "merged_at": "2025-04-02T09:29:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3219"
  },
  {
    "number": 3218,
    "title": "chore: Add Vanilla MLA.",
    "user": "Tracin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T03:08:31Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3218"
  },
  {
    "number": 3217,
    "title": "fix: waive L0 test",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T02:57:23Z",
    "closed_at": "2025-04-02T03:16:23Z",
    "merged_at": "2025-04-02T03:16:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3217"
  },
  {
    "number": 3216,
    "title": "bump the base docker image from 25.01 to 25.03",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T02:56:56Z",
    "closed_at": "2025-04-03T01:01:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3216"
  },
  {
    "number": 3215,
    "title": "doc: refine integration test guide",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T02:34:19Z",
    "closed_at": "2025-04-03T07:36:14Z",
    "merged_at": "2025-04-03T07:36:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3215"
  },
  {
    "number": 3214,
    "title": "feat: support abort disconnected requests",
    "user": "pansicheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T01:48:48Z",
    "closed_at": "2025-04-07T08:14:58Z",
    "merged_at": "2025-04-07T08:14:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3214"
  },
  {
    "number": 3213,
    "title": "chore: refine fetch new requests method",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T00:20:26Z",
    "closed_at": "2025-04-02T02:46:02Z",
    "merged_at": "2025-04-02T02:46:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3213"
  },
  {
    "number": 3212,
    "title": "perf: Add total token throughput metric.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-02T00:06:37Z",
    "closed_at": "2025-04-05T05:18:00Z",
    "merged_at": "2025-04-05T05:18:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3212"
  },
  {
    "number": 3211,
    "title": "CI: Reduce test cases for deepseek",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T23:58:42Z",
    "closed_at": "2025-04-02T05:57:56Z",
    "merged_at": "2025-04-02T05:57:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3211"
  },
  {
    "number": 3210,
    "title": "chore: Remove build config from Pytorch kwargs.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T23:19:14Z",
    "closed_at": "2025-04-03T07:00:30Z",
    "merged_at": "2025-04-03T07:00:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3210"
  },
  {
    "number": 3209,
    "title": "feature: KV Cache GPUDirect Storage",
    "user": "arthurrasmusson",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T22:04:38Z",
    "closed_at": "2025-05-28T23:27:43Z",
    "merged_at": "2025-05-28T23:27:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3209"
  },
  {
    "number": 3208,
    "title": "Upgrade cmake minimum from 3.18 to 3.27",
    "user": "WilliamTambellini",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T16:24:10Z",
    "closed_at": "2025-04-02T07:14:37Z",
    "merged_at": "2025-04-02T07:14:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3208"
  },
  {
    "number": 3206,
    "title": "[TRTLLM-4460] test: Use Llama 3.2 1B for Llama C++ tests",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T15:25:31Z",
    "closed_at": "2025-04-30T21:31:09Z",
    "merged_at": "2025-04-30T21:31:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3206"
  },
  {
    "number": 3205,
    "title": "[BREAKING CHANGE][TRTLLM-4354] chore: Add output of first token to additional generation outputs",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T14:50:04Z",
    "closed_at": "2025-04-02T12:14:16Z",
    "merged_at": "2025-04-02T12:14:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3205"
  },
  {
    "number": 3203,
    "title": "test: enable some disagg test",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T13:04:53Z",
    "closed_at": "2025-04-02T22:10:50Z",
    "merged_at": "2025-04-02T22:10:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3203"
  },
  {
    "number": 3202,
    "title": "chore: Cursor ignore cubin in headers",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T12:48:44Z",
    "closed_at": "2025-04-01T15:42:19Z",
    "merged_at": "2025-04-01T15:42:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3202"
  },
  {
    "number": 3201,
    "title": "test: feat/lora_modules_tests",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T11:39:17Z",
    "closed_at": "2025-04-09T15:06:52Z",
    "merged_at": "2025-04-09T15:06:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3201"
  },
  {
    "number": 3200,
    "title": "lora_manager",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T10:42:24Z",
    "closed_at": "2025-04-01T10:42:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3200"
  },
  {
    "number": 3199,
    "title": "refactor: replace OpenAIServer's tokenizer with llm.tokenizer",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T10:17:45Z",
    "closed_at": "2025-04-08T06:55:02Z",
    "merged_at": "2025-04-08T06:55:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3199"
  },
  {
    "number": 3198,
    "title": "chore: refine broadcast new requests method",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T09:43:25Z",
    "closed_at": "2025-04-02T00:05:21Z",
    "merged_at": "2025-04-02T00:05:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3198"
  },
  {
    "number": 3197,
    "title": "doc: add a directory for scaffolding contributors",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T09:35:15Z",
    "closed_at": "2025-04-02T07:25:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3197"
  },
  {
    "number": 3196,
    "title": "[Do not merge] Test",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T09:33:05Z",
    "closed_at": "2025-04-01T12:37:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3196"
  },
  {
    "number": 3195,
    "title": "Reapply \"refactor: Replace DecoderFinishedEvent with CudaEvent in decoder clas…\" (#3183)",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T09:23:02Z",
    "closed_at": "2025-04-04T13:56:28Z",
    "merged_at": "2025-04-04T13:56:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3195"
  },
  {
    "number": 3194,
    "title": "doc: add doc about development on cloud or runpod",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T08:16:31Z",
    "closed_at": "2025-04-02T10:10:56Z",
    "merged_at": "2025-04-02T10:10:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3194"
  },
  {
    "number": 3193,
    "title": "update user list",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T07:45:10Z",
    "closed_at": "2025-04-01T08:41:16Z",
    "merged_at": "2025-04-01T08:41:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3193"
  },
  {
    "number": 3192,
    "title": "infra: [TRTLLM-4308] Add Bot help",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T07:23:42Z",
    "closed_at": "2025-04-03T09:48:25Z",
    "merged_at": "2025-04-03T09:48:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3192"
  },
  {
    "number": 3191,
    "title": "Add bot command help and check bot command",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T07:13:22Z",
    "closed_at": "2025-04-01T07:22:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3191"
  },
  {
    "number": 3190,
    "title": "feat: Add support for FP8 MLA on Hopper and Blackwell.",
    "user": "bobboli",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T06:44:22Z",
    "closed_at": "2025-04-07T07:14:13Z",
    "merged_at": "2025-04-07T07:14:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3190"
  },
  {
    "number": 3189,
    "title": "tests: change qa perf test to trtllm-bench",
    "user": "ruodil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T06:33:54Z",
    "closed_at": "2025-04-17T01:53:33Z",
    "merged_at": "2025-04-17T01:53:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3189"
  },
  {
    "number": 3188,
    "title": "zip zongfei 6 commits",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T06:20:53Z",
    "closed_at": "2025-04-01T06:57:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3188"
  },
  {
    "number": 3187,
    "title": "doc: refactor trtllm-serve examples and doc",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T06:10:18Z",
    "closed_at": "2025-04-04T03:40:43Z",
    "merged_at": "2025-04-04T03:40:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3187"
  },
  {
    "number": 3186,
    "title": "feat: Support PeftCacheManager in Torch",
    "user": "shaharmor98",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T05:12:17Z",
    "closed_at": "2025-04-04T04:38:08Z",
    "merged_at": "2025-04-04T04:38:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3186"
  },
  {
    "number": 3185,
    "title": "[draft] split prefill and decode",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T03:27:58Z",
    "closed_at": "2025-05-01T17:43:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3185"
  },
  {
    "number": 3184,
    "title": "fix: fix bug of nvbug 5196515 which is related to path of glm-4-9b in ci",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T03:25:26Z",
    "closed_at": "2025-04-01T08:58:42Z",
    "merged_at": "2025-04-01T08:58:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3184"
  },
  {
    "number": 3183,
    "title": "Revert \"refactor: Replace DecoderFinishedEvent with CudaEvent in deco…",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T02:15:30Z",
    "closed_at": "2025-04-01T04:49:27Z",
    "merged_at": "2025-04-01T04:49:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3183"
  },
  {
    "number": 3182,
    "title": "test: waive PP tests in LLM-API",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T02:09:19Z",
    "closed_at": "2025-04-01T02:25:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3182"
  },
  {
    "number": 3181,
    "title": "Revert \"refactor: Replace DecoderFinishedEvent with CudaEvent in decoder classes\"",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T01:58:01Z",
    "closed_at": "2025-04-01T02:16:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3181"
  },
  {
    "number": 3180,
    "title": "infra: add sync fork action",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T01:50:43Z",
    "closed_at": "2025-04-01T09:57:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3180"
  },
  {
    "number": 3179,
    "title": "doc: add supported-models on PyTorch example",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-01T00:54:30Z",
    "closed_at": "2025-04-03T13:09:26Z",
    "merged_at": "2025-04-03T13:09:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3179"
  },
  {
    "number": 3177,
    "title": "Draft: Changes for Phi4MM",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T23:06:03Z",
    "closed_at": "2025-04-05T02:25:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3177"
  },
  {
    "number": 3176,
    "title": "feat: Support CUDA graphs for EAGLE3",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T22:51:19Z",
    "closed_at": "2025-04-16T20:53:50Z",
    "merged_at": "2025-04-16T20:53:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3176"
  },
  {
    "number": 3171,
    "title": "chore: bump version to 0.19.0.dev2025040800",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T14:02:09Z",
    "closed_at": "2025-04-02T00:21:56Z",
    "merged_at": "2025-04-02T00:21:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3171"
  },
  {
    "number": 3167,
    "title": "test: Accuracy test improvement (Part 3.1): Extend accuracy test suite with LLM API and initial implementation of `trtllm-eval`",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T09:55:48Z",
    "closed_at": "2025-04-01T14:20:29Z",
    "merged_at": "2025-04-01T14:20:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3167"
  },
  {
    "number": 3166,
    "title": "feat: refactor scaffolding worker and support openai api worker",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T09:26:22Z",
    "closed_at": "2025-04-01T10:31:52Z",
    "merged_at": "2025-04-01T10:31:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3166"
  },
  {
    "number": 3165,
    "title": "chore: cutlass cleanup",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T07:11:21Z",
    "closed_at": "2025-04-01T05:57:38Z",
    "merged_at": "2025-04-01T05:57:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3165"
  },
  {
    "number": 3164,
    "title": "infra: add sync fork action workflow",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T06:30:29Z",
    "closed_at": "2025-04-01T01:51:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3164"
  },
  {
    "number": 3163,
    "title": "[DRAFT]Fp8 block gemm open source",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T06:14:42Z",
    "closed_at": "2025-04-02T02:54:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3163"
  },
  {
    "number": 3162,
    "title": "doc: update main readme",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T05:38:31Z",
    "closed_at": "2025-04-01T02:37:07Z",
    "merged_at": "2025-04-01T02:37:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3162"
  },
  {
    "number": 3161,
    "title": "fix: conditional disagg test name",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T03:10:13Z",
    "closed_at": "2025-04-02T07:34:31Z",
    "merged_at": "2025-04-02T07:34:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3161"
  },
  {
    "number": 3159,
    "title": "infra: Switch to urm.nvidia.com as a WAR for urm-rn.nvidia.com connection issue",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T02:45:58Z",
    "closed_at": "2025-03-31T05:05:29Z",
    "merged_at": "2025-03-31T05:05:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3159"
  },
  {
    "number": 3157,
    "title": "Chore: waive tests and fix multi-GPU tests",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T02:01:38Z",
    "closed_at": "2025-03-31T08:05:45Z",
    "merged_at": "2025-03-31T08:05:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3157"
  },
  {
    "number": 3156,
    "title": "feat: Add Qwen2.5-VL and refactor Qwen2-VL",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-31T01:47:54Z",
    "closed_at": "2025-04-09T20:09:04Z",
    "merged_at": "2025-04-09T20:09:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3156"
  },
  {
    "number": 3155,
    "title": "fix: disable cuda graph for overlap tests",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-30T22:53:18Z",
    "closed_at": "2025-03-31T18:35:36Z",
    "merged_at": "2025-03-31T18:35:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3155"
  },
  {
    "number": 3154,
    "title": "chore: Refactor test_disaggregated.py",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-30T21:07:14Z",
    "closed_at": "2025-04-18T03:04:07Z",
    "merged_at": "2025-04-18T03:04:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3154"
  },
  {
    "number": 3153,
    "title": "doc: use alert formatting",
    "user": "musvaage",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-30T20:04:56Z",
    "closed_at": "2025-03-30T23:30:52Z",
    "merged_at": "2025-03-30T23:30:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3153"
  },
  {
    "number": 3152,
    "title": "chore: bump version to 0.19.0.dev2025040100",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-30T17:00:37Z",
    "closed_at": "2025-03-31T08:36:06Z",
    "merged_at": "2025-03-31T08:36:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3152"
  },
  {
    "number": 3151,
    "title": "feat: Apply the new torch-flow compatible AutoTuner to both Fused MoE and NVFP4 Linear operators.",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-30T13:27:08Z",
    "closed_at": "2025-04-08T06:28:36Z",
    "merged_at": "2025-04-08T06:28:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3151"
  },
  {
    "number": 3147,
    "title": "perf: Use pinned H2D to reduce bubbles",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-29T09:01:57Z",
    "closed_at": "2025-04-04T14:23:11Z",
    "merged_at": "2025-04-04T14:23:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3147"
  },
  {
    "number": 3146,
    "title": "fix: Fix an error related to dummy request when MTP is used",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-29T08:30:49Z",
    "closed_at": "2025-04-03T03:08:12Z",
    "merged_at": "2025-04-03T03:08:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3146"
  },
  {
    "number": 3145,
    "title": "feat: LogitsProcessor in PyTorch backend",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-29T07:27:40Z",
    "closed_at": "2025-05-01T21:15:30Z",
    "merged_at": "2025-05-01T21:15:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3145"
  },
  {
    "number": 3144,
    "title": "fix: Update FP8 sf layout for Blackwell and relax blockwise GEMM assertions",
    "user": "chang-l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-29T07:02:52Z",
    "closed_at": "2025-04-01T20:08:30Z",
    "merged_at": "2025-04-01T20:08:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3144"
  },
  {
    "number": 3141,
    "title": "test: support use file name in test list",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-28T12:22:12Z",
    "closed_at": "2025-10-17T07:04:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3141"
  },
  {
    "number": 3140,
    "title": "fix: fix single-node cannot quit issue on slurm",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-28T11:04:01Z",
    "closed_at": "2025-03-31T02:15:27Z",
    "merged_at": "2025-03-31T02:15:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3140"
  },
  {
    "number": 3139,
    "title": "refactor: Expose DecoderState via bindings and integrate in TRTLLMDecoder",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-28T08:05:57Z",
    "closed_at": "2025-04-04T23:42:36Z",
    "merged_at": "2025-04-04T23:42:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3139"
  },
  {
    "number": 3136,
    "title": "chore: Revert PR #3053",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-28T03:51:33Z",
    "closed_at": "2025-04-18T06:02:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3136"
  },
  {
    "number": 3135,
    "title": "doc: add MGMN doc",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-28T02:13:59Z",
    "closed_at": "2025-04-01T06:31:57Z",
    "merged_at": "2025-04-01T06:31:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3135"
  },
  {
    "number": 3134,
    "title": "perf: Optimisations for PP + attention DP",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-28T01:56:54Z",
    "closed_at": "2025-04-01T00:59:17Z",
    "merged_at": "2025-04-01T00:59:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3134"
  },
  {
    "number": 3133,
    "title": "feat: Variable-Beam-Width-Search (VBWS) Part2",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-28T01:38:45Z",
    "closed_at": "2025-04-02T04:31:29Z",
    "merged_at": "2025-04-02T04:31:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3133"
  },
  {
    "number": 3132,
    "title": "chore: cleanup py_executor code",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-27T22:51:00Z",
    "closed_at": "2025-04-01T01:27:05Z",
    "merged_at": "2025-04-01T01:27:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3132"
  },
  {
    "number": 3129,
    "title": "infra: add pre-commit check to github actions",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-27T17:54:48Z",
    "closed_at": "2025-04-10T22:40:53Z",
    "merged_at": "2025-04-10T22:40:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3129"
  },
  {
    "number": 3128,
    "title": "feat: Cohere2ForCausalLM support (Command-A, Command-R7B)",
    "user": "aikitoria",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-27T15:43:00Z",
    "closed_at": "2025-06-05T19:11:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3128"
  },
  {
    "number": 3127,
    "title": "feat: FP8 Rowwise quantization support for Cohere models",
    "user": "aikitoria",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-27T15:42:58Z",
    "closed_at": "2025-06-05T20:08:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3127"
  },
  {
    "number": 3126,
    "title": "[draft]chore: update libcutlass library with FP4 quantize linear layout change",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-27T14:34:10Z",
    "closed_at": "2025-04-01T05:31:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3126"
  },
  {
    "number": 3120,
    "title": "infra: update concurrency control",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-27T09:41:12Z",
    "closed_at": "2025-03-30T15:28:51Z",
    "merged_at": "2025-03-30T15:28:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3120"
  },
  {
    "number": 3119,
    "title": "fix: fix hang in mgmn with trtllm-llmapi-launch command",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-27T07:59:12Z",
    "closed_at": "2025-03-27T10:45:43Z",
    "merged_at": "2025-03-27T10:45:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3119"
  },
  {
    "number": 3117,
    "title": "chore: Stabilize ABI boundary for internal kernel library",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-27T04:50:59Z",
    "closed_at": "2025-04-11T07:07:51Z",
    "merged_at": "2025-04-11T07:07:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3117"
  },
  {
    "number": 3116,
    "title": "fix: Reverse CUDA graph size order",
    "user": "jiahanc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-27T01:43:48Z",
    "closed_at": "2025-04-01T03:28:36Z",
    "merged_at": "2025-04-01T03:28:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3116"
  },
  {
    "number": 3115,
    "title": "refactor:[AutoDeploy] Enhance RoPE support",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T21:11:07Z",
    "closed_at": "2025-04-11T15:51:24Z",
    "merged_at": "2025-04-11T15:51:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3115"
  },
  {
    "number": 3114,
    "title": "chore: add sqlite to rocky container",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T19:52:24Z",
    "closed_at": "2025-04-10T05:30:24Z",
    "merged_at": "2025-04-10T05:30:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3114"
  },
  {
    "number": 3113,
    "title": "fix: Early exit cmake if find_library() does not find any lib",
    "user": "WilliamTambellini",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T18:55:49Z",
    "closed_at": "2025-03-29T11:59:04Z",
    "merged_at": "2025-03-29T11:59:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3113"
  },
  {
    "number": 3110,
    "title": "citest",
    "user": "tensorrt-cicd",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T16:43:32Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3110"
  },
  {
    "number": 3107,
    "title": "infra: [TRTLLM-4308] Add Bot help",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T13:04:30Z",
    "closed_at": "2025-04-01T07:10:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3107"
  },
  {
    "number": 3106,
    "title": " fix: Fix C++ decoder synchronization in PyTorch",
    "user": "dcampora",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T13:01:41Z",
    "closed_at": "2025-04-23T15:55:28Z",
    "merged_at": "2025-04-23T15:55:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3106"
  },
  {
    "number": 3104,
    "title": "feat: Optionally split MoE inputs into chunks to reduce GPU memory usage",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T12:05:01Z",
    "closed_at": "2025-04-01T08:07:03Z",
    "merged_at": "2025-04-01T08:07:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3104"
  },
  {
    "number": 3103,
    "title": "refactor: Simplify disableLookahead and improve numDecodingEngineTokens handling",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T11:18:04Z",
    "closed_at": "2025-04-01T10:47:32Z",
    "merged_at": "2025-04-01T10:47:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3103"
  },
  {
    "number": 3102,
    "title": "breaking change: perf: Make ipc_periodically the default responses_handler",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T10:19:36Z",
    "closed_at": "2025-04-08T02:36:40Z",
    "merged_at": "2025-04-08T02:36:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3102"
  },
  {
    "number": 3101,
    "title": "chore: Ucx ip port remove mpi depend",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T10:15:02Z",
    "closed_at": "2025-04-02T01:42:29Z",
    "merged_at": "2025-04-02T01:42:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3101"
  },
  {
    "number": 3100,
    "title": "CI: Waive for https://nvbugspro.nvidia.com/bug/5189673",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T09:47:09Z",
    "closed_at": "2025-03-26T11:13:44Z",
    "merged_at": "2025-03-26T11:13:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3100"
  },
  {
    "number": 3098,
    "title": "infra: Support get file change for github PR",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T08:47:32Z",
    "closed_at": "2025-04-02T02:35:34Z",
    "merged_at": "2025-04-02T02:35:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3098"
  },
  {
    "number": 3097,
    "title": "infra: Add test list name check",
    "user": "EmmaQiaoCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T08:17:07Z",
    "closed_at": "2025-04-20T15:02:17Z",
    "merged_at": "2025-04-20T15:02:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3097"
  },
  {
    "number": 3096,
    "title": "feat: add support for MTP+cuda_graph_padding",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T07:57:54Z",
    "closed_at": "2025-03-27T08:06:14Z",
    "merged_at": "2025-03-27T08:06:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3096"
  },
  {
    "number": 3095,
    "title": "bug: Fix hang bug when context server doesn't have enough capacity for KV Cache",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T07:49:28Z",
    "closed_at": "2025-04-21T07:16:55Z",
    "merged_at": "2025-04-21T07:16:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3095"
  },
  {
    "number": 3094,
    "title": "test: tests[CI]: remove closed bugs",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T07:29:27Z",
    "closed_at": "2025-03-31T03:42:46Z",
    "merged_at": "2025-03-31T03:42:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3094"
  },
  {
    "number": 3093,
    "title": "perf: Add optimizations for deepseek in min latency mode",
    "user": "zongfeijing",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T07:15:35Z",
    "closed_at": "2025-04-02T01:05:24Z",
    "merged_at": "2025-04-02T01:05:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3093"
  },
  {
    "number": 3092,
    "title": "feat: Run PyExecutor's inference flow to estimate max_num_tokens for kv_cache_manager",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T06:50:17Z",
    "closed_at": "2025-04-10T10:29:41Z",
    "merged_at": "2025-04-10T10:29:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3092"
  },
  {
    "number": 3091,
    "title": "chore: Blossom debug hook",
    "user": "yiqingy0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T06:47:30Z",
    "closed_at": "2025-04-07T07:47:48Z",
    "merged_at": "2025-04-07T07:47:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3091"
  },
  {
    "number": 3090,
    "title": "test: fix QA TRT integration testlist mismatch issue",
    "user": "kxdc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T04:51:21Z",
    "closed_at": "2025-03-26T06:03:21Z",
    "merged_at": "2025-03-26T06:03:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3090"
  },
  {
    "number": 3089,
    "title": "teats[CI]: demo",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T02:54:40Z",
    "closed_at": "2025-03-26T05:30:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3089"
  },
  {
    "number": 3088,
    "title": "teats[CI]: demo",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T02:46:03Z",
    "closed_at": "2025-03-26T02:52:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3088"
  },
  {
    "number": 3087,
    "title": "fix: dist-serving streaming mode returns http 500",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T02:36:28Z",
    "closed_at": "2025-04-02T02:08:08Z",
    "merged_at": "2025-04-02T02:08:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3087"
  },
  {
    "number": 3086,
    "title": "teats[CI]: demo",
    "user": "xinhe-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T02:34:12Z",
    "closed_at": "2025-03-26T02:45:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3086"
  },
  {
    "number": 3085,
    "title": "feat: no-cache attention in PyTorch workflow",
    "user": "qixiang-99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T02:28:05Z",
    "closed_at": "2025-04-04T17:54:33Z",
    "merged_at": "2025-04-04T17:54:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3085"
  },
  {
    "number": 3084,
    "title": "feat: improve scaffolding shutdown process",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T02:04:43Z",
    "closed_at": "2025-03-31T12:39:21Z",
    "merged_at": "2025-03-31T12:39:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3084"
  },
  {
    "number": 3083,
    "title": "chore: move some models to examples/models/contrib/ and clean some ci of qa test",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T01:53:27Z",
    "closed_at": "2025-03-31T06:30:42Z",
    "merged_at": "2025-03-31T06:30:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3083"
  },
  {
    "number": 3082,
    "title": "feat: Variable-Beam-Width-Search (VBWS) part1",
    "user": "wili-65535",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-26T01:03:35Z",
    "closed_at": "2025-03-26T15:31:29Z",
    "merged_at": "2025-03-26T15:31:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3082"
  },
  {
    "number": 3081,
    "title": "chore: move nvrtc_wrapper to conan",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T22:23:02Z",
    "closed_at": "2025-04-03T22:27:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3081"
  },
  {
    "number": 3080,
    "title": "chore: Handle qwen2audio inputs ids expansion during processing",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T21:47:42Z",
    "closed_at": "2025-03-26T07:00:28Z",
    "merged_at": "2025-03-26T07:00:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3080"
  },
  {
    "number": 3079,
    "title": "feat:[AutoDeploy] Add support for Phi3/4 Model Family",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T21:47:20Z",
    "closed_at": "2025-05-22T18:58:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3079"
  },
  {
    "number": 3078,
    "title": "refactor: Replace DecoderFinishedEvent with CudaEvent in decoder classes",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T21:29:52Z",
    "closed_at": "2025-03-28T06:50:53Z",
    "merged_at": "2025-03-28T06:50:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3078"
  },
  {
    "number": 3077,
    "title": "refactor: Improve decoder finalize function",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T21:19:49Z",
    "closed_at": "2025-03-28T06:34:00Z",
    "merged_at": "2025-03-28T06:34:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3077"
  },
  {
    "number": 3076,
    "title": "refactor: simplify forward methods in GptDecoderBatched",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T21:13:14Z",
    "closed_at": "2025-03-26T12:45:06Z",
    "merged_at": "2025-03-26T12:45:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3076"
  },
  {
    "number": 3075,
    "title": "infra: Devcontainer productivity improvements",
    "user": "lucaslie",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T20:10:20Z",
    "closed_at": "2025-04-02T18:23:38Z",
    "merged_at": "2025-04-02T18:23:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3075"
  },
  {
    "number": 3074,
    "title": "2025-03-25 update CI allowlist",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T18:11:15Z",
    "closed_at": "2025-03-26T00:13:01Z",
    "merged_at": "2025-03-26T00:13:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3074"
  },
  {
    "number": 3073,
    "title": "chore: Refactor disaggregated serving scripts",
    "user": "pcastonguay",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T17:34:21Z",
    "closed_at": "2025-04-03T18:55:05Z",
    "merged_at": "2025-04-03T18:55:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3073"
  },
  {
    "number": 3072,
    "title": "fix: [AutoDeploy] Update README.md",
    "user": "Fridah-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T16:32:39Z",
    "closed_at": "2025-04-01T23:16:37Z",
    "merged_at": "2025-04-01T23:16:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3072"
  },
  {
    "number": 3071,
    "title": "feat: Open source fp8_blockscale_gemm",
    "user": "lucifer1004",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T16:18:13Z",
    "closed_at": "2025-04-02T04:12:52Z",
    "merged_at": "2025-04-02T04:12:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3071"
  },
  {
    "number": 3070,
    "title": "feat: Add BW measurement",
    "user": "BatshevaBlack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T16:07:03Z",
    "closed_at": "2025-03-28T02:53:00Z",
    "merged_at": "2025-03-28T02:53:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3070"
  },
  {
    "number": 3069,
    "title": "test: [TRTLLM-4334] Create 1.0 criteria scope from API stability references",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T14:02:41Z",
    "closed_at": "2025-03-26T10:14:38Z",
    "merged_at": "2025-03-26T10:14:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3069"
  },
  {
    "number": 3068,
    "title": "feat: Add LoRA support for gemma",
    "user": "amirkl94",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T13:29:21Z",
    "closed_at": "2025-04-01T11:15:55Z",
    "merged_at": "2025-04-01T11:15:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3068"
  },
  {
    "number": 3067,
    "title": "Fix: fuse message not aligned on different processes",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T13:06:27Z",
    "closed_at": "2025-03-26T09:15:28Z",
    "merged_at": "2025-03-26T09:15:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3067"
  },
  {
    "number": 3066,
    "title": "fix: Waive twoshot to fix acc issue",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T12:19:49Z",
    "closed_at": "2025-03-27T13:38:53Z",
    "merged_at": "2025-03-27T13:38:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3066"
  },
  {
    "number": 3065,
    "title": "feat: Adding UCX support for cacheTransceiver",
    "user": "RoeyAzran1992",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T11:57:06Z",
    "closed_at": "2025-04-10T04:39:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3065"
  },
  {
    "number": 3064,
    "title": "feat: allreduce and fusion kernel development",
    "user": "yilin-void",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T10:14:18Z",
    "closed_at": "2025-04-08T11:33:53Z",
    "merged_at": "2025-04-08T11:33:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3064"
  },
  {
    "number": 3063,
    "title": "test: Add gpqa tests for DeepSeek models",
    "user": "lfr-0531",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T10:02:19Z",
    "closed_at": "2025-03-27T11:47:07Z",
    "merged_at": "2025-03-27T11:47:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3063"
  },
  {
    "number": 3062,
    "title": "test: waive flaky test_kv_cache_event_async_api",
    "user": "tongyuantongyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T10:00:08Z",
    "closed_at": "2025-03-25T10:41:30Z",
    "merged_at": "2025-03-25T10:41:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3062"
  },
  {
    "number": 3061,
    "title": "test: Move gptj feature tests to LLaMA models",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T08:44:58Z",
    "closed_at": "2025-03-25T13:55:09Z",
    "merged_at": "2025-03-25T13:55:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3061"
  },
  {
    "number": 3060,
    "title": "doc: document running CI stage locally",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T07:51:20Z",
    "closed_at": "2025-03-25T08:18:18Z",
    "merged_at": "2025-03-25T08:18:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3060"
  },
  {
    "number": 3057,
    "title": "test: remove test_llm_gptj_fp8_manage_weights test case",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T07:28:29Z",
    "closed_at": "2025-03-25T07:41:27Z",
    "merged_at": "2025-03-25T07:41:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3057"
  },
  {
    "number": 3056,
    "title": "fix: amend the test list",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T05:29:43Z",
    "closed_at": "2025-03-25T06:17:37Z",
    "merged_at": "2025-03-25T06:17:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3056"
  },
  {
    "number": 3055,
    "title": "test: add random image test for llama-3.2-11b-vision",
    "user": "crazydemo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T05:15:44Z",
    "closed_at": "2025-03-26T07:38:16Z",
    "merged_at": "2025-03-26T07:38:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3055"
  },
  {
    "number": 3054,
    "title": "feat: Add EXAONE-Deep",
    "user": "yechank-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T04:04:50Z",
    "closed_at": "2025-03-26T06:24:05Z",
    "merged_at": "2025-03-26T06:24:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3054"
  },
  {
    "number": 3053,
    "title": "fix: Set correct draft_token_nums to dummy requests for torch compilation with MTP",
    "user": "HuiGao-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T03:31:11Z",
    "closed_at": "2025-03-26T03:32:58Z",
    "merged_at": "2025-03-26T03:32:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3053"
  },
  {
    "number": 3052,
    "title": "doc: Update DeepSeekV3 doc",
    "user": "xiaoweiw-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T03:30:24Z",
    "closed_at": "2025-03-25T10:17:26Z",
    "merged_at": "2025-03-25T10:17:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3052"
  },
  {
    "number": 3051,
    "title": "chore: upgrade transformers to 4.50.0",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T03:29:55Z",
    "closed_at": "2025-04-14T16:10:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3051"
  },
  {
    "number": 3050,
    "title": "chore: fix bug of test_phi",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T03:29:06Z",
    "closed_at": "2025-03-25T05:12:06Z",
    "merged_at": "2025-03-25T05:12:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3050"
  },
  {
    "number": 3049,
    "title": "fix: AllReduce CUDA Graph Fix + Kernel Clean up",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T02:42:08Z",
    "closed_at": "2025-08-05T07:01:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3049"
  },
  {
    "number": 3048,
    "title": "doc: Add README.md for scaffolding",
    "user": "WeiHaocheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-25T01:51:42Z",
    "closed_at": "2025-03-25T05:58:01Z",
    "merged_at": "2025-03-25T05:58:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3048"
  },
  {
    "number": 3046,
    "title": "feat: Support prequantized fp8 ckpt for nemotron-mini-4b-instruct",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T22:53:41Z",
    "closed_at": "2025-04-01T06:52:10Z",
    "merged_at": "2025-04-01T06:52:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3046"
  },
  {
    "number": 3045,
    "title": "Feat: Support Linear block scale layout in FP4 quantization",
    "user": "yibinl-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T22:40:22Z",
    "closed_at": "2025-04-03T17:13:54Z",
    "merged_at": "2025-04-03T17:13:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3045"
  },
  {
    "number": 3044,
    "title": "feat: Pytorch PP + attention DP support",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T22:20:09Z",
    "closed_at": "2025-03-27T16:11:25Z",
    "merged_at": "2025-03-27T16:11:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3044"
  },
  {
    "number": 3043,
    "title": "chore: Add second possible output for llava",
    "user": "amukkara",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T22:16:48Z",
    "closed_at": "2025-03-25T19:59:27Z",
    "merged_at": "2025-03-25T19:59:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3043"
  },
  {
    "number": 3042,
    "title": "fix: [NVBUG 5087143] Fix vila test",
    "user": "yuanjings-nvda",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T21:38:57Z",
    "closed_at": "2025-04-04T06:30:06Z",
    "merged_at": "2025-04-04T06:30:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3042"
  },
  {
    "number": 3041,
    "title": "perf: [AutoDeploy] Enable AutoDeploy as a backend in trtllm-bench",
    "user": "suyoggupta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T21:37:50Z",
    "closed_at": "2025-03-26T21:33:15Z",
    "merged_at": "2025-03-26T21:33:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3041"
  },
  {
    "number": 3040,
    "title": "chore: Only gather responses on rank 0",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T21:22:45Z",
    "closed_at": "2025-03-25T04:54:51Z",
    "merged_at": "2025-03-25T04:54:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3040"
  },
  {
    "number": 3039,
    "title": "perf: Readd iteration logging for trtllm-bench.",
    "user": "FrankD412",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T19:31:45Z",
    "closed_at": "2025-04-01T00:13:10Z",
    "merged_at": "2025-04-01T00:13:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3039"
  },
  {
    "number": 3038,
    "title": "chore: Fix logits dtype in assert",
    "user": "achartier",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T18:54:51Z",
    "closed_at": "2025-03-25T02:35:21Z",
    "merged_at": "2025-03-25T02:35:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3038"
  },
  {
    "number": 3036,
    "title": "chore: [TRTLLM-3694] Move functional args to llmargs",
    "user": "hchings",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T18:15:23Z",
    "closed_at": "2025-03-28T18:20:18Z",
    "merged_at": "2025-03-28T18:20:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3036"
  },
  {
    "number": 3035,
    "title": "feat: Add initial EAGLE-3 implementation",
    "user": "mikeiovine",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T17:00:13Z",
    "closed_at": "2025-03-29T14:31:25Z",
    "merged_at": "2025-03-29T14:31:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3035"
  },
  {
    "number": 3033,
    "title": "doc: Update CONTRIBUTING.md",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T15:34:17Z",
    "closed_at": "2025-03-25T00:06:24Z",
    "merged_at": "2025-03-25T00:06:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3033"
  },
  {
    "number": 3032,
    "title": "feat: Unify two versions of allreduce custom op",
    "user": "hyukn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T14:43:06Z",
    "closed_at": "2025-04-22T13:58:42Z",
    "merged_at": "2025-04-22T13:58:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3032"
  },
  {
    "number": 3030,
    "title": "doc:add version.txt for internal cutlass library and nvrtc_wrapper so…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T13:32:31Z",
    "closed_at": "2025-03-24T15:44:22Z",
    "merged_at": "2025-03-24T15:44:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3030"
  },
  {
    "number": 3029,
    "title": "infra: [CI] - Only checkout the Git sourcecodes once in the CI pipeline",
    "user": "chzblych",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T12:31:16Z",
    "closed_at": "2025-03-25T13:30:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3029"
  },
  {
    "number": 3028,
    "title": "feat: allocate minimal blocks per window size",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T12:10:31Z",
    "closed_at": "2025-04-17T08:04:57Z",
    "merged_at": "2025-04-17T08:04:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3028"
  },
  {
    "number": 3027,
    "title": "test: [TRTLLM-4000] Port multi GPU changes to GitHub",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T11:26:34Z",
    "closed_at": "2025-03-26T21:55:04Z",
    "merged_at": "2025-03-26T21:55:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3027"
  },
  {
    "number": 3026,
    "title": "feat:lora_modules_support",
    "user": "danielafrimi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T10:51:00Z",
    "closed_at": "2025-04-01T11:40:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3026"
  },
  {
    "number": 3025,
    "title": "chore: refactor the LlmArgs with Pydantic and migrate remaining pybinding configs to python",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T10:43:51Z",
    "closed_at": "2025-04-05T05:31:49Z",
    "merged_at": "2025-04-05T05:31:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3025"
  },
  {
    "number": 3024,
    "title": "refactor: Remove speculative decoding parameters from stateful decoders",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T10:35:48Z",
    "closed_at": "2025-03-26T12:16:27Z",
    "merged_at": "2025-03-26T12:16:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3024"
  },
  {
    "number": 3023,
    "title": "chore: remove useless param",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T09:59:53Z",
    "closed_at": "2025-03-25T00:36:45Z",
    "merged_at": "2025-03-25T00:36:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3023"
  },
  {
    "number": 3022,
    "title": "chore: add git lfs to build_wheel.py to avoid confusing error",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T09:52:24Z",
    "closed_at": "2025-03-25T00:19:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3022"
  },
  {
    "number": 3021,
    "title": "fix: disable KV cache reuse if using attention sink",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T09:50:55Z",
    "closed_at": "2025-04-15T19:07:33Z",
    "merged_at": "2025-04-15T19:07:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3021"
  },
  {
    "number": 3020,
    "title": "feat: Support cos_sin_cache in all cases.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T09:44:21Z",
    "closed_at": "2025-04-14T04:20:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3020"
  },
  {
    "number": 3019,
    "title": "chore: bump version to \"0.19.0.dev2025032500\"",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T09:39:17Z",
    "closed_at": "2025-03-25T02:04:17Z",
    "merged_at": "2025-03-25T02:04:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3019"
  },
  {
    "number": 3018,
    "title": "fix: creating output of dataset generator in current directory",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T09:20:13Z",
    "closed_at": "2025-04-25T13:01:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3018"
  },
  {
    "number": 3017,
    "title": "fix: segfault in cudaDriverWrapper",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T09:10:22Z",
    "closed_at": "2025-04-02T06:55:19Z",
    "merged_at": "2025-04-02T06:55:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3017"
  },
  {
    "number": 3016,
    "title": "chore: Improve trtllm-serve import pattern",
    "user": "LinPoly",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T09:00:55Z",
    "closed_at": "2025-04-08T15:33:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3016"
  },
  {
    "number": 3015,
    "title": "chore: Refactor imports inside tensorrt_llm._torch.",
    "user": "yuxianq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T08:37:01Z",
    "closed_at": "2025-03-26T03:01:07Z",
    "merged_at": "2025-03-26T03:01:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3015"
  },
  {
    "number": 3014,
    "title": "feat : reduce trt engine build time in testing",
    "user": "peaceh-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T08:01:41Z",
    "closed_at": "2025-03-26T05:02:55Z",
    "merged_at": "2025-03-26T05:02:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3014"
  },
  {
    "number": 3013,
    "title": "test: [TRTLLM-3994] Support only run pytorch tests",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T07:53:43Z",
    "closed_at": "2025-04-03T05:46:10Z",
    "merged_at": "2025-04-03T05:46:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3013"
  },
  {
    "number": 3012,
    "title": "feat: add conditional disaggregation test",
    "user": "zhengd-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T07:33:00Z",
    "closed_at": "2025-03-26T07:55:34Z",
    "merged_at": "2025-03-26T07:55:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3012"
  },
  {
    "number": 3011,
    "title": "chore: fix bug of model paths in confset.py",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T07:21:11Z",
    "closed_at": "2025-03-25T09:00:45Z",
    "merged_at": "2025-03-25T09:00:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3011"
  },
  {
    "number": 3010,
    "title": "perf: Enable CUDA graphs when attention DP is used and active requests on different GPUs are uneven",
    "user": "jinyangyuan-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T06:44:59Z",
    "closed_at": "2025-03-26T13:09:25Z",
    "merged_at": "2025-03-26T13:09:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3010"
  },
  {
    "number": 3009,
    "title": "feat: Update logits bitmask kernel to v3",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T06:39:34Z",
    "closed_at": "2025-03-26T07:21:30Z",
    "merged_at": "2025-03-26T07:21:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3009"
  },
  {
    "number": 3008,
    "title": "fix: [MLA] fix the bug with fp8 MLA kernels on Blackwell.",
    "user": "PerkzZheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T06:25:14Z",
    "closed_at": "2025-03-25T10:03:29Z",
    "merged_at": "2025-03-25T10:03:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3008"
  },
  {
    "number": 3007,
    "title": "fix: gpus_per_node in trtllm-bench when world_size < device_count",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T05:59:56Z",
    "closed_at": "2025-03-27T01:31:41Z",
    "merged_at": "2025-03-27T01:31:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3007"
  },
  {
    "number": 3006,
    "title": "fix：fix illeagel memory access when mtp >= 2",
    "user": "dongjiyingdjy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T05:30:06Z",
    "closed_at": "2025-04-01T05:36:46Z",
    "merged_at": "2025-04-01T05:36:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3006"
  },
  {
    "number": 3005,
    "title": "feat: update allreduce benchmark",
    "user": "yizhang-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T04:50:51Z",
    "closed_at": "2025-08-05T07:00:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3005"
  },
  {
    "number": 3004,
    "title": "feat: MLA FP8 KV Cache on Blackwell",
    "user": "DylanChen-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T03:57:38Z",
    "closed_at": "2025-04-11T02:18:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3004"
  },
  {
    "number": 3003,
    "title": "fix: Fix path to constraints.txt in bloom/requirements.txt",
    "user": "Pradeep-18062002",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T03:49:43Z",
    "closed_at": "2025-03-24T15:03:42Z",
    "merged_at": "2025-03-24T15:03:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3003"
  },
  {
    "number": 3002,
    "title": "fix: fix for cp > kvHeadNum",
    "user": "DylanChen-NV",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T03:27:21Z",
    "closed_at": "2025-03-26T04:39:03Z",
    "merged_at": "2025-03-26T04:39:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3002"
  },
  {
    "number": 3001,
    "title": "chore: add ratelimit in workflow",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T03:02:28Z",
    "closed_at": "2025-03-24T07:54:12Z",
    "merged_at": "2025-03-24T07:54:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3001"
  },
  {
    "number": 3000,
    "title": "chore: Simplify quickstart of PyTorch flow",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T02:59:33Z",
    "closed_at": "2025-03-24T06:32:18Z",
    "merged_at": "2025-03-24T06:32:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3000"
  },
  {
    "number": 2999,
    "title": "chore: Deprecate model_api examples",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T02:57:30Z",
    "closed_at": "2025-03-25T01:37:20Z",
    "merged_at": "2025-03-25T01:37:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2999"
  },
  {
    "number": 2998,
    "title": "test: wait long time for disagg test",
    "user": "chuangz0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T02:54:08Z",
    "closed_at": "2025-03-25T12:52:39Z",
    "merged_at": "2025-03-25T12:52:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2998"
  },
  {
    "number": 2997,
    "title": "feat: Add several pure python configs to LlmArgs",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T02:51:05Z",
    "closed_at": "2025-03-24T08:16:17Z",
    "merged_at": "2025-03-24T08:16:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2997"
  },
  {
    "number": 2996,
    "title": "test: reorganize tests folder hierarchy",
    "user": "VALLIS-NERIA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T02:34:43Z",
    "closed_at": "2025-03-27T04:07:53Z",
    "merged_at": "2025-03-27T04:07:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2996"
  },
  {
    "number": 2995,
    "title": "feat: Add one-shot version for UB AR NORM FP16/BF16",
    "user": "liji-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T02:29:34Z",
    "closed_at": "2025-03-31T03:16:04Z",
    "merged_at": "2025-03-31T03:16:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2995"
  },
  {
    "number": 2994,
    "title": "chore: update approver list",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T02:23:45Z",
    "closed_at": "2025-03-24T04:51:27Z",
    "merged_at": "2025-03-24T04:51:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2994"
  },
  {
    "number": 2992,
    "title": "chore: relax the limitation of setuptools",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-24T01:34:34Z",
    "closed_at": "2025-03-24T05:36:10Z",
    "merged_at": "2025-03-24T05:36:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2992"
  },
  {
    "number": 2991,
    "title": "test:Add Eagle tests with untrained heads",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T19:54:22Z",
    "closed_at": "2025-04-01T03:42:00Z",
    "merged_at": "2025-04-01T03:42:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2991"
  },
  {
    "number": 2990,
    "title": "feat: Add support for Phi-4-mini",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T19:29:54Z",
    "closed_at": "2025-04-02T00:34:39Z",
    "merged_at": "2025-04-02T00:34:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2990"
  },
  {
    "number": 2989,
    "title": "fix: synthetic data generation improvements and driver wrapper segfault fix",
    "user": "hypdeb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T19:03:01Z",
    "closed_at": "2025-03-24T09:21:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2989"
  },
  {
    "number": 2988,
    "title": "test: Add tests for Ministral, Codestral, Mistral Small",
    "user": "brb-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T18:42:06Z",
    "closed_at": "2025-06-12T16:33:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2988"
  },
  {
    "number": 2987,
    "title": "test:remove opt/mpt/gptj/gptneox/bloom/falcon/baichuan/internlm/deep_…",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T15:19:21Z",
    "closed_at": "2025-03-24T06:18:06Z",
    "merged_at": "2025-03-24T06:18:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2987"
  },
  {
    "number": 2986,
    "title": "chore: Refactor return of first gen token in PD",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T14:56:45Z",
    "closed_at": "2025-04-01T04:28:28Z",
    "merged_at": "2025-04-01T04:28:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2986"
  },
  {
    "number": 2985,
    "title": "feat: Add support of chat completion in PD",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T14:54:24Z",
    "closed_at": "2025-04-11T09:53:29Z",
    "merged_at": "2025-04-11T09:53:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2985"
  },
  {
    "number": 2984,
    "title": "feat: AddKVCacheTransfer",
    "user": "Shunkangz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T14:18:29Z",
    "closed_at": "2025-03-25T13:45:37Z",
    "merged_at": "2025-03-25T13:45:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2984"
  },
  {
    "number": 2983,
    "title": "chore: disable kv cache reuse when minimum window size is reached, instead of maximum window size",
    "user": "netanel-haber",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T14:00:54Z",
    "closed_at": "2025-03-24T14:49:54Z",
    "merged_at": "2025-03-24T14:49:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2983"
  },
  {
    "number": 2982,
    "title": "test: Accuracy test improvement (Part 2): Incorporate mmlu to accuracy test suite",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T11:36:39Z",
    "closed_at": "2025-03-24T23:34:11Z",
    "merged_at": "2025-03-24T23:34:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2982"
  },
  {
    "number": 2981,
    "title": "feat: Update cutlass",
    "user": "Funatiq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T11:10:38Z",
    "closed_at": "2025-03-26T14:36:27Z",
    "merged_at": "2025-03-26T14:36:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2981"
  },
  {
    "number": 2980,
    "title": "Update the CONTRIBUTING.md as the ramp-up for TensorRT-LLM github fir…",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T08:57:15Z",
    "closed_at": "2025-03-23T11:58:16Z",
    "merged_at": "2025-03-23T11:58:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2980"
  },
  {
    "number": 2979,
    "title": "Test update doc",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T08:46:51Z",
    "closed_at": "2025-03-23T11:57:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2979"
  },
  {
    "number": 2978,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-23T08:34:16Z",
    "closed_at": "2025-03-23T08:39:36Z",
    "merged_at": "2025-03-23T08:39:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2978"
  },
  {
    "number": 2977,
    "title": "fix: The constructor checks useDynamicTree but doesn’t validate dynamicTreeMaxTopK if set",
    "user": "A-transformer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-21T17:50:44Z",
    "closed_at": "2025-06-05T19:09:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2977"
  },
  {
    "number": 2975,
    "title": "chore: fix default path for compile commands in clangd",
    "user": "MartinMarciniszyn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-21T16:17:31Z",
    "closed_at": "2025-03-24T05:13:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2975"
  },
  {
    "number": 2973,
    "title": "DO NOT MERGE, domb CI test",
    "user": "DomBrown",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-21T10:18:20Z",
    "closed_at": "2025-03-23T08:59:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2973"
  },
  {
    "number": 2972,
    "title": "CI Test",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-21T09:32:25Z",
    "closed_at": "2025-03-23T09:03:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2972"
  },
  {
    "number": 2971,
    "title": "prtest",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-21T09:17:50Z",
    "closed_at": "2025-03-23T09:03:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2971"
  },
  {
    "number": 2969,
    "title": "fix bug of setuptools",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-21T03:01:36Z",
    "closed_at": "2025-03-23T09:03:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2969"
  },
  {
    "number": 2966,
    "title": "Update requirements.txt",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-21T02:29:47Z",
    "closed_at": "2025-03-21T02:35:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2966"
  },
  {
    "number": 2965,
    "title": "[Testing]  Remove deepseek v1 example",
    "user": "QiJune",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-21T02:12:00Z",
    "closed_at": "2025-03-24T00:42:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2965"
  },
  {
    "number": 2963,
    "title": "Pr1",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-20T15:06:19Z",
    "closed_at": "2025-03-23T09:07:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2963"
  },
  {
    "number": 2962,
    "title": "Fix: LLM API logits processor example comments",
    "user": "syuoni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-20T13:33:05Z",
    "closed_at": "2025-03-24T04:22:12Z",
    "merged_at": "2025-03-24T04:22:12Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2962"
  },
  {
    "number": 2961,
    "title": "CI test",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-20T12:57:21Z",
    "closed_at": "2025-03-23T09:04:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2961"
  },
  {
    "number": 2959,
    "title": "Test failed tests",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-20T10:22:11Z",
    "closed_at": "2025-03-23T09:04:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2959"
  },
  {
    "number": 2958,
    "title": "CI failure test",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-20T10:07:02Z",
    "closed_at": "2025-03-23T09:04:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2958"
  },
  {
    "number": 2957,
    "title": "Test kill",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-20T08:36:32Z",
    "closed_at": "2025-03-23T09:04:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2957"
  },
  {
    "number": 2956,
    "title": "Test multi-gpu",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-20T07:09:47Z",
    "closed_at": "2025-03-23T09:05:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2956"
  },
  {
    "number": 2955,
    "title": "Test failed tests",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-20T07:05:05Z",
    "closed_at": "2025-03-23T09:05:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2955"
  },
  {
    "number": 2954,
    "title": "Test conflict",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-20T06:35:06Z",
    "closed_at": "2025-03-23T09:05:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2954"
  },
  {
    "number": 2951,
    "title": "Regular bot run test",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T14:04:42Z",
    "closed_at": "2025-03-23T09:06:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2951"
  },
  {
    "number": 2950,
    "title": "Test",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T11:26:00Z",
    "closed_at": "2025-03-19T13:36:24Z",
    "merged_at": "2025-03-19T13:36:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2950"
  },
  {
    "number": 2949,
    "title": "chore: reference URL refactor",
    "user": "A-transformer",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T10:43:18Z",
    "closed_at": "2025-06-05T19:09:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2949"
  },
  {
    "number": 2948,
    "title": "fix l0 pipeline",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T08:10:38Z",
    "closed_at": "2025-03-20T06:12:33Z",
    "merged_at": "2025-03-20T06:12:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2948"
  },
  {
    "number": 2947,
    "title": "doc enhancement for disagg",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T06:17:01Z",
    "closed_at": "2025-03-19T11:26:36Z",
    "merged_at": "2025-03-19T11:26:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2947"
  },
  {
    "number": 2946,
    "title": "status testing",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T03:34:30Z",
    "closed_at": "2025-03-23T09:06:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2946"
  },
  {
    "number": 2945,
    "title": "feat: Add canary recipe support canary-1b, canary-1b flash and canary-180m flash with new prompt format",
    "user": "anand-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T03:29:31Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2945"
  },
  {
    "number": 2944,
    "title": "Ci test",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T03:25:27Z",
    "closed_at": "2025-03-23T09:06:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2944"
  },
  {
    "number": 2943,
    "title": "update workflow allowlist",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T02:13:29Z",
    "closed_at": "2025-03-19T02:20:47Z",
    "merged_at": "2025-03-19T02:20:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2943"
  },
  {
    "number": 2942,
    "title": "bot testing",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T01:42:06Z",
    "closed_at": "2025-03-23T09:06:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2942"
  },
  {
    "number": 2941,
    "title": "Break stuff",
    "user": "zeroepoch",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-18T18:54:40Z",
    "closed_at": "2025-03-21T05:12:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2941"
  },
  {
    "number": 2940,
    "title": "doc: update doc",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-18T15:59:47Z",
    "closed_at": "2025-03-23T09:06:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2940"
  },
  {
    "number": 2939,
    "title": "[NOT MERGE]Test Bot and CI",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-18T15:34:49Z",
    "closed_at": "2025-03-23T09:06:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2939"
  },
  {
    "number": 2938,
    "title": "[NOT MERGE]Test Bot and CI",
    "user": "ZhanruiSunCh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-18T15:30:00Z",
    "closed_at": "2025-03-22T02:58:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2938"
  },
  {
    "number": 2936,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-18T13:12:07Z",
    "closed_at": "2025-03-18T13:25:19Z",
    "merged_at": "2025-03-18T13:25:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2936"
  },
  {
    "number": 2935,
    "title": "doc: update doc",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-18T11:45:57Z",
    "closed_at": "2025-03-18T12:28:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2935"
  },
  {
    "number": 2934,
    "title": "DCO testing",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-18T06:42:30Z",
    "closed_at": "2025-03-18T13:38:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2934"
  },
  {
    "number": 2933,
    "title": "Test",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-18T04:48:21Z",
    "closed_at": "2025-03-18T12:31:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2933"
  },
  {
    "number": 2931,
    "title": "update github workflow",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-17T15:06:36Z",
    "closed_at": "2025-03-17T15:11:10Z",
    "merged_at": "2025-03-17T15:11:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2931"
  },
  {
    "number": 2930,
    "title": "Merge request test",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-17T14:28:23Z",
    "closed_at": "2025-03-23T09:06:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2930"
  },
  {
    "number": 2929,
    "title": "Merge request test",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-17T13:12:08Z",
    "closed_at": "2025-03-22T02:58:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2929"
  },
  {
    "number": 2927,
    "title": "rebase testing 1",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-17T08:53:23Z",
    "closed_at": "2025-03-18T13:38:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2927"
  },
  {
    "number": 2926,
    "title": "rebase testing 0",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-17T08:53:05Z",
    "closed_at": "2025-03-18T13:38:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2926"
  },
  {
    "number": 2924,
    "title": "stress testing thread-11",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:14:43Z",
    "closed_at": "2025-03-18T13:38:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2924"
  },
  {
    "number": 2923,
    "title": "stress testing thread-10",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:11:08Z",
    "closed_at": "2025-03-18T13:38:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2923"
  },
  {
    "number": 2922,
    "title": "stress testing thread-11",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:10:36Z",
    "closed_at": "2025-03-16T14:13:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2922"
  },
  {
    "number": 2921,
    "title": "stress testing thread-12",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:10:08Z",
    "closed_at": "2025-03-18T13:38:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2921"
  },
  {
    "number": 2920,
    "title": "stress testing thread-13",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:09:38Z",
    "closed_at": "2025-03-18T13:38:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2920"
  },
  {
    "number": 2919,
    "title": "stress testing thread-14",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:09:09Z",
    "closed_at": "2025-03-18T13:38:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2919"
  },
  {
    "number": 2918,
    "title": "stress testing thread-15",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:08:30Z",
    "closed_at": "2025-03-18T13:38:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2918"
  },
  {
    "number": 2917,
    "title": "stress testing thread-16",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:08:05Z",
    "closed_at": "2025-03-18T13:38:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2917"
  },
  {
    "number": 2916,
    "title": "stress testing thread-17",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:07:40Z",
    "closed_at": "2025-03-18T13:38:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2916"
  },
  {
    "number": 2915,
    "title": "stress testing thread-18",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:07:10Z",
    "closed_at": "2025-03-18T13:38:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2915"
  },
  {
    "number": 2914,
    "title": "stress testing thread-19",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-16T14:06:30Z",
    "closed_at": "2025-03-18T13:38:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2914"
  },
  {
    "number": 2910,
    "title": "stress testing thread-1",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T03:13:57Z",
    "closed_at": "2025-03-18T13:38:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2910"
  },
  {
    "number": 2909,
    "title": "stress testing thread-2",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T03:13:21Z",
    "closed_at": "2025-03-18T13:38:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2909"
  },
  {
    "number": 2908,
    "title": "stress testing thread-3",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T03:12:54Z",
    "closed_at": "2025-03-18T13:38:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2908"
  },
  {
    "number": 2907,
    "title": "stress testing thread-4",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T03:12:17Z",
    "closed_at": "2025-03-18T13:38:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2907"
  },
  {
    "number": 2906,
    "title": "stress testing thread-5",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T03:11:18Z",
    "closed_at": "2025-03-18T13:38:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2906"
  },
  {
    "number": 2905,
    "title": "stress testing thread-6",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T03:10:51Z",
    "closed_at": "2025-03-18T13:38:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2905"
  },
  {
    "number": 2904,
    "title": "stress testing thread-7",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T03:09:55Z",
    "closed_at": "2025-03-18T13:38:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2904"
  },
  {
    "number": 2903,
    "title": "stress testing thread-8",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T03:09:34Z",
    "closed_at": "2025-03-18T13:38:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2903"
  },
  {
    "number": 2902,
    "title": "stress testing thread-9",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T03:09:05Z",
    "closed_at": "2025-03-18T13:39:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2902"
  },
  {
    "number": 2901,
    "title": "minor change, for github CI validation only",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T02:50:37Z",
    "closed_at": "2025-03-22T02:58:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2901"
  },
  {
    "number": 2900,
    "title": "chroe:add debug msg test thread 4",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T01:58:02Z",
    "closed_at": "2025-03-14T02:52:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2900"
  },
  {
    "number": 2899,
    "title": "chroe:add debug msg test thread 3",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T01:57:04Z",
    "closed_at": "2025-03-14T02:52:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2899"
  },
  {
    "number": 2898,
    "title": "chroe:add debug msg test thread 2",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T01:56:35Z",
    "closed_at": "2025-03-14T02:51:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2898"
  },
  {
    "number": 2897,
    "title": "chroe:add debug msg test thread 1",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-14T01:55:55Z",
    "closed_at": "2025-03-14T02:47:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2897"
  },
  {
    "number": 2895,
    "title": "chroe:add debug msg test thread 0",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-13T12:03:30Z",
    "closed_at": "2025-03-18T13:39:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2895"
  },
  {
    "number": 2893,
    "title": "chroe:add debug msg",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-13T11:42:25Z",
    "closed_at": "2025-03-13T12:01:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2893"
  },
  {
    "number": 2892,
    "title": "chroe:add debug msg",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-13T11:41:11Z",
    "closed_at": "2025-03-13T12:01:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2892"
  },
  {
    "number": 2891,
    "title": "chroe:add debug msg",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-13T11:40:39Z",
    "closed_at": "2025-03-13T12:01:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2891"
  },
  {
    "number": 2890,
    "title": "chroe:add debug msg",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-13T11:39:56Z",
    "closed_at": "2025-03-13T12:01:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2890"
  },
  {
    "number": 2889,
    "title": "User/guomingz/ci robust 1",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-13T11:24:20Z",
    "closed_at": "2025-03-13T12:02:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2889"
  },
  {
    "number": 2888,
    "title": "User/guomingz/ci robust",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-13T11:22:53Z",
    "closed_at": "2025-03-13T12:01:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2888"
  },
  {
    "number": 2887,
    "title": "Test for Github firstly CI",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-13T11:14:17Z",
    "closed_at": "2025-03-14T02:30:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2887"
  },
  {
    "number": 2884,
    "title": "dev-main initialization",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-13T02:56:04Z",
    "closed_at": "2025-03-18T12:24:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2884"
  },
  {
    "number": 2882,
    "title": "dev-main initialization",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-13T02:09:55Z",
    "closed_at": "2025-03-13T02:41:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2882"
  },
  {
    "number": 2877,
    "title": "doc: Claim support for QwQ 32B",
    "user": "laikhtewari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-11T23:00:37Z",
    "closed_at": "2025-03-24T05:05:15Z",
    "merged_at": "2025-03-24T05:05:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2877"
  },
  {
    "number": 2876,
    "title": "chore: Update setup.py",
    "user": "ovuruska",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-11T18:54:25Z",
    "closed_at": "2025-03-24T05:10:53Z",
    "merged_at": "2025-03-24T05:10:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2876"
  },
  {
    "number": 2873,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-11T11:42:47Z",
    "closed_at": "2025-03-11T13:13:43Z",
    "merged_at": "2025-03-11T13:13:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2873"
  },
  {
    "number": 2872,
    "title": "fix: Fix converting EXAONE when using model_weights_loader",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-10T15:46:04Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2872"
  },
  {
    "number": 2862,
    "title": "Update quantization/README.md",
    "user": "mikekgfb",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-07T03:07:26Z",
    "closed_at": "2025-03-24T05:46:09Z",
    "merged_at": "2025-03-24T05:46:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2862"
  },
  {
    "number": 2858,
    "title": "enable l0-test.yml",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-06T07:23:23Z",
    "closed_at": "2025-03-06T07:24:40Z",
    "merged_at": "2025-03-06T07:24:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2858"
  },
  {
    "number": 2853,
    "title": "Fix googletest github",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-04T14:25:59Z",
    "closed_at": "2025-03-04T14:41:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2853"
  },
  {
    "number": 2852,
    "title": "Fix .gitmodules",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-04T14:20:58Z",
    "closed_at": "2025-03-04T14:34:09Z",
    "merged_at": "2025-03-04T14:34:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2852"
  },
  {
    "number": 2849,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-04T10:36:03Z",
    "closed_at": "2025-03-04T10:44:01Z",
    "merged_at": "2025-03-04T10:44:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2849"
  },
  {
    "number": 2841,
    "title": "[Do not merge] Test review rules",
    "user": "kevinch-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-28T20:56:46Z",
    "closed_at": "2025-03-03T18:01:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2841"
  },
  {
    "number": 2839,
    "title": "Add doc owners",
    "user": "kevinch-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-28T18:30:46Z",
    "closed_at": "2025-02-28T19:19:09Z",
    "merged_at": "2025-02-28T19:19:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2839"
  },
  {
    "number": 2837,
    "title": "fix blossom-ci",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-28T09:49:19Z",
    "closed_at": "2025-02-28T09:49:27Z",
    "merged_at": "2025-02-28T09:49:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2837"
  },
  {
    "number": 2836,
    "title": "fix blossom-ci",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-28T09:35:44Z",
    "closed_at": "2025-02-28T09:35:53Z",
    "merged_at": "2025-02-28T09:35:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2836"
  },
  {
    "number": 2835,
    "title": "check team membership in .yml file",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-28T09:31:25Z",
    "closed_at": "2025-02-28T09:31:46Z",
    "merged_at": "2025-02-28T09:31:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2835"
  },
  {
    "number": 2830,
    "title": "Use NVIDIA-gha runners to collect test results for CI",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-27T00:44:49Z",
    "closed_at": "2025-02-28T04:02:02Z",
    "merged_at": "2025-02-28T04:02:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2830"
  },
  {
    "number": 2828,
    "title": "Add CODEOWNERs file for rule testing",
    "user": "kevinch-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-26T19:19:44Z",
    "closed_at": "2025-02-26T20:19:37Z",
    "merged_at": "2025-02-26T20:19:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2828"
  },
  {
    "number": 2823,
    "title": "Add R1 perf data to latest news page",
    "user": "laikhtewari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-26T00:37:12Z",
    "closed_at": "2025-02-26T00:50:19Z",
    "merged_at": "2025-02-26T00:50:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2823"
  },
  {
    "number": 2820,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-25T13:03:32Z",
    "closed_at": "2025-02-25T13:21:50Z",
    "merged_at": "2025-02-25T13:21:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2820"
  },
  {
    "number": 2808,
    "title": "allow build command arguments",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-21T02:22:42Z",
    "closed_at": "2025-02-21T02:38:49Z",
    "merged_at": "2025-02-21T02:38:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2808"
  },
  {
    "number": 2807,
    "title": "[do not merge] test CI",
    "user": "tburt-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-20T23:40:08Z",
    "closed_at": "2025-03-22T02:58:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2807"
  },
  {
    "number": 2806,
    "title": "chore: better quantization calibration loop for modelopt ",
    "user": "michaelfeil",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-20T23:06:23Z",
    "closed_at": "2025-05-28T05:38:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2806"
  },
  {
    "number": 2792,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-18T13:19:22Z",
    "closed_at": "2025-02-18T13:27:39Z",
    "merged_at": "2025-02-18T13:27:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2792"
  },
  {
    "number": 2787,
    "title": "Fix Incorrect Batch Slot Usage in addCumLogProbs Kernel",
    "user": "aotman",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-14T15:00:40Z",
    "closed_at": "2025-03-19T02:49:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2787"
  },
  {
    "number": 2783,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-13T10:33:22Z",
    "closed_at": "2025-02-13T10:40:22Z",
    "merged_at": "2025-02-13T10:40:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2783"
  },
  {
    "number": 2779,
    "title": "Update",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-12T14:36:13Z",
    "closed_at": "2025-02-12T14:46:15Z",
    "merged_at": "2025-02-12T14:46:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2779"
  },
  {
    "number": 2770,
    "title": "fix invalid link on torch.md",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-11T03:53:03Z",
    "closed_at": "2025-02-11T05:55:43Z",
    "merged_at": "2025-02-11T05:55:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2770"
  },
  {
    "number": 2768,
    "title": "fix: Add missing parameter for WeightOnlyQuantRowLinear module",
    "user": "liquanfeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-09T12:48:27Z",
    "closed_at": "2025-03-31T08:20:31Z",
    "merged_at": "2025-03-31T08:20:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2768"
  },
  {
    "number": 2766,
    "title": "FP8 perf improvement",
    "user": "yingcanw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-08T12:13:59Z",
    "closed_at": "2025-02-08T12:45:00Z",
    "merged_at": "2025-02-08T12:45:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2766"
  },
  {
    "number": 2764,
    "title": "Update gh-pages",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-07T07:01:00Z",
    "closed_at": "2025-02-07T07:02:18Z",
    "merged_at": "2025-02-07T07:02:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2764"
  },
  {
    "number": 2762,
    "title": "Update DeepSeek V3 README",
    "user": "yingcanw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-07T06:19:00Z",
    "closed_at": "2025-02-07T06:24:29Z",
    "merged_at": "2025-02-07T06:24:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2762"
  },
  {
    "number": 2761,
    "title": "Fix github io pages",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-07T06:16:36Z",
    "closed_at": "2025-02-07T06:21:07Z",
    "merged_at": "2025-02-07T06:21:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2761"
  },
  {
    "number": 2755,
    "title": "Update TensorRT-LLM",
    "user": "DanBlanaru",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-06T12:40:23Z",
    "closed_at": "2025-02-06T13:01:37Z",
    "merged_at": "2025-02-06T13:01:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2755"
  },
  {
    "number": 2752,
    "title": "Fix moe_normalization_mode on Mixtral and Arctic",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-06T04:48:41Z",
    "closed_at": "2025-02-10T02:09:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2752"
  },
  {
    "number": 2751,
    "title": "update README.md for 0.17",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-06T03:53:48Z",
    "closed_at": "2025-02-06T08:27:25Z",
    "merged_at": "2025-02-06T08:27:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2751"
  },
  {
    "number": 2747,
    "title": "[None][chore] Only read cfg json once",
    "user": "LetsGoFir",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-05T11:50:10Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2747"
  },
  {
    "number": 2746,
    "title": "gh pages update",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-04T18:46:08Z",
    "closed_at": "2025-02-04T20:31:39Z",
    "merged_at": "2025-02-04T20:31:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2746"
  },
  {
    "number": 2744,
    "title": "Update the docs to workaround the extra-index-url issue",
    "user": "zeroepoch",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-04T07:48:57Z",
    "closed_at": "2025-02-04T07:59:51Z",
    "merged_at": "2025-02-04T07:59:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2744"
  },
  {
    "number": 2743,
    "title": "update gh pages",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-04T00:56:44Z",
    "closed_at": "2025-02-04T15:12:28Z",
    "merged_at": "2025-02-04T15:12:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2743"
  },
  {
    "number": 2742,
    "title": "Add note for blackwell",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-04T00:49:27Z",
    "closed_at": "2025-02-04T00:50:30Z",
    "merged_at": "2025-02-04T00:50:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2742"
  },
  {
    "number": 2741,
    "title": "Update gh pages",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-03T22:44:31Z",
    "closed_at": "2025-02-03T22:51:07Z",
    "merged_at": "2025-02-03T22:51:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2741"
  },
  {
    "number": 2736,
    "title": "TensorRT-LLM v0.17.0.post1 Release",
    "user": "zeroepoch",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-02T05:54:59Z",
    "closed_at": "2025-02-02T06:14:53Z",
    "merged_at": "2025-02-02T06:14:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2736"
  },
  {
    "number": 2734,
    "title": "update the cutlass kernel library",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-01T16:44:36Z",
    "closed_at": "2025-02-02T01:33:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2734"
  },
  {
    "number": 2732,
    "title": "Update docs",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-31T20:18:16Z",
    "closed_at": "2025-01-31T21:03:49Z",
    "merged_at": "2025-01-31T21:03:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2732"
  },
  {
    "number": 2731,
    "title": "feat: add chunked context/prefill runtime option to trtllm-serve",
    "user": "tsnyder-sps",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-31T17:30:08Z",
    "closed_at": "2025-03-26T13:12:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2731"
  },
  {
    "number": 2725,
    "title": "TensorRT-LLM v0.17 Release",
    "user": "schetlur-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-30T21:20:39Z",
    "closed_at": "2025-01-30T21:32:36Z",
    "merged_at": "2025-01-30T21:32:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2725"
  },
  {
    "number": 2724,
    "title": "fix ifb issue in deepseek",
    "user": "Tabrizian",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-30T18:58:29Z",
    "closed_at": "2025-01-31T02:35:06Z",
    "merged_at": "2025-01-31T02:35:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2724"
  },
  {
    "number": 2719,
    "title": "DeepSeek V3 FP8 Support",
    "user": "yingcanw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-24T18:02:31Z",
    "closed_at": "2025-01-24T20:16:16Z",
    "merged_at": "2025-01-24T20:16:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2719"
  },
  {
    "number": 2715,
    "title": "feat: deepseek_v1 gqa and correct normalization mode",
    "user": "akhoroshev",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-23T12:54:08Z",
    "closed_at": "2025-03-28T11:07:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2715"
  },
  {
    "number": 2712,
    "title": "fix: gptattentionplugin onnxparser compatability",
    "user": "jl749",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-23T03:48:20Z",
    "closed_at": "2025-02-14T02:58:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2712"
  },
  {
    "number": 2701,
    "title": "update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-16T14:33:34Z",
    "closed_at": "2025-01-16T14:34:28Z",
    "merged_at": "2025-01-16T14:34:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2701"
  },
  {
    "number": 2700,
    "title": "Revert \"doc:fix llm api reference blank issue.\"",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-16T13:49:05Z",
    "closed_at": "2025-01-16T13:53:14Z",
    "merged_at": "2025-01-16T13:53:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2700"
  },
  {
    "number": 2697,
    "title": "Fix  kv cache config issue",
    "user": "yingcanw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-16T03:52:53Z",
    "closed_at": "2025-01-16T09:46:04Z",
    "merged_at": "2025-01-16T09:46:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2697"
  },
  {
    "number": 2691,
    "title": "Fix kwarg name",
    "user": "topenkoff",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-14T10:28:13Z",
    "closed_at": "2025-01-20T04:18:27Z",
    "merged_at": "2025-01-20T04:18:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2691"
  },
  {
    "number": 2682,
    "title": "use selected index past past key value in attention when using contin…",
    "user": "Eayne",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-13T05:39:19Z",
    "closed_at": "2025-05-28T05:37:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2682"
  },
  {
    "number": 2669,
    "title": "doc:fix llm api reference blank issue.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-08T06:40:57Z",
    "closed_at": "2025-01-08T06:45:46Z",
    "merged_at": "2025-01-08T06:45:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2669"
  },
  {
    "number": 2668,
    "title": "Update README.md",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-08T06:40:25Z",
    "closed_at": "2025-01-08T06:41:00Z",
    "merged_at": "2025-01-08T06:40:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2668"
  },
  {
    "number": 2661,
    "title": "Update HF ckpt BF16 conversion.",
    "user": "yingcanw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-06T10:25:08Z",
    "closed_at": "2025-01-06T10:28:34Z",
    "merged_at": "2025-01-06T10:28:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2661"
  },
  {
    "number": 2654,
    "title": "Update gh-pages",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-03T07:38:34Z",
    "closed_at": "2025-01-03T08:09:29Z",
    "merged_at": "2025-01-03T08:09:29Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2654"
  },
  {
    "number": 2653,
    "title": "Update linux.md",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-03T07:34:16Z",
    "closed_at": "2025-01-03T08:01:33Z",
    "merged_at": "2025-01-03T08:01:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2653"
  },
  {
    "number": 2651,
    "title": "Update gh-pages",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-03T07:04:35Z",
    "closed_at": "2025-01-03T07:12:40Z",
    "merged_at": "2025-01-03T07:12:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2651"
  },
  {
    "number": 2650,
    "title": "Update disaggregated-service.md",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-03T06:57:15Z",
    "closed_at": "2025-01-03T07:07:32Z",
    "merged_at": "2025-01-03T07:07:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2650"
  },
  {
    "number": 2646,
    "title": "Update gh-pages",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-02T09:31:13Z",
    "closed_at": "2025-01-02T09:53:21Z",
    "merged_at": "2025-01-02T09:53:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2646"
  },
  {
    "number": 2645,
    "title": "Update Disaggregated-Service Document",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-02T09:06:12Z",
    "closed_at": "2025-01-02T09:53:18Z",
    "merged_at": "2025-01-02T09:53:18Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2645"
  },
  {
    "number": 2644,
    "title": "Fix model name mapping (#2)",
    "user": "yingcanw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-02T08:29:38Z",
    "closed_at": "2025-01-02T08:31:21Z",
    "merged_at": "2025-01-02T08:31:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2644"
  },
  {
    "number": 2643,
    "title": "Implement Min-P sampling and late temperature adjustment as a fused sampling layer",
    "user": "aikitoria",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-01-02T04:28:45Z",
    "closed_at": "2025-02-09T03:59:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2643"
  },
  {
    "number": 2633,
    "title": "Custom samplingconfig addition",
    "user": "buddhapuneeth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-27T00:57:09Z",
    "closed_at": "2025-03-28T11:05:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2633"
  },
  {
    "number": 2629,
    "title": "Support DeepSeek-V3 in TRT-LLM",
    "user": "yingcanw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-26T02:50:53Z",
    "closed_at": "2024-12-26T02:53:09Z",
    "merged_at": "2024-12-26T02:53:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2629"
  },
  {
    "number": 2625,
    "title": "Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-25T05:42:15Z",
    "closed_at": "2024-12-25T05:44:02Z",
    "merged_at": "2024-12-25T05:44:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2625"
  },
  {
    "number": 2623,
    "title": "Update windows doc for release 0.16",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-25T01:46:10Z",
    "closed_at": "2024-12-25T03:29:20Z",
    "merged_at": "2024-12-25T03:29:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2623"
  },
  {
    "number": 2618,
    "title": "Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-24T10:29:27Z",
    "closed_at": "2024-12-24T10:31:19Z",
    "merged_at": "2024-12-24T10:31:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2618"
  },
  {
    "number": 2612,
    "title": "PR test",
    "user": "yingcanw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-24T07:35:54Z",
    "closed_at": "2024-12-24T07:36:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2612"
  },
  {
    "number": 2611,
    "title": "TensorRT-LLM v0.16 Release",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-24T07:11:29Z",
    "closed_at": "2024-12-24T07:58:43Z",
    "merged_at": "2024-12-24T07:58:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2611"
  },
  {
    "number": 2601,
    "title": "Create c-cpp.yml",
    "user": "TNGBBK",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-20T14:56:57Z",
    "closed_at": "2025-03-14T09:41:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2601"
  },
  {
    "number": 2582,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-17T05:11:55Z",
    "closed_at": "2024-12-17T05:50:47Z",
    "merged_at": "2024-12-17T05:50:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2582"
  },
  {
    "number": 2573,
    "title": "[LLM] sampling_params should be setup only if end_id is None and tokenizer is not None",
    "user": "mfuntowicz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-12T22:23:38Z",
    "closed_at": "2024-12-24T15:05:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2573"
  },
  {
    "number": 2570,
    "title": "Constraint `pynvml` version",
    "user": "MahmoudAshraf97",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-12T09:58:42Z",
    "closed_at": "2025-02-06T22:16:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2570"
  },
  {
    "number": 2566,
    "title": "Add issue triage workflows",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-11T17:19:22Z",
    "closed_at": "2024-12-11T17:27:40Z",
    "merged_at": "2024-12-11T17:27:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2566"
  },
  {
    "number": 2562,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-11T07:43:56Z",
    "closed_at": "2024-12-11T08:31:05Z",
    "merged_at": "2024-12-11T08:31:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2562"
  },
  {
    "number": 2532,
    "title": "Update TensorRT-LLM",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-04T11:31:27Z",
    "closed_at": "2024-12-04T13:16:58Z",
    "merged_at": "2024-12-04T13:16:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2532"
  },
  {
    "number": 2530,
    "title": "Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-04T06:10:01Z",
    "closed_at": "2024-12-04T06:25:19Z",
    "merged_at": "2024-12-04T06:25:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2530"
  },
  {
    "number": 2529,
    "title": "TensorRT-LLM Release 0.15.0",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-04T05:29:23Z",
    "closed_at": "2024-12-04T05:44:56Z",
    "merged_at": "2024-12-04T05:44:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2529"
  },
  {
    "number": 2516,
    "title": "fix NV bench output len was garbage value",
    "user": "ekagra-ranjan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-29T21:56:19Z",
    "closed_at": "2025-03-28T11:04:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2516"
  },
  {
    "number": 2513,
    "title": "[DO NOT MERGE] test CI",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-29T07:21:40Z",
    "closed_at": "2025-03-24T05:12:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2513"
  },
  {
    "number": 2512,
    "title": "Add blossom-ci.yml",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-29T06:49:47Z",
    "closed_at": "2024-11-29T07:01:26Z",
    "merged_at": "2024-11-29T07:01:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2512"
  },
  {
    "number": 2502,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-26T08:29:18Z",
    "closed_at": "2024-11-26T08:51:34Z",
    "merged_at": "2024-11-26T08:51:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2502"
  },
  {
    "number": 2498,
    "title": "Add issue triage workflows",
    "user": "kevinch-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-26T00:03:44Z",
    "closed_at": "2024-12-04T15:50:46Z",
    "merged_at": "2024-12-04T15:50:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2498"
  },
  {
    "number": 2493,
    "title": "feat(qwen): add trust_remote_code argument support",
    "user": "ghost",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-24T23:17:49Z",
    "closed_at": "2025-02-05T16:09:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2493"
  },
  {
    "number": 2485,
    "title": "The clamp in-place operation cannot modify the weight_scales tensor directly.",
    "user": "StarrickLiu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-22T08:15:00Z",
    "closed_at": "2024-11-23T08:54:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2485"
  },
  {
    "number": 2484,
    "title": "bugfix/incorrect lora out dims",
    "user": "akhoroshev",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-22T06:55:26Z",
    "closed_at": "2025-03-28T11:00:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2484"
  },
  {
    "number": 2473,
    "title": "Fix minor typo",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-20T15:31:32Z",
    "closed_at": "2024-12-02T02:11:27Z",
    "merged_at": "2024-12-02T02:11:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2473"
  },
  {
    "number": 2470,
    "title": "Fix prompt_table_data empty tensor shape error",
    "user": "BasicCoder",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-20T07:50:10Z",
    "closed_at": "2025-08-19T21:32:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2470"
  },
  {
    "number": 2468,
    "title": "Ci test",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-20T02:51:13Z",
    "closed_at": "2024-11-20T02:51:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2468"
  },
  {
    "number": 2464,
    "title": "add blossom-ci.yml",
    "user": "niukuo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-19T14:23:32Z",
    "closed_at": "2024-11-20T02:26:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2464"
  },
  {
    "number": 2460,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-19T10:21:58Z",
    "closed_at": "2024-11-19T10:30:34Z",
    "merged_at": "2024-11-19T10:30:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2460"
  },
  {
    "number": 2446,
    "title": "Create INT8 KV Cache on Qserve",
    "user": "lej970703",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-14T12:47:44Z",
    "closed_at": "2025-08-19T21:26:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2446"
  },
  {
    "number": 2436,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-12T07:14:54Z",
    "closed_at": "2024-11-12T07:27:49Z",
    "merged_at": "2024-11-12T07:27:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2436"
  },
  {
    "number": 2418,
    "title": "add the missing files ",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-06T06:14:45Z",
    "closed_at": "2024-11-06T06:22:53Z",
    "merged_at": "2024-11-06T06:22:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2418"
  },
  {
    "number": 2413,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-05T07:38:55Z",
    "closed_at": "2024-11-05T08:27:07Z",
    "merged_at": "2024-11-05T08:27:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2413"
  },
  {
    "number": 2410,
    "title": "update llm api reference page.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-04T17:14:32Z",
    "closed_at": "2024-11-05T06:01:36Z",
    "merged_at": "2024-11-05T06:01:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2410"
  },
  {
    "number": 2409,
    "title": "fix documents issues",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-04T06:26:41Z",
    "closed_at": "2024-11-04T07:10:33Z",
    "merged_at": "2024-11-04T07:10:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2409"
  },
  {
    "number": 2404,
    "title": "Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-01T12:29:19Z",
    "closed_at": "2024-11-01T12:31:16Z",
    "merged_at": "2024-11-01T12:31:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2404"
  },
  {
    "number": 2401,
    "title": "Update TensorRT-LLM v0.14.0",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-01T10:09:09Z",
    "closed_at": "2024-11-01T11:48:44Z",
    "merged_at": "2024-11-01T11:48:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2401"
  },
  {
    "number": 2397,
    "title": "th::optional -> std::optional",
    "user": "r-barnes",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-31T14:44:49Z",
    "closed_at": "2025-11-02T03:25:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2397"
  },
  {
    "number": 2394,
    "title": "add support internvl2",
    "user": "Jeremy-J-J",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-31T07:20:58Z",
    "closed_at": "2024-11-13T04:20:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2394"
  },
  {
    "number": 2391,
    "title": "Update the latest news",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-29T15:17:43Z",
    "closed_at": "2024-10-29T15:23:03Z",
    "merged_at": "2024-10-29T15:23:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2391"
  },
  {
    "number": 2389,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-29T12:25:21Z",
    "closed_at": "2024-10-29T14:24:39Z",
    "merged_at": "2024-10-29T14:24:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2389"
  },
  {
    "number": 2384,
    "title": "attention mechanism toggle added",
    "user": "Aaryanverma",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-28T06:35:32Z",
    "closed_at": "2025-08-19T19:46:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2384"
  },
  {
    "number": 2382,
    "title": "[None][fix] fix load_model_on_cpu on qwen/convert_checkpoint.py",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-27T09:52:51Z",
    "closed_at": "2025-09-19T04:54:26Z",
    "merged_at": "2025-09-19T04:54:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2382"
  },
  {
    "number": 2378,
    "title": "network: fix broken onnx export ",
    "user": "ishandhanani",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-25T20:37:39Z",
    "closed_at": "2024-11-13T04:22:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2378"
  },
  {
    "number": 2370,
    "title": "Fix errors when using smoothquant to quantize Qwen2 model",
    "user": "Missmiaom",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-24T11:22:57Z",
    "closed_at": "2025-06-05T20:04:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2370"
  },
  {
    "number": 2366,
    "title": "Allow for LoRA modules with different rank dimensions when using HF format",
    "user": "AlessioNetti",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-23T09:10:00Z",
    "closed_at": "2024-10-24T09:02:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2366"
  },
  {
    "number": 2363,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-22T12:17:54Z",
    "closed_at": "2024-10-22T12:27:35Z",
    "merged_at": "2024-10-22T12:27:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2363"
  },
  {
    "number": 2352,
    "title": "Passing gpt_variant to model conversion",
    "user": "tonylek",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-18T09:35:00Z",
    "closed_at": "2024-10-22T12:28:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2352"
  },
  {
    "number": 2343,
    "title": "Specify Llama 3.x information in example readme",
    "user": "laikhtewari",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-16T17:51:45Z",
    "closed_at": "2024-10-25T08:10:57Z",
    "merged_at": "2024-10-25T08:10:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2343"
  },
  {
    "number": 2333,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-15T07:15:31Z",
    "closed_at": "2024-10-15T07:28:40Z",
    "merged_at": "2024-10-15T07:28:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2333"
  },
  {
    "number": 2316,
    "title": "docs: clarify the slurm case",
    "user": "stas00",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-10T20:49:55Z",
    "closed_at": "2024-10-14T05:06:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2316"
  },
  {
    "number": 2315,
    "title": "Add missing headers for mpiUtils.h to compile with gcc13",
    "user": "mfuntowicz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-10T14:53:10Z",
    "closed_at": "2024-10-17T07:50:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2315"
  },
  {
    "number": 2297,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-08T09:58:39Z",
    "closed_at": "2024-10-08T10:19:19Z",
    "merged_at": "2024-10-08T10:19:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2297"
  },
  {
    "number": 2290,
    "title": "Fixed minor typo in advanced docs",
    "user": "SachinVarghese",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-05T22:19:37Z",
    "closed_at": "2024-10-14T05:07:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2290"
  },
  {
    "number": 2285,
    "title": "doc: add the missing BF16",
    "user": "stas00",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-03T20:57:00Z",
    "closed_at": "2024-10-14T05:07:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2285"
  },
  {
    "number": 2276,
    "title": "Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-30T15:05:34Z",
    "closed_at": "2024-09-30T17:28:29Z",
    "merged_at": "2024-09-30T17:28:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2276"
  },
  {
    "number": 2275,
    "title": "Add the known issue to windows installation guide",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-30T14:36:56Z",
    "closed_at": "2024-09-30T14:44:32Z",
    "merged_at": "2024-09-30T14:44:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2275"
  },
  {
    "number": 2273,
    "title": "Update TensorRT-LLM",
    "user": "DanBlanaru",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-30T11:27:44Z",
    "closed_at": "2024-09-30T11:51:20Z",
    "merged_at": "2024-09-30T11:51:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2273"
  },
  {
    "number": 2271,
    "title": "update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-30T09:16:16Z",
    "closed_at": "2024-09-30T09:25:23Z",
    "merged_at": "2024-09-30T09:25:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2271"
  },
  {
    "number": 2269,
    "title": "TensorRT-LLM v0.13 Update",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-30T07:54:43Z",
    "closed_at": "2024-09-30T08:20:23Z",
    "merged_at": "2024-09-30T08:20:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2269"
  },
  {
    "number": 2264,
    "title": "Fix errors when quantizing Llama model",
    "user": "lej970703",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-28T08:22:56Z",
    "closed_at": "2024-11-14T13:06:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2264"
  },
  {
    "number": 2259,
    "title": "fix: none prompt to string",
    "user": "dongs0104",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-26T07:12:33Z",
    "closed_at": "2024-12-04T16:20:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2259"
  },
  {
    "number": 2258,
    "title": "Bump version to `0.14.0.dev2024092401`",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-26T01:04:44Z",
    "closed_at": "2024-09-26T02:26:16Z",
    "merged_at": "2024-09-26T02:26:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2258"
  },
  {
    "number": 2253,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-24T14:59:54Z",
    "closed_at": "2024-09-24T15:27:32Z",
    "merged_at": "2024-09-24T15:27:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2253"
  },
  {
    "number": 2244,
    "title": "README.md: Add 3rd Party Inference Speed Dashboard",
    "user": "matichon-vultureprime",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-22T10:48:29Z",
    "closed_at": "2025-08-19T19:11:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2244"
  },
  {
    "number": 2243,
    "title": "fix: add support for passing calib sequence length, and num samples + fixing use of custom calibration dataset for smoothquant in llama",
    "user": "Bhuvanesh09",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-19T11:22:05Z",
    "closed_at": "2024-10-08T14:42:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2243"
  },
  {
    "number": 2234,
    "title": "Bump version to `0.14.0.dev2024091700`",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-18T00:46:07Z",
    "closed_at": "2024-09-18T00:58:36Z",
    "merged_at": "2024-09-18T00:58:36Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2234"
  },
  {
    "number": 2232,
    "title": "Fix check_share_embedding",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-17T16:01:25Z",
    "closed_at": "2024-09-26T08:16:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2232"
  },
  {
    "number": 2230,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-17T06:27:53Z",
    "closed_at": "2024-09-17T06:39:09Z",
    "merged_at": "2024-09-17T06:39:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2230"
  },
  {
    "number": 2219,
    "title": "Fix kv_cache_type issue",
    "user": "qingquansong",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-11T20:05:01Z",
    "closed_at": "2024-09-27T16:48:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2219"
  },
  {
    "number": 2215,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-10T10:00:06Z",
    "closed_at": "2024-09-10T10:21:22Z",
    "merged_at": "2024-09-10T10:21:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2215"
  },
  {
    "number": 2213,
    "title": "Modify small-batched weight only quantization",
    "user": "dasistwo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-10T04:38:57Z",
    "closed_at": "2025-11-02T03:25:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2213"
  },
  {
    "number": 2199,
    "title": "mv 'paged_kv_cache=paged_kv_cache' to 'kv_cache_type=KVCacheType.PAGED'",
    "user": "Jeremy-J-J",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-05T21:24:14Z",
    "closed_at": "2024-09-17T03:10:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2199"
  },
  {
    "number": 2191,
    "title": "Add module __repr__ methods",
    "user": "1ytic",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-04T17:01:13Z",
    "closed_at": "2024-09-27T16:48:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2191"
  },
  {
    "number": 2188,
    "title": "Fix extra-index-url for torch",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-03T19:55:33Z",
    "closed_at": "2024-10-14T05:07:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2188"
  },
  {
    "number": 2187,
    "title": "[examples/bert/build.py]: Load weights for BertModel and RobertaModel if `--model_dir` is provided",
    "user": "tkhanipov",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-03T12:05:44Z",
    "closed_at": "2025-04-13T10:35:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2187"
  },
  {
    "number": 2184,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-03T09:49:03Z",
    "closed_at": "2024-09-03T10:14:23Z",
    "merged_at": "2024-09-03T10:14:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2184"
  },
  {
    "number": 2182,
    "title": "Fix duplicated import module",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-03T02:08:38Z",
    "closed_at": "2024-09-17T06:42:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2182"
  },
  {
    "number": 2175,
    "title": "docs: Fix README",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-31T20:35:27Z",
    "closed_at": "2024-09-02T06:48:45Z",
    "merged_at": "2024-09-02T06:48:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2175"
  },
  {
    "number": 2169,
    "title": "Add blog for Tuning TensorRT-LLM for Optimal Serving",
    "user": "Sherlock113",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-30T04:31:10Z",
    "closed_at": "2024-09-14T06:26:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2169"
  },
  {
    "number": 2168,
    "title": "Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-29T15:42:51Z",
    "closed_at": "2024-08-30T05:09:14Z",
    "merged_at": "2024-08-30T05:09:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2168"
  },
  {
    "number": 2165,
    "title": "Add Windows library for release 0.12",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-29T09:43:39Z",
    "closed_at": "2024-08-29T15:00:20Z",
    "merged_at": "2024-08-29T15:00:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2165"
  },
  {
    "number": 2164,
    "title": "TensorRT-LLM v0.12 Update",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-29T09:10:36Z",
    "closed_at": "2024-08-29T09:25:07Z",
    "merged_at": "2024-08-29T09:25:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2164"
  },
  {
    "number": 2156,
    "title": "Update TensorRT-LLM",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-27T09:13:50Z",
    "closed_at": "2024-08-27T10:20:59Z",
    "merged_at": "2024-08-27T10:20:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2156"
  },
  {
    "number": 2154,
    "title": "Create sync.yml",
    "user": "inkimikoko",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-27T07:00:46Z",
    "closed_at": "2025-06-05T19:12:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2154"
  },
  {
    "number": 2152,
    "title": "Update model_weights_loader.py",
    "user": "wangkuiyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-25T15:13:36Z",
    "closed_at": "2024-09-17T06:43:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2152"
  },
  {
    "number": 2151,
    "title": "Update quick-start-guide.md",
    "user": "wangkuiyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-24T01:17:06Z",
    "closed_at": "2024-09-09T06:13:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2151"
  },
  {
    "number": 2146,
    "title": "Add workaround instruction for a known issue of v0.11 on Windows",
    "user": "pamelap-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-23T03:42:10Z",
    "closed_at": "2024-11-13T04:21:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2146"
  },
  {
    "number": 2130,
    "title": "Update TensorRT-LLM",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-20T10:23:40Z",
    "closed_at": "2024-08-20T10:55:15Z",
    "merged_at": "2024-08-20T10:55:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2130"
  },
  {
    "number": 2113,
    "title": "[Fix] Match exclude_modules pattern in convert_utils.py to quantize.py changes.",
    "user": "fjosw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-14T07:47:48Z",
    "closed_at": "2024-08-20T11:28:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2113"
  },
  {
    "number": 2110,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-13T13:13:25Z",
    "closed_at": "2024-08-13T14:34:33Z",
    "merged_at": "2024-08-13T14:34:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2110"
  },
  {
    "number": 2099,
    "title": "fix wrong buffer for `oneShotAllReduceKernel` under `PUSH_MODE`",
    "user": "YconquestY",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-08T08:23:40Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2099"
  },
  {
    "number": 2097,
    "title": "Fix the workspace size calculation for quantization plugins",
    "user": "ZhangGe6",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-07T15:47:42Z",
    "closed_at": "2024-10-14T05:08:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2097"
  },
  {
    "number": 2094,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-07T00:45:56Z",
    "closed_at": "2024-08-07T08:44:44Z",
    "merged_at": "2024-08-07T08:44:44Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2094"
  },
  {
    "number": 2086,
    "title": "Merge branch 'NVIDIA-main' into mlp",
    "user": "dasistwo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-05T10:48:25Z",
    "closed_at": "2024-08-05T10:48:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2086"
  },
  {
    "number": 2085,
    "title": "INT8 kv cache dequant SCALE_QP_INSTEAD_OF_KV",
    "user": "lishicheng1996",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-05T08:09:44Z",
    "closed_at": "2025-11-03T03:27:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2085"
  },
  {
    "number": 2081,
    "title": "Include use_fused_mlp when constructing BuildConfig from dict",
    "user": "ethnzhng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-02T18:59:25Z",
    "closed_at": "2024-10-14T05:08:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2081"
  },
  {
    "number": 2077,
    "title": "Remove unnecessary space in install_requirements.sh",
    "user": "KeitaW",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-01T06:33:39Z",
    "closed_at": "2024-09-27T16:47:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2077"
  },
  {
    "number": 2075,
    "title": "typo fix quick-start-guide.md",
    "user": "sweetning0809",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-01T05:41:48Z",
    "closed_at": "2024-11-13T04:27:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2075"
  },
  {
    "number": 2070,
    "title": "fix GemmFpAIntB MMa::IteratorB::Layout",
    "user": "luliyucoordinate",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-31T13:27:49Z",
    "closed_at": "2025-08-06T16:15:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2070"
  },
  {
    "number": 2065,
    "title": "[Bugfix] remove un-need calls for mark_weights_refittable",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-31T08:48:21Z",
    "closed_at": "2024-07-31T10:15:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2065"
  },
  {
    "number": 2057,
    "title": "fix wrong arg in Engine Building Command in docs/source/performance/perf-overview.md",
    "user": "RuibaiXu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-30T15:43:32Z",
    "closed_at": "2025-05-28T05:32:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2057"
  },
  {
    "number": 2056,
    "title": "[Fix] Propagate QuantConfig.exclude_modules to weight only quantization",
    "user": "fjosw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-30T15:26:52Z",
    "closed_at": "2024-08-13T09:09:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2056"
  },
  {
    "number": 2053,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-30T13:20:45Z",
    "closed_at": "2024-07-30T13:25:01Z",
    "merged_at": "2024-07-30T13:25:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2053"
  },
  {
    "number": 2049,
    "title": "fix wrong arg in Engine Building Command in docs/source/performance/perf-overview.md",
    "user": "RuibaiXu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-30T07:49:13Z",
    "closed_at": "2024-07-30T15:32:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2049"
  },
  {
    "number": 2039,
    "title": "Fix segfault in TopP sampling layer",
    "user": "akhoroshev",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-28T07:05:10Z",
    "closed_at": "2024-09-27T16:49:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2039"
  },
  {
    "number": 2033,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-26T08:17:22Z",
    "closed_at": "2024-07-26T08:19:24Z",
    "merged_at": "2024-07-26T08:19:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2033"
  },
  {
    "number": 2028,
    "title": "update links in overview section of README",
    "user": "Tayef-Shah",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-25T19:46:57Z",
    "closed_at": "2024-09-27T16:49:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2028"
  },
  {
    "number": 2016,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-24T11:44:39Z",
    "closed_at": "2024-07-24T11:50:28Z",
    "merged_at": "2024-07-24T11:50:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2016"
  },
  {
    "number": 2012,
    "title": "Update README",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-24T01:23:09Z",
    "closed_at": "2024-07-24T01:31:28Z",
    "merged_at": "2024-07-24T01:31:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2012"
  },
  {
    "number": 2010,
    "title": "Update TensorRT-LLM",
    "user": "dongxuy04",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-23T21:45:41Z",
    "closed_at": "2024-07-23T21:48:06Z",
    "merged_at": "2024-07-23T21:48:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2010"
  },
  {
    "number": 2008,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-23T15:01:02Z",
    "closed_at": "2024-07-23T15:05:09Z",
    "merged_at": "2024-07-23T15:05:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2008"
  },
  {
    "number": 2000,
    "title": "Add support for interleaved moe",
    "user": "Macchiato123000",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-22T22:09:00Z",
    "closed_at": "2024-09-27T16:38:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2000"
  },
  {
    "number": 1987,
    "title": "fix auto parallel cluster info typo",
    "user": "saeyoonoh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-19T12:49:50Z",
    "closed_at": "2024-07-24T05:54:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1987"
  },
  {
    "number": 1979,
    "title": "Update gh-pages for windows part doc.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-18T02:49:05Z",
    "closed_at": "2024-07-18T03:18:09Z",
    "merged_at": "2024-07-18T03:18:09Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1979"
  },
  {
    "number": 1977,
    "title": "Update gh-pages for windows part doc.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-18T02:15:59Z",
    "closed_at": "2024-07-18T02:17:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1977"
  },
  {
    "number": 1975,
    "title": "fix : v0.11 windows docs",
    "user": "tp5uiuc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-17T16:34:01Z",
    "closed_at": "2024-07-18T01:53:56Z",
    "merged_at": "2024-07-18T01:53:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1975"
  },
  {
    "number": 1973,
    "title": "Update gh-pages for pic hyperlinks.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-17T14:55:26Z",
    "closed_at": "2024-07-17T14:57:46Z",
    "merged_at": "2024-07-17T14:57:46Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1973"
  },
  {
    "number": 1972,
    "title": "Update gh-pages.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-17T14:29:53Z",
    "closed_at": "2024-07-17T14:32:55Z",
    "merged_at": "2024-07-17T14:32:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1972"
  },
  {
    "number": 1971,
    "title": "Update gh-pages for 0.11 release.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-17T13:17:48Z",
    "closed_at": "2024-07-17T13:29:04Z",
    "merged_at": "2024-07-17T13:29:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1971"
  },
  {
    "number": 1969,
    "title": "TensorRT-LLM v0.11 Update",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-17T12:21:36Z",
    "closed_at": "2024-07-17T12:45:02Z",
    "merged_at": "2024-07-17T12:45:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1969"
  },
  {
    "number": 1966,
    "title": "Update the latest news",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-17T08:00:12Z",
    "closed_at": "2024-07-17T12:39:41Z",
    "merged_at": "2024-07-17T12:39:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1966"
  },
  {
    "number": 1958,
    "title": "Fixed == -> = typo",
    "user": "mrdrprofuroboros",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-16T18:32:08Z",
    "closed_at": "2024-07-25T06:41:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1958"
  },
  {
    "number": 1954,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-16T06:44:19Z",
    "closed_at": "2024-07-16T07:30:25Z",
    "merged_at": "2024-07-16T07:30:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1954"
  },
  {
    "number": 1952,
    "title": "Feat tiktoken integration",
    "user": "nguyenhoangthuan99",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-16T06:12:37Z",
    "closed_at": "2024-07-16T06:13:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1952"
  },
  {
    "number": 1939,
    "title": "chore(docs): fix typos",
    "user": "lfz941",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-12T03:20:41Z",
    "closed_at": "2024-09-27T16:47:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1939"
  },
  {
    "number": 1937,
    "title": "chore: remove duplicate flag",
    "user": "hattizai",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-12T02:48:35Z",
    "closed_at": "2024-07-25T06:41:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1937"
  },
  {
    "number": 1936,
    "title": "Correct the version",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-12T02:18:22Z",
    "closed_at": "2024-10-09T03:14:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1936"
  },
  {
    "number": 1935,
    "title": "Fix default min length",
    "user": "akhoroshev",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-11T13:32:52Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1935"
  },
  {
    "number": 1927,
    "title": "Add support for custom tokenizer and batch size",
    "user": "uppalutkarsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-09T14:34:22Z",
    "closed_at": "2025-08-06T16:15:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1927"
  },
  {
    "number": 1926,
    "title": "Add support for falcon2",
    "user": "puneeshkhanna",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-09T13:16:09Z",
    "closed_at": "2024-10-14T01:21:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1926"
  },
  {
    "number": 1918,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-09T06:18:00Z",
    "closed_at": "2024-07-09T06:42:22Z",
    "merged_at": "2024-07-09T06:42:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1918"
  },
  {
    "number": 1909,
    "title": "add `chunk_length` parameter to Whisper",
    "user": "MahmoudAshraf97",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-07T19:24:18Z",
    "closed_at": "2024-08-28T12:27:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1909"
  },
  {
    "number": 1902,
    "title": "Update setup_build_env.ps1",
    "user": "nero-dv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-05T11:38:25Z",
    "closed_at": "2024-09-29T10:48:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1902"
  },
  {
    "number": 1897,
    "title": "Support gelu_pytorch_tanh activation function",
    "user": "ttim",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-04T20:56:44Z",
    "closed_at": "2024-07-25T06:42:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1897"
  },
  {
    "number": 1891,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-04T06:28:57Z",
    "closed_at": "2024-07-04T06:37:20Z",
    "merged_at": "2024-07-04T06:37:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1891"
  },
  {
    "number": 1880,
    "title": "Dev sm87 trt101",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-03T02:50:49Z",
    "closed_at": "2025-11-01T03:22:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1880"
  },
  {
    "number": 1854,
    "title": "commit",
    "user": "Abhi20003",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-27T11:04:15Z",
    "closed_at": "2024-06-27T11:04:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1854"
  },
  {
    "number": 1851,
    "title": "Add FAST_BUILD comment at #endif",
    "user": "lkm2835",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-27T07:43:05Z",
    "closed_at": "2024-06-28T06:26:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1851"
  },
  {
    "number": 1836,
    "title": "Bump pillow from 10.2.0 to 10.3.0",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-06-25T13:13:28Z",
    "closed_at": "2024-07-04T06:38:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1836"
  },
  {
    "number": 1835,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-25T11:51:43Z",
    "closed_at": "2024-06-25T13:10:31Z",
    "merged_at": "2024-06-25T13:10:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1835"
  },
  {
    "number": 1834,
    "title": "support remove_input_padding for BertForSequenceClassification models",
    "user": "Altair-Alpha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-25T11:30:59Z",
    "closed_at": "2024-07-16T09:47:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1834"
  },
  {
    "number": 1815,
    "title": "[ModelRunner] Fix stop and bad words list contiguous for offsets",
    "user": "Marks101",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-20T09:32:44Z",
    "closed_at": "2024-07-04T15:21:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1815"
  },
  {
    "number": 1811,
    "title": "Update install_requirements.sh",
    "user": "KantaHayashiAI",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-19T18:13:05Z",
    "closed_at": "2024-06-23T23:58:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1811"
  },
  {
    "number": 1799,
    "title": "[GEMMA] `from_hugging_face` not setting `share_embedding_table` to True leading to incapacity to load Gemma",
    "user": "mfuntowicz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-18T14:55:22Z",
    "closed_at": "2024-06-25T13:24:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1799"
  },
  {
    "number": 1793,
    "title": "Update TensorRT-LLM",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-18T09:21:15Z",
    "closed_at": "2024-06-18T10:18:23Z",
    "merged_at": "2024-06-18T10:18:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1793"
  },
  {
    "number": 1769,
    "title": "Release/0.10.0",
    "user": "WilliamEricCheung",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-12T03:03:00Z",
    "closed_at": "2024-06-12T07:04:18Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1769"
  },
  {
    "number": 1763,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-11T07:00:12Z",
    "closed_at": "2024-06-11T08:59:02Z",
    "merged_at": "2024-06-11T08:59:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1763"
  },
  {
    "number": 1762,
    "title": "Support custom calibration datasets",
    "user": "DreamGenX",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-11T04:38:46Z",
    "closed_at": "2024-06-13T14:23:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1762"
  },
  {
    "number": 1758,
    "title": "DeepSeek MoE support",
    "user": "akhoroshev",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-09T18:05:31Z",
    "closed_at": "2024-09-25T02:32:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1758"
  },
  {
    "number": 1751,
    "title": "Bump gradio from 4.19.2 to 4.36.0 in /examples/qwen",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-06-06T22:20:17Z",
    "closed_at": "2024-06-15T13:49:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1751"
  },
  {
    "number": 1742,
    "title": "Reference input randomSeeds by idx rather than batchSlot",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-05T16:44:10Z",
    "closed_at": "2024-06-12T21:17:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1742"
  },
  {
    "number": 1737,
    "title": "Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-05T13:55:30Z",
    "closed_at": "2024-06-05T13:59:38Z",
    "merged_at": "2024-06-05T13:59:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1737"
  },
  {
    "number": 1734,
    "title": "TensorRT-LLM v0.10 update",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-05T12:19:30Z",
    "closed_at": "2024-06-05T12:43:25Z",
    "merged_at": "2024-06-05T12:43:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1734"
  },
  {
    "number": 1725,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-04T12:01:28Z",
    "closed_at": "2024-06-04T12:26:32Z",
    "merged_at": "2024-06-04T12:26:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1725"
  },
  {
    "number": 1723,
    "title": "Fix pre-norm weight conversion for nmt",
    "user": "Pzzzzz5142",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-04T08:27:52Z",
    "closed_at": "2024-06-06T11:48:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1723"
  },
  {
    "number": 1712,
    "title": "fix SmoothQuantGatedMLP ffn_hidden_size bug",
    "user": "michael200892458",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-03T02:42:40Z",
    "closed_at": "2024-06-05T09:44:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1712"
  },
  {
    "number": 1708,
    "title": "fix:missing kwargs in ModelRunner.generate()",
    "user": "lausannel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-31T18:15:11Z",
    "closed_at": "2024-05-31T18:16:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1708"
  },
  {
    "number": 1689,
    "title": "Bump transformers from 4.36.2 to 4.38.0 in /examples/multimodal",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-05-28T12:08:43Z",
    "closed_at": "2025-04-26T07:58:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1689"
  },
  {
    "number": 1688,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-28T10:20:02Z",
    "closed_at": "2024-05-28T12:07:49Z",
    "merged_at": "2024-05-28T12:07:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1688"
  },
  {
    "number": 1685,
    "title": "add cached generation buffer",
    "user": "michael200892458",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-28T06:31:08Z",
    "closed_at": "2025-11-01T03:22:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1685"
  },
  {
    "number": 1679,
    "title": "Fix missing medusa arg in run_profiling",
    "user": "PanZaifeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-26T14:17:17Z",
    "closed_at": "2024-05-27T11:27:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1679"
  },
  {
    "number": 1674,
    "title": "Add Huggingface model zoo from community",
    "user": "matichon-vultureprime",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-25T09:47:45Z",
    "closed_at": "2024-06-03T12:04:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1674"
  },
  {
    "number": 1669,
    "title": "Fixed rslora scaling in lora_manager",
    "user": "TheCodeWrangler",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-24T19:55:26Z",
    "closed_at": "2024-06-03T12:18:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1669"
  },
  {
    "number": 1660,
    "title": "Fix nmt weight conversion",
    "user": "Pzzzzz5142",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-24T05:41:55Z",
    "closed_at": "2024-05-28T12:11:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1660"
  },
  {
    "number": 1655,
    "title": "Make Executor timeout configurable",
    "user": "DreamGenX",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-23T09:27:22Z",
    "closed_at": "2024-06-10T12:03:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1655"
  },
  {
    "number": 1650,
    "title": "Fix llama conversion with smooth quant",
    "user": "lopuhin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-22T18:52:19Z",
    "closed_at": "2024-05-28T12:32:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1650"
  },
  {
    "number": 1646,
    "title": "Optimize python benchmark logging",
    "user": "michaelnny",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-22T10:06:51Z",
    "closed_at": "2024-09-01T11:22:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1646"
  },
  {
    "number": 1640,
    "title": "Bump gradio from 3.40.1 to 4.19.2 in /examples/qwen",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-05-21T09:52:13Z",
    "closed_at": "2024-06-03T12:06:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1640"
  },
  {
    "number": 1639,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-21T09:30:22Z",
    "closed_at": "2024-05-21T09:51:02Z",
    "merged_at": "2024-05-21T09:51:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1639"
  },
  {
    "number": 1637,
    "title": "fix up qkv.bias error when use qwen1.5-32b-gptq-int4",
    "user": "Tlntin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-21T06:18:32Z",
    "closed_at": "2024-06-04T03:06:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1637"
  },
  {
    "number": 1629,
    "title": "Fix CUDA OOM when creating Mixtral checkpoint",
    "user": "VivekBits2210",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-19T21:00:25Z",
    "closed_at": "2025-06-12T22:05:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1629"
  },
  {
    "number": 1621,
    "title": "Fix some issue when build whl for windows",
    "user": "shizidushu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-17T02:40:38Z",
    "closed_at": "2024-06-05T09:56:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1621"
  },
  {
    "number": 1615,
    "title": "enable medusa int8 weight only quantization",
    "user": "XiaobingSuper",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-16T06:47:25Z",
    "closed_at": "2024-06-05T09:53:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1615"
  },
  {
    "number": 1611,
    "title": "Add support for non-power-of-two heads with Alibi",
    "user": "vmarkovtsev",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-15T18:42:54Z",
    "closed_at": "2024-10-14T05:17:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1611"
  },
  {
    "number": 1598,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-14T08:25:26Z",
    "closed_at": "2024-05-14T08:43:41Z",
    "merged_at": "2024-05-14T08:43:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1598"
  },
  {
    "number": 1587,
    "title": "Fix missing link in perf-best-practices.md",
    "user": "bloodeagle40234",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-13T09:52:10Z",
    "closed_at": "2024-05-28T01:25:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1587"
  },
  {
    "number": 1583,
    "title": "Fix the error of Ada traits for fpA_intB.",
    "user": "JamesTheZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-12T11:00:26Z",
    "closed_at": "2024-06-03T12:05:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1583"
  },
  {
    "number": 1568,
    "title": "[feat]: Support weight only gemm with 2bit",
    "user": "gavinchen430",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-09T11:05:46Z",
    "closed_at": "2025-06-05T20:03:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1568"
  },
  {
    "number": 1558,
    "title": "Update customAllReduceKernels.cu - line 120's typo was edited",
    "user": "sjbae1999",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-08T04:07:32Z",
    "closed_at": "2024-05-20T09:56:57Z",
    "merged_at": "2024-05-20T09:56:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1558"
  },
  {
    "number": 1554,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-07T12:56:36Z",
    "closed_at": "2024-05-07T15:34:28Z",
    "merged_at": "2024-05-07T15:34:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1554"
  },
  {
    "number": 1551,
    "title": "Use cls variable instead of ModelRunner",
    "user": "bloodeagle40234",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-07T10:10:27Z",
    "closed_at": "2024-05-15T10:46:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1551"
  },
  {
    "number": 1545,
    "title": "Update perf-best-practices.md",
    "user": "sam-india-007",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-06T10:17:05Z",
    "closed_at": "2024-05-20T10:05:14Z",
    "merged_at": "2024-05-20T10:05:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1545"
  },
  {
    "number": 1537,
    "title": "[fix] export failure with CUDA driver < 526 and pynvml>=11.5.0",
    "user": "CoderHam",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-03T00:36:23Z",
    "closed_at": "2024-05-28T12:31:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1537"
  },
  {
    "number": 1536,
    "title": "Use first bad_words as extra parameters, and implement min-p",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-02T23:26:23Z",
    "closed_at": "2025-02-07T07:47:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1536"
  },
  {
    "number": 1535,
    "title": "Loading Medusa Safetensors + AWQ Conversion correction",
    "user": "Tushar-ml",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-02T08:13:07Z",
    "closed_at": "2024-06-03T11:59:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1535"
  },
  {
    "number": 1534,
    "title": "Define hf_config explisitly for convert_hf_mpt_legacy",
    "user": "bloodeagle40234",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-02T05:39:59Z",
    "closed_at": "2024-06-05T09:53:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1534"
  },
  {
    "number": 1530,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-30T08:07:18Z",
    "closed_at": "2024-04-30T09:19:10Z",
    "merged_at": "2024-04-30T09:19:10Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1530"
  },
  {
    "number": 1522,
    "title": "Add note on build Llama v3",
    "user": "sammcj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-29T06:25:19Z",
    "closed_at": "2024-06-05T23:19:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1522"
  },
  {
    "number": 1521,
    "title": "Update perf-overview.md",
    "user": "snowmanwwg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-29T06:17:21Z",
    "closed_at": "2024-05-28T01:28:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1521"
  },
  {
    "number": 1514,
    "title": "Support SDXL and its distributed inference",
    "user": "Zars19",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-28T14:50:02Z",
    "closed_at": "2024-11-22T03:42:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1514"
  },
  {
    "number": 1508,
    "title": "Remove the <s> token from post_prompt of multimodal",
    "user": "yupbank",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-26T23:12:55Z",
    "closed_at": "2025-11-01T03:22:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1508"
  },
  {
    "number": 1495,
    "title": "fix: correct cudaSetDevice error when GPUs per node are fewer than their ranks in inter-node inference",
    "user": "littlefatfat",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-24T10:00:22Z",
    "closed_at": "2025-04-28T09:38:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1495"
  },
  {
    "number": 1492,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-24T06:26:21Z",
    "closed_at": "2024-04-24T06:44:22Z",
    "merged_at": "2024-04-24T06:44:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1492"
  },
  {
    "number": 1486,
    "title": "[ModelRunner] Fix stop & bad word list pointer offset.",
    "user": "fjosw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-22T14:32:51Z",
    "closed_at": "2024-05-22T15:17:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1486"
  },
  {
    "number": 1464,
    "title": "Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T06:38:47Z",
    "closed_at": "2024-04-17T06:59:33Z",
    "merged_at": "2024-04-17T06:59:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1464"
  },
  {
    "number": 1462,
    "title": "Fix perf-overview.md",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T04:52:26Z",
    "closed_at": "2024-04-17T05:00:20Z",
    "merged_at": "2024-04-17T05:00:20Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1462"
  },
  {
    "number": 1461,
    "title": "Update documents for release 0.9",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T03:40:12Z",
    "closed_at": "2024-04-17T03:51:50Z",
    "merged_at": "2024-04-17T03:51:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1461"
  },
  {
    "number": 1460,
    "title": "Fix llama2 convert_checkpoint.py --load_model_on_cpu",
    "user": "aikitoria",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T02:23:50Z",
    "closed_at": "2024-04-30T13:07:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1460"
  },
  {
    "number": 1455,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-16T11:28:04Z",
    "closed_at": "2024-04-16T11:40:08Z",
    "merged_at": "2024-04-16T11:40:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1455"
  },
  {
    "number": 1445,
    "title": "Update TensorRT-LLM Release branch",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-12T08:40:52Z",
    "closed_at": "2024-04-12T09:59:19Z",
    "merged_at": "2024-04-12T09:59:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1445"
  },
  {
    "number": 1435,
    "title": "Update model_runner_cpp.py",
    "user": "RoyHe",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-09T15:02:23Z",
    "closed_at": "2024-05-15T10:06:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1435"
  },
  {
    "number": 1427,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-09T07:22:13Z",
    "closed_at": "2024-04-09T09:03:34Z",
    "merged_at": "2024-04-09T09:03:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1427"
  },
  {
    "number": 1406,
    "title": "Update summarize.py",
    "user": "focusunsink",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-06T17:00:51Z",
    "closed_at": "2024-05-15T10:05:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1406"
  },
  {
    "number": 1403,
    "title": "serialize rotary base to config",
    "user": "tonylek",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-04T15:32:29Z",
    "closed_at": "2024-05-15T10:10:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1403"
  },
  {
    "number": 1392,
    "title": "Support internlm2",
    "user": "RunningLeon",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-02T10:01:19Z",
    "closed_at": "2024-06-03T12:17:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1392"
  },
  {
    "number": 1387,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T08:27:03Z",
    "closed_at": "2024-04-01T08:39:43Z",
    "merged_at": "2024-04-01T08:39:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1387"
  },
  {
    "number": 1385,
    "title": "llama convert add rotary_scaling param in cli_args",
    "user": "activezhao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T03:09:25Z",
    "closed_at": "2025-05-27T23:49:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1385"
  },
  {
    "number": 1373,
    "title": "[Doc] Fix mistral v0.1 build instructions",
    "user": "minwhoo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T07:31:54Z",
    "closed_at": "2024-05-20T10:16:02Z",
    "merged_at": "2024-05-20T10:16:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1373"
  },
  {
    "number": 1371,
    "title": "add time stamp print function to generation.py",
    "user": "seethon",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T00:28:05Z",
    "closed_at": "2024-04-26T17:03:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1371"
  },
  {
    "number": 1366,
    "title": "Add SmoothQuant for T5 (decoder only right now)",
    "user": "eycheung",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T21:50:43Z",
    "closed_at": "2025-08-18T20:23:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1366"
  },
  {
    "number": 1364,
    "title": "\tmodified:   3rdparty/cutlass",
    "user": "Wladoo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T17:23:32Z",
    "closed_at": "2024-03-27T17:23:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1364"
  },
  {
    "number": 1363,
    "title": "Support for `DBRX`",
    "user": "megha95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T14:48:11Z",
    "closed_at": "2024-04-09T09:06:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1363"
  },
  {
    "number": 1358,
    "title": "Update TensorRT-LLM",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-26T12:20:31Z",
    "closed_at": "2024-03-26T12:47:14Z",
    "merged_at": "2024-03-26T12:47:14Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1358"
  },
  {
    "number": 1347,
    "title": "support qwen2",
    "user": "RobotGF",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T02:26:21Z",
    "closed_at": "2024-03-26T08:59:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1347"
  },
  {
    "number": 1346,
    "title": "Relax python dependencies",
    "user": "tdeboissiere",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-24T22:41:16Z",
    "closed_at": "2025-03-31T07:09:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1346"
  },
  {
    "number": 1337,
    "title": "[feat]: Add Option to convert and run distil-whisper large-v3",
    "user": "IbrahimAmin1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-22T19:47:00Z",
    "closed_at": "2024-05-28T12:29:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1337"
  },
  {
    "number": 1329,
    "title": "Fix top_k type (float => int32) executor.py",
    "user": "vonjackustc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-21T11:58:56Z",
    "closed_at": "2024-05-21T09:52:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1329"
  },
  {
    "number": 1328,
    "title": "Fix typo in examples/whisper, Fix examples/whisper/run_faster_whisper.py",
    "user": "Pzzzzz5142",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-21T07:05:16Z",
    "closed_at": "2024-05-28T12:10:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1328"
  },
  {
    "number": 1327,
    "title": "[WIP]Add download model from www.modelscope.cn build TensorRT-LLM model.",
    "user": "liuyhwangyh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-21T01:26:48Z",
    "closed_at": "2024-03-24T01:56:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1327"
  },
  {
    "number": 1325,
    "title": "Feature/centos7 build fix",
    "user": "akhoroshev",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T19:39:50Z",
    "closed_at": "2024-03-22T08:07:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1325"
  },
  {
    "number": 1315,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-19T09:25:59Z",
    "closed_at": "2024-03-19T09:36:43Z",
    "merged_at": "2024-03-19T09:36:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1315"
  },
  {
    "number": 1296,
    "title": "Fix examples/server.py returning only one token",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T19:46:33Z",
    "closed_at": "2024-05-21T07:48:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1296"
  },
  {
    "number": 1295,
    "title": "Fix assertion in engine/executor by using TransformersTokenizer",
    "user": "pathorn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T19:46:25Z",
    "closed_at": "2024-05-21T07:50:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1295"
  },
  {
    "number": 1281,
    "title": "Update precision.md",
    "user": "BasicCoder",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-12T12:24:59Z",
    "closed_at": "2024-05-21T07:51:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1281"
  },
  {
    "number": 1278,
    "title": "Chore: Update README",
    "user": "hahuyhoang411",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-12T08:57:22Z",
    "closed_at": "2024-03-12T08:58:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1278"
  },
  {
    "number": 1275,
    "title": "Update links to Nitro Download and LlamaCorn Model Engine",
    "user": "dan-menlo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-12T08:02:42Z",
    "closed_at": "2024-03-12T08:02:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1275"
  },
  {
    "number": 1274,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-12T07:54:18Z",
    "closed_at": "2024-03-12T10:15:52Z",
    "merged_at": "2024-03-12T10:15:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1274"
  },
  {
    "number": 1270,
    "title": "Fix embedding weights for NMT's datatype",
    "user": "Dao007forever",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-11T20:25:49Z",
    "closed_at": "2024-05-21T08:03:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1270"
  },
  {
    "number": 1263,
    "title": "docs: add inflight batching support to llama doc",
    "user": "wxsms",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-11T06:53:47Z",
    "closed_at": "2024-06-04T03:11:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1263"
  },
  {
    "number": 1259,
    "title": "docs: Integrate with jan",
    "user": "freelerobot",
    "user_type": "User",
    "is_human": false,
    "created_at": "2024-03-08T14:37:01Z",
    "closed_at": "2024-03-08T14:37:17Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1259"
  },
  {
    "number": 1258,
    "title": "Fix for runing python from folders containing spaces e.g \"Program Fil…",
    "user": "jonny2027",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-08T08:38:16Z",
    "closed_at": "2024-05-28T01:36:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1258"
  },
  {
    "number": 1257,
    "title": "WIP: First Draft of README for nitro-tensorrt-llm",
    "user": "dan-menlo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-08T06:29:39Z",
    "closed_at": "2024-03-08T06:30:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1257"
  },
  {
    "number": 1248,
    "title": "Update requirements.txt",
    "user": "ngoanpv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-07T04:27:15Z",
    "closed_at": "2024-06-03T12:03:18Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1248"
  },
  {
    "number": 1238,
    "title": "Adding debug options to trtllm-build to visualize the TRT Network before Engine build",
    "user": "Lokiiiiii",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-06T00:18:31Z",
    "closed_at": "2024-06-05T09:48:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1238"
  },
  {
    "number": 1233,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-05T07:20:28Z",
    "closed_at": "2024-03-05T10:32:54Z",
    "merged_at": "2024-03-05T10:32:54Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1233"
  },
  {
    "number": 1230,
    "title": "TensorRT-llm nitro",
    "user": "tikikun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-05T01:54:46Z",
    "closed_at": "2024-03-05T01:55:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1230"
  },
  {
    "number": 1202,
    "title": "Add 0.8 batch manager static lib for Windows ",
    "user": "tp5uiuc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T22:16:13Z",
    "closed_at": "2024-03-01T02:44:05Z",
    "merged_at": "2024-03-01T02:44:05Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1202"
  },
  {
    "number": 1196,
    "title": "Update gh-pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T12:55:15Z",
    "closed_at": "2024-02-29T12:56:27Z",
    "merged_at": "2024-02-29T12:56:26Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1196"
  },
  {
    "number": 1192,
    "title": "Update TensorRT-LLM Release branch",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T09:14:13Z",
    "closed_at": "2024-02-29T09:20:56Z",
    "merged_at": "2024-02-29T09:20:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1192"
  },
  {
    "number": 1177,
    "title": "fix: wrong request processing order",
    "user": "prnake",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-28T05:17:38Z",
    "closed_at": "2025-08-08T16:33:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1177"
  },
  {
    "number": 1168,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-27T07:57:13Z",
    "closed_at": "2024-02-27T09:37:34Z",
    "merged_at": "2024-02-27T09:37:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1168"
  },
  {
    "number": 1157,
    "title": "Fix Server",
    "user": "nivibilla",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-25T22:12:12Z",
    "closed_at": "2024-03-17T10:15:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1157"
  },
  {
    "number": 1148,
    "title": "Specify the head_size from the config when importing Gemma from Hugging Face.",
    "user": "mfuntowicz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-23T16:30:45Z",
    "closed_at": "2024-06-04T03:07:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1148"
  },
  {
    "number": 1147,
    "title": "Make Gemma importable from `transformers` Gemma implementation",
    "user": "mfuntowicz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-23T16:25:51Z",
    "closed_at": "2024-06-04T03:08:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1147"
  },
  {
    "number": 1146,
    "title": "For issue 1145",
    "user": "a5hwinjs",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-23T16:23:00Z",
    "closed_at": "2024-02-27T14:09:31Z",
    "merged_at": "2024-02-27T14:09:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1146"
  },
  {
    "number": 1142,
    "title": "ModelRunnerCpp run fails with `gather_context_logits`",
    "user": "megha95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-23T06:59:00Z",
    "closed_at": "2024-03-25T19:49:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1142"
  },
  {
    "number": 1136,
    "title": "Bump gradio from 3.40.1 to 4.19.2 in /examples/qwen",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-02-22T22:13:58Z",
    "closed_at": "2024-03-01T01:45:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1136"
  },
  {
    "number": 1128,
    "title": "Update gemma README",
    "user": "tp5uiuc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-22T00:22:20Z",
    "closed_at": "2024-02-22T01:26:04Z",
    "merged_at": "2024-02-22T01:26:04Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1128"
  },
  {
    "number": 1126,
    "title": "Update README.md",
    "user": "byshiue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-21T13:55:55Z",
    "closed_at": "2024-02-21T13:59:16Z",
    "merged_at": "2024-02-21T13:59:16Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1126"
  },
  {
    "number": 1122,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-21T11:25:40Z",
    "closed_at": "2024-02-21T13:30:55Z",
    "merged_at": "2024-02-21T13:30:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1122"
  },
  {
    "number": 1115,
    "title": "modify for main_0f041b7b57_jetson",
    "user": "sunnyqgg",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-20T14:14:23Z",
    "closed_at": "2025-05-28T00:01:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1115"
  },
  {
    "number": 1107,
    "title": "Run with respective int4 weights",
    "user": "spoonbobo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-20T03:36:18Z",
    "closed_at": "2024-05-12T14:34:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1107"
  },
  {
    "number": 1098,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-18T06:35:24Z",
    "closed_at": "2024-02-18T07:48:08Z",
    "merged_at": "2024-02-18T07:48:08Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1098"
  },
  {
    "number": 1096,
    "title": "Substitute deprecated nvidia-docker",
    "user": "HeAndres",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-17T14:46:16Z",
    "closed_at": "2024-02-26T03:08:49Z",
    "merged_at": "2024-02-26T03:08:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1096"
  },
  {
    "number": 1094,
    "title": "fix import error in parallel build",
    "user": "llan-ml",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-17T07:43:01Z",
    "closed_at": "2025-06-05T19:51:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1094"
  },
  {
    "number": 1091,
    "title": "moe router tp removed",
    "user": "megha95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-16T21:43:42Z",
    "closed_at": "2024-03-12T23:50:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1091"
  },
  {
    "number": 1089,
    "title": "Small update to benchmarking build docs to correct usage",
    "user": "julianmack",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-16T09:45:14Z",
    "closed_at": "2024-06-05T09:50:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1089"
  },
  {
    "number": 1076,
    "title": "[fix] avoid the overflow issue when supporting 32k sequence length",
    "user": "llsj14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-11T12:03:48Z",
    "closed_at": "2025-02-22T08:59:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1076"
  },
  {
    "number": 1075,
    "title": "update einops in mpt requirements script",
    "user": "hllj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-10T14:38:46Z",
    "closed_at": "2025-06-05T19:51:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1075"
  },
  {
    "number": 1073,
    "title": "Update README.md account for new cuDNN version - installation instruction only works with Archive version",
    "user": "ewandel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-09T21:44:20Z",
    "closed_at": "2024-06-13T00:57:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1073"
  },
  {
    "number": 1061,
    "title": "Adding distil-whisper model support to TensorRT-LLM",
    "user": "Bhuvanesh09",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-07T03:29:00Z",
    "closed_at": "2024-04-16T04:32:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1061"
  },
  {
    "number": 1055,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-06T10:28:45Z",
    "closed_at": "2024-02-06T10:38:07Z",
    "merged_at": "2024-02-06T10:38:07Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1055"
  },
  {
    "number": 1051,
    "title": "[Fix] Explicitly check if output['generation_logits'] is an empty list",
    "user": "fjosw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-05T12:14:19Z",
    "closed_at": "2024-03-08T12:06:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1051"
  },
  {
    "number": 1050,
    "title": "Update README.md",
    "user": "MustaphaU",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-05T07:43:58Z",
    "closed_at": "2024-06-13T01:06:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1050"
  },
  {
    "number": 1032,
    "title": "Automate cuDNN setup",
    "user": "Muhtasham",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T16:27:14Z",
    "closed_at": "2025-07-08T12:47:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1032"
  },
  {
    "number": 1025,
    "title": "Fix:  should check the return value of  cudaFuncAttributeMaxDynamicSharedMemorySize,",
    "user": "thesues",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T03:16:47Z",
    "closed_at": "2024-02-03T00:31:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1025"
  },
  {
    "number": 1022,
    "title": "Fix args for `decode_batch` in Blip2",
    "user": "Lucius-THU",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T02:21:16Z",
    "closed_at": "2024-03-26T09:09:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1022"
  },
  {
    "number": 1019,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T12:53:20Z",
    "closed_at": "2024-01-31T13:55:33Z",
    "merged_at": "2024-01-31T13:55:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1019"
  },
  {
    "number": 1017,
    "title": "Fix a dependency error",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T09:31:46Z",
    "closed_at": "2024-01-31T09:48:47Z",
    "merged_at": "2024-01-31T09:48:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1017"
  },
  {
    "number": 1014,
    "title": "fix a typo",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T07:03:29Z",
    "closed_at": "2024-01-31T07:03:41Z",
    "merged_at": "2024-01-31T07:03:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1014"
  },
  {
    "number": 1010,
    "title": "Fix typo in help messages for --fp8_kv_cache option",
    "user": "ethnzhng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-30T23:34:25Z",
    "closed_at": "2024-05-20T17:23:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1010"
  },
  {
    "number": 1009,
    "title": "Doc update 20240130",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-30T19:40:03Z",
    "closed_at": "2024-01-30T19:40:23Z",
    "merged_at": "2024-01-30T19:40:22Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1009"
  },
  {
    "number": 1001,
    "title": "Update README.md",
    "user": "MustaphaU",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-29T23:27:44Z",
    "closed_at": "2024-10-26T14:42:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1001"
  },
  {
    "number": 992,
    "title": "Fix enc_dec bug and Make several improvements to whisper",
    "user": "Eddie-Wang1120",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-28T14:48:48Z",
    "closed_at": "2024-06-02T02:45:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/992"
  },
  {
    "number": 985,
    "title": "Add weight-only quantization for T5 models",
    "user": "eycheung",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-27T05:30:42Z",
    "closed_at": "2025-05-27T23:26:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/985"
  },
  {
    "number": 972,
    "title": "fix: fix qwen-vl deploy bug in A10.",
    "user": "xiaochus",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-25T13:48:24Z",
    "closed_at": "2025-08-06T16:13:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/972"
  },
  {
    "number": 968,
    "title": "Pretty Print Module",
    "user": "liquanfeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-25T12:12:41Z",
    "closed_at": "2025-08-18T20:05:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/968"
  },
  {
    "number": 949,
    "title": "Solve llama SmoothQuant error of medusa_packed_mask and medusa_positi…",
    "user": "activezhao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-24T09:21:56Z",
    "closed_at": "2024-03-22T04:08:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/949"
  },
  {
    "number": 947,
    "title": "add gelu_pytorch_tanh for starcoder",
    "user": "nullxjx",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-24T07:41:18Z",
    "closed_at": "2025-05-27T23:11:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/947"
  },
  {
    "number": 941,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-23T14:07:53Z",
    "closed_at": "2024-01-23T15:22:35Z",
    "merged_at": "2024-01-23T15:22:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/941"
  },
  {
    "number": 929,
    "title": "Fix Int4 AWQ quantization model saving",
    "user": "Broyojo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-22T05:37:06Z",
    "closed_at": "2025-05-27T23:10:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/929"
  },
  {
    "number": 923,
    "title": "fix : paralle build example error",
    "user": "park12sj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-20T11:04:43Z",
    "closed_at": "2025-08-06T16:12:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/923"
  },
  {
    "number": 916,
    "title": "Dg main mege",
    "user": "forrestjgq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T03:12:24Z",
    "closed_at": "2024-01-19T03:12:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/916"
  },
  {
    "number": 915,
    "title": "chore : add memory required in build from source guide",
    "user": "park12sj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T03:10:21Z",
    "closed_at": "2025-11-01T03:22:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/915"
  },
  {
    "number": 909,
    "title": "FEA: support use_paged_context_fmha feature to enable enable_kv_cache…",
    "user": "yunzhongyan0",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-18T04:16:28Z",
    "closed_at": "2025-08-18T19:51:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/909"
  },
  {
    "number": 900,
    "title": "fix typo",
    "user": "park12sj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-17T10:45:14Z",
    "closed_at": "2024-01-28T12:56:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/900"
  },
  {
    "number": 891,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-16T11:37:04Z",
    "closed_at": "2024-01-16T12:03:12Z",
    "merged_at": "2024-01-16T12:03:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/891"
  },
  {
    "number": 890,
    "title": "IA3 in Python-Runtime",
    "user": "vladnosiv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-16T10:11:33Z",
    "closed_at": "2024-01-16T10:11:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/890"
  },
  {
    "number": 873,
    "title": "Update README.md in multimodal example for LLaVA",
    "user": "isaac-vidas",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-12T16:30:55Z",
    "closed_at": "2024-01-28T22:03:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/873"
  },
  {
    "number": 859,
    "title": "fixed cutlass preprocessors typo",
    "user": "whitelok",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-11T03:23:45Z",
    "closed_at": "2024-02-01T06:27:58Z",
    "merged_at": "2024-02-01T06:27:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/859"
  },
  {
    "number": 857,
    "title": "Update perf_best_practices.md",
    "user": "BasicCoder",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-11T02:34:00Z",
    "closed_at": "2024-02-01T01:58:39Z",
    "merged_at": "2024-02-01T01:58:39Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/857"
  },
  {
    "number": 848,
    "title": "Add build command to HLAPI's example",
    "user": "Superjomn",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-09T14:42:06Z",
    "closed_at": "2024-01-09T14:48:49Z",
    "merged_at": "2024-01-09T14:48:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/848"
  },
  {
    "number": 846,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-09T12:09:37Z",
    "closed_at": "2024-01-09T13:03:35Z",
    "merged_at": "2024-01-09T13:03:35Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/846"
  },
  {
    "number": 834,
    "title": "Fix build llama",
    "user": "ngoanpv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-08T03:46:45Z",
    "closed_at": "2024-01-14T03:53:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/834"
  },
  {
    "number": 831,
    "title": "add method get_engine_version",
    "user": "ngoanpv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-07T12:26:09Z",
    "closed_at": "2025-08-18T19:33:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/831"
  },
  {
    "number": 817,
    "title": "[Bug] fix awq weight scale bug",
    "user": "lizhiyuanUSTC",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-05T03:06:14Z",
    "closed_at": "2025-11-01T03:22:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/817"
  },
  {
    "number": 814,
    "title": "Add batch manager static lib for Windows",
    "user": "sestephens-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-04T22:15:13Z",
    "closed_at": "2024-01-05T01:49:57Z",
    "merged_at": "2024-01-05T01:49:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/814"
  },
  {
    "number": 813,
    "title": "sentence reformulation",
    "user": "doracsillag",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-04T16:12:32Z",
    "closed_at": "2024-01-04T16:18:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/813"
  },
  {
    "number": 810,
    "title": "[Bug]Fixed SQ bug",
    "user": "wjj19950828",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-04T07:27:57Z",
    "closed_at": "2025-08-06T16:12:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/810"
  },
  {
    "number": 794,
    "title": "Add Weight-Only Support To Whisper",
    "user": "Eddie-Wang1120",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-02T16:21:54Z",
    "closed_at": "2024-01-17T13:49:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/794"
  },
  {
    "number": 787,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-02T08:10:55Z",
    "closed_at": "2024-01-02T09:54:32Z",
    "merged_at": "2024-01-02T09:54:32Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/787"
  },
  {
    "number": 779,
    "title": "Fix broken link in Mixtral doc",
    "user": "jrinder42",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-30T03:58:51Z",
    "closed_at": "2024-06-13T00:58:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/779"
  },
  {
    "number": 778,
    "title": "Add Roberta and few new tests for Bert",
    "user": "erenup",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-29T23:44:42Z",
    "closed_at": "2024-01-31T14:20:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/778"
  },
  {
    "number": 755,
    "title": "fix bug in examples/openai_triton/manual_plugin/fmha_triton.py",
    "user": "zhoutianzi666",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-27T09:45:38Z",
    "closed_at": "2025-05-27T22:54:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/755"
  },
  {
    "number": 754,
    "title": "Update TensorRT-LLM main branch",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-27T08:12:28Z",
    "closed_at": "2023-12-27T09:41:24Z",
    "merged_at": "2023-12-27T09:41:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/754"
  },
  {
    "number": 750,
    "title": "Update TensorRT-LLM",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-26T12:30:07Z",
    "closed_at": "2023-12-26T12:31:40Z",
    "merged_at": "2023-12-26T12:31:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/750"
  },
  {
    "number": 745,
    "title": "Update TensorRT-LLM Release branch",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-26T08:15:22Z",
    "closed_at": "2023-12-26T11:42:17Z",
    "merged_at": "2023-12-26T11:42:17Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/745"
  },
  {
    "number": 735,
    "title": "refactor: use torch to get device capability considering older NVIDIA driver",
    "user": "ApsarasX",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-25T07:49:34Z",
    "closed_at": "2025-07-08T12:48:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/735"
  },
  {
    "number": 727,
    "title": "Rewrite the repetition penalty kernel for the larger maxSeqLen",
    "user": "StarrickLiu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-22T11:57:07Z",
    "closed_at": "2024-06-20T02:35:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/727"
  },
  {
    "number": 721,
    "title": "Bump gradio from 3.40.1 to 4.11.0 in /examples/qwen",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-21T18:37:38Z",
    "closed_at": "2023-12-26T12:37:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/721"
  },
  {
    "number": 719,
    "title": "Fix a docker build error",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-21T11:21:08Z",
    "closed_at": "2023-12-22T00:00:11Z",
    "merged_at": "2023-12-22T00:00:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/719"
  },
  {
    "number": 712,
    "title": "Bump transformers from 4.33.1 to 4.36.0",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:12:53Z",
    "closed_at": "2023-12-26T12:38:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/712"
  },
  {
    "number": 710,
    "title": "fix int8_t quant bug",
    "user": "JamesLim-sy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T15:52:40Z",
    "closed_at": "2023-12-27T02:39:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/710"
  },
  {
    "number": 708,
    "title": "Update TensorRT-LLM",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T08:12:44Z",
    "closed_at": "2023-12-20T08:38:28Z",
    "merged_at": "2023-12-20T08:38:28Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/708"
  },
  {
    "number": 690,
    "title": "fix banRepeatNgram",
    "user": "AAAves",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-18T11:14:58Z",
    "closed_at": "2025-08-06T16:12:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/690"
  },
  {
    "number": 687,
    "title": "support llava",
    "user": "forrestjgq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-18T08:21:53Z",
    "closed_at": "2023-12-18T08:22:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/687"
  },
  {
    "number": 669,
    "title": "add `--allow-change-held-packages` ",
    "user": "pfldy2850",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-15T14:30:09Z",
    "closed_at": "2025-08-06T16:12:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/669"
  },
  {
    "number": 667,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-15T09:06:14Z",
    "closed_at": "2023-12-15T14:14:52Z",
    "merged_at": "2023-12-15T14:14:52Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/667"
  },
  {
    "number": 631,
    "title": "Update README.md",
    "user": "xxyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-11T10:36:09Z",
    "closed_at": "2024-06-13T01:09:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/631"
  },
  {
    "number": 615,
    "title": "pass rotary_emb_base to gpt_attention",
    "user": "FightingMan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T13:59:36Z",
    "closed_at": "2025-08-06T16:11:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/615"
  },
  {
    "number": 613,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T09:05:29Z",
    "closed_at": "2023-12-08T09:49:25Z",
    "merged_at": "2023-12-08T09:49:25Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/613"
  },
  {
    "number": 608,
    "title": "[Enhancement] StoppingCriteria and LogitsProcessor",
    "user": "zhang-ge-hao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-07T23:39:16Z",
    "closed_at": "2023-12-08T09:55:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/608"
  },
  {
    "number": 603,
    "title": "Include an option when no quantization mode is needed",
    "user": "miguelusque",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-07T12:03:31Z",
    "closed_at": "2025-11-01T03:23:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/603"
  },
  {
    "number": 597,
    "title": "WMT Example",
    "user": "yuting-wang-1000",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T17:50:03Z",
    "closed_at": "2023-12-06T18:21:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/597"
  },
  {
    "number": 596,
    "title": "Openai whisper to TensorRT-LLM model",
    "user": "makaveli10",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T17:36:51Z",
    "closed_at": "2023-12-12T18:48:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/596"
  },
  {
    "number": 590,
    "title": "Fix warning pynvml message",
    "user": "miguelusque",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T09:50:26Z",
    "closed_at": "2024-06-13T00:59:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/590"
  },
  {
    "number": 586,
    "title": "MPT convert from HF directly",
    "user": "megha95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T08:08:06Z",
    "closed_at": "2024-03-25T19:49:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/586"
  },
  {
    "number": 569,
    "title": "Add batch manager static lib for Windows",
    "user": "sestephens-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-05T19:02:43Z",
    "closed_at": "2023-12-06T05:43:30Z",
    "merged_at": "2023-12-06T05:43:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/569"
  },
  {
    "number": 559,
    "title": "Refactor CUDA cast function for float to int8_t conversion",
    "user": "kubed-arch",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T23:57:49Z",
    "closed_at": "2023-12-09T18:53:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/559"
  },
  {
    "number": 554,
    "title": "support int4-awq/gptq for Qwen",
    "user": "Tlntin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T16:32:52Z",
    "closed_at": "2023-12-18T08:49:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/554"
  },
  {
    "number": 552,
    "title": "Update badge",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T14:28:37Z",
    "closed_at": "2023-12-04T14:35:49Z",
    "merged_at": "2023-12-04T14:35:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/552"
  },
  {
    "number": 551,
    "title": "Update badge",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T14:26:27Z",
    "closed_at": "2023-12-04T14:27:41Z",
    "merged_at": "2023-12-04T14:27:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/551"
  },
  {
    "number": 550,
    "title": "Quick doc update",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T14:14:31Z",
    "closed_at": "2023-12-04T14:15:23Z",
    "merged_at": "2023-12-04T14:15:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/550"
  },
  {
    "number": 549,
    "title": "Update the latest news",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T13:59:58Z",
    "closed_at": "2023-12-04T14:04:01Z",
    "merged_at": "2023-12-04T14:04:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/549"
  },
  {
    "number": 546,
    "title": "Update TensorRT-LLM",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T10:52:32Z",
    "closed_at": "2023-12-04T10:59:41Z",
    "merged_at": "2023-12-04T10:59:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/546"
  },
  {
    "number": 545,
    "title": "skip special token during inference",
    "user": "littletomatodonkey",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T10:51:20Z",
    "closed_at": "2025-08-06T16:11:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/545"
  },
  {
    "number": 544,
    "title": "Update GitHub pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T10:17:36Z",
    "closed_at": "2023-12-04T10:59:41Z",
    "merged_at": "2023-12-04T10:59:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/544"
  },
  {
    "number": 540,
    "title": "Update GitHub pages",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T08:16:53Z",
    "closed_at": "2023-12-04T08:26:13Z",
    "merged_at": "2023-12-04T08:26:13Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/540"
  },
  {
    "number": 539,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T07:45:21Z",
    "closed_at": "2023-12-04T10:07:00Z",
    "merged_at": "2023-12-04T10:06:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/539"
  },
  {
    "number": 525,
    "title": "Update aarch64 batch manager libraries",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T08:15:55Z",
    "closed_at": "2023-12-01T08:27:03Z",
    "merged_at": "2023-12-01T08:27:03Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/525"
  },
  {
    "number": 524,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T07:54:30Z",
    "closed_at": "2023-12-01T14:27:51Z",
    "merged_at": "2023-12-01T14:27:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/524"
  },
  {
    "number": 506,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T06:31:37Z",
    "closed_at": "2023-11-30T08:46:23Z",
    "merged_at": "2023-11-30T08:46:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/506"
  },
  {
    "number": 497,
    "title": "[Docs] fix docs information error",
    "user": "BasicCoder",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T03:58:52Z",
    "closed_at": "2024-10-14T05:33:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/497"
  },
  {
    "number": 475,
    "title": "Fix an issue of mpi4py",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T02:20:39Z",
    "closed_at": "2023-11-27T07:30:15Z",
    "merged_at": "2023-11-27T07:30:15Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/475"
  },
  {
    "number": 465,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-24T08:36:53Z",
    "closed_at": "2023-11-24T14:12:27Z",
    "merged_at": "2023-11-24T14:12:27Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/465"
  },
  {
    "number": 459,
    "title": "[Enhancement] StoppingCriteria and LogitsProcessor",
    "user": "zhang-ge-hao",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T10:24:09Z",
    "closed_at": "2023-12-08T09:55:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/459"
  },
  {
    "number": 437,
    "title": "add cudnn_root arguments for build_wheel.py for not build TensorRT-LL…",
    "user": "ycsos",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-20T05:58:13Z",
    "closed_at": "2025-05-19T15:12:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/437"
  },
  {
    "number": 432,
    "title": "add cudnn_root arguments for build_wheel.py ",
    "user": "ycsos",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-20T03:29:50Z",
    "closed_at": "2023-11-20T03:47:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/432"
  },
  {
    "number": 428,
    "title": "[docs] add a diagram illustrating tensorrt-llm and tensorrt",
    "user": "lastweek",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-19T05:14:08Z",
    "closed_at": "2025-06-06T17:58:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/428"
  },
  {
    "number": 422,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T14:16:54Z",
    "closed_at": "2023-11-17T16:05:55Z",
    "merged_at": "2023-11-17T16:05:55Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/422"
  },
  {
    "number": 417,
    "title": "Fix baichuan smoothquant/INT8 KV cache build error",
    "user": "BasicCoder",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T03:25:06Z",
    "closed_at": "2025-06-06T01:54:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/417"
  },
  {
    "number": 384,
    "title": "[WIP] feat: add rwkv model.",
    "user": "SanftMonster",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-14T18:35:26Z",
    "closed_at": "2025-06-05T19:50:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/384"
  },
  {
    "number": 379,
    "title": "Update latest news",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-14T08:22:39Z",
    "closed_at": "2023-11-14T09:15:06Z",
    "merged_at": "2023-11-14T09:15:06Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/379"
  },
  {
    "number": 378,
    "title": "Update latest news",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-14T08:17:08Z",
    "closed_at": "2023-11-14T09:15:37Z",
    "merged_at": "2023-11-14T09:15:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/378"
  },
  {
    "number": 372,
    "title": "support Yi models (01-ai/Yi)",
    "user": "learninmou",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-14T04:29:27Z",
    "closed_at": "2023-11-27T08:08:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/372"
  },
  {
    "number": 368,
    "title": "Add Latest News section",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T13:09:07Z",
    "closed_at": "2023-11-13T13:11:11Z",
    "merged_at": "2023-11-13T13:11:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/368"
  },
  {
    "number": 367,
    "title": "Add Latest News section",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T13:07:47Z",
    "closed_at": "2023-11-13T13:10:01Z",
    "merged_at": "2023-11-13T13:10:01Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/367"
  },
  {
    "number": 366,
    "title": "Add Latest News section",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T12:54:33Z",
    "closed_at": "2023-11-13T12:56:37Z",
    "merged_at": "2023-11-13T12:56:37Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/366"
  },
  {
    "number": 365,
    "title": "Add Latest News section",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T12:54:30Z",
    "closed_at": "2023-11-13T12:56:23Z",
    "merged_at": "2023-11-13T12:56:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/365"
  },
  {
    "number": 362,
    "title": "Add Latest News section",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T07:13:07Z",
    "closed_at": "2023-11-13T07:17:23Z",
    "merged_at": "2023-11-13T07:17:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/362"
  },
  {
    "number": 361,
    "title": "Add Latest News section",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T07:13:06Z",
    "closed_at": "2023-11-13T07:15:24Z",
    "merged_at": "2023-11-13T07:15:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/361"
  },
  {
    "number": 357,
    "title": "[Docs] fix docs typo",
    "user": "DefTruth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-12T12:20:12Z",
    "closed_at": "2024-06-13T00:51:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/357"
  },
  {
    "number": 351,
    "title": "Add support for Flan-T5 models",
    "user": "mlmonk",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-10T16:58:16Z",
    "closed_at": "2023-11-18T03:35:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/351"
  },
  {
    "number": 349,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-10T12:44:59Z",
    "closed_at": "2023-11-10T14:30:32Z",
    "merged_at": "2023-11-10T14:30:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/349"
  },
  {
    "number": 332,
    "title": "fix: GPTBenchmark object has no attribute num_kv_heads",
    "user": "NaNAGISaSA",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-09T09:41:27Z",
    "closed_at": "2025-06-06T08:41:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/332"
  },
  {
    "number": 315,
    "title": "Add Latest News section",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T06:38:07Z",
    "closed_at": "2023-11-08T07:04:34Z",
    "merged_at": "2023-11-08T07:04:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/315"
  },
  {
    "number": 314,
    "title": "Add Latest News section",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T06:36:48Z",
    "closed_at": "2023-11-08T07:04:31Z",
    "merged_at": "2023-11-08T07:04:31Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/314"
  },
  {
    "number": 309,
    "title": "build: Update Windows torch versions",
    "user": "sestephens-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T17:26:49Z",
    "closed_at": "2023-11-08T03:56:38Z",
    "merged_at": "2023-11-08T03:56:38Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/309"
  },
  {
    "number": 302,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T10:03:42Z",
    "closed_at": "2023-11-07T11:51:58Z",
    "merged_at": "2023-11-07T11:51:58Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/302"
  },
  {
    "number": 293,
    "title": "Fix: llama2 fp8 quantization, fix invalid transformers syntax for padding/truncating to length",
    "user": "0xymoro",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T23:04:06Z",
    "closed_at": "2023-12-15T14:42:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/293"
  },
  {
    "number": 291,
    "title": "Add support for loading from pretrained model to BertModel example build script",
    "user": "oliverholworthy",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T13:43:23Z",
    "closed_at": "2025-08-06T16:10:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/291"
  },
  {
    "number": 279,
    "title": "Update the file paths to be absolute paths",
    "user": "changhuilin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-04T07:15:16Z",
    "closed_at": "2024-06-13T01:04:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/279"
  },
  {
    "number": 273,
    "title": "Support the gather_all_token_logits flag for Llama",
    "user": "eycheung",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-03T18:08:21Z",
    "closed_at": "2025-05-19T07:56:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/273"
  },
  {
    "number": 266,
    "title": "Support InternLM models",
    "user": "wangruohui",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-03T07:22:18Z",
    "closed_at": "2023-11-07T12:22:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/266"
  },
  {
    "number": 259,
    "title": "H/print",
    "user": "hayleyhu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T22:46:12Z",
    "closed_at": "2025-05-16T22:14:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/259"
  },
  {
    "number": 245,
    "title": "patch for commit f84d5fe",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T06:14:54Z",
    "closed_at": "2023-11-02T06:50:51Z",
    "merged_at": "2023-11-02T06:50:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/245"
  },
  {
    "number": 221,
    "title": "Add batch manager lib",
    "user": "sestephens-nv",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T16:11:16Z",
    "closed_at": "2023-11-02T05:49:42Z",
    "merged_at": "2023-11-02T05:49:42Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/221"
  },
  {
    "number": 205,
    "title": "Add GQA support to MPT (and GPT) models",
    "user": "bheilbrun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T00:57:16Z",
    "closed_at": "2023-11-18T03:34:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/205"
  },
  {
    "number": 188,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-30T02:07:42Z",
    "closed_at": "2023-10-30T08:06:41Z",
    "merged_at": "2023-10-30T08:06:41Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/188"
  },
  {
    "number": 177,
    "title": "Update README.md",
    "user": "FarukhS52",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-28T15:12:03Z",
    "closed_at": "2024-06-13T00:52:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/177"
  },
  {
    "number": 173,
    "title": "Activation Function Implementations",
    "user": "AndreSlavescu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-28T03:37:50Z",
    "closed_at": "2025-06-09T01:37:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/173"
  },
  {
    "number": 172,
    "title": "Activation Operators",
    "user": "AndreSlavescu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-28T01:40:02Z",
    "closed_at": "2023-10-28T03:31:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/172"
  },
  {
    "number": 163,
    "title": "Conv Operator Mappings - Pytorch API",
    "user": "AndreSlavescu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T09:44:23Z",
    "closed_at": "2025-06-05T19:50:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/163"
  },
  {
    "number": 152,
    "title": "update the batch manager for release/0.5.0",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T03:52:53Z",
    "closed_at": "2023-10-27T04:15:49Z",
    "merged_at": "2023-10-27T04:15:49Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/152"
  },
  {
    "number": 148,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T03:07:33Z",
    "closed_at": "2023-10-27T04:10:00Z",
    "merged_at": "2023-10-27T04:10:00Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/148"
  },
  {
    "number": 130,
    "title": "Support paged kv cache for benchmarks",
    "user": "yunfeng-scale",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T04:50:02Z",
    "closed_at": "2025-11-01T03:23:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/130"
  },
  {
    "number": 123,
    "title": "Support InternLM models (deprecated and move to #266)",
    "user": "wangruohui",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T14:08:05Z",
    "closed_at": "2024-06-13T01:08:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/123"
  },
  {
    "number": 115,
    "title": "correction of typo",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T11:42:14Z",
    "closed_at": "2023-10-25T11:55:43Z",
    "merged_at": "2023-10-25T11:55:43Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/115"
  },
  {
    "number": 114,
    "title": "fix doc typo.",
    "user": "nv-guomingz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T10:51:20Z",
    "closed_at": "2023-10-25T11:31:51Z",
    "merged_at": "2023-10-25T11:31:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/114"
  },
  {
    "number": 78,
    "title": "Donglu branch",
    "user": "dongluw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-23T22:33:13Z",
    "closed_at": "2023-10-23T22:34:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/78"
  },
  {
    "number": 76,
    "title": "Add Python bindings to `GptManager`",
    "user": "linden-li",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-23T16:56:42Z",
    "closed_at": "2025-03-28T06:51:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/76"
  },
  {
    "number": 63,
    "title": "Improved Readability of Readme.md File",
    "user": "Sanyam-2026",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-22T18:20:43Z",
    "closed_at": "2024-06-13T00:53:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/63"
  },
  {
    "number": 60,
    "title": "Update windows related documentation to main branch",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-22T01:23:59Z",
    "closed_at": "2023-10-22T01:24:59Z",
    "merged_at": "2023-10-22T01:24:59Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/60"
  },
  {
    "number": 59,
    "title": "Update windows related documentation",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-22T01:20:27Z",
    "closed_at": "2023-10-22T01:24:53Z",
    "merged_at": "2023-10-22T01:24:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/59"
  },
  {
    "number": 58,
    "title": "Perceived",
    "user": "RichardScottOZ",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-21T22:45:03Z",
    "closed_at": "2024-06-13T01:00:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/58"
  },
  {
    "number": 56,
    "title": "Fix typo in batchScheduler.h",
    "user": "eltociear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-21T17:58:58Z",
    "closed_at": "2024-06-13T00:55:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/56"
  },
  {
    "number": 46,
    "title": "fix Forward Compatibility mode is UNAVAILABLE error",
    "user": "BasicCoder",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T12:49:26Z",
    "closed_at": "2024-06-13T01:02:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/46"
  },
  {
    "number": 40,
    "title": "Update docs/source/batch_manager.md",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T09:32:18Z",
    "closed_at": "2023-10-20T10:08:45Z",
    "merged_at": "2023-10-20T10:08:45Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/40"
  },
  {
    "number": 38,
    "title": "Cherry-pick https://github.com/NVIDIA/TensorRT-LLM/pull/21 to main branch",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T08:43:26Z",
    "closed_at": "2023-10-20T08:45:51Z",
    "merged_at": "2023-10-20T08:45:51Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/38"
  },
  {
    "number": 30,
    "title": "Fix link jump in windows readme.md",
    "user": "yuanlehome",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T04:47:13Z",
    "closed_at": "2023-10-30T02:56:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/30"
  },
  {
    "number": 21,
    "title": "Fix two deadlinks in README.md",
    "user": "wangkuiyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T01:52:59Z",
    "closed_at": "2023-10-20T02:13:21Z",
    "merged_at": "2023-10-20T02:13:21Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/21"
  },
  {
    "number": 20,
    "title": "Main doc fix",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T00:48:53Z",
    "closed_at": "2023-10-20T00:52:11Z",
    "merged_at": "2023-10-20T00:52:11Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/20"
  },
  {
    "number": 19,
    "title": "Fix small doc issue",
    "user": "juney-nvidia",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T00:45:53Z",
    "closed_at": "2023-10-20T00:52:03Z",
    "merged_at": "2023-10-20T00:52:02Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/19"
  },
  {
    "number": 15,
    "title": "Fix the link to the documentation",
    "user": "jdemouth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T15:45:09Z",
    "closed_at": "2023-10-19T15:46:57Z",
    "merged_at": "2023-10-19T15:46:57Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/15"
  },
  {
    "number": 14,
    "title": "revise the homepage (release)",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T08:32:02Z",
    "closed_at": "2023-10-19T08:40:23Z",
    "merged_at": "2023-10-19T08:40:23Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/14"
  },
  {
    "number": 13,
    "title": "revise the homepage (main)",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T08:31:52Z",
    "closed_at": "2023-10-19T08:39:30Z",
    "merged_at": "2023-10-19T08:39:30Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/13"
  },
  {
    "number": 12,
    "title": "add git-lfs dependency for binaries (main branch)",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T05:51:19Z",
    "closed_at": "2023-10-19T06:35:48Z",
    "merged_at": "2023-10-19T06:35:48Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/12"
  },
  {
    "number": 11,
    "title": "add git-lfs dependency for binaries (release)",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T05:51:06Z",
    "closed_at": "2023-10-19T06:34:56Z",
    "merged_at": "2023-10-19T06:34:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/11"
  },
  {
    "number": 10,
    "title": "update aarch64 batch manager libraries to release/0.5.0",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T16:10:38Z",
    "closed_at": "2023-10-18T16:15:40Z",
    "merged_at": "2023-10-18T16:15:40Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/10"
  },
  {
    "number": 9,
    "title": "update aarch64 batch manager libraries to main",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T16:07:41Z",
    "closed_at": "2023-10-18T16:15:48Z",
    "merged_at": "2023-10-18T16:15:47Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/9"
  },
  {
    "number": 8,
    "title": "Fix memory leak in falcon weight loader",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T15:57:38Z",
    "closed_at": "2023-10-18T15:58:34Z",
    "merged_at": "2023-10-18T15:58:34Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/8"
  },
  {
    "number": 7,
    "title": "update aarch64 libraries to release/0.5.0 branch",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T15:14:33Z",
    "closed_at": "2023-10-18T15:21:19Z",
    "merged_at": "2023-10-18T15:21:19Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7"
  },
  {
    "number": 6,
    "title": "update aarch64 libraries to main branch",
    "user": "Shixiaowei02",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T15:11:52Z",
    "closed_at": "2023-10-18T15:21:50Z",
    "merged_at": "2023-10-18T15:21:50Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/6"
  },
  {
    "number": 5,
    "title": "Kaiyu/update main",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T14:20:33Z",
    "closed_at": "2023-10-18T14:38:53Z",
    "merged_at": "2023-10-18T14:38:53Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/5"
  },
  {
    "number": 4,
    "title": "Update TensorRT-LLM",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T06:26:45Z",
    "closed_at": "2023-10-11T08:03:33Z",
    "merged_at": "2023-10-11T08:03:33Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/4"
  },
  {
    "number": 3,
    "title": "Update TRT-LLM code",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-28T16:00:53Z",
    "closed_at": "2023-09-28T17:46:56Z",
    "merged_at": "2023-09-28T17:46:56Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/3"
  },
  {
    "number": 2,
    "title": "Add static libraries for batch manager",
    "user": "kaiyux",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T03:32:02Z",
    "closed_at": "2023-09-21T03:52:24Z",
    "merged_at": "2023-09-21T03:52:24Z",
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/2"
  },
  {
    "number": 1,
    "title": "Bump onnx from 1.12.0 to 1.13.0",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-09-20T07:34:02Z",
    "closed_at": "2023-12-26T12:38:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/1"
  }
]