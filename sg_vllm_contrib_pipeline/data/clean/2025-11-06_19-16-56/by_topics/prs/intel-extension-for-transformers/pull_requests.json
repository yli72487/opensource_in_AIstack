[
  {
    "number": 1698,
    "title": "Bump langchain from 0.1.11 to 0.2.10 in /intel_extension_for_transformers/neural_chat/tests",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-09-17T21:41:00Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1698"
  },
  {
    "number": 1693,
    "title": "Add readme for inc 3.0 xpu device usage",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-26T02:21:35Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1693"
  },
  {
    "number": 1691,
    "title": "Update utils.py",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-23T07:57:12Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1691"
  },
  {
    "number": 1687,
    "title": "fix search issue of online doc",
    "user": "NeoZhangJianyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-13T04:32:09Z",
    "closed_at": "2024-08-13T05:41:36Z",
    "merged_at": "2024-08-13T05:41:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1687"
  },
  {
    "number": 1686,
    "title": "Adapt `INCWeightOnlyLinear` ",
    "user": "Kaihui-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-09T14:05:22Z",
    "closed_at": "2024-08-11T10:14:02Z",
    "merged_at": "2024-08-11T10:14:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1686"
  },
  {
    "number": 1685,
    "title": "Update LLM recipes to align INC 3.0",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-09T08:48:32Z",
    "closed_at": "2024-08-09T13:50:05Z",
    "merged_at": "2024-08-09T13:50:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1685"
  },
  {
    "number": 1684,
    "title": "Fix intel tbb",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-08T05:24:40Z",
    "closed_at": "2024-08-08T07:54:24Z",
    "merged_at": "2024-08-08T07:54:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1684"
  },
  {
    "number": 1683,
    "title": "Add `use_mse_search` to GPTQ Config",
    "user": "Kaihui-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-08T01:14:53Z",
    "closed_at": "2024-08-09T01:31:30Z",
    "merged_at": "2024-08-09T01:31:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1683"
  },
  {
    "number": 1682,
    "title": "fix int8 skip module config",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-05T04:20:11Z",
    "closed_at": "2024-08-09T03:14:08Z",
    "merged_at": "2024-08-09T03:14:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1682"
  },
  {
    "number": 1679,
    "title": "Bump torch from 1.13.1 to 2.2.0 in /workflows/compression_aware_training",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-07-25T10:50:50Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1679"
  },
  {
    "number": 1678,
    "title": "Bump langchain-community from 0.0.27 to 0.2.9 in /intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-07-24T17:38:39Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1678"
  },
  {
    "number": 1677,
    "title": "Update publication.md",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-24T13:46:29Z",
    "closed_at": "2024-07-25T08:30:53Z",
    "merged_at": "2024-07-25T08:30:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1677"
  },
  {
    "number": 1675,
    "title": "Fix args.tasks type check issue",
    "user": "duanshengliu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-23T13:07:20Z",
    "closed_at": "2024-07-24T09:19:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1675"
  },
  {
    "number": 1674,
    "title": "test ci",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-19T02:07:16Z",
    "closed_at": "2024-07-30T02:11:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1674"
  },
  {
    "number": 1673,
    "title": "For test",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-18T06:14:46Z",
    "closed_at": "2024-07-18T06:52:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1673"
  },
  {
    "number": 1672,
    "title": "qbits deprecate clip postfix",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-17T08:26:24Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1672"
  },
  {
    "number": 1671,
    "title": "Adapt quant lm head",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-17T08:11:14Z",
    "closed_at": "2024-08-01T08:57:31Z",
    "merged_at": "2024-08-01T08:57:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1671"
  },
  {
    "number": 1670,
    "title": "Support lmhead int4",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-17T05:33:22Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1670"
  },
  {
    "number": 1669,
    "title": "Adapt INC autoround changes",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-17T03:26:40Z",
    "closed_at": "2024-07-25T06:15:45Z",
    "merged_at": "2024-07-25T06:15:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1669"
  },
  {
    "number": 1668,
    "title": "Fix CI inc3.x install",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-16T03:34:01Z",
    "closed_at": "2024-07-16T08:55:21Z",
    "merged_at": "2024-07-16T08:55:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1668"
  },
  {
    "number": 1667,
    "title": "Bump setuptools from 69.5.1 to 70.0.0",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-07-15T19:39:19Z",
    "closed_at": "2024-07-17T02:56:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1667"
  },
  {
    "number": 1663,
    "title": "A beginner friendly quantize and text embeddings tutorial for XPUs",
    "user": "sleepingcat4",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-14T04:42:51Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1663"
  },
  {
    "number": 1662,
    "title": "improve SQ/WOQ examples",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-11T07:27:43Z",
    "closed_at": "2024-07-12T09:35:46Z",
    "merged_at": "2024-07-12T09:35:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1662"
  },
  {
    "number": 1661,
    "title": "Bump protobuf from 3.20 to 3.20.2 in /intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-07-10T05:31:34Z",
    "closed_at": "2024-07-22T01:26:23Z",
    "merged_at": "2024-07-22T01:26:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1661"
  },
  {
    "number": 1660,
    "title": "Update publication.md",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-10T01:45:55Z",
    "closed_at": "2024-07-10T05:29:52Z",
    "merged_at": "2024-07-10T05:29:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1660"
  },
  {
    "number": 1658,
    "title": "update lm-eval to 0.4.3",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-09T03:05:37Z",
    "closed_at": "2024-07-11T06:56:58Z",
    "merged_at": "2024-07-11T06:56:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1658"
  },
  {
    "number": 1657,
    "title": "Apply regreSSHion mitigation",
    "user": "tylertitsworth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-08T16:45:03Z",
    "closed_at": "2024-07-09T01:21:00Z",
    "merged_at": "2024-07-09T01:21:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1657"
  },
  {
    "number": 1655,
    "title": "Enable modelscope for itrex",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-05T03:40:54Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1655"
  },
  {
    "number": 1654,
    "title": "Fix optimum-intel version for INC v3.0",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-05T02:38:42Z",
    "closed_at": "2024-07-05T05:13:16Z",
    "merged_at": "2024-07-05T05:13:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1654"
  },
  {
    "number": 1653,
    "title": "qbits support f4 weight repack",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-04T12:42:18Z",
    "closed_at": "2024-07-10T02:39:21Z",
    "merged_at": "2024-07-10T02:39:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1653"
  },
  {
    "number": 1652,
    "title": "for test",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-04T01:52:31Z",
    "closed_at": "2024-07-04T05:20:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1652"
  },
  {
    "number": 1651,
    "title": "remove optimum-intel version limit",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-03T05:46:55Z",
    "closed_at": "2024-07-04T06:00:47Z",
    "merged_at": "2024-07-04T06:00:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1651"
  },
  {
    "number": 1650,
    "title": "update ipex api",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-03T01:31:49Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1650"
  },
  {
    "number": 1648,
    "title": "Unify the 'iters' and 'calib_iters' in AutoRound config",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-02T09:21:38Z",
    "closed_at": "2024-07-04T04:12:53Z",
    "merged_at": "2024-07-04T04:12:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1648"
  },
  {
    "number": 1647,
    "title": "Set lm-eval to 0.4.2",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-02T05:22:00Z",
    "closed_at": "2024-07-02T09:29:30Z",
    "merged_at": "2024-07-02T09:29:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1647"
  },
  {
    "number": 1646,
    "title": "[pre-commit.ci] pre-commit autoupdate",
    "user": "pre-commit-ci[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-07-02T00:19:39Z",
    "closed_at": "2024-07-04T01:28:20Z",
    "merged_at": "2024-07-04T01:28:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1646"
  },
  {
    "number": 1645,
    "title": "Clean INC import",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-07-01T04:50:33Z",
    "closed_at": "2024-07-02T05:13:47Z",
    "merged_at": "2024-07-02T05:13:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1645"
  },
  {
    "number": 1642,
    "title": "[CI]fix neural engine error",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-28T06:34:53Z",
    "closed_at": "2024-06-28T10:56:06Z",
    "merged_at": "2024-06-28T10:56:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1642"
  },
  {
    "number": 1641,
    "title": "Fix Qwen neural speed",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-27T07:50:11Z",
    "closed_at": "2024-06-28T10:55:29Z",
    "merged_at": "2024-06-28T10:55:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1641"
  },
  {
    "number": 1640,
    "title": "Improve MPT series SQ",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-27T06:51:49Z",
    "closed_at": "2024-06-28T09:54:14Z",
    "merged_at": "2024-06-28T09:54:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1640"
  },
  {
    "number": 1639,
    "title": "Support phi series SQ",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-27T06:25:28Z",
    "closed_at": "2024-06-28T01:29:26Z",
    "merged_at": "2024-06-28T01:29:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1639"
  },
  {
    "number": 1638,
    "title": "fix xpu version itrex detect",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-27T02:41:20Z",
    "closed_at": "2024-06-28T09:07:12Z",
    "merged_at": "2024-06-28T09:07:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1638"
  },
  {
    "number": 1637,
    "title": "Fix docs online build",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-26T09:11:14Z",
    "closed_at": "2024-06-27T02:59:31Z",
    "merged_at": "2024-06-27T02:59:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1637"
  },
  {
    "number": 1636,
    "title": "Fix  SQ bloom",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-26T09:00:43Z",
    "closed_at": "2024-06-27T03:30:51Z",
    "merged_at": "2024-06-27T03:30:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1636"
  },
  {
    "number": 1635,
    "title": "update torch version to 2.3.0 in example/requirements.txt",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-26T03:20:16Z",
    "closed_at": "2024-06-26T05:19:35Z",
    "merged_at": "2024-06-26T05:19:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1635"
  },
  {
    "number": 1634,
    "title": "fix  RAG requirements",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-25T03:55:48Z",
    "closed_at": "2024-07-10T05:30:23Z",
    "merged_at": "2024-07-10T05:30:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1634"
  },
  {
    "number": 1633,
    "title": "Bump neural docker base",
    "user": "Chris-Sigopt",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-21T23:42:00Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1633"
  },
  {
    "number": 1632,
    "title": "fix codescan",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-21T08:00:11Z",
    "closed_at": "2024-06-21T08:29:54Z",
    "merged_at": "2024-06-21T08:29:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1632"
  },
  {
    "number": 1631,
    "title": "remove unused binary",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-21T05:56:56Z",
    "closed_at": "2024-06-21T07:55:03Z",
    "merged_at": "2024-06-21T07:55:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1631"
  },
  {
    "number": 1629,
    "title": "solve codescan#5-42",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-20T07:44:23Z",
    "closed_at": "2024-06-21T05:51:14Z",
    "merged_at": "2024-06-21T05:51:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1629"
  },
  {
    "number": 1628,
    "title": "solve codescan#43-45",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-20T07:26:45Z",
    "closed_at": "2024-06-21T05:50:26Z",
    "merged_at": "2024-06-21T05:50:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1628"
  },
  {
    "number": 1627,
    "title": "solve codescan#53-56",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-20T07:06:18Z",
    "closed_at": "2024-06-21T05:49:04Z",
    "merged_at": "2024-06-21T05:49:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1627"
  },
  {
    "number": 1626,
    "title": "solve codescan#57",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-20T06:57:02Z",
    "closed_at": "2024-06-21T05:48:00Z",
    "merged_at": "2024-06-21T05:48:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1626"
  },
  {
    "number": 1625,
    "title": "[StepSecurity] ci: Harden GitHub Actions",
    "user": "step-security-bot",
    "user_type": "User",
    "is_human": false,
    "created_at": "2024-06-20T06:17:01Z",
    "closed_at": "2024-06-20T08:15:03Z",
    "merged_at": "2024-06-20T08:15:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1625"
  },
  {
    "number": 1624,
    "title": "remove scorecard",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-20T06:16:29Z",
    "closed_at": "2024-06-20T06:16:58Z",
    "merged_at": "2024-06-20T06:16:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1624"
  },
  {
    "number": 1623,
    "title": "Support INC layerwise quant",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-19T13:29:01Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1623"
  },
  {
    "number": 1622,
    "title": "add contributors",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-19T08:50:41Z",
    "closed_at": "2024-06-20T05:43:51Z",
    "merged_at": "2024-06-20T05:43:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1622"
  },
  {
    "number": 1621,
    "title": "Fix typo at README.md",
    "user": "LucasHBG",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-18T19:23:12Z",
    "closed_at": "2024-06-19T08:32:02Z",
    "merged_at": "2024-06-19T08:32:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1621"
  },
  {
    "number": 1620,
    "title": "Fix typo at README.md",
    "user": "LucasHBG",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-18T19:18:31Z",
    "closed_at": "2024-06-19T08:31:49Z",
    "merged_at": "2024-06-19T08:31:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1620"
  },
  {
    "number": 1619,
    "title": "Create scorecard.yml",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-18T12:44:32Z",
    "closed_at": "2024-06-19T08:21:47Z",
    "merged_at": "2024-06-19T08:21:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1619"
  },
  {
    "number": 1618,
    "title": "Add OpenSSF Badge",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-18T12:27:25Z",
    "closed_at": "2024-06-19T08:20:16Z",
    "merged_at": "2024-06-19T08:20:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1618"
  },
  {
    "number": 1617,
    "title": "fix deepspeed script and add a optimum-intel option",
    "user": "jiqing-feng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-17T08:15:22Z",
    "closed_at": "2024-06-20T05:39:15Z",
    "merged_at": "2024-06-20T05:39:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1617"
  },
  {
    "number": 1615,
    "title": "limit setuptools version",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-17T06:24:08Z",
    "closed_at": "2024-06-18T03:19:11Z",
    "merged_at": "2024-06-18T03:19:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1615"
  },
  {
    "number": 1614,
    "title": "Fixed issue of loading woq model for intel GPU",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-15T16:02:14Z",
    "closed_at": "2024-06-21T07:14:56Z",
    "merged_at": "2024-06-21T07:14:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1614"
  },
  {
    "number": 1613,
    "title": "Bump langchain-community from 0.0.27 to 0.2.5 in /intel_extension_for_transformers/neural_chat/tests",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-06-14T23:28:28Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1613"
  },
  {
    "number": 1612,
    "title": "update recipes to align INC2.6",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-14T08:40:37Z",
    "closed_at": "2024-07-09T06:02:29Z",
    "merged_at": "2024-07-09T06:02:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1612"
  },
  {
    "number": 1611,
    "title": "disable qbits lib import when using gpu version itrex",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-14T07:01:48Z",
    "closed_at": "2024-06-21T07:26:22Z",
    "merged_at": "2024-06-21T07:26:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1611"
  },
  {
    "number": 1610,
    "title": "Remove permissions.",
    "user": "ZePan110",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-14T04:53:44Z",
    "closed_at": "2024-06-18T07:21:28Z",
    "merged_at": "2024-06-18T07:21:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1610"
  },
  {
    "number": 1609,
    "title": "Remove premissions for .github/workflows/probot.yml",
    "user": "ZePan110",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-14T02:40:39Z",
    "closed_at": "2024-06-14T02:48:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1609"
  },
  {
    "number": 1608,
    "title": "Bump qdrant-client from 1.8.2 to 1.9.0 in /intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-06-14T01:20:13Z",
    "closed_at": "2024-06-14T02:32:12Z",
    "merged_at": "2024-06-14T02:32:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1608"
  },
  {
    "number": 1607,
    "title": "qbits support torch version compatiblity check",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-13T03:37:02Z",
    "closed_at": "2024-06-14T04:18:40Z",
    "merged_at": "2024-06-14T04:18:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1607"
  },
  {
    "number": 1606,
    "title": "Migrate SQ and WOQ to INC 3.x API.",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-12T11:28:17Z",
    "closed_at": "2024-07-11T05:43:32Z",
    "merged_at": "2024-07-11T05:43:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1606"
  },
  {
    "number": 1605,
    "title": "Migrate trainer INC 1.x API to 2.x",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-12T10:30:13Z",
    "closed_at": "2024-06-24T02:08:46Z",
    "merged_at": "2024-06-24T02:08:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1605"
  },
  {
    "number": 1604,
    "title": "[DOC] Update example doc",
    "user": "xiguiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-12T02:05:00Z",
    "closed_at": "2024-06-14T04:20:13Z",
    "merged_at": "2024-06-14T04:20:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1604"
  },
  {
    "number": 1603,
    "title": "add publications",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-11T15:44:22Z",
    "closed_at": "2024-06-14T02:02:25Z",
    "merged_at": "2024-06-14T02:02:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1603"
  },
  {
    "number": 1602,
    "title": "Enable vllm backend.",
    "user": "ZePan110",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-11T01:55:03Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1602"
  },
  {
    "number": 1601,
    "title": "Update requirements.txt",
    "user": "denniszhen1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-10T21:04:31Z",
    "closed_at": "2024-06-10T21:05:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1601"
  },
  {
    "number": 1600,
    "title": "Improve SQ model restored from json",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-07T05:03:02Z",
    "closed_at": "2024-06-11T05:38:13Z",
    "merged_at": "2024-06-11T05:38:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1600"
  },
  {
    "number": 1599,
    "title": "update qbits doc",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-07T02:57:52Z",
    "closed_at": "2024-06-07T03:28:55Z",
    "merged_at": "2024-06-07T03:28:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1599"
  },
  {
    "number": 1598,
    "title": "improve lm_eval get chatglm2 tokenizer from local saved files",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-06T07:49:38Z",
    "closed_at": "2024-06-07T02:49:03Z",
    "merged_at": "2024-06-07T02:49:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1598"
  },
  {
    "number": 1597,
    "title": "Fix SQ baichuan without position_ids for torch and ipex 2.3.0",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-06T06:31:05Z",
    "closed_at": "2024-06-11T03:13:44Z",
    "merged_at": "2024-06-11T03:13:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1597"
  },
  {
    "number": 1595,
    "title": "remove INC 1.X API usage in neuralchat doc",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-05T06:57:13Z",
    "closed_at": "2024-06-06T05:41:45Z",
    "merged_at": "2024-06-06T05:41:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1595"
  },
  {
    "number": 1594,
    "title": "Unify the woq config weight_dtype for int4 and fp4 on different devices",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-05T06:19:43Z",
    "closed_at": "2024-06-07T08:42:48Z",
    "merged_at": "2024-06-07T08:42:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1594"
  },
  {
    "number": 1593,
    "title": "[NeuralChat] Refine path in AskDoc server",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-05T02:06:41Z",
    "closed_at": "2024-06-14T05:44:52Z",
    "merged_at": "2024-06-14T05:44:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1593"
  },
  {
    "number": 1592,
    "title": "Bump qdrant-client from 1.8.2 to 1.9.0 in /intel_extension_for_transformers/neural_chat/tests",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-06-04T17:54:50Z",
    "closed_at": "2024-06-14T01:18:51Z",
    "merged_at": "2024-06-14T01:18:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1592"
  },
  {
    "number": 1591,
    "title": "improve SQ to adapt optimum-intel 1.16.1",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-04T09:09:34Z",
    "closed_at": "2024-06-06T02:08:05Z",
    "merged_at": "2024-06-06T02:08:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1591"
  },
  {
    "number": 1590,
    "title": "Set optimum-intel to 1.16.1",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-04T07:45:53Z",
    "closed_at": "2024-06-04T09:07:53Z",
    "merged_at": "2024-06-04T09:07:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1590"
  },
  {
    "number": 1589,
    "title": "Fix CI due to optimum-intel upgrade",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-04T06:40:53Z",
    "closed_at": "2024-06-04T06:56:15Z",
    "merged_at": "2024-06-04T06:56:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1589"
  },
  {
    "number": 1588,
    "title": "Add shanghainese ASR TTS finetuning and inference example",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-04T03:50:14Z",
    "closed_at": "2024-06-05T01:17:20Z",
    "merged_at": "2024-06-05T01:17:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1588"
  },
  {
    "number": 1587,
    "title": "Remove NAS and AutoDistillation due to INC API update.",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-03T10:37:14Z",
    "closed_at": "2024-06-05T05:02:04Z",
    "merged_at": "2024-06-05T05:02:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1587"
  },
  {
    "number": 1586,
    "title": "Fix version",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-03T06:47:39Z",
    "closed_at": "2024-06-04T09:35:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1586"
  },
  {
    "number": 1585,
    "title": "add model_type tinyllama",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-31T11:10:48Z",
    "closed_at": "2024-05-31T12:24:03Z",
    "merged_at": "2024-05-31T12:24:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1585"
  },
  {
    "number": 1584,
    "title": "update auto-round version to 0.2",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-31T07:21:06Z",
    "closed_at": "2024-06-05T05:01:23Z",
    "merged_at": "2024-06-05T05:01:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1584"
  },
  {
    "number": 1583,
    "title": "[CI]Add model test on arc ",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-31T01:52:28Z",
    "closed_at": "2024-06-07T08:45:03Z",
    "merged_at": "2024-06-07T08:45:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1583"
  },
  {
    "number": 1581,
    "title": "[vLLM] QBits Perf Enhence",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-30T10:08:28Z",
    "closed_at": "2024-06-05T01:41:08Z",
    "merged_at": "2024-06-05T01:41:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1581"
  },
  {
    "number": 1580,
    "title": "Support huggingface popular weight format for weight-only quantization",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-30T07:29:29Z",
    "closed_at": "2024-07-05T09:20:55Z",
    "merged_at": "2024-07-05T09:20:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1580"
  },
  {
    "number": 1579,
    "title": "Add a dependency for  Neural_chat on CPU device",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-30T03:05:53Z",
    "closed_at": "2024-06-07T09:53:50Z",
    "merged_at": "2024-06-07T09:53:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1579"
  },
  {
    "number": 1578,
    "title": "[DOC]Add modelscope example",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-30T02:11:03Z",
    "closed_at": "2024-06-14T04:19:49Z",
    "merged_at": "2024-06-14T04:19:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1578"
  },
  {
    "number": 1576,
    "title": "[AskGM] Using ICX to analyze the relevance of the retrieval document",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-29T11:53:27Z",
    "closed_at": "2024-05-30T05:56:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1576"
  },
  {
    "number": 1575,
    "title": "Gaudi Tensor split for memory optimization",
    "user": "ClarkChin08",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-29T08:46:49Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1575"
  },
  {
    "number": 1573,
    "title": "Add Habana HPU check in setup.py",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-27T09:38:55Z",
    "closed_at": "2024-05-28T02:37:57Z",
    "merged_at": "2024-05-28T02:37:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1573"
  },
  {
    "number": 1572,
    "title": "add vllm_deploy",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-27T02:00:35Z",
    "closed_at": "2024-06-14T02:45:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1572"
  },
  {
    "number": 1571,
    "title": "Update README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-24T07:47:37Z",
    "closed_at": "2024-06-04T08:46:10Z",
    "merged_at": "2024-06-04T08:46:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1571"
  },
  {
    "number": 1570,
    "title": "Update README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-24T07:39:33Z",
    "closed_at": "2024-05-24T07:40:05Z",
    "merged_at": "2024-05-24T07:40:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1570"
  },
  {
    "number": 1569,
    "title": "Add GGUF support in ITREX readme",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-24T07:04:54Z",
    "closed_at": "2024-05-24T07:36:20Z",
    "merged_at": "2024-05-24T07:36:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1569"
  },
  {
    "number": 1568,
    "title": "Update WOQ AutoRoundConfig parameter",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-24T03:54:25Z",
    "closed_at": "2024-05-24T09:46:43Z",
    "merged_at": "2024-05-24T09:46:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1568"
  },
  {
    "number": 1567,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-24T02:50:12Z",
    "closed_at": "2024-05-24T02:56:20Z",
    "merged_at": "2024-05-24T02:56:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1567"
  },
  {
    "number": 1566,
    "title": "Fix gptq true_sequential default False",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-23T09:11:04Z",
    "closed_at": "2024-05-23T10:46:20Z",
    "merged_at": "2024-05-23T10:46:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1566"
  },
  {
    "number": 1565,
    "title": "Align gguf_file with HF API",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-23T08:32:01Z",
    "closed_at": "2024-05-24T02:09:56Z",
    "merged_at": "2024-05-24T02:09:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1565"
  },
  {
    "number": 1564,
    "title": "update onnxruntime version for engine",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-22T09:11:29Z",
    "closed_at": "2024-05-22T12:30:17Z",
    "merged_at": "2024-05-22T12:30:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1564"
  },
  {
    "number": 1563,
    "title": "remove limitation for torch",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-22T07:45:51Z",
    "closed_at": "2024-05-22T08:24:28Z",
    "merged_at": "2024-05-22T08:24:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1563"
  },
  {
    "number": 1562,
    "title": "fix CI-summary branch",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-22T01:54:20Z",
    "closed_at": "2024-05-22T01:56:35Z",
    "merged_at": "2024-05-22T01:56:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1562"
  },
  {
    "number": 1560,
    "title": "update requirement",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-21T09:23:52Z",
    "closed_at": "2024-05-22T03:44:00Z",
    "merged_at": "2024-05-22T03:44:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1560"
  },
  {
    "number": 1559,
    "title": "add true_sequential for WOQ GPTQ",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-21T08:20:29Z",
    "closed_at": "2024-05-22T11:56:41Z",
    "merged_at": "2024-05-22T11:56:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1559"
  },
  {
    "number": 1558,
    "title": "[Gaudi] Add LLAMA Streaming LLM in Gaudi",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-21T06:13:52Z",
    "closed_at": "2024-05-22T01:57:52Z",
    "merged_at": "2024-05-22T01:57:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1558"
  },
  {
    "number": 1557,
    "title": "Update README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-21T03:04:38Z",
    "closed_at": "2024-05-24T02:50:43Z",
    "merged_at": "2024-05-24T02:50:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1557"
  },
  {
    "number": 1556,
    "title": "[NeuralChat][UI]: updated side_by_side client",
    "user": "cepera",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-20T15:06:27Z",
    "closed_at": "2024-06-07T09:54:40Z",
    "merged_at": "2024-06-07T09:54:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1556"
  },
  {
    "number": 1555,
    "title": "update BDBA",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-20T09:49:40Z",
    "closed_at": "2024-05-21T05:35:16Z",
    "merged_at": "2024-05-21T05:35:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1555"
  },
  {
    "number": 1554,
    "title": "adapt autoround release",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-20T09:45:58Z",
    "closed_at": "2024-05-22T02:50:47Z",
    "merged_at": "2024-05-22T02:50:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1554"
  },
  {
    "number": 1553,
    "title": "Update run_generation_gpu_woq.py",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-20T07:53:17Z",
    "closed_at": "2024-05-21T02:58:15Z",
    "merged_at": "2024-05-21T02:58:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1553"
  },
  {
    "number": 1552,
    "title": "add more SDL scan",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-20T06:39:27Z",
    "closed_at": "2024-05-20T07:24:24Z",
    "merged_at": "2024-05-20T07:24:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1552"
  },
  {
    "number": 1551,
    "title": "[vLLM] Support vLLM CPU backend and provide QBits acceleration",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-20T03:43:32Z",
    "closed_at": "2024-05-24T08:18:01Z",
    "merged_at": "2024-05-24T08:18:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1551"
  },
  {
    "number": 1549,
    "title": "Remove the useless code about ipex cpu woq",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-16T07:21:59Z",
    "closed_at": "2024-05-17T03:02:06Z",
    "merged_at": "2024-05-17T03:02:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1549"
  },
  {
    "number": 1548,
    "title": "[vLLM] optimizing vLLM models by qbits.",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-15T06:58:15Z",
    "closed_at": "2024-05-20T03:43:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1548"
  },
  {
    "number": 1547,
    "title": "fix url issue",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-15T03:55:01Z",
    "closed_at": "2024-05-15T07:48:13Z",
    "merged_at": "2024-05-15T07:48:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1547"
  },
  {
    "number": 1546,
    "title": "Support ipex cpu WOQ backend",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-14T09:16:36Z",
    "closed_at": "2024-05-16T02:08:44Z",
    "merged_at": "2024-05-16T02:08:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1546"
  },
  {
    "number": 1545,
    "title": "add schema to control asym/sym",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-13T08:13:16Z",
    "closed_at": "2024-05-14T03:52:39Z",
    "merged_at": "2024-05-14T03:52:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1545"
  },
  {
    "number": 1544,
    "title": "Update config.py due to qbits expanded asym scheme and scale scope",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-13T06:11:51Z",
    "closed_at": "2024-05-14T10:32:18Z",
    "merged_at": "2024-05-14T10:32:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1544"
  },
  {
    "number": 1543,
    "title": "lm-eval for llama.cpp enhancement.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-12T09:55:28Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1543"
  },
  {
    "number": 1542,
    "title": "requirements.txt for intel_extension_for_transformers/neural_chat/examples/helloworld",
    "user": "fbaldassarri",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-11T22:02:50Z",
    "closed_at": "2024-06-07T08:50:47Z",
    "merged_at": "2024-06-07T08:50:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1542"
  },
  {
    "number": 1541,
    "title": "[GPU] script for phi3 on windows",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-11T05:57:07Z",
    "closed_at": "2024-05-11T06:39:44Z",
    "merged_at": "2024-05-11T06:39:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1541"
  },
  {
    "number": 1540,
    "title": "Update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-11T04:12:18Z",
    "closed_at": "2024-05-11T04:35:50Z",
    "merged_at": "2024-05-11T04:35:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1540"
  },
  {
    "number": 1539,
    "title": "Update to 2.3.0",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-11T00:02:09Z",
    "closed_at": "2024-05-13T02:49:40Z",
    "merged_at": "2024-05-13T02:49:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1539"
  },
  {
    "number": 1537,
    "title": "Fix CPU WOQ scheme setting",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-09T09:12:05Z",
    "closed_at": "2024-05-09T09:19:58Z",
    "merged_at": "2024-05-09T09:19:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1537"
  },
  {
    "number": 1536,
    "title": "lm-eval supports tuple output return",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-09T08:41:57Z",
    "closed_at": "2024-05-10T05:11:32Z",
    "merged_at": "2024-05-10T05:11:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1536"
  },
  {
    "number": 1535,
    "title": "QBits adapt to the latest BesTLA",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-09T01:28:19Z",
    "closed_at": "2024-05-13T06:00:16Z",
    "merged_at": "2024-05-13T06:00:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1535"
  },
  {
    "number": 1534,
    "title": "Update requirements.txt",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-08T08:35:42Z",
    "closed_at": "2024-05-13T02:53:30Z",
    "merged_at": "2024-05-13T02:53:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1534"
  },
  {
    "number": 1532,
    "title": "skip ut for specific version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-08T04:21:16Z",
    "closed_at": "2024-05-08T08:54:50Z",
    "merged_at": "2024-05-08T08:54:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1532"
  },
  {
    "number": 1531,
    "title": "[NeuralChat] Revise the variable name",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-07T01:45:42Z",
    "closed_at": "2024-05-08T06:15:17Z",
    "merged_at": "2024-05-08T06:15:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1531"
  },
  {
    "number": 1530,
    "title": "move limitation of gradio to readme",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-07T01:10:04Z",
    "closed_at": "2024-05-07T05:39:21Z",
    "merged_at": "2024-05-07T05:39:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1530"
  },
  {
    "number": 1529,
    "title": "update deepspeed version for habana docker",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-06T14:06:16Z",
    "closed_at": "2024-05-07T01:05:42Z",
    "merged_at": "2024-05-07T01:05:42Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1529"
  },
  {
    "number": 1528,
    "title": "Update dependency",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-06T09:37:36Z",
    "closed_at": "2024-05-06T10:21:18Z",
    "merged_at": "2024-05-06T10:21:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1528"
  },
  {
    "number": 1527,
    "title": "fix is_intel_gpu_available",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-06T06:34:41Z",
    "closed_at": "2024-05-07T02:46:56Z",
    "merged_at": "2024-05-07T02:46:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1527"
  },
  {
    "number": 1526,
    "title": "catch prepack error and fallback to torch bf16",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-06T04:06:38Z",
    "closed_at": "2024-06-07T09:55:13Z",
    "merged_at": "2024-06-07T09:55:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1526"
  },
  {
    "number": 1523,
    "title": "add streamingllm doc",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-30T07:53:41Z",
    "closed_at": "2024-05-06T07:28:13Z",
    "merged_at": "2024-05-06T07:28:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1523"
  },
  {
    "number": 1521,
    "title": "update talkingbot pc notebook",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-29T04:35:42Z",
    "closed_at": "2024-05-13T02:55:01Z",
    "merged_at": "2024-05-13T02:55:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1521"
  },
  {
    "number": 1520,
    "title": "Fix to use token latency to measure performance ",
    "user": "louie-tsai",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-27T21:38:50Z",
    "closed_at": "2024-05-13T06:04:57Z",
    "merged_at": "2024-05-13T06:04:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1520"
  },
  {
    "number": 1519,
    "title": "Add scale and weight dtype check for quantization config",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-26T08:47:46Z",
    "closed_at": "2024-04-28T05:49:00Z",
    "merged_at": "2024-04-28T05:49:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1519"
  },
  {
    "number": 1516,
    "title": "update",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-26T02:59:14Z",
    "closed_at": "2024-04-26T09:22:59Z",
    "merged_at": "2024-04-26T09:22:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1516"
  },
  {
    "number": 1515,
    "title": "enhance voicechat api with multilang tts streaming support",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-25T06:07:27Z",
    "closed_at": "2024-04-29T06:52:45Z",
    "merged_at": "2024-04-29T06:52:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1515"
  },
  {
    "number": 1514,
    "title": "add qbits ci",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-25T05:42:43Z",
    "closed_at": "2024-04-25T09:30:08Z",
    "merged_at": "2024-04-25T09:30:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1514"
  },
  {
    "number": 1513,
    "title": "add bias internal convertion in qbits",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-25T04:29:08Z",
    "closed_at": "2024-04-29T06:52:07Z",
    "merged_at": "2024-04-29T06:52:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1513"
  },
  {
    "number": 1512,
    "title": "Ipex pvc",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-25T02:39:49Z",
    "closed_at": "2024-05-13T03:04:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1512"
  },
  {
    "number": 1511,
    "title": "fix tf autodistill bug of transformers>=4.37",
    "user": "n1ck-guo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-25T02:19:48Z",
    "closed_at": "2024-04-25T03:22:01Z",
    "merged_at": "2024-04-25T03:22:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1511"
  },
  {
    "number": 1510,
    "title": "restrain qdrant-client to 1.8.2 and langchain-core to 0.1.35",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-24T07:20:55Z",
    "closed_at": "2024-04-24T07:40:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1510"
  },
  {
    "number": 1509,
    "title": "update requirements",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-24T06:51:28Z",
    "closed_at": "2024-04-24T06:57:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1509"
  },
  {
    "number": 1508,
    "title": "restrain torch to 2.2.0",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-24T04:38:59Z",
    "closed_at": "2024-04-24T08:49:10Z",
    "merged_at": "2024-04-24T08:49:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1508"
  },
  {
    "number": 1505,
    "title": "Add DynamicQuantConfig and QuantAwareTrainingConfig",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-23T07:23:00Z",
    "closed_at": "2024-04-25T06:44:46Z",
    "merged_at": "2024-04-25T06:44:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1505"
  },
  {
    "number": 1504,
    "title": "Integrate EAGLE with ITREX",
    "user": "siddhivelankar23",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-23T06:04:31Z",
    "closed_at": "2024-05-09T14:44:20Z",
    "merged_at": "2024-05-09T14:44:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1504"
  },
  {
    "number": 1503,
    "title": "Fixed QLoRA CPU issue due to internal API change",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-23T02:27:26Z",
    "closed_at": "2024-04-30T07:16:59Z",
    "merged_at": "2024-04-30T07:16:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1503"
  },
  {
    "number": 1502,
    "title": "Support ipex pvc test",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-22T14:54:55Z",
    "closed_at": "2024-04-25T02:40:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1502"
  },
  {
    "number": 1501,
    "title": "Add StaticQuantConfig",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-22T07:30:06Z",
    "closed_at": "2024-04-23T07:12:49Z",
    "merged_at": "2024-04-23T07:12:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1501"
  },
  {
    "number": 1500,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-21T08:37:39Z",
    "closed_at": "2024-04-21T08:42:09Z",
    "merged_at": "2024-04-21T08:42:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1500"
  },
  {
    "number": 1499,
    "title": "improve low bits model loading",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-19T14:10:57Z",
    "closed_at": "2024-04-21T02:50:24Z",
    "merged_at": "2024-04-21T02:50:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1499"
  },
  {
    "number": 1498,
    "title": "Refine the README for TPP usage",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-19T12:28:54Z",
    "closed_at": "2024-04-19T14:28:06Z",
    "merged_at": "2024-04-19T14:28:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1498"
  },
  {
    "number": 1497,
    "title": "update ci script",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-19T09:04:31Z",
    "closed_at": "2024-04-19T09:05:11Z",
    "merged_at": "2024-04-19T09:05:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1497"
  },
  {
    "number": 1496,
    "title": "update neuralchat notebook on cpu",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-19T08:33:33Z",
    "closed_at": "2024-04-19T11:49:09Z",
    "merged_at": "2024-04-19T11:49:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1496"
  },
  {
    "number": 1495,
    "title": "[Doc] update news",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-19T06:42:21Z",
    "closed_at": "2024-04-19T09:58:24Z",
    "merged_at": "2024-04-19T09:58:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1495"
  },
  {
    "number": 1494,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-19T02:01:08Z",
    "closed_at": "2024-04-19T02:21:25Z",
    "merged_at": "2024-04-19T02:21:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1494"
  },
  {
    "number": 1493,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-18T14:23:24Z",
    "closed_at": "2024-04-18T14:24:20Z",
    "merged_at": "2024-04-18T14:24:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1493"
  },
  {
    "number": 1492,
    "title": "Fix example llm recipes and autoround default value.",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-18T07:26:02Z",
    "closed_at": "2024-04-19T12:10:06Z",
    "merged_at": "2024-04-19T12:10:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1492"
  },
  {
    "number": 1491,
    "title": "doc: add example for autoround and llama3 on intel gpu",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-18T03:25:37Z",
    "closed_at": "2024-04-19T06:51:28Z",
    "merged_at": "2024-04-19T06:51:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1491"
  },
  {
    "number": 1489,
    "title": "Support multiple weight dtype",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T08:44:29Z",
    "closed_at": "2024-04-19T03:56:08Z",
    "merged_at": "2024-04-19T03:56:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1489"
  },
  {
    "number": 1488,
    "title": "Fixed loading issue of woq model with parameters",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T07:18:48Z",
    "closed_at": "2024-04-19T11:07:45Z",
    "merged_at": "2024-04-19T11:07:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1488"
  },
  {
    "number": 1487,
    "title": "[NeuralChat] Update README and fix retrieval dependency",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T05:56:33Z",
    "closed_at": "2024-04-17T07:52:45Z",
    "merged_at": "2024-04-17T07:52:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1487"
  },
  {
    "number": 1486,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T02:19:07Z",
    "closed_at": "2024-04-17T02:44:20Z",
    "merged_at": "2024-04-17T02:44:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1486"
  },
  {
    "number": 1485,
    "title": "Update Test Requirement ",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T01:42:09Z",
    "closed_at": "2024-04-17T03:33:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1485"
  },
  {
    "number": 1483,
    "title": "update notebooks for cpu",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-15T08:31:44Z",
    "closed_at": "2024-04-19T03:53:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1483"
  },
  {
    "number": 1482,
    "title": "Removed fallback for lm_head op",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-15T02:50:46Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1482"
  },
  {
    "number": 1481,
    "title": "Update SPR chatbot notebook",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-14T15:37:46Z",
    "closed_at": "2024-04-17T09:07:00Z",
    "merged_at": "2024-04-17T09:07:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1481"
  },
  {
    "number": 1479,
    "title": "move get_gpu_family to is_gpu_available",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-13T17:25:08Z",
    "closed_at": "2024-04-17T02:12:18Z",
    "merged_at": "2024-04-17T02:12:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1479"
  },
  {
    "number": 1478,
    "title": "Add text-gen finetune workflow for glue mnli",
    "user": "mini-goel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-12T06:54:55Z",
    "closed_at": "2024-06-07T08:52:27Z",
    "merged_at": "2024-06-07T08:52:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1478"
  },
  {
    "number": 1477,
    "title": "[NeuralChat] Update code for running on Windows",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-12T03:33:48Z",
    "closed_at": "2024-04-17T06:12:20Z",
    "merged_at": "2024-04-17T06:12:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1477"
  },
  {
    "number": 1476,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-12T02:15:02Z",
    "closed_at": "2024-04-12T02:16:53Z",
    "merged_at": "2024-04-12T02:16:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1476"
  },
  {
    "number": 1475,
    "title": "delete Intel font",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-11T10:47:47Z",
    "closed_at": "2024-04-29T12:39:46Z",
    "merged_at": "2024-04-29T12:39:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1475"
  },
  {
    "number": 1474,
    "title": "[NeuralChat] Fix TPP support for single socket",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-11T08:07:27Z",
    "closed_at": "2024-04-17T06:14:11Z",
    "merged_at": "2024-04-17T06:14:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1474"
  },
  {
    "number": 1473,
    "title": "fix QBits actshuf buf overflow under large batch",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-11T06:11:27Z",
    "closed_at": "2024-04-15T07:39:52Z",
    "merged_at": "2024-04-15T07:39:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1473"
  },
  {
    "number": 1472,
    "title": "Update neural-chat example",
    "user": "xiguiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-11T02:54:02Z",
    "closed_at": "2024-04-12T09:07:21Z",
    "merged_at": "2024-04-12T09:07:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1472"
  },
  {
    "number": 1471,
    "title": "Adapted textual inversion distillation for quantization example to latest transformers and diffusers packages",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-11T01:43:01Z",
    "closed_at": "2024-04-15T01:40:23Z",
    "merged_at": "2024-04-15T01:40:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1471"
  },
  {
    "number": 1470,
    "title": "Bump transformers from 4.36.2 to 4.38.0 in /intel_extension_for_transformers/neural_chat/tests",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-04-10T23:00:35Z",
    "closed_at": "2024-04-11T05:20:31Z",
    "merged_at": "2024-04-11T05:20:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1470"
  },
  {
    "number": 1468,
    "title": "h2o for kv cache compression",
    "user": "n1ck-guo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-10T07:37:20Z",
    "closed_at": "2024-07-29T05:58:47Z",
    "merged_at": "2024-07-29T05:58:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1468"
  },
  {
    "number": 1467,
    "title": "Added example for table extraction, and enabled multi-page table handling pipeline",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-10T06:48:44Z",
    "closed_at": "2024-04-10T12:05:32Z",
    "merged_at": "2024-04-10T12:05:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1467"
  },
  {
    "number": 1466,
    "title": "[Transformers] Refine Model `from_pretrained` When `use_neural_speed`",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-09T09:34:23Z",
    "closed_at": "2024-04-11T03:46:37Z",
    "merged_at": "2024-04-11T03:46:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1466"
  },
  {
    "number": 1465,
    "title": "update vision demo ui",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-09T05:30:17Z",
    "closed_at": "2024-04-09T07:34:46Z",
    "merged_at": "2024-04-09T07:34:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1465"
  },
  {
    "number": 1462,
    "title": "upgrade lm-eval to 0.4.2",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-08T12:00:50Z",
    "closed_at": "2024-04-18T05:44:25Z",
    "merged_at": "2024-04-18T05:44:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1462"
  },
  {
    "number": 1460,
    "title": "support ipex arc test",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-08T08:09:57Z",
    "closed_at": "2024-04-11T15:47:28Z",
    "merged_at": "2024-04-11T15:47:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1460"
  },
  {
    "number": 1459,
    "title": "get packed-weight size via config",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-08T06:14:32Z",
    "closed_at": "2024-04-10T12:04:31Z",
    "merged_at": "2024-04-10T12:04:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1459"
  },
  {
    "number": 1458,
    "title": "[NeuralChat] Add restriction for torch version ",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-08T05:59:26Z",
    "closed_at": "2024-04-09T01:44:48Z",
    "merged_at": "2024-04-09T01:44:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1458"
  },
  {
    "number": 1457,
    "title": "support qwen and baichuan with neuralchat ns=False",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-08T05:10:01Z",
    "closed_at": "2024-04-08T07:06:15Z",
    "merged_at": "2024-04-08T07:06:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1457"
  },
  {
    "number": 1456,
    "title": "add visison demo front_end code",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-07T13:09:47Z",
    "closed_at": "2024-04-08T12:58:48Z",
    "merged_at": "2024-04-08T12:58:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1456"
  },
  {
    "number": 1455,
    "title": "[NeuralChat] Add new customized chabot UI",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-03T14:05:45Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1455"
  },
  {
    "number": 1454,
    "title": "Update run_generation_gpu_woq.py for auto round",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-03T03:13:13Z",
    "closed_at": "2024-04-03T05:59:16Z",
    "merged_at": "2024-04-03T05:59:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1454"
  },
  {
    "number": 1453,
    "title": "Update docker",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-03T01:05:26Z",
    "closed_at": "2024-04-03T02:45:10Z",
    "merged_at": "2024-04-03T02:45:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1453"
  },
  {
    "number": 1452,
    "title": "Fix param",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-03T00:33:43Z",
    "closed_at": "2024-04-03T00:45:07Z",
    "merged_at": "2024-04-03T00:45:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1452"
  },
  {
    "number": 1451,
    "title": "fix typing",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-02T09:58:15Z",
    "closed_at": "2024-04-03T03:01:15Z",
    "merged_at": "2024-04-03T03:01:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1451"
  },
  {
    "number": 1450,
    "title": "fix lm_eval params",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-02T08:00:11Z",
    "closed_at": "2024-04-02T08:13:33Z",
    "merged_at": "2024-04-02T08:13:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1450"
  },
  {
    "number": 1449,
    "title": "[Transformers] Support load mode from HF Hub when use Neural Speed",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-02T07:59:21Z",
    "closed_at": "2024-04-03T14:36:37Z",
    "merged_at": "2024-04-03T14:36:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1449"
  },
  {
    "number": 1448,
    "title": "Bump transformers from 4.35.2 to 4.36.0 in /examples/huggingface/pytorch/text-generation/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-04-02T05:57:53Z",
    "closed_at": "2024-04-03T02:46:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1448"
  },
  {
    "number": 1447,
    "title": "[NeuralChat] Suport pptx format for RAG",
    "user": "xmx-521",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-02T02:48:06Z",
    "closed_at": "2024-06-14T04:17:58Z",
    "merged_at": "2024-06-14T04:17:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1447"
  },
  {
    "number": 1446,
    "title": "[pre-commit.ci] pre-commit autoupdate",
    "user": "pre-commit-ci[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-04-01T22:15:19Z",
    "closed_at": "2024-04-09T06:15:44Z",
    "merged_at": "2024-04-09T06:15:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1446"
  },
  {
    "number": 1445,
    "title": "[NeuralChat] Support TPP for Xeon Tensor Parallel",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T16:38:00Z",
    "closed_at": "2024-04-10T12:05:10Z",
    "merged_at": "2024-04-10T12:05:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1445"
  },
  {
    "number": 1444,
    "title": "Update tensorflow examples",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T08:17:39Z",
    "closed_at": "2024-04-02T07:31:08Z",
    "merged_at": "2024-04-02T07:31:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1444"
  },
  {
    "number": 1443,
    "title": "[NeuralChat] Support deepspeed for textchat API",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T07:15:57Z",
    "closed_at": "2024-04-02T09:56:26Z",
    "merged_at": "2024-04-02T09:56:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1443"
  },
  {
    "number": 1442,
    "title": "add FP8Config",
    "user": "mengniwang95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T02:56:24Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1442"
  },
  {
    "number": 1441,
    "title": "[NeuralChat] Add notebook for chatbot with TP",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T02:54:55Z",
    "closed_at": "2024-04-17T06:14:43Z",
    "merged_at": "2024-04-17T06:14:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1441"
  },
  {
    "number": 1440,
    "title": "[doc] add mtl example and update get_gpu_family",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T00:40:50Z",
    "closed_at": "2024-04-09T10:11:49Z",
    "merged_at": "2024-04-09T10:11:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1440"
  },
  {
    "number": 1439,
    "title": "Support load WOQ model from HF model hub",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-31T10:03:49Z",
    "closed_at": "2024-04-02T08:41:39Z",
    "merged_at": "2024-04-02T08:41:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1439"
  },
  {
    "number": 1438,
    "title": "add gaudi modeling support in itrex",
    "user": "ClarkChin08",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T06:11:51Z",
    "closed_at": "2024-05-24T02:36:52Z",
    "merged_at": "2024-05-24T02:36:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1438"
  },
  {
    "number": 1437,
    "title": "Unify the parameters for woq examples by CPU and GPU",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T15:02:10Z",
    "closed_at": "2024-03-29T03:03:21Z",
    "merged_at": "2024-03-29T03:03:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1437"
  },
  {
    "number": 1435,
    "title": "Checkmarx fix",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T11:44:28Z",
    "closed_at": "2024-03-29T03:28:53Z",
    "merged_at": "2024-03-29T03:28:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1435"
  },
  {
    "number": 1433,
    "title": "[NeuralChat] Add file management in RAG api",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T02:26:58Z",
    "closed_at": "2024-03-29T07:48:19Z",
    "merged_at": "2024-03-29T07:48:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1433"
  },
  {
    "number": 1432,
    "title": "Bump langchain-core from 0.1.34 to 0.1.35 in /intel_extension_for_transformers/langchain/langchain_community/embeddings",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-03-27T18:01:56Z",
    "closed_at": "2024-03-28T02:30:30Z",
    "merged_at": "2024-03-28T02:30:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1432"
  },
  {
    "number": 1431,
    "title": "limit onnx version to 1.15 before onnxruntime next release to match onnx 1.16",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T12:26:29Z",
    "closed_at": "2024-03-27T13:01:39Z",
    "merged_at": "2024-03-27T13:01:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1431"
  },
  {
    "number": 1430,
    "title": "Fix example dependency version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T07:25:47Z",
    "closed_at": "2024-03-27T10:52:21Z",
    "merged_at": "2024-03-27T10:52:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1430"
  },
  {
    "number": 1429,
    "title": "refine the doc",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T06:26:26Z",
    "closed_at": "2024-04-03T07:14:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1429"
  },
  {
    "number": 1428,
    "title": "Support AutoRound quantization method for intel GPU",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T05:27:46Z",
    "closed_at": "2024-04-02T05:56:46Z",
    "merged_at": "2024-04-02T05:56:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1428"
  },
  {
    "number": 1427,
    "title": "fix code-generation params issue",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T03:40:49Z",
    "closed_at": "2024-04-01T06:10:39Z",
    "merged_at": "2024-04-01T06:10:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1427"
  },
  {
    "number": 1426,
    "title": "[Example] Update Neural_Speed Eval",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T02:40:39Z",
    "closed_at": "2024-03-28T02:49:36Z",
    "merged_at": "2024-03-28T02:49:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1426"
  },
  {
    "number": 1425,
    "title": "Bump langchain-core from 0.1.30 to 0.1.34 in /intel_extension_for_transformers/langchain/langchain_community/embeddings",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-03-26T21:43:44Z",
    "closed_at": "2024-03-27T02:40:59Z",
    "merged_at": "2024-03-27T02:40:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1425"
  },
  {
    "number": 1424,
    "title": "[NeuralChat] Fix code generation code format issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-26T15:35:22Z",
    "closed_at": "2024-03-27T02:32:12Z",
    "merged_at": "2024-03-27T02:32:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1424"
  },
  {
    "number": 1423,
    "title": "[NeuralChat] Save the parsed html file into a local jsonl file for better knowledge management",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-26T09:41:15Z",
    "closed_at": "2024-03-27T03:02:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1423"
  },
  {
    "number": 1422,
    "title": "fix a bug in run_generation_gpu_woq.py",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-26T03:38:28Z",
    "closed_at": "2024-03-28T08:33:06Z",
    "merged_at": "2024-03-28T08:33:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1422"
  },
  {
    "number": 1420,
    "title": "update LLM recipes",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T11:36:44Z",
    "closed_at": "2024-03-26T09:52:08Z",
    "merged_at": "2024-03-26T09:52:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1420"
  },
  {
    "number": 1419,
    "title": "Fix woq autoround last layer quant issue",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T09:19:09Z",
    "closed_at": "2024-03-27T02:03:50Z",
    "merged_at": "2024-03-27T02:03:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1419"
  },
  {
    "number": 1418,
    "title": "Update requirement",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T09:04:51Z",
    "closed_at": "2024-03-25T09:05:34Z",
    "merged_at": "2024-03-25T09:05:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1418"
  },
  {
    "number": 1417,
    "title": "[NeuralChat] Enable RAG's table extraction and summary",
    "user": "xmx-521",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T07:19:30Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1417"
  },
  {
    "number": 1416,
    "title": "[Doc] Fix README ITREX V1.3 NS link",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T07:09:23Z",
    "closed_at": "2024-03-25T08:41:27Z",
    "merged_at": "2024-03-25T08:41:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1416"
  },
  {
    "number": 1415,
    "title": "[NeuralChat] Update retrieval requirements",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T07:05:47Z",
    "closed_at": "2024-03-25T08:41:40Z",
    "merged_at": "2024-03-25T08:41:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1415"
  },
  {
    "number": 1414,
    "title": "check and convert contiguous tensor when model saving",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T04:50:33Z",
    "closed_at": "2024-03-25T09:14:15Z",
    "merged_at": "2024-03-25T09:14:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1414"
  },
  {
    "number": 1413,
    "title": "Fixed exmple error for Intel GPU WOQ",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T03:41:34Z",
    "closed_at": "2024-03-25T03:50:56Z",
    "merged_at": "2024-03-25T03:50:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1413"
  },
  {
    "number": 1412,
    "title": "Fixed import error for neural_speed",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T02:14:39Z",
    "closed_at": "2024-03-25T07:40:31Z",
    "merged_at": "2024-03-25T07:40:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1412"
  },
  {
    "number": 1411,
    "title": "Changed regular expression",
    "user": "igeni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-23T12:21:37Z",
    "closed_at": "2024-03-25T01:37:29Z",
    "merged_at": "2024-03-25T01:37:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1411"
  },
  {
    "number": 1410,
    "title": "Remove redundant parameters for WOQ saving config and fix GPTQ issue",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-22T08:29:20Z",
    "closed_at": "2024-03-25T00:49:39Z",
    "merged_at": "2024-03-25T00:49:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1410"
  },
  {
    "number": 1409,
    "title": "update requirement",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-22T06:31:03Z",
    "closed_at": "2024-03-22T06:41:11Z",
    "merged_at": "2024-03-22T06:41:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1409"
  },
  {
    "number": 1408,
    "title": "update sparseGPT example",
    "user": "WeiweiZhang1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-22T01:19:30Z",
    "closed_at": "2024-03-25T01:37:16Z",
    "merged_at": "2024-03-25T01:37:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1408"
  },
  {
    "number": 1406,
    "title": "Fix SQ model restore loading",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-21T09:35:32Z",
    "closed_at": "2024-03-22T07:32:36Z",
    "merged_at": "2024-03-22T07:32:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1406"
  },
  {
    "number": 1405,
    "title": "[Neural Speed] Add the model_type list for GPTQ",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-21T07:34:01Z",
    "closed_at": "2024-03-22T03:39:04Z",
    "merged_at": "2024-03-22T03:39:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1405"
  },
  {
    "number": 1404,
    "title": "Update woq code for intel GPU",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-21T07:06:36Z",
    "closed_at": "2024-03-22T08:48:34Z",
    "merged_at": "2024-03-22T08:48:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1404"
  },
  {
    "number": 1403,
    "title": "restrain transformers",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-21T03:35:35Z",
    "closed_at": "2024-03-21T04:01:27Z",
    "merged_at": "2024-03-21T04:01:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1403"
  },
  {
    "number": 1402,
    "title": "[Transformers] Add utils.py in tools",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-21T02:27:00Z",
    "closed_at": "2024-03-21T05:32:52Z",
    "merged_at": "2024-03-21T05:32:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1402"
  },
  {
    "number": 1401,
    "title": "Add neuralchat quick start example",
    "user": "xiguiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T09:15:51Z",
    "closed_at": "2024-03-27T06:15:31Z",
    "merged_at": "2024-03-27T06:15:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1401"
  },
  {
    "number": 1400,
    "title": "Fix WOQ huggingface model loading",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T07:43:20Z",
    "closed_at": "2024-03-21T01:30:33Z",
    "merged_at": "2024-03-21T01:30:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1400"
  },
  {
    "number": 1399,
    "title": "Promote qbits as itrex module",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T03:22:31Z",
    "closed_at": "2024-03-21T09:25:24Z",
    "merged_at": "2024-03-21T09:25:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1399"
  },
  {
    "number": 1398,
    "title": "[NeuralChat] Fix UT TestClient.client=None issue",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T02:43:47Z",
    "closed_at": "2024-03-20T05:54:13Z",
    "merged_at": "2024-03-20T05:54:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1398"
  },
  {
    "number": 1397,
    "title": "[NeuralChat] Fix path in UT",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T01:59:35Z",
    "closed_at": "2024-03-21T01:31:24Z",
    "merged_at": "2024-03-21T01:31:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1397"
  },
  {
    "number": 1396,
    "title": "update test",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T01:48:54Z",
    "closed_at": "2024-03-21T01:29:45Z",
    "merged_at": "2024-03-21T01:29:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1396"
  },
  {
    "number": 1395,
    "title": "Fix gptq desc_act and static_group",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-19T08:45:13Z",
    "closed_at": "2024-03-20T00:54:17Z",
    "merged_at": "2024-03-20T00:54:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1395"
  },
  {
    "number": 1394,
    "title": "[Neural Speed] Add the model_type list for GPTQ.",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-19T08:38:13Z",
    "closed_at": "2024-03-21T07:35:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1394"
  },
  {
    "number": 1393,
    "title": "Fix WOQ int8 unpack weight",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-19T07:42:35Z",
    "closed_at": "2024-03-20T00:53:59Z",
    "merged_at": "2024-03-20T00:53:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1393"
  },
  {
    "number": 1391,
    "title": "add layerwise for WOQ Rtn&GPTQ",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-19T06:03:36Z",
    "closed_at": "2024-03-21T05:34:21Z",
    "merged_at": "2024-03-21T05:34:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1391"
  },
  {
    "number": 1390,
    "title": "Update README for intel GPU weight only quantization",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-18T10:29:16Z",
    "closed_at": "2024-03-19T03:54:19Z",
    "merged_at": "2024-03-19T03:54:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1390"
  },
  {
    "number": 1389,
    "title": "Clean dependencies in requirements.txt and setup.py",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-18T06:36:35Z",
    "closed_at": "2024-03-18T09:58:59Z",
    "merged_at": "2024-03-18T09:58:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1389"
  },
  {
    "number": 1388,
    "title": "[NeuralChat] Fix tgi endpoint in test",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-18T06:30:25Z",
    "closed_at": "2024-03-18T07:54:08Z",
    "merged_at": "2024-03-18T07:54:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1388"
  },
  {
    "number": 1387,
    "title": "[NeuralChat] Fix index error in Child-parent retriever",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-18T05:56:07Z",
    "closed_at": "2024-03-18T07:53:37Z",
    "merged_at": "2024-03-18T07:53:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1387"
  },
  {
    "number": 1386,
    "title": "[NeuralChat] Refine document",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-18T02:03:59Z",
    "closed_at": "2024-03-18T06:17:13Z",
    "merged_at": "2024-03-18T06:17:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1386"
  },
  {
    "number": 1385,
    "title": "Bump langchain-core from 0.1.18 to 0.1.30 in /intel_extension_for_transformers/langchain/embeddings",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-03-14T23:17:19Z",
    "closed_at": "2024-03-15T07:10:55Z",
    "merged_at": "2024-03-15T07:10:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1385"
  },
  {
    "number": 1384,
    "title": "modify readme",
    "user": "sramakintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-14T17:58:07Z",
    "closed_at": "2024-03-19T03:34:05Z",
    "merged_at": "2024-03-19T03:34:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1384"
  },
  {
    "number": 1383,
    "title": "fix readme",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-14T09:16:47Z",
    "closed_at": "2024-03-14T09:48:25Z",
    "merged_at": "2024-03-14T09:48:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1383"
  },
  {
    "number": 1382,
    "title": "[Neural Speed]load model from modelscope",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-14T08:46:29Z",
    "closed_at": "2024-03-27T11:45:50Z",
    "merged_at": "2024-03-27T11:45:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1382"
  },
  {
    "number": 1380,
    "title": "Support IPEX bf16 & fp32 optimization for emebedding model ",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-14T02:35:14Z",
    "closed_at": "2024-03-14T06:40:22Z",
    "merged_at": "2024-03-14T06:40:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1380"
  },
  {
    "number": 1378,
    "title": "[NeuralChat] Add readme, add content length filter, fix build error",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T16:11:22Z",
    "closed_at": "2024-03-18T01:48:16Z",
    "merged_at": "2024-03-18T01:48:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1378"
  },
  {
    "number": 1376,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T10:57:23Z",
    "closed_at": "2024-03-13T11:01:09Z",
    "merged_at": "2024-03-13T11:01:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1376"
  },
  {
    "number": 1375,
    "title": "Improve WOQ model saving and loading",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T10:29:17Z",
    "closed_at": "2024-03-18T14:15:09Z",
    "merged_at": "2024-03-18T14:15:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1375"
  },
  {
    "number": 1374,
    "title": "add version limit for WOQConfig",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T07:49:23Z",
    "closed_at": "2024-03-14T02:10:14Z",
    "merged_at": "2024-03-14T02:10:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1374"
  },
  {
    "number": 1373,
    "title": "Fixed weight-only config save issue",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T07:24:37Z",
    "closed_at": "2024-03-13T09:41:41Z",
    "merged_at": "2024-03-13T09:41:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1373"
  },
  {
    "number": 1372,
    "title": "[NeuralChat] Enabled image2text finetuning and added an example",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T06:32:13Z",
    "closed_at": "2024-03-14T06:47:30Z",
    "merged_at": "2024-03-14T06:47:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1372"
  },
  {
    "number": 1371,
    "title": "restrain langchain community version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T03:31:15Z",
    "closed_at": "2024-03-13T05:15:08Z",
    "merged_at": "2024-03-13T05:15:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1371"
  },
  {
    "number": 1370,
    "title": "update auto-round version",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T02:13:42Z",
    "closed_at": "2024-03-14T02:31:19Z",
    "merged_at": "2024-03-14T02:31:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1370"
  },
  {
    "number": 1369,
    "title": "[Neural Speed]add load model from modelscope api",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-12T11:27:48Z",
    "closed_at": "2024-03-14T08:46:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1369"
  },
  {
    "number": 1368,
    "title": "[Example] update the deprate example dataset load",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-12T09:23:32Z",
    "closed_at": "2024-03-12T11:16:01Z",
    "merged_at": "2024-03-12T11:16:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1368"
  },
  {
    "number": 1367,
    "title": "[NeuralChat] Fix finetuning unit test issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-12T08:15:34Z",
    "closed_at": "2024-03-12T11:14:22Z",
    "merged_at": "2024-03-12T11:14:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1367"
  },
  {
    "number": 1366,
    "title": "[NeuralChat] Enlarge the context window for HPU graph recompile",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-12T06:04:32Z",
    "closed_at": "2024-03-12T11:15:17Z",
    "merged_at": "2024-03-12T11:15:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1366"
  },
  {
    "number": 1364,
    "title": "[NeuralChat]Update the character checking function to enable the Chinese character",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-11T11:11:19Z",
    "closed_at": "2024-03-12T05:17:02Z",
    "merged_at": "2024-03-12T05:17:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1364"
  },
  {
    "number": 1363,
    "title": "[NeuralChat] enable lm_eval during training.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-11T08:38:02Z",
    "closed_at": "2024-03-15T02:27:32Z",
    "merged_at": "2024-03-15T02:27:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1363"
  },
  {
    "number": 1362,
    "title": "fix neuralchat docker readme",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-11T07:44:10Z",
    "closed_at": "2024-03-12T01:42:38Z",
    "merged_at": "2024-03-12T01:42:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1362"
  },
  {
    "number": 1361,
    "title": "[NeuralChat] Support language detection & translation for RAG chat",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-11T07:40:51Z",
    "closed_at": "2024-03-25T08:41:58Z",
    "merged_at": "2024-03-25T08:41:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1361"
  },
  {
    "number": 1360,
    "title": "[Reorg Folder] reorg transformers and langchain folders",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-11T02:41:32Z",
    "closed_at": "2024-03-19T08:37:41Z",
    "merged_at": "2024-03-19T08:37:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1360"
  },
  {
    "number": 1359,
    "title": "Fix lm-eval neuralspeed loading model",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-11T02:41:08Z",
    "closed_at": "2024-03-11T06:02:15Z",
    "merged_at": "2024-03-11T06:02:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1359"
  },
  {
    "number": 1358,
    "title": "[NeuralChat] Adding LLaVA-NeXT",
    "user": "dillonalaird",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-10T22:26:18Z",
    "closed_at": "2024-03-27T05:29:34Z",
    "merged_at": "2024-03-27T05:29:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1358"
  },
  {
    "number": 1356,
    "title": "Refine WOQ Config",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-07T09:16:42Z",
    "closed_at": "2024-03-13T05:48:35Z",
    "merged_at": "2024-03-13T05:48:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1356"
  },
  {
    "number": 1355,
    "title": "Fix modeling_auto trust_remote_code issue",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-07T08:17:59Z",
    "closed_at": "2024-03-07T10:56:09Z",
    "merged_at": "2024-03-07T10:56:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1355"
  },
  {
    "number": 1354,
    "title": "update demo for ipex gpu",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-07T07:11:01Z",
    "closed_at": "2024-03-08T09:23:22Z",
    "merged_at": "2024-03-08T09:23:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1354"
  },
  {
    "number": 1353,
    "title": "delete package-lock.json file",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-07T02:11:24Z",
    "closed_at": "2024-03-07T03:13:06Z",
    "merged_at": "2024-03-07T03:13:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1353"
  },
  {
    "number": 1351,
    "title": "Bump langchain from 0.0.354 to 0.1.11 in /intel_extension_for_transformers/neural_chat/tests",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-03-06T16:40:12Z",
    "closed_at": "2024-03-12T06:15:05Z",
    "merged_at": "2024-03-12T06:15:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1351"
  },
  {
    "number": 1350,
    "title": "Bump langchain from 0.0.354 to 0.1.11 in /intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-03-06T16:40:02Z",
    "closed_at": "2024-03-12T10:48:50Z",
    "merged_at": "2024-03-12T10:48:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1350"
  },
  {
    "number": 1349,
    "title": "weight only quantization",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-06T10:01:57Z",
    "closed_at": "2024-03-09T00:04:37Z",
    "merged_at": "2024-03-09T00:04:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1349"
  },
  {
    "number": 1348,
    "title": "improve autoround args",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-06T07:39:38Z",
    "closed_at": "2024-03-07T07:47:43Z",
    "merged_at": "2024-03-07T07:47:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1348"
  },
  {
    "number": 1347,
    "title": "[NeuralChat] add example to use RAG+OpenAI LLM",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-06T05:36:36Z",
    "closed_at": "2024-03-07T02:11:31Z",
    "merged_at": "2024-03-07T02:11:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1347"
  },
  {
    "number": 1346,
    "title": "add more ns examples",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-06T04:20:38Z",
    "closed_at": "2024-03-13T05:35:55Z",
    "merged_at": "2024-03-13T05:35:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1346"
  },
  {
    "number": 1344,
    "title": "[NeuralChat] Added description of task argument for each finetuning dataset in NeuralChat",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-05T06:19:47Z",
    "closed_at": "2024-03-05T13:06:27Z",
    "merged_at": "2024-03-05T13:06:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1344"
  },
  {
    "number": 1343,
    "title": "Update WOQ doc and example README",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-05T03:24:01Z",
    "closed_at": "2024-03-07T02:17:40Z",
    "merged_at": "2024-03-07T02:17:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1343"
  },
  {
    "number": 1342,
    "title": "[NeuralChat] Fix audio plugin sample code issue and provide a way to set tts/asr model path",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-04T14:33:10Z",
    "closed_at": "2024-03-05T06:33:51Z",
    "merged_at": "2024-03-05T06:33:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1342"
  },
  {
    "number": 1341,
    "title": "Bump undici and @sveltejs/kit in /intel_extension_for_transformers/neural_chat/ui/customized/side_by_side",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-03-04T12:46:58Z",
    "closed_at": "2024-03-15T07:22:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1341"
  },
  {
    "number": 1340,
    "title": "Bump postcss from 8.4.27 to 8.4.31 in /intel_extension_for_transformers/neural_chat/ui/customized/side_by_side",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-03-04T12:46:11Z",
    "closed_at": "2024-03-07T03:16:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1340"
  },
  {
    "number": 1339,
    "title": "Bump vite from 4.4.9 to 4.5.2 in /intel_extension_for_transformers/neural_chat/ui/customized/side_by_side",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-03-04T12:46:00Z",
    "closed_at": "2024-03-07T03:14:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1339"
  },
  {
    "number": 1338,
    "title": "Add is_autoround_available to improve package install",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-04T09:16:47Z",
    "closed_at": "2024-03-04T15:08:12Z",
    "merged_at": "2024-03-04T15:08:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1338"
  },
  {
    "number": 1337,
    "title": "Add CI summary",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-04T07:33:45Z",
    "closed_at": "2024-03-11T03:16:45Z",
    "merged_at": "2024-03-11T03:16:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1337"
  },
  {
    "number": 1336,
    "title": "Update requirement",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-01T09:37:14Z",
    "closed_at": "2024-03-04T09:07:12Z",
    "merged_at": "2024-03-04T09:07:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1336"
  },
  {
    "number": 1335,
    "title": "Enhance embedding to support  jit model",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-01T09:24:13Z",
    "closed_at": "2024-03-08T12:37:29Z",
    "merged_at": "2024-03-08T12:37:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1335"
  },
  {
    "number": 1334,
    "title": "[Neuralchat] Support RAG with HF endpoint & add notebook",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-01T09:00:47Z",
    "closed_at": "2024-03-03T23:40:18Z",
    "merged_at": "2024-03-03T23:40:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1334"
  },
  {
    "number": 1333,
    "title": "[NeuralChat] RAG evaluation",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-01T03:11:59Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1333"
  },
  {
    "number": 1332,
    "title": "update requirement",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-01T03:09:27Z",
    "closed_at": "2024-03-01T06:39:04Z",
    "merged_at": "2024-03-01T06:39:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1332"
  },
  {
    "number": 1331,
    "title": "Update Quickstart NeuralChat notebook for workshops",
    "user": "alexsin368",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T22:59:21Z",
    "closed_at": "2024-03-09T00:05:28Z",
    "merged_at": "2024-03-09T00:05:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1331"
  },
  {
    "number": 1330,
    "title": "Improve WOQ algo autoround",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T12:35:13Z",
    "closed_at": "2024-03-01T12:27:40Z",
    "merged_at": "2024-03-01T12:27:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1330"
  },
  {
    "number": 1329,
    "title": "customized side by side",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T06:09:05Z",
    "closed_at": "2024-03-04T12:44:49Z",
    "merged_at": "2024-03-04T12:44:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1329"
  },
  {
    "number": 1328,
    "title": "[NeuralChat] Added finetuning example for gemma-2b on ARC",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T06:07:51Z",
    "closed_at": "2024-02-29T07:33:18Z",
    "merged_at": "2024-02-29T07:33:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1328"
  },
  {
    "number": 1327,
    "title": "[NeuralChat] Support microsoft/biogpt model as per the request",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-28T15:04:11Z",
    "closed_at": "2024-02-29T04:57:27Z",
    "merged_at": "2024-02-29T04:57:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1327"
  },
  {
    "number": 1326,
    "title": "fix distilgpt2 tf signature issue",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-28T03:54:21Z",
    "closed_at": "2024-02-29T05:55:48Z",
    "merged_at": "2024-02-29T05:55:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1326"
  },
  {
    "number": 1325,
    "title": "Add User input + max tokens requested exceeds model context window error response",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-28T03:23:14Z",
    "closed_at": "2024-02-29T06:36:29Z",
    "merged_at": "2024-02-29T06:36:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1325"
  },
  {
    "number": 1324,
    "title": "Fix clm model with transformers 4.38.1",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-27T09:53:40Z",
    "closed_at": "2024-02-28T05:46:56Z",
    "merged_at": "2024-02-28T05:46:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1324"
  },
  {
    "number": 1323,
    "title": "Update requirements for Chinese model NormalizedConfig",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-27T08:49:55Z",
    "closed_at": "2024-03-13T03:21:24Z",
    "merged_at": "2024-03-13T03:21:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1323"
  },
  {
    "number": 1322,
    "title": "[NeuralChat] enable RAG + ChatGPT flow",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-27T07:03:41Z",
    "closed_at": "2024-02-28T11:31:06Z",
    "merged_at": "2024-02-28T11:31:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1322"
  },
  {
    "number": 1321,
    "title": "[NeuralChat] Configure TGI endpoint from YAML",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-27T07:00:11Z",
    "closed_at": "2024-02-29T07:33:38Z",
    "merged_at": "2024-02-29T07:33:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1321"
  },
  {
    "number": 1320,
    "title": "[NeuralChat] fix speechbrain version",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-27T06:56:58Z",
    "closed_at": "2024-02-27T08:51:08Z",
    "merged_at": "2024-02-27T08:51:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1320"
  },
  {
    "number": 1319,
    "title": "[NeuralChat] support evaluation perplexity during training.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-27T04:26:02Z",
    "closed_at": "2024-02-28T07:07:23Z",
    "merged_at": "2024-02-28T07:07:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1319"
  },
  {
    "number": 1318,
    "title": "Add MLPerf Example",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-27T01:10:35Z",
    "closed_at": "2024-02-27T12:18:08Z",
    "merged_at": "2024-02-27T12:18:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1318"
  },
  {
    "number": 1317,
    "title": "Bump langchain from 0.0.354 to 0.1.0 in /intel_extension_for_transformers/neural_chat/tests",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-02-26T21:17:05Z",
    "closed_at": "2024-03-06T16:40:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1317"
  },
  {
    "number": 1316,
    "title": "Bump langchain from 0.0.354 to 0.1.0 in /intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-02-26T21:15:43Z",
    "closed_at": "2024-03-06T16:40:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1316"
  },
  {
    "number": 1314,
    "title": "[NeuralChat]Update the torch version for RAG",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-26T12:14:06Z",
    "closed_at": "2024-02-29T12:46:16Z",
    "merged_at": "2024-02-29T12:46:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1314"
  },
  {
    "number": 1313,
    "title": "[NeuralChat] Add bm25 into enabled retrievers and add Uts",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-26T11:33:11Z",
    "closed_at": "2024-02-29T12:47:12Z",
    "merged_at": "2024-02-29T12:47:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1313"
  },
  {
    "number": 1312,
    "title": "WOQ support autoround algo on cpu device",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-26T10:16:14Z",
    "closed_at": "2024-02-27T06:02:01Z",
    "merged_at": "2024-02-27T06:02:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1312"
  },
  {
    "number": 1310,
    "title": "[NeuralChat] Support chatbot inference with huggingface endpoint",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-26T09:32:18Z",
    "closed_at": "2024-02-27T12:22:40Z",
    "merged_at": "2024-02-27T12:22:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1310"
  },
  {
    "number": 1307,
    "title": "fix ut",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-24T02:46:10Z",
    "closed_at": "2024-02-24T03:52:55Z",
    "merged_at": "2024-02-24T03:52:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1307"
  },
  {
    "number": 1305,
    "title": "Update requirements",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-23T04:08:17Z",
    "closed_at": "2024-02-23T10:24:10Z",
    "merged_at": "2024-02-23T10:24:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1305"
  },
  {
    "number": 1304,
    "title": "Bump gradio from 3.36.0 to 4.19.2 in /intel_extension_for_transformers/neural_chat/ui/gradio/basic",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-02-23T02:46:44Z",
    "closed_at": "2024-02-23T10:25:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1304"
  },
  {
    "number": 1303,
    "title": "[NeuralChat] Fix Qwen chat model output undesirable on HPU issue caused by input padding",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-23T01:57:26Z",
    "closed_at": "2024-02-23T11:50:18Z",
    "merged_at": "2024-02-23T11:50:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1303"
  },
  {
    "number": 1302,
    "title": "Update interface.hpp",
    "user": "eltociear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-22T16:33:23Z",
    "closed_at": "2024-02-22T23:28:07Z",
    "merged_at": "2024-02-22T23:28:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1302"
  },
  {
    "number": 1301,
    "title": "[NeuralChat] Fix chat history issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-22T16:06:24Z",
    "closed_at": "2024-02-22T23:29:01Z",
    "merged_at": "2024-02-22T23:29:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1301"
  },
  {
    "number": 1299,
    "title": "[NeuralChat] Refined readme of ROME",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-22T08:30:51Z",
    "closed_at": "2024-02-22T23:29:51Z",
    "merged_at": "2024-02-22T23:29:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1299"
  },
  {
    "number": 1298,
    "title": "[API Changed] Dispatch the backend based on model_type",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-22T08:07:52Z",
    "closed_at": "2024-02-23T13:33:52Z",
    "merged_at": "2024-02-23T13:33:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1298"
  },
  {
    "number": 1297,
    "title": "Update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-21T13:58:32Z",
    "closed_at": "2024-02-21T14:00:33Z",
    "merged_at": "2024-02-21T14:00:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1297"
  },
  {
    "number": 1296,
    "title": "fix face animation error with new torch version",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-21T08:37:33Z",
    "closed_at": "2024-02-22T09:00:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1296"
  },
  {
    "number": 1295,
    "title": "[NeuralChat] enable mixtral lora finetuning on Gaudi2.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-21T06:51:46Z",
    "closed_at": "2024-03-14T06:43:58Z",
    "merged_at": "2024-03-14T06:43:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1295"
  },
  {
    "number": 1294,
    "title": "update requirements_xpu.txt in neural chat to add neural_speed and ip",
    "user": "huiyan2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-21T03:03:27Z",
    "closed_at": "2024-02-23T10:54:07Z",
    "merged_at": "2024-02-23T10:54:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1294"
  },
  {
    "number": 1293,
    "title": "[NeuralChat] CUDA serving with Triton Inference Server",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-21T02:40:18Z",
    "closed_at": "2024-02-23T05:24:21Z",
    "merged_at": "2024-02-23T05:24:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1293"
  },
  {
    "number": 1292,
    "title": "[NeuralChat] Support Triton on HPU",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-20T08:55:58Z",
    "closed_at": "2024-02-23T05:23:44Z",
    "merged_at": "2024-02-23T05:23:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1292"
  },
  {
    "number": 1291,
    "title": "[LLM examples] Add GPTQ static_groups",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-20T07:05:22Z",
    "closed_at": "2024-02-21T12:18:03Z",
    "merged_at": "2024-02-21T12:18:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1291"
  },
  {
    "number": 1290,
    "title": "[NeuralChat] update finetuning validated model",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-20T07:01:40Z",
    "closed_at": "2024-02-20T08:14:08Z",
    "merged_at": "2024-02-20T08:14:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1290"
  },
  {
    "number": 1289,
    "title": "[NeuralChat] Fix local model check issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-20T06:15:15Z",
    "closed_at": "2024-02-23T02:44:12Z",
    "merged_at": "2024-02-23T02:44:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1289"
  },
  {
    "number": 1286,
    "title": "Update neuralchat readme",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-19T08:59:01Z",
    "closed_at": "2024-02-20T01:52:40Z",
    "merged_at": "2024-02-20T01:52:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1286"
  },
  {
    "number": 1285,
    "title": "[NeuralChat] Fix pydub library import issues",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-19T08:14:49Z",
    "closed_at": "2024-02-20T01:53:13Z",
    "merged_at": "2024-02-20T01:53:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1285"
  },
  {
    "number": 1284,
    "title": "remove optimum-intel commit limit",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-19T05:43:25Z",
    "closed_at": "2024-02-19T06:05:46Z",
    "merged_at": "2024-02-19T06:05:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1284"
  },
  {
    "number": 1283,
    "title": "[NeuralChat] Support Assisted Generation on Multi-nodes",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-19T03:30:17Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1283"
  },
  {
    "number": 1282,
    "title": "[LLM] update code_lm_eval to bigcode_eval",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-19T03:24:55Z",
    "closed_at": "2024-02-20T06:33:09Z",
    "merged_at": "2024-02-20T06:33:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1282"
  },
  {
    "number": 1281,
    "title": "fix readme",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-19T01:53:45Z",
    "closed_at": "2024-02-19T03:13:24Z",
    "merged_at": "2024-02-19T03:13:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1281"
  },
  {
    "number": 1280,
    "title": "upgrade pytorch/ipex to 2.2.0",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-18T08:16:22Z",
    "closed_at": "2024-02-26T05:55:46Z",
    "merged_at": "2024-02-26T05:55:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1280"
  },
  {
    "number": 1279,
    "title": "[NeuralChat] Update gradio app to sync with backend change",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-18T05:37:38Z",
    "closed_at": "2024-02-23T02:46:06Z",
    "merged_at": "2024-02-23T02:46:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1279"
  },
  {
    "number": 1278,
    "title": "[NeuralChat] Fix IPEX version mismatch with Pytorch issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-18T02:26:47Z",
    "closed_at": "2024-02-18T03:18:12Z",
    "merged_at": "2024-02-18T03:18:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1278"
  },
  {
    "number": 1277,
    "title": "Update installation",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-18T02:12:23Z",
    "closed_at": "2024-02-18T02:50:16Z",
    "merged_at": "2024-02-18T02:50:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1277"
  },
  {
    "number": 1275,
    "title": "[NeuralChat] Update openai-python API usage for library API upgrade",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-14T14:57:59Z",
    "closed_at": "2024-02-15T06:41:22Z",
    "merged_at": "2024-02-15T06:41:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1275"
  },
  {
    "number": 1273,
    "title": "update system libs",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-13T13:25:06Z",
    "closed_at": "2024-02-15T06:39:31Z",
    "merged_at": "2024-02-15T06:39:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1273"
  },
  {
    "number": 1272,
    "title": "[NeuralChat] Fix MMMU eval",
    "user": "dillonalaird",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-12T21:58:29Z",
    "closed_at": "2024-03-10T21:41:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1272"
  },
  {
    "number": 1271,
    "title": "Revert \"[LLM] update code_lm_eval to bigcode_eval\"",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-08T10:07:00Z",
    "closed_at": "2024-02-08T10:07:09Z",
    "merged_at": "2024-02-08T10:07:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1271"
  },
  {
    "number": 1270,
    "title": "[NeuralChat] Enable bm25 sparse retrieval, enable retrieval then rerank pipeline",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-08T05:01:27Z",
    "closed_at": "2024-02-23T07:38:08Z",
    "merged_at": "2024-02-23T07:38:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1270"
  },
  {
    "number": 1269,
    "title": "quickstart neuralchat notebook for workshops",
    "user": "kminhta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-07T23:33:38Z",
    "closed_at": "2024-02-08T22:50:18Z",
    "merged_at": "2024-02-08T22:50:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1269"
  },
  {
    "number": 1268,
    "title": "resolve woq quantization error when running neuralchat",
    "user": "kminhta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-07T21:45:07Z",
    "closed_at": "2024-02-08T02:42:32Z",
    "merged_at": "2024-02-08T02:42:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1268"
  },
  {
    "number": 1266,
    "title": "[LLM] Support WOQ scheme asym",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-07T08:30:23Z",
    "closed_at": "2024-02-08T06:27:25Z",
    "merged_at": "2024-02-08T06:27:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1266"
  },
  {
    "number": 1265,
    "title": "[NeuralChat] add time tracer decorator",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-07T05:59:59Z",
    "closed_at": "2024-02-19T05:18:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1265"
  },
  {
    "number": 1264,
    "title": "[NeuralChat] Retrieval pdf figure to text",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-06T09:01:38Z",
    "closed_at": "2024-02-22T07:24:17Z",
    "merged_at": "2024-02-22T07:24:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1264"
  },
  {
    "number": 1263,
    "title": "update readme and scripts",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-06T08:33:32Z",
    "closed_at": "2024-02-06T09:18:14Z",
    "merged_at": "2024-02-06T09:18:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1263"
  },
  {
    "number": 1262,
    "title": "Fix text-generation example accuracy test",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-06T07:57:58Z",
    "closed_at": "2024-02-06T11:55:27Z",
    "merged_at": "2024-02-06T11:55:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1262"
  },
  {
    "number": 1261,
    "title": "Bump fastapi from 0.103.2 to 0.109.1 in /intel_extension_for_transformers/neural_chat/tests",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-02-05T17:17:34Z",
    "closed_at": "2024-02-22T09:02:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1261"
  },
  {
    "number": 1260,
    "title": "Bump fastapi from 0.103.2 to 0.109.1 in /intel_extension_for_transformers/neural_chat",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2024-02-05T17:17:34Z",
    "closed_at": "2024-02-22T09:03:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1260"
  },
  {
    "number": 1259,
    "title": "[NeuralChat] Enable llava mmmu evaluation on Gaudi2.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-05T16:52:24Z",
    "closed_at": "2024-02-06T11:56:39Z",
    "merged_at": "2024-02-06T11:56:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1259"
  },
  {
    "number": 1258,
    "title": "[LLM] update code_lm_eval to bigcode_eval",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-05T10:10:36Z",
    "closed_at": "2024-02-08T09:11:36Z",
    "merged_at": "2024-02-08T09:11:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1258"
  },
  {
    "number": 1257,
    "title": "Test issues comment",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-05T08:10:44Z",
    "closed_at": "2024-02-05T08:11:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1257"
  },
  {
    "number": 1256,
    "title": "[NeuralChat] fix system prompt",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-05T03:48:24Z",
    "closed_at": "2024-02-05T03:52:31Z",
    "merged_at": "2024-02-05T03:52:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1256"
  },
  {
    "number": 1255,
    "title": "Upgrade sentense_transformers and langchain_core  version ",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-05T03:11:51Z",
    "closed_at": "2024-02-05T09:15:08Z",
    "merged_at": "2024-02-05T09:15:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1255"
  },
  {
    "number": 1254,
    "title": "Autoround/refine docs",
    "user": "WeiweiZhang1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-04T09:44:34Z",
    "closed_at": "2024-02-04T11:52:11Z",
    "merged_at": "2024-02-04T11:52:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1254"
  },
  {
    "number": 1253,
    "title": "Fix RAG wrong input path",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-04T07:47:06Z",
    "closed_at": "2024-02-05T04:04:03Z",
    "merged_at": "2024-02-05T04:04:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1253"
  },
  {
    "number": 1251,
    "title": "[NeuralChat] Support deepseek-coder models in NeuralChat",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-03T15:06:37Z",
    "closed_at": "2024-02-04T00:52:39Z",
    "merged_at": "2024-02-04T00:52:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1251"
  },
  {
    "number": 1250,
    "title": "Fix Qdrant bug caused by langchain_core upgrade",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-02T10:38:14Z",
    "closed_at": "2024-02-02T11:27:00Z",
    "merged_at": "2024-02-02T11:27:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1250"
  },
  {
    "number": 1249,
    "title": "[Doc] explain the change of neuralspeed",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-02T08:06:09Z",
    "closed_at": "2024-03-08T12:36:53Z",
    "merged_at": "2024-03-08T12:36:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1249"
  },
  {
    "number": 1248,
    "title": "Qbits woq ref impl for debug",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-02T07:55:01Z",
    "closed_at": "2024-02-04T03:38:12Z",
    "merged_at": "2024-02-04T03:38:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1248"
  },
  {
    "number": 1247,
    "title": "remove pyproject.toml",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-02T07:46:07Z",
    "closed_at": "2024-02-04T03:42:05Z",
    "merged_at": "2024-02-04T03:42:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1247"
  },
  {
    "number": 1246,
    "title": "[Neural Speed]change the graph name to neural speed",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-02T07:30:09Z",
    "closed_at": "2024-02-04T04:47:55Z",
    "merged_at": "2024-02-04T04:47:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1246"
  },
  {
    "number": 1245,
    "title": "[NeuralChat] enable gramma check and query polish to enhance RAG performance",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-02T07:14:18Z",
    "closed_at": "2024-02-23T08:10:49Z",
    "merged_at": "2024-02-23T08:10:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1245"
  },
  {
    "number": 1243,
    "title": "[NeuralChat] [WiP] Add generation for LLaVA models",
    "user": "dillonalaird",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-02T06:03:54Z",
    "closed_at": "2024-02-05T17:38:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1243"
  },
  {
    "number": 1242,
    "title": "Update docs",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-02T04:51:00Z",
    "closed_at": "2024-02-04T08:03:14Z",
    "merged_at": "2024-02-04T08:03:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1242"
  },
  {
    "number": 1241,
    "title": "[LLM Runtime]updata neural speed example",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-02T03:18:26Z",
    "closed_at": "2024-02-02T03:37:04Z",
    "merged_at": "2024-02-02T03:37:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1241"
  },
  {
    "number": 1240,
    "title": "[GPU] dispatch to external IPEX link",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-02T00:13:24Z",
    "closed_at": "2024-02-02T01:12:04Z",
    "merged_at": "2024-02-02T01:12:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1240"
  },
  {
    "number": 1238,
    "title": "Fix Readme for Assisted Generation",
    "user": "danielkorat",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T18:36:39Z",
    "closed_at": "2024-02-02T14:45:23Z",
    "merged_at": "2024-02-02T14:45:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1238"
  },
  {
    "number": 1237,
    "title": "[NeuralChat] Add langchain extension example and update notebook",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T15:43:09Z",
    "closed_at": "2024-02-02T07:23:30Z",
    "merged_at": "2024-02-02T07:23:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1237"
  },
  {
    "number": 1236,
    "title": "[NeuralChat] Add Docker use case for CodeGen",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T13:53:45Z",
    "closed_at": "2024-02-02T14:47:45Z",
    "merged_at": "2024-02-02T14:47:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1236"
  },
  {
    "number": 1235,
    "title": "[NeuralChat] Add requirements.txt for retrieval plugin (#1217)",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T11:12:32Z",
    "closed_at": "2024-02-01T11:14:39Z",
    "merged_at": "2024-02-01T11:14:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1235"
  },
  {
    "number": 1234,
    "title": "[NeuralChat] Support SQL generation",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T09:01:27Z",
    "closed_at": "2024-02-02T14:46:57Z",
    "merged_at": "2024-02-02T14:46:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1234"
  },
  {
    "number": 1233,
    "title": "[NeuralChat] Add Docker use case for Text Generation",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T08:53:11Z",
    "closed_at": "2024-02-05T09:11:46Z",
    "merged_at": "2024-02-05T09:11:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1233"
  },
  {
    "number": 1232,
    "title": "[LLM Runtime]add neural speed example",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T07:19:42Z",
    "closed_at": "2024-02-01T13:26:34Z",
    "merged_at": "2024-02-01T13:26:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1232"
  },
  {
    "number": 1231,
    "title": "[NeuralChat] Add ROME implementation and example",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T06:49:53Z",
    "closed_at": "2024-02-15T08:17:22Z",
    "merged_at": "2024-02-15T08:17:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1231"
  },
  {
    "number": 1229,
    "title": "[NeuralChat] fix itrex install",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T06:39:05Z",
    "closed_at": "2024-02-01T07:51:19Z",
    "merged_at": "2024-02-01T07:51:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1229"
  },
  {
    "number": 1228,
    "title": "[NeuralChat] Support DeciLM-7B and DeciLM-7B-instruct",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T06:18:14Z",
    "closed_at": "2024-02-04T22:45:09Z",
    "merged_at": "2024-02-04T22:45:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1228"
  },
  {
    "number": 1227,
    "title": "Set trainer.save_model state_dict format to safetensors",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T06:00:41Z",
    "closed_at": "2024-02-05T09:31:22Z",
    "merged_at": "2024-02-05T09:31:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1227"
  },
  {
    "number": 1226,
    "title": "[NeuralChat] OpenAI compatible audio API",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T05:42:22Z",
    "closed_at": "2024-02-02T03:37:21Z",
    "merged_at": "2024-02-02T03:37:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1226"
  },
  {
    "number": 1225,
    "title": "Dev/ipex woq",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T05:24:36Z",
    "closed_at": "2024-02-24T01:56:20Z",
    "merged_at": "2024-02-24T01:56:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1225"
  },
  {
    "number": 1224,
    "title": "[NeuralChat] Fixed broken links, model name typo",
    "user": "okhleif-10",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-01T00:41:31Z",
    "closed_at": "2024-02-01T05:36:44Z",
    "merged_at": "2024-02-01T05:36:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1224"
  },
  {
    "number": 1223,
    "title": "Error handling with yield",
    "user": "itayariel1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T15:05:46Z",
    "closed_at": "2024-04-19T02:20:24Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1223"
  },
  {
    "number": 1222,
    "title": "add neuralspeed json",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T09:18:46Z",
    "closed_at": "2024-02-04T06:19:03Z",
    "merged_at": "2024-02-04T06:19:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1222"
  },
  {
    "number": 1221,
    "title": "[NeuralChat] Fix response issue of model.predict",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T08:55:39Z",
    "closed_at": "2024-02-08T09:14:07Z",
    "merged_at": "2024-02-08T09:14:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1221"
  },
  {
    "number": 1219,
    "title": "add autoround example",
    "user": "WeiweiZhang1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T08:03:15Z",
    "closed_at": "2024-02-04T01:19:31Z",
    "merged_at": "2024-02-04T01:19:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1219"
  },
  {
    "number": 1218,
    "title": "[LLM Runtime] Extend API for GGUF",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T07:51:50Z",
    "closed_at": "2024-02-02T07:17:52Z",
    "merged_at": "2024-02-02T07:17:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1218"
  },
  {
    "number": 1217,
    "title": "[NeuralChat] Add requirements.txt for retrieval plugin",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T07:34:36Z",
    "closed_at": "2024-02-01T09:03:29Z",
    "merged_at": "2024-02-01T09:03:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1217"
  },
  {
    "number": 1216,
    "title": "Add precommit config",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T05:38:43Z",
    "closed_at": "2024-01-31T09:18:18Z",
    "merged_at": "2024-01-31T09:18:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1216"
  },
  {
    "number": 1215,
    "title": "reuse INC pre-commit",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T03:28:56Z",
    "closed_at": "2024-01-31T05:20:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1215"
  },
  {
    "number": 1214,
    "title": "update torch version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-31T02:12:01Z",
    "closed_at": "2024-01-31T02:42:07Z",
    "merged_at": "2024-01-31T02:42:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1214"
  },
  {
    "number": 1213,
    "title": "[NeuralChat] Fix errors in trainer save",
    "user": "dillonalaird",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-30T20:56:04Z",
    "closed_at": "2024-01-31T14:28:52Z",
    "merged_at": "2024-01-31T14:28:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1213"
  },
  {
    "number": 1212,
    "title": "rewrite finetuning data preprocessing with static shape for Gaudi2.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-30T14:19:56Z",
    "closed_at": "2024-01-31T05:47:23Z",
    "merged_at": "2024-01-31T05:47:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1212"
  },
  {
    "number": 1211,
    "title": "[LLM] Support woq model save and load",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-30T13:10:38Z",
    "closed_at": "2024-02-01T03:33:08Z",
    "merged_at": "2024-02-01T03:33:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1211"
  },
  {
    "number": 1210,
    "title": "[NeuralChat] Reconstruct multi_cpu_server.py",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-30T08:52:56Z",
    "closed_at": "2024-02-05T01:27:21Z",
    "merged_at": "2024-02-05T01:27:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1210"
  },
  {
    "number": 1209,
    "title": "recover validation data",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-30T08:51:44Z",
    "closed_at": "2024-01-30T09:26:46Z",
    "merged_at": "2024-01-30T09:26:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1209"
  },
  {
    "number": 1208,
    "title": "[NeuralChat] Support Neuralchat-TGI serving with Docker",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-30T08:16:25Z",
    "closed_at": "2024-01-31T02:03:51Z",
    "merged_at": "2024-01-31T02:03:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1208"
  },
  {
    "number": 1207,
    "title": "Update weight only quant doc",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-30T03:17:43Z",
    "closed_at": "2024-03-14T10:13:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1207"
  },
  {
    "number": 1206,
    "title": "[NeuralChat] Support GPTQ, AWQ model in NeuralChat",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-29T15:49:57Z",
    "closed_at": "2024-02-01T05:12:25Z",
    "merged_at": "2024-02-01T05:12:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1206"
  },
  {
    "number": 1205,
    "title": "[NeuralChat] Sync RESTful API with latest OpenAI protocol",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-29T15:41:04Z",
    "closed_at": "2024-01-31T11:20:52Z",
    "merged_at": "2024-01-31T11:20:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1205"
  },
  {
    "number": 1204,
    "title": "[NeuralChat] Remove redundant knowledge id in audio plugin api",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-29T09:25:29Z",
    "closed_at": "2024-01-29T18:02:43Z",
    "merged_at": "2024-01-29T18:02:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1204"
  },
  {
    "number": 1203,
    "title": "Fix WOQ IPEX condition",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-29T07:37:33Z",
    "closed_at": "2024-01-29T17:53:00Z",
    "merged_at": "2024-01-29T17:53:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1203"
  },
  {
    "number": 1202,
    "title": "[LLM Runtime] Update parameters for llm runtime",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-29T05:45:14Z",
    "closed_at": "2024-01-29T17:54:43Z",
    "merged_at": "2024-01-29T17:54:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1202"
  },
  {
    "number": 1200,
    "title": "[NeuralChat] Support GGUF model in NeuralChat",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-27T14:46:10Z",
    "closed_at": "2024-02-08T09:16:07Z",
    "merged_at": "2024-02-08T09:16:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1200"
  },
  {
    "number": 1199,
    "title": "[NeuralChat] Fix backprop error for text only examples",
    "user": "dillonalaird",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-26T22:36:50Z",
    "closed_at": "2024-01-28T03:38:35Z",
    "merged_at": "2024-01-28T03:38:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1199"
  },
  {
    "number": 1198,
    "title": "[NeuralChat] Use unk token instead of eos token",
    "user": "dillonalaird",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-26T19:06:46Z",
    "closed_at": "2024-01-29T17:53:16Z",
    "merged_at": "2024-01-29T17:53:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1198"
  },
  {
    "number": 1197,
    "title": "Update README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-26T10:09:47Z",
    "closed_at": "2024-01-26T10:09:54Z",
    "merged_at": "2024-01-26T10:09:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1197"
  },
  {
    "number": 1196,
    "title": "Update README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-26T09:51:20Z",
    "closed_at": "2024-01-26T23:21:25Z",
    "merged_at": "2024-01-26T23:21:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1196"
  },
  {
    "number": 1195,
    "title": "[NeuralChat] Embedding model finetuning",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-26T08:01:49Z",
    "closed_at": "2024-02-04T22:53:20Z",
    "merged_at": "2024-02-04T22:53:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1195"
  },
  {
    "number": 1194,
    "title": "Change the default value for XPU weight-only quantization",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-26T07:22:53Z",
    "closed_at": "2024-01-26T08:43:19Z",
    "merged_at": "2024-01-26T08:43:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1194"
  },
  {
    "number": 1193,
    "title": "[NeuralChat] Fixed off by one error on masking",
    "user": "dillonalaird",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-26T03:56:20Z",
    "closed_at": "2024-01-26T09:39:03Z",
    "merged_at": "2024-01-26T09:39:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1193"
  },
  {
    "number": 1192,
    "title": "Revert \"Add GPTJ Finetuning and eval workflow example\"",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-25T13:18:37Z",
    "closed_at": "2024-01-25T13:20:49Z",
    "merged_at": "2024-01-25T13:20:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1192"
  },
  {
    "number": 1191,
    "title": "update requirement",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-25T08:47:59Z",
    "closed_at": "2024-01-25T08:57:01Z",
    "merged_at": "2024-01-25T08:57:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1191"
  },
  {
    "number": 1190,
    "title": "[NeuralChat] add neural_speed in requirements",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-25T06:08:46Z",
    "closed_at": "2024-01-29T07:43:16Z",
    "merged_at": "2024-01-29T07:43:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1190"
  },
  {
    "number": 1189,
    "title": "[NeuralChat] Enhance Retrieval UTs",
    "user": "xmx-521",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-25T05:43:44Z",
    "closed_at": "2024-02-23T02:50:38Z",
    "merged_at": "2024-02-23T02:50:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1189"
  },
  {
    "number": 1188,
    "title": "add pre-commit-ci codespell check",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-24T08:33:27Z",
    "closed_at": "2024-01-29T09:25:48Z",
    "merged_at": "2024-01-29T09:25:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1188"
  },
  {
    "number": 1187,
    "title": "[NeuralChat] Support Neuralchat-vLLM serving with Docker ",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-24T08:19:54Z",
    "closed_at": "2024-02-05T05:41:26Z",
    "merged_at": "2024-02-05T05:41:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1187"
  },
  {
    "number": 1186,
    "title": "Update requirements.txt",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-24T08:01:18Z",
    "closed_at": "2024-01-25T08:49:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1186"
  },
  {
    "number": 1185,
    "title": "add pypi install neural-speed",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-24T06:34:32Z",
    "closed_at": "2024-01-24T09:35:16Z",
    "merged_at": "2024-01-24T09:35:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1185"
  },
  {
    "number": 1184,
    "title": "[NeuralChat] pip install neural_speed from wheel",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-24T03:13:40Z",
    "closed_at": "2024-01-24T05:29:38Z",
    "merged_at": "2024-01-24T05:29:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1184"
  },
  {
    "number": 1183,
    "title": "fixed example of cpu woq accuracy issue",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-24T03:07:55Z",
    "closed_at": "2024-01-25T13:09:30Z",
    "merged_at": "2024-01-25T13:09:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1183"
  },
  {
    "number": 1182,
    "title": "[Neuralchat] Notebook retrieval update",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-24T02:47:35Z",
    "closed_at": "2024-02-23T06:17:19Z",
    "merged_at": "2024-02-23T06:17:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1182"
  },
  {
    "number": 1180,
    "title": "[NeuralChat] Support Neuralchat-TGI serving with Docker",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-23T09:37:10Z",
    "closed_at": "2024-01-30T06:14:13Z",
    "merged_at": "2024-01-30T06:14:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1180"
  },
  {
    "number": 1179,
    "title": "[NeuralChat] add customized system prompts",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-23T09:07:47Z",
    "closed_at": "2024-02-04T22:49:10Z",
    "merged_at": "2024-02-04T22:49:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1179"
  },
  {
    "number": 1178,
    "title": "fix of calling is_ipex_available",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-23T07:33:14Z",
    "closed_at": "2024-01-24T08:08:07Z",
    "merged_at": "2024-01-24T08:08:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1178"
  },
  {
    "number": 1177,
    "title": "[NeuralChat] fix latest transformers whisper forced_decoder_ids error",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-23T06:11:01Z",
    "closed_at": "2024-01-26T08:44:02Z",
    "merged_at": "2024-01-26T08:44:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1177"
  },
  {
    "number": 1176,
    "title": "restrain transformers to 4.36.2",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-23T05:52:22Z",
    "closed_at": "2024-01-24T01:44:31Z",
    "merged_at": "2024-01-24T01:44:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1176"
  },
  {
    "number": 1174,
    "title": "[NeuralChat] Minimize dependencies for running a chatbot",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-22T07:45:36Z",
    "closed_at": "2024-01-25T13:15:06Z",
    "merged_at": "2024-01-25T13:15:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1174"
  },
  {
    "number": 1173,
    "title": "fix docker clean when generating report",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-22T06:26:08Z",
    "closed_at": "2024-01-24T06:29:03Z",
    "merged_at": "2024-01-24T06:29:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1173"
  },
  {
    "number": 1171,
    "title": "Add trust_remote_code args for lm_eval of woq example",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-22T05:52:24Z",
    "closed_at": "2024-01-22T08:07:31Z",
    "merged_at": "2024-01-22T08:07:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1171"
  },
  {
    "number": 1170,
    "title": "[NeuralChat] Fix nightly unit test issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-22T05:21:59Z",
    "closed_at": "2024-01-22T06:03:41Z",
    "merged_at": "2024-01-22T06:03:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1170"
  },
  {
    "number": 1169,
    "title": "Fix LLM example model eval",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-22T03:11:10Z",
    "closed_at": "2024-01-25T08:50:12Z",
    "merged_at": "2024-01-25T08:50:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1169"
  },
  {
    "number": 1168,
    "title": "add validated sw",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-21T06:37:39Z",
    "closed_at": "2024-01-24T09:36:11Z",
    "merged_at": "2024-01-24T09:36:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1168"
  },
  {
    "number": 1167,
    "title": "update conda meta version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T16:08:05Z",
    "closed_at": "2024-01-19T16:12:52Z",
    "merged_at": "2024-01-19T16:12:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1167"
  },
  {
    "number": 1166,
    "title": "Fix starcoder accuracy",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T15:10:53Z",
    "closed_at": "2024-01-19T15:54:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1166"
  },
  {
    "number": 1165,
    "title": "[NeuralChat] Fix UT issues",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T12:43:37Z",
    "closed_at": "2024-01-19T14:02:20Z",
    "merged_at": "2024-01-19T14:02:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1165"
  },
  {
    "number": 1164,
    "title": "Fix TEQ UT",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T08:55:48Z",
    "closed_at": "2024-01-19T14:08:09Z",
    "merged_at": "2024-01-19T14:08:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1164"
  },
  {
    "number": 1163,
    "title": "remove GPTQ llama dependence",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T06:43:17Z",
    "closed_at": "2024-01-19T11:43:34Z",
    "merged_at": "2024-01-19T11:43:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1163"
  },
  {
    "number": 1162,
    "title": "[NeuralChat] fix finetuning example import issue.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T06:42:45Z",
    "closed_at": "2024-01-19T08:24:30Z",
    "merged_at": "2024-01-19T08:24:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1162"
  },
  {
    "number": 1161,
    "title": "[NeuralChat] Support TGI Serving",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T06:33:54Z",
    "closed_at": "2024-01-19T15:25:04Z",
    "merged_at": "2024-01-19T15:25:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1161"
  },
  {
    "number": 1160,
    "title": "[LLM Runtime]Add GGUF API UT",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T04:30:29Z",
    "closed_at": "2024-01-19T15:24:27Z",
    "merged_at": "2024-01-19T15:24:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1160"
  },
  {
    "number": 1159,
    "title": "[Runtime] Add NS PPL example",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T03:35:33Z",
    "closed_at": "2024-01-19T06:49:43Z",
    "merged_at": "2024-01-19T06:49:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1159"
  },
  {
    "number": 1158,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-19T02:28:40Z",
    "closed_at": "2024-01-19T03:58:46Z",
    "merged_at": "2024-01-19T03:58:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1158"
  },
  {
    "number": 1157,
    "title": "[NeuralChat] Fix GPT-J model name issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-18T14:22:17Z",
    "closed_at": "2024-01-19T14:12:24Z",
    "merged_at": "2024-01-19T14:12:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1157"
  },
  {
    "number": 1156,
    "title": "Neural Speed related Document",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-18T06:54:05Z",
    "closed_at": "2024-01-18T07:15:38Z",
    "merged_at": "2024-01-18T07:15:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1156"
  },
  {
    "number": 1155,
    "title": "change install queue",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-18T03:29:56Z",
    "closed_at": "2024-01-18T04:13:12Z",
    "merged_at": "2024-01-18T04:13:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1155"
  },
  {
    "number": 1154,
    "title": "Limit sentence-transformers version",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-18T02:43:39Z",
    "closed_at": "2024-01-18T06:42:26Z",
    "merged_at": "2024-01-18T06:42:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1154"
  },
  {
    "number": 1153,
    "title": "Support weight-only kernel with IPEX for intel GPU",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-18T02:31:50Z",
    "closed_at": "2024-01-19T16:29:59Z",
    "merged_at": "2024-01-19T16:29:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1153"
  },
  {
    "number": 1152,
    "title": "[NeuralChat] Add transformers and langchain extension api notebooks",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-17T14:52:21Z",
    "closed_at": "2024-01-19T14:33:00Z",
    "merged_at": "2024-01-19T14:33:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1152"
  },
  {
    "number": 1151,
    "title": "[LLM Runtime] Add python API for GGUF",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-17T07:47:50Z",
    "closed_at": "2024-01-19T02:29:34Z",
    "merged_at": "2024-01-19T02:29:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1151"
  },
  {
    "number": 1150,
    "title": "[NeuralChat] Add neural-speed dependency",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-17T07:12:57Z",
    "closed_at": "2024-01-17T08:08:37Z",
    "merged_at": "2024-01-17T08:08:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1150"
  },
  {
    "number": 1149,
    "title": "[LLM Runtime]update README and requirements",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-17T07:03:23Z",
    "closed_at": "2024-01-19T14:41:09Z",
    "merged_at": "2024-01-19T14:41:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1149"
  },
  {
    "number": 1147,
    "title": "[NeuralChat] Restructure Retrieval UTs",
    "user": "xmx-521",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-17T02:59:08Z",
    "closed_at": "2024-01-25T05:44:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1147"
  },
  {
    "number": 1146,
    "title": "[NeuralChat] Continue refine documents",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-16T12:21:36Z",
    "closed_at": "2024-01-19T14:19:03Z",
    "merged_at": "2024-01-19T14:19:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1146"
  },
  {
    "number": 1143,
    "title": "remove transformers==4.34.1",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-15T14:12:36Z",
    "closed_at": "2024-01-19T03:59:17Z",
    "merged_at": "2024-01-19T03:59:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1143"
  },
  {
    "number": 1141,
    "title": "Add loading int8 model for AutoModelForCausalLM",
    "user": "Kaihui-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-15T09:29:11Z",
    "closed_at": "2024-03-11T06:16:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1141"
  },
  {
    "number": 1140,
    "title": "Fix doc about LLM example recipes.",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-15T07:12:34Z",
    "closed_at": "2024-01-15T08:05:05Z",
    "merged_at": "2024-01-15T08:05:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1140"
  },
  {
    "number": 1139,
    "title": "[NeuralChat] Multi-language TTS support",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-15T02:22:02Z",
    "closed_at": "2024-03-13T05:37:07Z",
    "merged_at": "2024-03-13T05:37:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1139"
  },
  {
    "number": 1138,
    "title": "Add GPTJ Finetuning and eval workflow example",
    "user": "mini-goel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-12T16:29:45Z",
    "closed_at": "2024-01-25T13:16:47Z",
    "merged_at": "2024-01-25T13:16:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1138"
  },
  {
    "number": 1137,
    "title": "[NeuralChat] Support Phi-2 model",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-12T13:27:39Z",
    "closed_at": "2024-01-13T00:06:20Z",
    "merged_at": "2024-01-13T00:06:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1137"
  },
  {
    "number": 1135,
    "title": "[NeuralChat] Integrate backend code of Askdoc",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-12T06:40:55Z",
    "closed_at": "2024-01-29T18:11:35Z",
    "merged_at": "2024-01-29T18:11:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1135"
  },
  {
    "number": 1134,
    "title": "fix Qwen-72b eval precision issue",
    "user": "WeiweiZhang1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-12T02:03:52Z",
    "closed_at": "2024-01-12T09:00:19Z",
    "merged_at": "2024-01-12T09:00:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1134"
  },
  {
    "number": 1133,
    "title": "Add accuracy test",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-11T12:35:18Z",
    "closed_at": "2024-01-31T07:26:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1133"
  },
  {
    "number": 1132,
    "title": "[Neuralchat] Improve error code UT coverage",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-11T08:30:26Z",
    "closed_at": "2024-01-18T13:32:52Z",
    "merged_at": "2024-01-18T13:32:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1132"
  },
  {
    "number": 1131,
    "title": "Fix neuralchat ut requirement",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-11T07:23:00Z",
    "closed_at": "2024-01-11T08:10:54Z",
    "merged_at": "2024-01-11T08:10:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1131"
  },
  {
    "number": 1130,
    "title": "try remove paddle speed limitation",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-11T02:28:18Z",
    "closed_at": "2024-01-11T05:12:53Z",
    "merged_at": "2024-01-11T05:12:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1130"
  },
  {
    "number": 1129,
    "title": "[LLM Runtime] neural_speed_replace_graph",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-11T01:49:10Z",
    "closed_at": "2024-01-17T03:46:43Z",
    "merged_at": "2024-01-17T03:46:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1129"
  },
  {
    "number": 1128,
    "title": "Fix missing documentation in LLM runtime, and a broken link + some typos",
    "user": "aahouzi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-10T11:02:56Z",
    "closed_at": "2024-01-11T01:38:23Z",
    "merged_at": "2024-01-11T01:38:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1128"
  },
  {
    "number": 1127,
    "title": "[NeuralChat] Refine Document",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-10T11:00:08Z",
    "closed_at": "2024-01-13T09:52:50Z",
    "merged_at": "2024-01-13T09:52:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1127"
  },
  {
    "number": 1126,
    "title": "Qbits use bestla as 3rd lib in cmake",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-10T07:20:04Z",
    "closed_at": "2024-01-12T05:59:51Z",
    "merged_at": "2024-01-12T05:59:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1126"
  },
  {
    "number": 1125,
    "title": "fix nightly ut",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-10T06:19:56Z",
    "closed_at": "2024-01-10T09:13:55Z",
    "merged_at": "2024-01-10T09:13:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1125"
  },
  {
    "number": 1123,
    "title": "[Runtime] calculate accuracy of runtime",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-09T23:38:56Z",
    "closed_at": "2024-01-15T06:26:20Z",
    "merged_at": "2024-01-15T06:26:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1123"
  },
  {
    "number": 1122,
    "title": "add paper documents",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-09T09:23:45Z",
    "closed_at": "2024-01-09T12:13:24Z",
    "merged_at": "2024-01-09T12:13:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1122"
  },
  {
    "number": 1121,
    "title": "Fix unexpected keyword argument 'trust_remote_code' in embedding",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-09T07:03:44Z",
    "closed_at": "2024-01-09T07:34:17Z",
    "merged_at": "2024-01-09T07:34:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1121"
  },
  {
    "number": 1120,
    "title": "[NeuralChat] Support vLLM serving",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-09T06:29:20Z",
    "closed_at": "2024-01-19T14:19:25Z",
    "merged_at": "2024-01-19T14:19:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1120"
  },
  {
    "number": 1119,
    "title": "[NeuralChat] Update the readme for RAG",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-07T19:15:02Z",
    "closed_at": "2024-01-09T05:26:16Z",
    "merged_at": "2024-01-09T05:26:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1119"
  },
  {
    "number": 1118,
    "title": "[NeuralChat] Fix langchain version issue since langchain upgrade to v0.1.0",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-06T06:23:34Z",
    "closed_at": "2024-01-07T14:07:29Z",
    "merged_at": "2024-01-07T14:07:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1118"
  },
  {
    "number": 1116,
    "title": "[NeuralChat] Fix unit test issues caused by the activation of different plugins",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-05T13:59:08Z",
    "closed_at": "2024-01-05T23:10:51Z",
    "merged_at": "2024-01-05T23:10:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1116"
  },
  {
    "number": 1114,
    "title": "upgrade LLM model list to IPEX 2.2",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-05T09:38:50Z",
    "closed_at": "2024-02-19T03:02:40Z",
    "merged_at": "2024-02-19T03:02:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1114"
  },
  {
    "number": 1113,
    "title": "update prompt and ci scripts",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-05T08:07:52Z",
    "closed_at": "2024-01-05T11:34:08Z",
    "merged_at": "2024-01-05T11:34:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1113"
  },
  {
    "number": 1112,
    "title": "[NeuralChat] Support compatible stats format",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-04T14:54:51Z",
    "closed_at": "2024-01-16T01:24:50Z",
    "merged_at": "2024-01-16T01:24:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1112"
  },
  {
    "number": 1111,
    "title": "[NeuralChat] Fix RAG example for retrieval plugin parameter change",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-04T09:55:40Z",
    "closed_at": "2024-01-04T12:43:37Z",
    "merged_at": "2024-01-04T12:43:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1111"
  },
  {
    "number": 1110,
    "title": "[NeuralChat] Update out of storage threshold value",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-04T09:44:50Z",
    "closed_at": "2024-01-18T14:50:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1110"
  },
  {
    "number": 1109,
    "title": "update Llama-2-13b-hf command",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-04T03:54:29Z",
    "closed_at": "2024-01-04T09:03:56Z",
    "merged_at": "2024-01-04T09:03:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1109"
  },
  {
    "number": 1108,
    "title": "add accelerate for tensorflow examples",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-04T03:47:54Z",
    "closed_at": "2024-01-04T03:50:20Z",
    "merged_at": "2024-01-04T03:50:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1108"
  },
  {
    "number": 1107,
    "title": "Add WOQ GPTQ frontend and example",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-03T10:13:27Z",
    "closed_at": "2024-01-19T02:45:51Z",
    "merged_at": "2024-01-19T02:45:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1107"
  },
  {
    "number": 1103,
    "title": "remove SharedDDP as it is deprecated",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-02T07:01:16Z",
    "closed_at": "2024-01-12T01:28:24Z",
    "merged_at": "2024-01-12T01:28:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1103"
  },
  {
    "number": 1102,
    "title": "add accelerate",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-01-02T06:35:58Z",
    "closed_at": "2024-01-02T06:43:21Z",
    "merged_at": "2024-01-02T06:43:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1102"
  },
  {
    "number": 1100,
    "title": "[LLM Runtime] Fix convert mistral script missing parameter issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-31T08:38:08Z",
    "closed_at": "2024-01-05T02:07:43Z",
    "merged_at": "2024-01-05T02:07:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1100"
  },
  {
    "number": 1098,
    "title": "[NeuralChat] Support LLM runtime ggml int4",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-29T14:15:12Z",
    "closed_at": "2024-01-01T23:35:59Z",
    "merged_at": "2024-01-01T23:35:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1098"
  },
  {
    "number": 1096,
    "title": "Qbits Backend support act shuffle",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-29T08:41:40Z",
    "closed_at": "2024-01-03T05:23:01Z",
    "merged_at": "2024-01-03T05:23:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1096"
  },
  {
    "number": 1095,
    "title": "Add documentation for LLM quantization recipes.",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-29T05:29:15Z",
    "closed_at": "2023-12-29T09:36:11Z",
    "merged_at": "2023-12-29T09:36:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1095"
  },
  {
    "number": 1092,
    "title": "[NeuralChat] Align inference parameters num_beams with HF transformers",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-28T13:31:34Z",
    "closed_at": "2024-01-02T12:00:55Z",
    "merged_at": "2024-01-02T12:00:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1092"
  },
  {
    "number": 1089,
    "title": "[LLM Runtime]YaRN rope",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-28T08:25:57Z",
    "closed_at": "2024-01-15T13:58:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1089"
  },
  {
    "number": 1088,
    "title": "[NeuralChat] Fix tts crash with messy retrieval input and enhance normalizer",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-28T06:44:41Z",
    "closed_at": "2024-01-05T13:53:43Z",
    "merged_at": "2024-01-05T13:53:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1088"
  },
  {
    "number": 1087,
    "title": "[LLM example] add calib_shuffle args for text-generation example",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-28T03:13:27Z",
    "closed_at": "2023-12-28T06:13:48Z",
    "merged_at": "2023-12-28T06:13:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1087"
  },
  {
    "number": 1086,
    "title": "Fix magicoder model tokenizer issue and remove codegen streaming redundant end format",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-27T15:52:37Z",
    "closed_at": "2023-12-27T23:15:09Z",
    "merged_at": "2023-12-27T23:15:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1086"
  },
  {
    "number": 1085,
    "title": "Feature/support older intel mac book pro with gcc 13",
    "user": "nezda",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-27T11:37:41Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1085"
  },
  {
    "number": 1084,
    "title": "[NeuralChat] remove coverage check tts cn",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-27T09:52:12Z",
    "closed_at": "2023-12-28T01:39:28Z",
    "merged_at": "2023-12-28T01:39:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1084"
  },
  {
    "number": 1083,
    "title": "[NeuralChat] Add ut",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-27T05:57:57Z",
    "closed_at": "2024-01-10T06:05:35Z",
    "merged_at": "2024-01-10T06:05:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1083"
  },
  {
    "number": 1082,
    "title": "[LLM Runtime] support neural chat on windows with magicoder",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-27T04:59:22Z",
    "closed_at": "2023-12-27T10:32:15Z",
    "merged_at": "2023-12-27T10:32:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1082"
  },
  {
    "number": 1080,
    "title": "[NeuralChat] Optimize the prompt follow the suggestion from cnvrg.io team",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-26T10:08:36Z",
    "closed_at": "2024-01-07T14:17:21Z",
    "merged_at": "2024-01-07T14:17:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1080"
  },
  {
    "number": 1079,
    "title": "[LLM Runtime]fix mistarl-7b memory error when inference 2012 tokens",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-26T09:02:40Z",
    "closed_at": "2024-01-15T13:59:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1079"
  },
  {
    "number": 1078,
    "title": "[LLM Runtime] Circumvent dependabot check for chatglm/baichuan/baichuan2",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-26T07:11:59Z",
    "closed_at": "2023-12-26T11:38:33Z",
    "merged_at": "2023-12-26T11:38:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1078"
  },
  {
    "number": 1077,
    "title": "update datasets version for docker",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-26T06:21:51Z",
    "closed_at": "2023-12-27T01:12:47Z",
    "merged_at": "2023-12-27T01:12:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1077"
  },
  {
    "number": 1076,
    "title": "Enable Qdrant vectorstore",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-26T02:13:35Z",
    "closed_at": "2024-01-07T14:08:36Z",
    "merged_at": "2024-01-07T14:08:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1076"
  },
  {
    "number": 1075,
    "title": "[NeuralChat] Fix magicoder model tokenizer issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-25T15:27:01Z",
    "closed_at": "2023-12-25T23:05:46Z",
    "merged_at": "2023-12-25T23:05:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1075"
  },
  {
    "number": 1074,
    "title": "Bump transformers from 4.33.1 to 4.36.0 in /intel_extension_for_transformers/llm/runtime/graph/scripts/requirements",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-25T12:36:00Z",
    "closed_at": "2023-12-25T13:15:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1074"
  },
  {
    "number": 1073,
    "title": "[NeuralChat] Add Multi-Socket LLM Inference Example",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-25T08:38:21Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1073"
  },
  {
    "number": 1072,
    "title": "[LLM WOQ] add QuantizedLinearQbits activation contiguous check",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-25T07:57:09Z",
    "closed_at": "2023-12-25T08:52:47Z",
    "merged_at": "2023-12-25T08:52:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1072"
  },
  {
    "number": 1071,
    "title": "[Doc] Add doc of save & load low bit model in CPU",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-25T06:41:04Z",
    "closed_at": "2023-12-25T12:32:09Z",
    "merged_at": "2023-12-25T12:32:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1071"
  },
  {
    "number": 1070,
    "title": "[LLM] Update utils.py",
    "user": "eltociear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-24T15:18:20Z",
    "closed_at": "2023-12-24T23:29:10Z",
    "merged_at": "2023-12-24T23:29:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1070"
  },
  {
    "number": 1069,
    "title": "[NeuralChat] Support SOLAR-10.7B-Instruct-v1.0 model",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-24T02:13:09Z",
    "closed_at": "2023-12-24T23:23:33Z",
    "merged_at": "2023-12-24T23:23:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1069"
  },
  {
    "number": 1067,
    "title": "[NeuralChat] Support magicoder model",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-22T12:23:13Z",
    "closed_at": "2023-12-25T12:33:25Z",
    "merged_at": "2023-12-25T12:33:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1067"
  },
  {
    "number": 1066,
    "title": "Remove the specified versions from requirements",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-22T05:26:23Z",
    "closed_at": "2023-12-26T06:10:21Z",
    "merged_at": "2023-12-26T06:10:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1066"
  },
  {
    "number": 1065,
    "title": "[NeuralChat] optimize prompt template to decrease model hallucination and enhance the ability of RAG",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-22T03:49:32Z",
    "closed_at": "2023-12-25T23:06:30Z",
    "merged_at": "2023-12-25T23:06:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1065"
  },
  {
    "number": 1064,
    "title": "Fixed example error for T5 text2text generation",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-22T01:43:59Z",
    "closed_at": "2023-12-22T05:22:20Z",
    "merged_at": "2023-12-22T05:22:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1064"
  },
  {
    "number": 1062,
    "title": "Bump gradio from 3.36.0 to 4.11.0 in /intel_extension_for_transformers/neural_chat/ui/gradio/basic",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-21T18:39:06Z",
    "closed_at": "2023-12-22T05:36:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1062"
  },
  {
    "number": 1061,
    "title": "Bump gradio from 3.36.0 to 4.11.0 in /workflows/chatbot/demo/basic_frontend",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-21T18:39:03Z",
    "closed_at": "2023-12-22T05:36:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1061"
  },
  {
    "number": 1060,
    "title": "Bump gradio from 3.34.0 to 4.11.0 in /intel_extension_for_transformers/neural_chat/ui/gradio/side_by_side",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-21T18:38:35Z",
    "closed_at": "2023-12-22T05:36:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1060"
  },
  {
    "number": 1059,
    "title": "[LLM Runtime] dynamic link the layer to compress binary size",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-21T16:18:44Z",
    "closed_at": "2023-12-22T05:22:43Z",
    "merged_at": "2023-12-22T05:22:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1059"
  },
  {
    "number": 1058,
    "title": "update requirement for UT",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-21T10:47:25Z",
    "closed_at": "2023-12-21T12:35:35Z",
    "merged_at": "2023-12-21T12:35:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1058"
  },
  {
    "number": 1057,
    "title": "[LLM Runtime] Fix the accuracy issue with group_size=-1",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-21T09:03:13Z",
    "closed_at": "2023-12-21T12:38:14Z",
    "merged_at": "2023-12-21T12:38:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1057"
  },
  {
    "number": 1056,
    "title": "Update run_generation.py",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-21T07:49:00Z",
    "closed_at": "2023-12-21T09:09:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1056"
  },
  {
    "number": 1055,
    "title": "[LLM] Save && Load INT4 model on CPU",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-21T03:44:15Z",
    "closed_at": "2023-12-25T00:54:43Z",
    "merged_at": "2023-12-25T00:54:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1055"
  },
  {
    "number": 1054,
    "title": "[LLM Runtime] Control printing information using NEURAL_SPEED_VERBOSE",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-21T02:34:12Z",
    "closed_at": "2024-01-05T09:17:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1054"
  },
  {
    "number": 1053,
    "title": "[LLM Runtime]Magicoder graph",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-21T01:24:53Z",
    "closed_at": "2023-12-26T08:09:05Z",
    "merged_at": "2023-12-26T08:09:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1053"
  },
  {
    "number": 1052,
    "title": "update transformers version to 4.36.0+",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-21T01:20:36Z",
    "closed_at": "2023-12-21T09:30:09Z",
    "merged_at": "2023-12-21T09:30:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1052"
  },
  {
    "number": 1051,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /intel_extension_for_transformers/neural_chat/ui/gradio/basic",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:16:30Z",
    "closed_at": "2023-12-21T01:30:31Z",
    "merged_at": "2023-12-21T01:30:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1051"
  },
  {
    "number": 1050,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/new_pruning",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:16:12Z",
    "closed_at": "2023-12-21T02:13:00Z",
    "merged_at": "2023-12-21T02:13:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1050"
  },
  {
    "number": 1049,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/distillation",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:16:04Z",
    "closed_at": "2023-12-21T02:13:14Z",
    "merged_at": "2023-12-21T02:13:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1049"
  },
  {
    "number": 1048,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/mrpc/bert_mini",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:16:03Z",
    "closed_at": "2023-12-21T02:51:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1048"
  },
  {
    "number": 1047,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/token-classification/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:16:02Z",
    "closed_at": "2023-12-21T02:06:45Z",
    "merged_at": "2023-12-21T02:06:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1047"
  },
  {
    "number": 1046,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/mrpc/bert_base_cased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:16:02Z",
    "closed_at": "2023-12-21T02:52:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1046"
  },
  {
    "number": 1045,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-generation/deployment",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:16:01Z",
    "closed_at": "2023-12-21T02:53:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1045"
  },
  {
    "number": 1044,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /intel_extension_for_transformers/neural_chat/examples/finetuning/instruction",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:16:00Z",
    "closed_at": "2023-12-21T03:19:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1044"
  },
  {
    "number": 1043,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/deployment/squad/length_adaptive_transformer",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:16:00Z",
    "closed_at": "2023-12-21T02:53:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1043"
  },
  {
    "number": 1042,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/pruning/longformer_triviaqa",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:56Z",
    "closed_at": "2023-12-21T02:08:21Z",
    "merged_at": "2023-12-21T02:08:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1042"
  },
  {
    "number": 1041,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /intel_extension_for_transformers/neural_chat/ui/gradio/side_by_side",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:52Z",
    "closed_at": "2023-12-21T01:31:54Z",
    "merged_at": "2023-12-21T01:31:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1041"
  },
  {
    "number": 1040,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/language-modeling/nas",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:49Z",
    "closed_at": "2023-12-21T02:08:41Z",
    "merged_at": "2023-12-21T02:08:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1040"
  },
  {
    "number": 1039,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/dynamic",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:42Z",
    "closed_at": "2023-12-21T02:09:10Z",
    "merged_at": "2023-12-21T02:09:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1039"
  },
  {
    "number": 1038,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-generation/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:41Z",
    "closed_at": "2023-12-21T02:12:35Z",
    "merged_at": "2023-12-21T02:12:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1038"
  },
  {
    "number": 1037,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/mrpc/roberta_base",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:39Z",
    "closed_at": "2023-12-21T02:54:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1037"
  },
  {
    "number": 1036,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/deployment/squad/bert_large",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:38Z",
    "closed_at": "2023-12-21T02:56:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1036"
  },
  {
    "number": 1035,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/emotion/distilbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:37Z",
    "closed_at": "2023-12-21T02:54:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1035"
  },
  {
    "number": 1034,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/summarization/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:36Z",
    "closed_at": "2023-12-21T02:16:19Z",
    "merged_at": "2023-12-21T02:16:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1034"
  },
  {
    "number": 1033,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/language-modeling/deployment/fill-mask/electra_base_chinese",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:36Z",
    "closed_at": "2023-12-21T02:56:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1033"
  },
  {
    "number": 1032,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/language-modeling/distillation",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:35Z",
    "closed_at": "2023-12-21T02:16:49Z",
    "merged_at": "2023-12-21T02:16:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1032"
  },
  {
    "number": 1031,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/onnxruntime/speech-recognition/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:34Z",
    "closed_at": "2023-12-21T01:38:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1031"
  },
  {
    "number": 1030,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:34Z",
    "closed_at": "2023-12-21T02:19:18Z",
    "merged_at": "2023-12-21T02:19:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1030"
  },
  {
    "number": 1029,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/orchestrate_optimizations",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:32Z",
    "closed_at": "2023-12-21T02:06:24Z",
    "merged_at": "2023-12-21T02:06:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1029"
  },
  {
    "number": 1028,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/sparse/distilbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:31Z",
    "closed_at": "2023-12-21T02:54:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1028"
  },
  {
    "number": 1027,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/tensorflow/token-classification/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:30Z",
    "closed_at": "2023-12-21T02:05:55Z",
    "merged_at": "2023-12-21T02:05:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1027"
  },
  {
    "number": 1026,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/image-classification/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:28Z",
    "closed_at": "2023-12-21T02:05:35Z",
    "merged_at": "2023-12-21T02:05:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1026"
  },
  {
    "number": 1025,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text2text-generation",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:26Z",
    "closed_at": "2023-12-21T03:31:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1025"
  },
  {
    "number": 1024,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /intel_extension_for_transformers/neural_chat/examples/deployment/plugin/image2image",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:24Z",
    "closed_at": "2023-12-21T03:22:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1024"
  },
  {
    "number": 1023,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /intel_extension_for_transformers/neural_chat/pipeline/plugins/ner",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:20Z",
    "closed_at": "2023-12-21T01:30:17Z",
    "merged_at": "2023-12-21T01:30:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1023"
  },
  {
    "number": 1022,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/distillation_for_quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:18Z",
    "closed_at": "2023-12-21T02:21:50Z",
    "merged_at": "2023-12-21T02:21:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1022"
  },
  {
    "number": 1021,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/orchestrate_optimizations",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:18Z",
    "closed_at": "2023-12-21T02:19:50Z",
    "merged_at": "2023-12-21T02:19:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1021"
  },
  {
    "number": 1020,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-embedding/deployment/mteb/bge",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:15Z",
    "closed_at": "2023-12-21T02:50:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1020"
  },
  {
    "number": 1019,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/sparse/bert_mini",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:14Z",
    "closed_at": "2023-12-21T02:56:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1019"
  },
  {
    "number": 1018,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/sst2/distilbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:14Z",
    "closed_at": "2023-12-21T02:59:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1018"
  },
  {
    "number": 1017,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/deployment/squad/ipex/bert_large",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:12Z",
    "closed_at": "2023-12-21T02:59:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1017"
  },
  {
    "number": 1016,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/textual-inversion/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:12Z",
    "closed_at": "2023-12-21T02:22:38Z",
    "merged_at": "2023-12-21T02:22:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1016"
  },
  {
    "number": 1015,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/language-modeling/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:10Z",
    "closed_at": "2023-12-21T02:59:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1015"
  },
  {
    "number": 1014,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/deployment/squad/ipex/distilbert_base_uncased_sparse",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:07Z",
    "closed_at": "2023-12-21T02:59:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1014"
  },
  {
    "number": 1013,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/tensorflow/text-classification/distillation/auto_distillation",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:06Z",
    "closed_at": "2023-12-21T02:24:18Z",
    "merged_at": "2023-12-21T02:24:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1013"
  },
  {
    "number": 1012,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/multiple-choice/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:15:04Z",
    "closed_at": "2023-12-21T02:24:28Z",
    "merged_at": "2023-12-21T02:24:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1012"
  },
  {
    "number": 1011,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/setfit",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:59Z",
    "closed_at": "2023-12-21T02:50:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1011"
  },
  {
    "number": 1010,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/distillation",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:59Z",
    "closed_at": "2023-12-21T02:24:35Z",
    "merged_at": "2023-12-21T02:24:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1010"
  },
  {
    "number": 1009,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/sst2/minilm_l6_h384_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:58Z",
    "closed_at": "2023-12-21T02:58:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1009"
  },
  {
    "number": 1008,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/sst2/bert_mini",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:45Z",
    "closed_at": "2023-12-21T02:58:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1008"
  },
  {
    "number": 1007,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/language-modeling/pruning",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:43Z",
    "closed_at": "2023-12-21T02:24:47Z",
    "merged_at": "2023-12-21T02:24:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1007"
  },
  {
    "number": 1006,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/mrpc/bert_base",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:37Z",
    "closed_at": "2023-12-21T02:58:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1006"
  },
  {
    "number": 1005,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/cascade-models",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:34Z",
    "closed_at": "2023-12-21T02:24:56Z",
    "merged_at": "2023-12-21T02:24:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1005"
  },
  {
    "number": 1004,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /intel_extension_for_transformers/llm/runtime/deprecated/test/pytest",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:13Z",
    "closed_at": "2023-12-21T01:52:57Z",
    "merged_at": "2023-12-21T01:52:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1004"
  },
  {
    "number": 1003,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/tensorflow/text-classification/distillation",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:12Z",
    "closed_at": "2023-12-21T02:25:05Z",
    "merged_at": "2023-12-21T02:25:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1003"
  },
  {
    "number": 1002,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /intel_extension_for_transformers/neural_chat/examples/finetuning/ppo_pipeline",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:10Z",
    "closed_at": "2023-12-21T03:21:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1002"
  },
  {
    "number": 1001,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:09Z",
    "closed_at": "2023-12-21T02:05:12Z",
    "merged_at": "2023-12-21T02:05:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1001"
  },
  {
    "number": 1000,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/early-exit",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:09Z",
    "closed_at": "2023-12-21T02:59:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1000"
  },
  {
    "number": 999,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/quantization/qat",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:08Z",
    "closed_at": "2023-12-21T02:20:53Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/999"
  },
  {
    "number": 998,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/pruning",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:04Z",
    "closed_at": "2023-12-21T02:25:15Z",
    "merged_at": "2023-12-21T02:25:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/998"
  },
  {
    "number": 997,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/tensorflow/text-classification/pruning",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:14:01Z",
    "closed_at": "2023-12-21T02:25:25Z",
    "merged_at": "2023-12-21T02:25:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/997"
  },
  {
    "number": 996,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /intel_extension_for_transformers/neural_chat/examples/finetuning/dpo_pipeline",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:55Z",
    "closed_at": "2023-12-21T03:20:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/996"
  },
  {
    "number": 995,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-to-image/deployment/stable_diffusion",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:54Z",
    "closed_at": "2023-12-21T02:59:18Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/995"
  },
  {
    "number": 994,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/quantization/ptq",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:54Z",
    "closed_at": "2023-12-21T02:20:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/994"
  },
  {
    "number": 993,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/tensorflow/text-classification/quantization/ptq",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:52Z",
    "closed_at": "2023-12-21T02:25:44Z",
    "merged_at": "2023-12-21T02:25:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/993"
  },
  {
    "number": 992,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/translation/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:52Z",
    "closed_at": "2023-12-21T02:25:55Z",
    "merged_at": "2023-12-21T02:25:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/992"
  },
  {
    "number": 991,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/tensorflow/language-modeling/quantization/ptq",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:47Z",
    "closed_at": "2023-12-21T02:26:05Z",
    "merged_at": "2023-12-21T02:26:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/991"
  },
  {
    "number": 990,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-to-image/quantization/qat",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:47Z",
    "closed_at": "2023-12-21T02:26:15Z",
    "merged_at": "2023-12-21T02:26:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/990"
  },
  {
    "number": 989,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/text-classification/deployment/mrpc/distilbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:43Z",
    "closed_at": "2023-12-21T02:59:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/989"
  },
  {
    "number": 988,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/tensorflow/multiple-choice/quantization",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:42Z",
    "closed_at": "2023-12-21T02:26:26Z",
    "merged_at": "2023-12-21T02:26:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/988"
  },
  {
    "number": 987,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/pruning/basic_magnitude",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:42Z",
    "closed_at": "2023-12-21T02:26:34Z",
    "merged_at": "2023-12-21T02:26:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/987"
  },
  {
    "number": 986,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/language-modeling/inference",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:38Z",
    "closed_at": "2023-12-21T02:26:52Z",
    "merged_at": "2023-12-21T02:26:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/986"
  },
  {
    "number": 985,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/question-answering/deployment/squad/ipex/distilbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:32Z",
    "closed_at": "2023-12-21T02:59:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/985"
  },
  {
    "number": 984,
    "title": "Bump transformers from 4.34.1 to 4.36.0 in /examples/huggingface/pytorch/image-classification/deployment/imagenet/vit",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:13:23Z",
    "closed_at": "2023-12-21T02:59:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/984"
  },
  {
    "number": 983,
    "title": "Bump transformers from 4.31.0 to 4.36.0 in /workflows/hf_finetuning_and_inference_nlp",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:12:36Z",
    "closed_at": "2023-12-21T01:25:37Z",
    "merged_at": "2023-12-21T01:25:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/983"
  },
  {
    "number": 982,
    "title": "Bump transformers from 4.32.1 to 4.36.0 in /intel_extension_for_transformers/neural_chat",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:12:34Z",
    "closed_at": "2023-12-21T01:33:13Z",
    "merged_at": "2023-12-21T01:33:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/982"
  },
  {
    "number": 981,
    "title": "Bump transformers from 4.30.0 to 4.36.0 in /workflows/compression_aware_training",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-12-20T21:09:00Z",
    "closed_at": "2023-12-21T01:21:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/981"
  },
  {
    "number": 980,
    "title": "Fix ORT whisper example",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T14:38:49Z",
    "closed_at": "2023-12-21T01:36:27Z",
    "merged_at": "2023-12-21T01:36:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/980"
  },
  {
    "number": 979,
    "title": "[LLM example]Parser args.recipes to dict and add args.use_llm_runtime",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T12:58:35Z",
    "closed_at": "2023-12-21T02:37:06Z",
    "merged_at": "2023-12-21T02:37:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/979"
  },
  {
    "number": 978,
    "title": "[LLM] Support recipes with calibration changes and add example parser args.",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T10:58:17Z",
    "closed_at": "2023-12-20T12:34:55Z",
    "merged_at": "2023-12-20T12:34:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/978"
  },
  {
    "number": 977,
    "title": "[Doc]refine example",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T10:29:22Z",
    "closed_at": "2023-12-20T12:06:21Z",
    "merged_at": "2023-12-20T12:06:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/977"
  },
  {
    "number": 976,
    "title": "Fix embeddings to support local file",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T10:29:08Z",
    "closed_at": "2023-12-21T08:22:11Z",
    "merged_at": "2023-12-21T08:22:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/976"
  },
  {
    "number": 975,
    "title": "[NeuralChat] Fix UT issues on Habana",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T09:43:28Z",
    "closed_at": "2023-12-21T06:20:18Z",
    "merged_at": "2023-12-21T06:20:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/975"
  },
  {
    "number": 974,
    "title": "[NeuralChat] Support Triton Inference Serving",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T06:14:30Z",
    "closed_at": "2023-12-20T12:04:08Z",
    "merged_at": "2023-12-20T12:04:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/974"
  },
  {
    "number": 973,
    "title": "[NeuralChat] Support Salesforce codegen model",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T06:04:18Z",
    "closed_at": "2023-12-20T11:54:31Z",
    "merged_at": "2023-12-20T11:54:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/973"
  },
  {
    "number": 972,
    "title": "[NeuralChat] Support Mixtral-8x7B-v0.1 model",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T05:39:22Z",
    "closed_at": "2023-12-25T23:08:57Z",
    "merged_at": "2023-12-25T23:08:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/972"
  },
  {
    "number": 971,
    "title": "[NeuralChat] fix wrong output of multi-round prediction",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T04:50:21Z",
    "closed_at": "2023-12-21T13:15:48Z",
    "merged_at": "2023-12-21T13:15:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/971"
  },
  {
    "number": 970,
    "title": "[NeuralChat] Fix llama model inference hang issue on Gaudi",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T03:39:27Z",
    "closed_at": "2023-12-20T05:53:51Z",
    "merged_at": "2023-12-20T05:53:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/970"
  },
  {
    "number": 969,
    "title": "[LLM Runtime]Improve Windows first token inference performance",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T03:01:39Z",
    "closed_at": "2023-12-20T10:44:16Z",
    "merged_at": "2023-12-20T10:44:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/969"
  },
  {
    "number": 968,
    "title": "add deepspeed support for ise-uiuc/Magicoder-S-CL-7B which only has s",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T02:48:26Z",
    "closed_at": "2023-12-20T05:34:31Z",
    "merged_at": "2023-12-20T05:34:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/968"
  },
  {
    "number": 967,
    "title": "Update README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-20T02:14:26Z",
    "closed_at": "2023-12-20T05:30:31Z",
    "merged_at": "2023-12-20T05:30:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/967"
  },
  {
    "number": 966,
    "title": "[LLM] Adapt models didn't need position ids do sq auto tune",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-19T12:32:01Z",
    "closed_at": "2023-12-20T01:48:35Z",
    "merged_at": "2023-12-20T01:48:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/966"
  },
  {
    "number": 965,
    "title": "[LLM Runtime] Update README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-19T08:13:08Z",
    "closed_at": "2023-12-19T09:02:55Z",
    "merged_at": "2023-12-19T09:02:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/965"
  },
  {
    "number": 964,
    "title": "[LLM Runtime] Fix mistral bf16",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-19T05:32:52Z",
    "closed_at": "2023-12-20T05:35:31Z",
    "merged_at": "2023-12-20T05:35:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/964"
  },
  {
    "number": 962,
    "title": "[NeuralChat] Fix path not exist when ut is skipped",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-19T02:22:37Z",
    "closed_at": "2023-12-20T12:07:30Z",
    "merged_at": "2023-12-20T12:07:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/962"
  },
  {
    "number": 961,
    "title": "[LLM Runtime] Sync inference scripts",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-19T02:06:48Z",
    "closed_at": "2023-12-25T12:35:25Z",
    "merged_at": "2023-12-25T12:35:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/961"
  },
  {
    "number": 960,
    "title": "[NeuralChat] Fix UT issues on Gaudi",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-18T15:11:02Z",
    "closed_at": "2023-12-18T23:00:08Z",
    "merged_at": "2023-12-18T23:00:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/960"
  },
  {
    "number": 959,
    "title": "[NeuralChat] Fix LLM runtime int4 issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-18T14:19:52Z",
    "closed_at": "2023-12-20T11:53:21Z",
    "merged_at": "2023-12-20T11:53:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/959"
  },
  {
    "number": 958,
    "title": "[LLM] Support bf16 inputs for WOQ",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-18T10:17:43Z",
    "closed_at": "2023-12-22T09:18:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/958"
  },
  {
    "number": 957,
    "title": "[NeuralChat] Specify local model path for nightly UT",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-18T08:10:10Z",
    "closed_at": "2023-12-21T01:47:52Z",
    "merged_at": "2023-12-21T01:47:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/957"
  },
  {
    "number": 956,
    "title": "[Doc] update data format and release data",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-18T06:49:46Z",
    "closed_at": "2023-12-21T13:29:30Z",
    "merged_at": "2023-12-21T13:29:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/956"
  },
  {
    "number": 955,
    "title": "[LLM] Fix llm models extension issue",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-18T05:57:51Z",
    "closed_at": "2023-12-21T10:38:23Z",
    "merged_at": "2023-12-21T10:38:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/955"
  },
  {
    "number": 954,
    "title": "Update qloracpu.md",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-18T03:14:35Z",
    "closed_at": "2023-12-20T11:47:18Z",
    "merged_at": "2023-12-20T11:47:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/954"
  },
  {
    "number": 952,
    "title": "[NeuralChat] Support user management in backend server",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-17T14:45:52Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/952"
  },
  {
    "number": 950,
    "title": "[NeuralChat] Improve model utils code coverage and fix ut issues",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-16T13:43:45Z",
    "closed_at": "2023-12-17T22:58:35Z",
    "merged_at": "2023-12-17T22:58:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/950"
  },
  {
    "number": 949,
    "title": "refract optimum habana available code for cpu/gpu/gaudi device conflict",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-15T13:24:21Z",
    "closed_at": "2023-12-19T11:00:04Z",
    "merged_at": "2023-12-19T11:00:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/949"
  },
  {
    "number": 948,
    "title": "[NeuralChat] support llama series model for llava finetuning.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-15T13:03:30Z",
    "closed_at": "2024-01-15T14:04:58Z",
    "merged_at": "2024-01-15T14:04:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/948"
  },
  {
    "number": 947,
    "title": "Client schedule",
    "user": "yuchengliu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-15T11:52:47Z",
    "closed_at": "2024-01-15T13:57:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/947"
  },
  {
    "number": 946,
    "title": "Fixed issue for T5 base model quantization issue with IPEX smoothquant",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-15T08:11:49Z",
    "closed_at": "2023-12-21T10:41:31Z",
    "merged_at": "2023-12-21T10:41:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/946"
  },
  {
    "number": 945,
    "title": "[LLM Runtime] Add MatMul data types combinations table",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-15T07:29:50Z",
    "closed_at": "2023-12-15T14:35:04Z",
    "merged_at": "2023-12-15T14:35:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/945"
  },
  {
    "number": 943,
    "title": "[NeuralChat] upgrade huggingface/transformers use_auth_token to token",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-14T13:02:29Z",
    "closed_at": "2023-12-15T02:57:09Z",
    "merged_at": "2023-12-15T02:57:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/943"
  },
  {
    "number": 942,
    "title": "[NeuralChat] Improve utils UT coverage",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-14T11:50:29Z",
    "closed_at": "2023-12-14T13:38:41Z",
    "merged_at": "2023-12-14T13:38:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/942"
  },
  {
    "number": 941,
    "title": "Fix llama series with transformers 4.36",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-14T08:03:27Z",
    "closed_at": "2023-12-15T12:58:40Z",
    "merged_at": "2023-12-15T12:58:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/941"
  },
  {
    "number": 940,
    "title": "[LLM Runtime] decoupling weight_type and scale_type in Qbits",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-14T07:53:49Z",
    "closed_at": "2023-12-16T08:00:56Z",
    "merged_at": "2023-12-16T08:00:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/940"
  },
  {
    "number": 939,
    "title": "[NeuralChat] Improve chatbot code coverage",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-14T07:42:01Z",
    "closed_at": "2023-12-17T02:03:57Z",
    "merged_at": "2023-12-17T02:03:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/939"
  },
  {
    "number": 938,
    "title": "[NeuralChat] Improve photoai UT coverage",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-14T05:56:06Z",
    "closed_at": "2023-12-15T05:58:13Z",
    "merged_at": "2023-12-15T05:58:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/938"
  },
  {
    "number": 937,
    "title": "[LLM Runtime] Fix PPL Test",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-14T05:51:02Z",
    "closed_at": "2023-12-15T12:57:33Z",
    "merged_at": "2023-12-15T12:57:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/937"
  },
  {
    "number": 936,
    "title": "[NeuralChat] Align inference generation configuration behaviour with HF",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-14T04:01:37Z",
    "closed_at": "2023-12-19T10:41:53Z",
    "merged_at": "2023-12-19T10:41:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/936"
  },
  {
    "number": 935,
    "title": "update neural compressor version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-14T03:09:49Z",
    "closed_at": "2023-12-14T07:51:02Z",
    "merged_at": "2023-12-14T07:51:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/935"
  },
  {
    "number": 934,
    "title": "Fix bug caused by sentense transformers upgrade",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-14T02:34:58Z",
    "closed_at": "2023-12-15T01:22:57Z",
    "merged_at": "2023-12-15T01:22:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/934"
  },
  {
    "number": 933,
    "title": "[NeuralChat] Improve infrastructure code coverage",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T15:23:05Z",
    "closed_at": "2023-12-14T02:54:56Z",
    "merged_at": "2023-12-14T02:54:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/933"
  },
  {
    "number": 932,
    "title": "Add gptj textgen finetuning and eval code",
    "user": "mini-goel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T14:47:59Z",
    "closed_at": "2024-01-12T07:58:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/932"
  },
  {
    "number": 931,
    "title": "[Doc] add contributors",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T12:02:56Z",
    "closed_at": "2023-12-14T06:29:40Z",
    "merged_at": "2023-12-14T06:29:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/931"
  },
  {
    "number": 930,
    "title": "[NeuralChat] finetuning ut supports gaudi",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T08:07:17Z",
    "closed_at": "2023-12-13T11:27:55Z",
    "merged_at": "2023-12-13T11:27:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/930"
  },
  {
    "number": 928,
    "title": "[NeuralChat] Update notebook",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T07:16:11Z",
    "closed_at": "2023-12-21T09:12:12Z",
    "merged_at": "2023-12-21T09:12:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/928"
  },
  {
    "number": 927,
    "title": "[LLM Runtime] Convert huggingface gptq model to jblas",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T06:35:40Z",
    "closed_at": "2023-12-19T07:19:23Z",
    "merged_at": "2023-12-19T07:19:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/927"
  },
  {
    "number": 926,
    "title": "[LLM Runtime] Add load_in_nbit test in nightly test",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T05:47:42Z",
    "closed_at": "2023-12-20T12:12:08Z",
    "merged_at": "2023-12-20T12:12:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/926"
  },
  {
    "number": 925,
    "title": "[LLM Runtime] GGUF format for ChatGLM2",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T05:47:29Z",
    "closed_at": "2024-01-10T06:51:52Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/925"
  },
  {
    "number": 924,
    "title": "Skip ut",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T05:12:45Z",
    "closed_at": "2023-12-13T05:20:20Z",
    "merged_at": "2023-12-13T05:20:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/924"
  },
  {
    "number": 923,
    "title": "skip UT to avoid block CI",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T03:11:47Z",
    "closed_at": "2023-12-13T03:30:20Z",
    "merged_at": "2023-12-13T03:30:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/923"
  },
  {
    "number": 922,
    "title": "[NeuralChat] Fix talking photo security",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-13T02:15:50Z",
    "closed_at": "2023-12-14T13:38:25Z",
    "merged_at": "2023-12-14T13:38:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/922"
  },
  {
    "number": 920,
    "title": "[NeuralChat] Fix PC codegen streaming issue and set 'Intel/neural-chat-7b-v3-1' as default model",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-12T14:41:30Z",
    "closed_at": "2023-12-14T02:56:51Z",
    "merged_at": "2023-12-14T02:56:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/920"
  },
  {
    "number": 919,
    "title": "[NeuralChat] Add Qwen model unit test case",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-12T14:22:12Z",
    "closed_at": "2023-12-14T02:52:31Z",
    "merged_at": "2023-12-14T02:52:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/919"
  },
  {
    "number": 916,
    "title": "adapt to windows",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-12T09:50:17Z",
    "closed_at": "2023-12-14T03:18:24Z",
    "merged_at": "2023-12-14T03:18:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/916"
  },
  {
    "number": 915,
    "title": "[LLM] Support fp8 config and update examples",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-12T08:52:56Z",
    "closed_at": "2023-12-14T03:19:47Z",
    "merged_at": "2023-12-14T03:19:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/915"
  },
  {
    "number": 914,
    "title": "[NeuralChat] Refactor ut server cases and improve code coverage",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-12T08:26:11Z",
    "closed_at": "2023-12-14T07:43:46Z",
    "merged_at": "2023-12-14T07:43:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/914"
  },
  {
    "number": 913,
    "title": "[NeuralChat] Refactor RAG code and structure",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-12T06:50:45Z",
    "closed_at": "2023-12-22T08:47:43Z",
    "merged_at": "2023-12-22T08:47:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/913"
  },
  {
    "number": 912,
    "title": "support woq fp8 config and update examples",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-12T06:47:33Z",
    "closed_at": "2023-12-12T06:48:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/912"
  },
  {
    "number": 911,
    "title": "[NeuralChat] Improve video/audio utils ut coverage",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-12T04:08:52Z",
    "closed_at": "2023-12-14T05:37:15Z",
    "merged_at": "2023-12-14T05:37:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/911"
  },
  {
    "number": 910,
    "title": "Support evaluation for onnx model exported with optimum >= 1.14.0",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-12T03:00:38Z",
    "closed_at": "2023-12-14T05:54:58Z",
    "merged_at": "2023-12-14T05:54:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/910"
  },
  {
    "number": 909,
    "title": "[NeuralChat] Fix the default values for the config parameters",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-11T14:37:10Z",
    "closed_at": "2023-12-14T02:57:58Z",
    "merged_at": "2023-12-14T02:57:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/909"
  },
  {
    "number": 908,
    "title": "add publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-11T13:37:57Z",
    "closed_at": "2023-12-13T11:27:01Z",
    "merged_at": "2023-12-13T11:27:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/908"
  },
  {
    "number": 907,
    "title": "[NeuralChat] Notebook Update",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-11T08:05:52Z",
    "closed_at": "2023-12-12T06:46:01Z",
    "merged_at": "2023-12-12T06:46:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/907"
  },
  {
    "number": 906,
    "title": "[NeuralChat] Modify yaml configuration",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-11T05:35:10Z",
    "closed_at": "2023-12-15T05:58:50Z",
    "merged_at": "2023-12-15T05:58:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/906"
  },
  {
    "number": 905,
    "title": "fix neuralchat nightly ut",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-11T03:24:59Z",
    "closed_at": "2023-12-11T06:29:45Z",
    "merged_at": "2023-12-11T06:29:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/905"
  },
  {
    "number": 904,
    "title": "fix imageio path issue",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-11T03:19:12Z",
    "closed_at": "2023-12-11T06:29:08Z",
    "merged_at": "2023-12-11T06:29:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/904"
  },
  {
    "number": 903,
    "title": "[LLM Runtime] make rms_norm_eps and freq_base as parameter",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-11T01:22:35Z",
    "closed_at": "2023-12-11T08:36:15Z",
    "merged_at": "2023-12-11T08:36:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/903"
  },
  {
    "number": 902,
    "title": "[NeuralChat] Fix codegen windows pc example",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-10T14:55:35Z",
    "closed_at": "2023-12-11T01:49:28Z",
    "merged_at": "2023-12-11T01:49:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/902"
  },
  {
    "number": 900,
    "title": "enlarge stack size",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-09T09:55:30Z",
    "closed_at": "2023-12-10T00:15:10Z",
    "merged_at": "2023-12-10T00:15:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/900"
  },
  {
    "number": 899,
    "title": "resolve azure disk structure issues",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-09T04:49:52Z",
    "closed_at": "2023-12-09T07:57:30Z",
    "merged_at": "2023-12-09T07:57:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/899"
  },
  {
    "number": 898,
    "title": "Fix code-generation woq example",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T10:35:44Z",
    "closed_at": "2023-12-11T01:54:42Z",
    "merged_at": "2023-12-11T01:54:42Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/898"
  },
  {
    "number": 897,
    "title": "fix gradient issue for qlora on seq2seq",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T07:49:05Z",
    "closed_at": "2023-12-09T02:33:27Z",
    "merged_at": "2023-12-09T02:33:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/897"
  },
  {
    "number": 896,
    "title": "[NeuralChat] Support assisted generation for NeuralChat",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T07:47:18Z",
    "closed_at": "2023-12-08T13:55:09Z",
    "merged_at": "2023-12-08T13:55:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/896"
  },
  {
    "number": 895,
    "title": "improve reorder cache for the ipex.optimize_transformers.",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T05:21:10Z",
    "closed_at": "2023-12-08T08:35:58Z",
    "merged_at": "2023-12-08T08:35:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/895"
  },
  {
    "number": 894,
    "title": "[LLM Runtime] Remove the identical branch",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T05:16:38Z",
    "closed_at": "2023-12-09T05:08:59Z",
    "merged_at": "2023-12-09T05:08:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/894"
  },
  {
    "number": 892,
    "title": "add constraint for peft",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T03:45:21Z",
    "closed_at": "2023-12-08T09:17:21Z",
    "merged_at": "2023-12-08T09:17:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/892"
  },
  {
    "number": 891,
    "title": "[LLM Runtime] Baichuan13B inference bug fix",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T02:29:20Z",
    "closed_at": "2023-12-08T04:55:10Z",
    "merged_at": "2023-12-08T04:55:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/891"
  },
  {
    "number": 890,
    "title": "Add auto slim example for bge models on STS tasks",
    "user": "YIYANGCAI",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T01:38:07Z",
    "closed_at": "2023-12-17T02:06:13Z",
    "merged_at": "2023-12-17T02:06:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/890"
  },
  {
    "number": 889,
    "title": "[NeuralChat] Enhance SafetyChecker to resolve can't find stopword.txt",
    "user": "xmx-521",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-08T01:14:06Z",
    "closed_at": "2023-12-08T06:53:19Z",
    "merged_at": "2023-12-08T06:53:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/889"
  },
  {
    "number": 888,
    "title": "remove last_token_acc",
    "user": "WeiweiZhang1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-07T16:25:32Z",
    "closed_at": "2023-12-07T23:02:51Z",
    "merged_at": "2023-12-07T23:02:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/888"
  },
  {
    "number": 887,
    "title": "add limitation for tensorflow and onnx for neuralchat",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-07T12:46:28Z",
    "closed_at": "2023-12-07T23:03:07Z",
    "merged_at": "2023-12-07T23:03:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/887"
  },
  {
    "number": 886,
    "title": "Fix miss example_inputs issue",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-07T12:40:03Z",
    "closed_at": "2023-12-08T01:20:45Z",
    "merged_at": "2023-12-08T01:20:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/886"
  },
  {
    "number": 885,
    "title": "[NeuralChat] Update README for OpenAI-Compatible RESTful API",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-07T11:52:39Z",
    "closed_at": "2023-12-09T04:36:58Z",
    "merged_at": "2023-12-09T04:36:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/885"
  },
  {
    "number": 884,
    "title": "add peft model support in deepspeed sharded mode",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-07T10:04:20Z",
    "closed_at": "2023-12-09T05:19:51Z",
    "merged_at": "2023-12-09T05:19:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/884"
  },
  {
    "number": 883,
    "title": "Update windows binary test",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-07T08:08:10Z",
    "closed_at": "2023-12-07T08:34:55Z",
    "merged_at": "2023-12-07T08:34:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/883"
  },
  {
    "number": 882,
    "title": "[NeuralChat] Add codegen restful API and clean code format for code generation scenario",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-07T06:59:03Z",
    "closed_at": "2023-12-07T12:55:36Z",
    "merged_at": "2023-12-07T12:55:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/882"
  },
  {
    "number": 880,
    "title": "update peft version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-07T05:41:20Z",
    "closed_at": "2023-12-07T07:20:00Z",
    "merged_at": "2023-12-07T07:20:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/880"
  },
  {
    "number": 879,
    "title": "[NeuralChat] Support image to image plugin as service",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T15:55:24Z",
    "closed_at": "2023-12-11T12:54:55Z",
    "merged_at": "2023-12-11T12:54:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/879"
  },
  {
    "number": 878,
    "title": "[Infra] add windows binary build test",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T13:32:14Z",
    "closed_at": "2023-12-07T06:03:35Z",
    "merged_at": "2023-12-07T06:03:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/878"
  },
  {
    "number": 877,
    "title": "[NeuralChat] Fix issue with private access preventing download of the bge int8 model",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T08:28:04Z",
    "closed_at": "2023-12-06T13:21:25Z",
    "merged_at": "2023-12-06T13:21:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/877"
  },
  {
    "number": 876,
    "title": "Add SparsityConfig class for PyTorch inference",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T08:26:21Z",
    "closed_at": "2023-12-08T05:22:45Z",
    "merged_at": "2023-12-08T05:22:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/876"
  },
  {
    "number": 875,
    "title": "update readme optimum commit due to Phi",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T07:38:29Z",
    "closed_at": "2023-12-06T11:21:11Z",
    "merged_at": "2023-12-06T11:21:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/875"
  },
  {
    "number": 874,
    "title": "sync with inc, refine data and example",
    "user": "WeiweiZhang1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T07:30:57Z",
    "closed_at": "2023-12-07T03:28:57Z",
    "merged_at": "2023-12-07T03:28:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/874"
  },
  {
    "number": 872,
    "title": "[LLM Runtime] Add MX-Format (FP8_E5M2, FP8_E4M3, FP4_E2M1, NF4)",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T06:38:21Z",
    "closed_at": "2023-12-15T04:42:53Z",
    "merged_at": "2023-12-15T04:42:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/872"
  },
  {
    "number": 871,
    "title": "[LLM Runtime] Update BGE README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T06:12:01Z",
    "closed_at": "2023-12-07T08:17:04Z",
    "merged_at": "2023-12-07T08:17:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/871"
  },
  {
    "number": 870,
    "title": "[NeuralChat] Dpo gaudi acceleration",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T01:25:38Z",
    "closed_at": "2023-12-06T03:19:42Z",
    "merged_at": "2023-12-06T03:19:42Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/870"
  },
  {
    "number": 869,
    "title": "[Neural Engine] Fix kernels softmax in int8 mha",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-06T01:11:57Z",
    "closed_at": "2023-12-06T05:27:40Z",
    "merged_at": "2023-12-06T05:27:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/869"
  },
  {
    "number": 868,
    "title": "[NeuralChat] Update Gradio version to fix malfunction in 3.34.0",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-05T15:57:49Z",
    "closed_at": "2023-12-05T22:57:13Z",
    "merged_at": "2023-12-05T22:57:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/868"
  },
  {
    "number": 867,
    "title": "[NeuralChat] Support multi cards streaming inference on Gaudi",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-05T15:45:52Z",
    "closed_at": "2023-12-06T06:30:17Z",
    "merged_at": "2023-12-06T06:30:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/867"
  },
  {
    "number": 865,
    "title": "[NeuralChat] Update readme and example yaml files",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-05T15:22:10Z",
    "closed_at": "2023-12-06T02:07:31Z",
    "merged_at": "2023-12-06T02:07:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/865"
  },
  {
    "number": 864,
    "title": "Update requirement",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-05T12:31:24Z",
    "closed_at": "2023-12-05T12:41:40Z",
    "merged_at": "2023-12-05T12:41:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/864"
  },
  {
    "number": 863,
    "title": "Fix task infer bug in export",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-05T08:42:51Z",
    "closed_at": "2023-12-05T12:41:58Z",
    "merged_at": "2023-12-05T12:41:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/863"
  },
  {
    "number": 862,
    "title": "[NeuralChat] Enable multi-cpu serving",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-05T07:14:56Z",
    "closed_at": "2023-12-05T22:59:09Z",
    "merged_at": "2023-12-05T22:59:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/862"
  },
  {
    "number": 861,
    "title": "[LLM Runtime] Remove the unnecessary branch",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-05T06:08:29Z",
    "closed_at": "2023-12-05T12:40:57Z",
    "merged_at": "2023-12-05T12:40:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/861"
  },
  {
    "number": 860,
    "title": "[LLM] Support restoring sq optimized model for text-generation ",
    "user": "Kaihui-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-05T01:58:33Z",
    "closed_at": "2023-12-08T05:35:25Z",
    "merged_at": "2023-12-08T05:35:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/860"
  },
  {
    "number": 859,
    "title": "[LLM] Fix chatglm accuracy regression and support chatglm2 prompt",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T10:28:34Z",
    "closed_at": "2023-12-04T13:24:09Z",
    "merged_at": "2023-12-04T13:24:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/859"
  },
  {
    "number": 858,
    "title": "[LLM] Fix code-generation benchmark",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T09:04:14Z",
    "closed_at": "2023-12-04T12:28:32Z",
    "merged_at": "2023-12-04T12:28:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/858"
  },
  {
    "number": 857,
    "title": "[LLM] Fix code-generaion example extension test cmd",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T07:42:23Z",
    "closed_at": "2023-12-04T07:49:36Z",
    "merged_at": "2023-12-04T07:49:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/857"
  },
  {
    "number": 856,
    "title": "[LLM Runtime] Fix windows compile error",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T07:07:45Z",
    "closed_at": "2023-12-04T07:50:13Z",
    "merged_at": "2023-12-04T07:50:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/856"
  },
  {
    "number": 855,
    "title": "[NeuralChat] update dpo dataset",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T05:39:54Z",
    "closed_at": "2023-12-04T06:01:02Z",
    "merged_at": "2023-12-04T06:01:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/855"
  },
  {
    "number": 854,
    "title": "update textual-inversion requirements",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T03:34:58Z",
    "closed_at": "2023-12-04T08:13:31Z",
    "merged_at": "2023-12-04T08:13:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/854"
  },
  {
    "number": 853,
    "title": "[LLM] make text-generation example woq default weight dtype to \"nf4\"",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T03:29:50Z",
    "closed_at": "2023-12-04T12:30:54Z",
    "merged_at": "2023-12-04T12:30:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/853"
  },
  {
    "number": 851,
    "title": "fix dpo finetuning path",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T02:20:18Z",
    "closed_at": "2023-12-04T02:39:59Z",
    "merged_at": "2023-12-04T02:39:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/851"
  },
  {
    "number": 850,
    "title": "fix potential thread conflict in makedirs",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T02:16:23Z",
    "closed_at": "2023-12-04T07:20:41Z",
    "merged_at": "2023-12-04T07:20:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/850"
  },
  {
    "number": 849,
    "title": "[LLM] add an option to enable fallback_add",
    "user": "yintong-lu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-04T00:54:54Z",
    "closed_at": "2023-12-04T03:39:24Z",
    "merged_at": "2023-12-04T03:39:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/849"
  },
  {
    "number": 848,
    "title": "Fix typo at installation.md",
    "user": "LucasHBG",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-03T17:30:03Z",
    "closed_at": "2023-12-03T23:53:47Z",
    "merged_at": "2023-12-03T23:53:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/848"
  },
  {
    "number": 847,
    "title": "Fix typo at third_party_programs.txt file",
    "user": "LucasHBG",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-03T16:34:39Z",
    "closed_at": "2023-12-04T01:17:24Z",
    "merged_at": "2023-12-04T01:17:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/847"
  },
  {
    "number": 844,
    "title": "[NeuralChat] Fix StarCoder running issue on Gaudi ",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T13:02:55Z",
    "closed_at": "2023-12-03T12:34:27Z",
    "merged_at": "2023-12-03T12:34:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/844"
  },
  {
    "number": 843,
    "title": "[LLM] add revision to text-generaion example",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T11:49:13Z",
    "closed_at": "2023-12-01T12:36:58Z",
    "merged_at": "2023-12-01T12:36:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/843"
  },
  {
    "number": 842,
    "title": "fix checkmarx issue",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T09:04:00Z",
    "closed_at": "2023-12-01T12:32:37Z",
    "merged_at": "2023-12-01T12:32:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/842"
  },
  {
    "number": 841,
    "title": "fix not define unwrapped_model",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T08:44:39Z",
    "closed_at": "2023-12-01T12:32:24Z",
    "merged_at": "2023-12-01T12:32:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/841"
  },
  {
    "number": 839,
    "title": "Fix bge-base-en-v1.5-sts-int8-static model ut issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T06:20:28Z",
    "closed_at": "2023-12-01T12:39:38Z",
    "merged_at": "2023-12-01T12:39:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/839"
  },
  {
    "number": 838,
    "title": "[NeuralChat] Improve UT coverage",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T06:18:01Z",
    "closed_at": "2023-12-01T12:40:02Z",
    "merged_at": "2023-12-01T12:40:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/838"
  },
  {
    "number": 837,
    "title": "[LLM] add chatglm and codellama extension test",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T05:46:56Z",
    "closed_at": "2023-12-01T06:31:37Z",
    "merged_at": "2023-12-01T06:31:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/837"
  },
  {
    "number": 836,
    "title": "[LLM] Enable Falcon SQ, which uses Multi-Query Attention (MQA), and therefore past_kv dimension is different and has its own if branch.",
    "user": "qgao007",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T04:56:35Z",
    "closed_at": "2023-12-06T04:33:18Z",
    "merged_at": "2023-12-06T04:33:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/836"
  },
  {
    "number": 835,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T03:15:09Z",
    "closed_at": "2023-12-01T06:44:06Z",
    "merged_at": "2023-12-01T06:44:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/835"
  },
  {
    "number": 834,
    "title": "[Infra] enhance CI scan",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T02:40:02Z",
    "closed_at": "2023-12-05T12:42:29Z",
    "merged_at": "2023-12-05T12:42:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/834"
  },
  {
    "number": 833,
    "title": "Move `optimum` package import into export function",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T02:20:58Z",
    "closed_at": "2023-12-01T03:39:08Z",
    "merged_at": "2023-12-01T03:39:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/833"
  },
  {
    "number": 832,
    "title": "[LLM Runtime] Baichuan7B Enabling",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-12-01T01:37:02Z",
    "closed_at": "2024-01-15T14:00:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/832"
  },
  {
    "number": 830,
    "title": "[NeuralChat] Add and refine codegen examples",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T15:31:21Z",
    "closed_at": "2023-11-30T23:48:25Z",
    "merged_at": "2023-11-30T23:48:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/830"
  },
  {
    "number": 829,
    "title": "[NeuralChat] Fix UT issues on Nvidia GPU",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T14:42:19Z",
    "closed_at": "2023-11-30T23:49:18Z",
    "merged_at": "2023-11-30T23:49:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/829"
  },
  {
    "number": 828,
    "title": "fix save issue of deepspeed zero3",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T13:22:15Z",
    "closed_at": "2023-11-30T23:50:08Z",
    "merged_at": "2023-11-30T23:50:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/828"
  },
  {
    "number": 827,
    "title": "improve prepare_generation_for_inputs for chatglm",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T12:59:15Z",
    "closed_at": "2023-12-01T01:49:23Z",
    "merged_at": "2023-12-01T01:49:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/827"
  },
  {
    "number": 826,
    "title": "Fix UT issue is_bitsandbytes_available for transformers version upgrade",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T09:59:25Z",
    "closed_at": "2023-11-30T11:08:18Z",
    "merged_at": "2023-11-30T11:08:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/826"
  },
  {
    "number": 825,
    "title": "minor fix",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T09:34:40Z",
    "closed_at": "2023-11-30T11:10:32Z",
    "merged_at": "2023-11-30T11:10:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/825"
  },
  {
    "number": 824,
    "title": "[NeuralChat] support full parameters finetuning",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T09:26:58Z",
    "closed_at": "2023-12-06T03:18:19Z",
    "merged_at": "2023-12-06T03:18:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/824"
  },
  {
    "number": 823,
    "title": "[LLM Runtime] Fix windows compile error",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T09:14:38Z",
    "closed_at": "2023-11-30T12:22:54Z",
    "merged_at": "2023-11-30T12:22:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/823"
  },
  {
    "number": 822,
    "title": "[NeuralChat] Fix ner nightly ut bug",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T08:51:24Z",
    "closed_at": "2023-11-30T12:31:03Z",
    "merged_at": "2023-11-30T12:31:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/822"
  },
  {
    "number": 821,
    "title": "escape sql string for SDL",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T08:19:41Z",
    "closed_at": "2023-11-30T09:03:38Z",
    "merged_at": "2023-11-30T09:03:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/821"
  },
  {
    "number": 820,
    "title": "fix finetuning instruction readme issue",
    "user": "huiyan2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T06:03:05Z",
    "closed_at": "2023-11-30T11:17:33Z",
    "merged_at": "2023-11-30T11:17:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/820"
  },
  {
    "number": 819,
    "title": "[NeuralChat] update llava model import path",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T03:47:57Z",
    "closed_at": "2023-11-30T05:34:47Z",
    "merged_at": "2023-11-30T05:34:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/819"
  },
  {
    "number": 818,
    "title": "[LLM Runtime] Remove use_cache in WOQ",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-30T02:50:55Z",
    "closed_at": "2023-12-01T09:17:24Z",
    "merged_at": "2023-12-01T09:17:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/818"
  },
  {
    "number": 817,
    "title": "[NeuralChat] Fix typo in photoai_services.py",
    "user": "eltociear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T15:24:42Z",
    "closed_at": "2023-11-29T22:59:17Z",
    "merged_at": "2023-11-29T22:59:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/817"
  },
  {
    "number": 816,
    "title": "[NeuralChat] update dpo dataset fields name.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T13:59:50Z",
    "closed_at": "2023-11-29T15:04:58Z",
    "merged_at": "2023-11-29T15:04:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/816"
  },
  {
    "number": 815,
    "title": "Update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T13:48:38Z",
    "closed_at": "2023-11-29T14:19:03Z",
    "merged_at": "2023-11-29T14:19:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/815"
  },
  {
    "number": 814,
    "title": "[LLM] Support chatglm/falcon series and unified int8 loading",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T11:37:34Z",
    "closed_at": "2023-11-30T06:23:44Z",
    "merged_at": "2023-11-30T06:23:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/814"
  },
  {
    "number": 812,
    "title": "[LLM Runtime]fix format",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T08:29:20Z",
    "closed_at": "2023-11-29T10:07:34Z",
    "merged_at": "2023-11-29T10:07:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/812"
  },
  {
    "number": 811,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T07:08:59Z",
    "closed_at": "2023-11-29T08:39:46Z",
    "merged_at": "2023-11-29T08:39:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/811"
  },
  {
    "number": 810,
    "title": "Add SetFit API in ITREX",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T06:45:48Z",
    "closed_at": "2023-12-08T06:51:46Z",
    "merged_at": "2023-12-08T06:51:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/810"
  },
  {
    "number": 809,
    "title": "[LLM Runtime] Add Whisper Example and Python API",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T06:33:01Z",
    "closed_at": "2024-01-03T01:38:00Z",
    "merged_at": "2024-01-03T01:38:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/809"
  },
  {
    "number": 808,
    "title": "[Doc] update README for Qwen chat",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T06:10:06Z",
    "closed_at": "2023-12-01T07:58:43Z",
    "merged_at": "2023-12-01T07:58:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/808"
  },
  {
    "number": 807,
    "title": "[LLM Runtime] add shared memory support ccl to parallel context",
    "user": "ClarkChin08",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T06:01:15Z",
    "closed_at": "2024-01-15T14:00:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/807"
  },
  {
    "number": 806,
    "title": "[NeuralChat]Support llm runtime int4 using existed local quantized model",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T05:46:01Z",
    "closed_at": "2023-11-29T09:15:21Z",
    "merged_at": "2023-11-29T09:15:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/806"
  },
  {
    "number": 805,
    "title": "[NeuralChat] Fix the print bug in retriever",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T05:40:30Z",
    "closed_at": "2023-11-29T09:04:45Z",
    "merged_at": "2023-11-29T09:04:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/805"
  },
  {
    "number": 804,
    "title": "fix tts and face animation UT on gpu",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-29T04:25:26Z",
    "closed_at": "2023-11-29T09:05:10Z",
    "merged_at": "2023-11-29T09:05:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/804"
  },
  {
    "number": 802,
    "title": "[NeuralChat] Support Gaudi model parallelism serving",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T15:00:32Z",
    "closed_at": "2023-11-29T11:13:46Z",
    "merged_at": "2023-11-29T11:13:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/802"
  },
  {
    "number": 801,
    "title": "recover test_Ner",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T12:42:37Z",
    "closed_at": "2023-11-30T09:07:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/801"
  },
  {
    "number": 800,
    "title": "[NeuralChat] use validation dataset for evaluation.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T10:46:00Z",
    "closed_at": "2023-11-28T11:25:15Z",
    "merged_at": "2023-11-28T11:25:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/800"
  },
  {
    "number": 799,
    "title": "[Doc] add gaudi2 in doc",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T07:17:26Z",
    "closed_at": "2023-12-13T11:06:38Z",
    "merged_at": "2023-12-13T11:06:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/799"
  },
  {
    "number": 798,
    "title": "[NeuralChat] Enable plugin as service",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T06:25:56Z",
    "closed_at": "2023-11-30T11:25:48Z",
    "merged_at": "2023-11-30T11:25:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/798"
  },
  {
    "number": 797,
    "title": "[LLM Runtime] Update weightonlyquant.md ",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T06:22:21Z",
    "closed_at": "2023-11-28T06:30:13Z",
    "merged_at": "2023-11-28T06:30:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/797"
  },
  {
    "number": 796,
    "title": "update Gaudi2 name.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T05:45:56Z",
    "closed_at": "2023-11-28T06:28:07Z",
    "merged_at": "2023-11-28T06:28:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/796"
  },
  {
    "number": 795,
    "title": "[LLM] Fix SmoothQuant auto tuning dataloader",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T04:59:23Z",
    "closed_at": "2023-11-28T06:29:22Z",
    "merged_at": "2023-11-28T06:29:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/795"
  },
  {
    "number": 794,
    "title": "[LLM Runtime] Fix develop doc and convert.py",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T03:33:31Z",
    "closed_at": "2023-11-28T05:17:52Z",
    "merged_at": "2023-11-28T05:17:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/794"
  },
  {
    "number": 793,
    "title": "[LLM Runtime]  fix added_tokens error",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T03:03:57Z",
    "closed_at": "2023-11-29T11:12:55Z",
    "merged_at": "2023-11-29T11:12:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/793"
  },
  {
    "number": 792,
    "title": "[LLM example] improve text-generation example",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T02:37:49Z",
    "closed_at": "2023-11-28T03:40:59Z",
    "merged_at": "2023-11-28T03:40:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/792"
  },
  {
    "number": 791,
    "title": "[LLM Runtime] GPTQ to jblas for llama2",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T02:26:03Z",
    "closed_at": "2023-12-13T07:19:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/791"
  },
  {
    "number": 790,
    "title": "Revert \"docs : reinforcement llm runtime graph devleoper guide (#786)\"",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T02:06:49Z",
    "closed_at": "2023-11-28T06:54:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/790"
  },
  {
    "number": 789,
    "title": "fix : init_from_bin example",
    "user": "park12sj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-28T00:00:12Z",
    "closed_at": "2023-11-28T08:34:47Z",
    "merged_at": "2023-11-28T08:34:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/789"
  },
  {
    "number": 788,
    "title": "fix : max output token",
    "user": "park12sj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T23:43:23Z",
    "closed_at": "2023-11-28T01:32:26Z",
    "merged_at": "2023-11-28T01:32:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/788"
  },
  {
    "number": 787,
    "title": "update docker container and docs for v1.3.0",
    "user": "tylertitsworth",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T23:39:02Z",
    "closed_at": "2023-11-28T02:35:26Z",
    "merged_at": "2023-11-28T02:35:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/787"
  },
  {
    "number": 786,
    "title": "[LLM Runtime] docs : reinforcement llm runtime graph devleoper guide",
    "user": "park12sj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T22:19:15Z",
    "closed_at": "2023-11-28T01:37:01Z",
    "merged_at": "2023-11-28T01:37:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/786"
  },
  {
    "number": 784,
    "title": "[NeuralChat] support llava multi-model training.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T15:28:13Z",
    "closed_at": "2023-11-29T13:00:28Z",
    "merged_at": "2023-11-29T13:00:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/784"
  },
  {
    "number": 783,
    "title": "[NeuralChat]  Add optimization pipeline for LLaMa model familly",
    "user": "mzyczyns",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T13:40:23Z",
    "closed_at": "2023-11-28T05:18:58Z",
    "merged_at": "2023-11-28T05:18:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/783"
  },
  {
    "number": 782,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T12:19:21Z",
    "closed_at": "2023-11-27T13:21:40Z",
    "merged_at": "2023-11-27T13:21:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/782"
  },
  {
    "number": 781,
    "title": "Update GPTQ into README",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T10:19:57Z",
    "closed_at": "2023-11-27T12:24:19Z",
    "merged_at": "2023-11-27T12:24:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/781"
  },
  {
    "number": 780,
    "title": "Enhance input and output names setting in onnx export",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T10:11:30Z",
    "closed_at": "2023-11-27T13:22:52Z",
    "merged_at": "2023-11-27T13:22:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/780"
  },
  {
    "number": 779,
    "title": "move llmruntime ut to azure for disk missing",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T03:27:35Z",
    "closed_at": "2023-11-27T03:33:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/779"
  },
  {
    "number": 778,
    "title": "[LLM Runtime] Check weight dtype and compute dtype",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T03:17:14Z",
    "closed_at": "2023-11-28T03:44:03Z",
    "merged_at": "2023-11-28T03:44:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/778"
  },
  {
    "number": 777,
    "title": "[Infra] update check UT status",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T03:03:32Z",
    "closed_at": "2023-11-27T06:33:23Z",
    "merged_at": "2023-11-27T06:33:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/777"
  },
  {
    "number": 776,
    "title": "[NeuralChat] Support NER with chatbot and upload_link to create new kb",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T02:35:21Z",
    "closed_at": "2023-11-27T07:02:30Z",
    "merged_at": "2023-11-27T07:02:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/776"
  },
  {
    "number": 775,
    "title": "Fix lm-eval optimum version",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-27T02:09:26Z",
    "closed_at": "2023-11-27T02:52:53Z",
    "merged_at": "2023-11-27T02:52:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/775"
  },
  {
    "number": 774,
    "title": "update installation.md",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-26T05:47:34Z",
    "closed_at": "2023-11-27T02:31:53Z",
    "merged_at": "2023-11-27T02:31:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/774"
  },
  {
    "number": 773,
    "title": "update to habana SW release 1.13 and enable llama 70b lora finetune",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-25T14:19:46Z",
    "closed_at": "2023-11-29T09:16:10Z",
    "merged_at": "2023-11-29T09:16:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/773"
  },
  {
    "number": 772,
    "title": "Enable Falcon, which use MQA and therefore past_kv dimension is different and has its own if branch. ",
    "user": "qgao007",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-25T10:41:42Z",
    "closed_at": "2023-12-03T20:02:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/772"
  },
  {
    "number": 771,
    "title": "Update ppo readme",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-25T09:15:57Z",
    "closed_at": "2023-11-30T12:32:46Z",
    "merged_at": "2023-11-30T12:32:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/771"
  },
  {
    "number": 769,
    "title": "[LLM Runtime] refactor itrex backend based on the latest Jblas",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-24T08:39:46Z",
    "closed_at": "2023-12-13T06:17:01Z",
    "merged_at": "2023-12-13T06:17:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/769"
  },
  {
    "number": 768,
    "title": "Update ci",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-24T08:36:06Z",
    "closed_at": "2023-11-24T09:34:08Z",
    "merged_at": "2023-11-24T09:34:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/768"
  },
  {
    "number": 767,
    "title": "fix tts ut random issue",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-24T08:27:02Z",
    "closed_at": "2023-11-24T13:06:52Z",
    "merged_at": "2023-11-24T13:06:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/767"
  },
  {
    "number": 766,
    "title": "Fix fastchat version 0.2.33 upgrade issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-24T08:16:58Z",
    "closed_at": "2023-11-24T12:00:55Z",
    "merged_at": "2023-11-24T12:00:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/766"
  },
  {
    "number": 765,
    "title": "Fix nightly CI issue",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-24T06:52:33Z",
    "closed_at": "2023-11-24T09:06:37Z",
    "merged_at": "2023-11-24T09:06:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/765"
  },
  {
    "number": 764,
    "title": "add neuralchat support ipex.optimize_transformers sq int8 model loading",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-24T05:56:57Z",
    "closed_at": "2023-11-24T09:06:53Z",
    "merged_at": "2023-11-24T09:06:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/764"
  },
  {
    "number": 763,
    "title": "update docker name and coverage threshold",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-24T05:21:20Z",
    "closed_at": "2023-11-24T06:00:51Z",
    "merged_at": "2023-11-24T06:00:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/763"
  },
  {
    "number": 761,
    "title": "[NeuralChat] Fix tokenizer issue for optimized model",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T14:05:53Z",
    "closed_at": "2023-11-24T00:55:43Z",
    "merged_at": "2023-11-24T00:55:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/761"
  },
  {
    "number": 759,
    "title": "fix circular import issue",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T14:04:26Z",
    "closed_at": "2023-11-24T07:07:29Z",
    "merged_at": "2023-11-24T07:07:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/759"
  },
  {
    "number": 758,
    "title": "Move the TSModelCausalLMForOPTLLM path",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T13:32:57Z",
    "closed_at": "2023-11-24T00:57:47Z",
    "merged_at": "2023-11-24T00:57:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/758"
  },
  {
    "number": 756,
    "title": "support dpo training on CPU (SPR)",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T09:42:40Z",
    "closed_at": "2023-11-23T12:29:34Z",
    "merged_at": "2023-11-23T12:29:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/756"
  },
  {
    "number": 755,
    "title": "update talkingbot",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T09:00:00Z",
    "closed_at": "2023-11-24T09:08:54Z",
    "merged_at": "2023-11-24T09:08:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/755"
  },
  {
    "number": 754,
    "title": "dockerfile refine",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T08:26:06Z",
    "closed_at": "2023-11-24T08:48:08Z",
    "merged_at": "2023-11-24T08:48:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/754"
  },
  {
    "number": 753,
    "title": "[Neural Chat] Added video for QLoRA running on client CPU",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T08:18:04Z",
    "closed_at": "2023-11-23T10:52:03Z",
    "merged_at": "2023-11-23T10:52:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/753"
  },
  {
    "number": 752,
    "title": "[Infra] use python logging",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T07:51:32Z",
    "closed_at": "2023-12-09T02:33:17Z",
    "merged_at": "2023-12-09T02:33:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/752"
  },
  {
    "number": 751,
    "title": "improve modeling_auto recipes setting.",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T06:53:17Z",
    "closed_at": "2023-11-23T11:16:19Z",
    "merged_at": "2023-11-23T11:16:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/751"
  },
  {
    "number": 750,
    "title": "SmoothQuantConfig support recipes",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T03:47:25Z",
    "closed_at": "2023-11-23T06:05:02Z",
    "merged_at": "2023-11-23T06:05:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/750"
  },
  {
    "number": 749,
    "title": "[BUG] fix UT path error",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T03:39:38Z",
    "closed_at": "2023-11-23T05:34:05Z",
    "merged_at": "2023-11-23T05:34:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/749"
  },
  {
    "number": 748,
    "title": "[NeuralChat] Update retrieval README.md",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-23T01:54:46Z",
    "closed_at": "2023-11-23T06:22:12Z",
    "merged_at": "2023-11-23T06:22:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/748"
  },
  {
    "number": 747,
    "title": "add dpo data process to clm finetune",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-22T13:59:26Z",
    "closed_at": "2023-11-23T02:14:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/747"
  },
  {
    "number": 746,
    "title": "[LLM Runtime] init ns setup.py",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-22T09:01:08Z",
    "closed_at": "2023-12-25T23:49:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/746"
  },
  {
    "number": 745,
    "title": "Add Assisted Generation Example",
    "user": "danielkorat",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-22T08:57:54Z",
    "closed_at": "2023-11-22T13:43:13Z",
    "merged_at": "2023-11-22T13:43:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/745"
  },
  {
    "number": 744,
    "title": "Move TSModelCausalLMForOPTLLM to lm-eval",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-22T07:32:08Z",
    "closed_at": "2023-11-22T07:40:56Z",
    "merged_at": "2023-11-22T07:40:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/744"
  },
  {
    "number": 742,
    "title": "fix QBits compile on windows",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-22T06:29:51Z",
    "closed_at": "2023-11-22T12:31:05Z",
    "merged_at": "2023-11-22T12:31:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/742"
  },
  {
    "number": 741,
    "title": "[LLM] Support gpt_neox ipex.optimize_transformers and update quantization workflow.",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-22T02:42:37Z",
    "closed_at": "2023-11-23T02:09:14Z",
    "merged_at": "2023-11-23T02:09:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/741"
  },
  {
    "number": 740,
    "title": "Bump joblib from 1.1.0 to 1.2.0 in /intel_extension_for_transformers/neural_chat/tests",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-11-22T01:00:34Z",
    "closed_at": "2023-11-22T01:48:28Z",
    "merged_at": "2023-11-22T01:48:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/740"
  },
  {
    "number": 739,
    "title": "Bump joblib from 1.1.0 to 1.2.0 in /intel_extension_for_transformers/neural_chat/pipeline/plugins/video/face_animation",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-11-22T01:00:14Z",
    "closed_at": "2023-11-22T01:52:46Z",
    "merged_at": "2023-11-22T01:52:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/739"
  },
  {
    "number": 738,
    "title": "Bump joblib from 1.1.0 to 1.2.0 in /intel_extension_for_transformers/neural_chat",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-11-22T01:00:08Z",
    "closed_at": "2023-11-22T02:56:00Z",
    "merged_at": "2023-11-22T02:56:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/738"
  },
  {
    "number": 737,
    "title": "allow group_size -1 in quantization_config",
    "user": "bearn01d",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-21T15:19:10Z",
    "closed_at": "2023-11-22T03:13:44Z",
    "merged_at": "2023-11-22T03:13:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/737"
  },
  {
    "number": 736,
    "title": "Update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-21T13:34:00Z",
    "closed_at": "2023-11-21T13:52:49Z",
    "merged_at": "2023-11-21T13:52:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/736"
  },
  {
    "number": 735,
    "title": "Extend langchain embedding API",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-21T09:39:02Z",
    "closed_at": "2023-11-27T08:38:29Z",
    "merged_at": "2023-11-27T08:38:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/735"
  },
  {
    "number": 734,
    "title": "[LLM Runtime] Beam Search Support of Fused Attention",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-21T09:09:30Z",
    "closed_at": "2023-11-27T04:37:30Z",
    "merged_at": "2023-11-27T04:37:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/734"
  },
  {
    "number": 733,
    "title": "Add dataset arguments description in readme",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-21T07:36:05Z",
    "closed_at": "2023-11-21T08:22:43Z",
    "merged_at": "2023-11-21T08:22:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/733"
  },
  {
    "number": 732,
    "title": "Add AWS support for neural chat multi-node fine-tuning",
    "user": "louie-tsai",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-21T07:31:12Z",
    "closed_at": "2023-11-22T03:14:36Z",
    "merged_at": "2023-11-22T03:14:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/732"
  },
  {
    "number": 730,
    "title": "add dpo data process to clm finetune",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-21T05:46:41Z",
    "closed_at": "2023-11-23T02:09:41Z",
    "merged_at": "2023-11-23T02:09:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/730"
  },
  {
    "number": 729,
    "title": "separate optimize UT and improve UT infra",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-21T03:14:54Z",
    "closed_at": "2023-11-22T12:42:57Z",
    "merged_at": "2023-11-22T12:42:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/729"
  },
  {
    "number": 728,
    "title": "add finetuning metrics doc",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-21T01:40:45Z",
    "closed_at": "2023-11-21T01:51:43Z",
    "merged_at": "2023-11-21T01:51:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/728"
  },
  {
    "number": 727,
    "title": "warp conv compute",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-21T01:38:25Z",
    "closed_at": "2023-11-21T01:38:38Z",
    "merged_at": "2023-11-21T01:38:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/727"
  },
  {
    "number": 725,
    "title": "Refactor NeuralChat UI directory structure",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-20T13:38:03Z",
    "closed_at": "2023-11-20T23:08:01Z",
    "merged_at": "2023-11-20T23:08:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/725"
  },
  {
    "number": 723,
    "title": "multilingual asr support",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-20T08:34:22Z",
    "closed_at": "2023-11-21T12:38:55Z",
    "merged_at": "2023-11-21T12:38:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/723"
  },
  {
    "number": 722,
    "title": "[NeuralChat] Remove unnecessary model load during optimizing model",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-20T06:51:51Z",
    "closed_at": "2023-11-21T14:03:16Z",
    "merged_at": "2023-11-21T14:03:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/722"
  },
  {
    "number": 720,
    "title": "Update retrieval readme on file farmat",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-20T03:28:11Z",
    "closed_at": "2023-11-20T08:02:54Z",
    "merged_at": "2023-11-20T08:02:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/720"
  },
  {
    "number": 719,
    "title": "[Infra] remove restriction of INC",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-20T02:45:17Z",
    "closed_at": "2023-11-20T05:39:27Z",
    "merged_at": "2023-11-20T05:39:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/719"
  },
  {
    "number": 718,
    "title": "[LLM Runtime] Optimize tests of llm runtime",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-20T02:15:10Z",
    "closed_at": "2023-11-20T10:43:18Z",
    "merged_at": "2023-11-20T10:43:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/718"
  },
  {
    "number": 717,
    "title": "add docker clean before run inference",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-20T00:39:59Z",
    "closed_at": "2023-11-20T01:07:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/717"
  },
  {
    "number": 716,
    "title": "remove chatbot test dependency on local scripts",
    "user": "chensuyue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-19T12:24:29Z",
    "closed_at": "2023-11-20T01:03:42Z",
    "merged_at": "2023-11-20T01:03:42Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/716"
  },
  {
    "number": 714,
    "title": "Fix some link issues in NeuralChat",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-19T09:49:27Z",
    "closed_at": "2023-11-20T01:01:39Z",
    "merged_at": "2023-11-20T01:01:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/714"
  },
  {
    "number": 713,
    "title": "update code for training neuralchat-7b-v3.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-19T05:57:57Z",
    "closed_at": "2023-11-22T03:15:38Z",
    "merged_at": "2023-11-22T03:15:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/713"
  },
  {
    "number": 711,
    "title": "Support CodeLlama model in NeuralChat",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T15:55:52Z",
    "closed_at": "2023-11-20T02:08:07Z",
    "merged_at": "2023-11-20T02:08:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/711"
  },
  {
    "number": 710,
    "title": "Support Mistral model in NeuralChat",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T13:48:53Z",
    "closed_at": "2023-11-20T06:23:32Z",
    "merged_at": "2023-11-20T06:23:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/710"
  },
  {
    "number": 709,
    "title": "Support security check for both origin query and translated query",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T13:06:24Z",
    "closed_at": "2023-11-23T11:12:10Z",
    "merged_at": "2023-11-23T11:12:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/709"
  },
  {
    "number": 708,
    "title": "update readme",
    "user": "yuchengliu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T09:35:59Z",
    "closed_at": "2023-11-17T12:05:50Z",
    "merged_at": "2023-11-17T12:05:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/708"
  },
  {
    "number": 707,
    "title": "[LLM Runtime] Optmized dropout operator",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T08:38:23Z",
    "closed_at": "2023-11-20T13:01:10Z",
    "merged_at": "2023-11-20T13:01:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/707"
  },
  {
    "number": 706,
    "title": "[NeuralChat]remove haystack dependency",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T06:39:25Z",
    "closed_at": "2023-11-17T07:38:24Z",
    "merged_at": "2023-11-17T07:38:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/706"
  },
  {
    "number": 705,
    "title": "fix tokenizer for code generation eval",
    "user": "violetch24",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T06:18:06Z",
    "closed_at": "2023-11-17T07:53:46Z",
    "merged_at": "2023-11-17T07:53:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/705"
  },
  {
    "number": 704,
    "title": "[NeuralChat] refine dockerfile",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T06:17:15Z",
    "closed_at": "2023-11-20T08:09:01Z",
    "merged_at": "2023-11-20T08:09:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/704"
  },
  {
    "number": 703,
    "title": "update code generation example script",
    "user": "violetch24",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T04:11:08Z",
    "closed_at": "2023-11-28T08:19:16Z",
    "merged_at": "2023-11-28T08:19:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/703"
  },
  {
    "number": 702,
    "title": "skip testllmruntime temperately",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-17T02:48:16Z",
    "closed_at": "2023-11-17T03:09:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/702"
  },
  {
    "number": 701,
    "title": "restrain INC version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-16T14:02:30Z",
    "closed_at": "2023-11-17T01:19:35Z",
    "merged_at": "2023-11-17T01:19:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/701"
  },
  {
    "number": 700,
    "title": "[LLM Runtime] ChatGLM-V1 multi-batch infer and batched greedy search generation",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-16T08:45:56Z",
    "closed_at": "2023-12-01T09:04:13Z",
    "merged_at": "2023-12-01T09:04:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/700"
  },
  {
    "number": 699,
    "title": "Trainer modification for the BGE",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-16T08:18:31Z",
    "closed_at": "2023-11-16T14:20:59Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/699"
  },
  {
    "number": 698,
    "title": "Support neural-chat-7b-v3 and neural-chat-7b-v3-1",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-16T08:03:33Z",
    "closed_at": "2023-11-17T12:14:08Z",
    "merged_at": "2023-11-17T12:14:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/698"
  },
  {
    "number": 697,
    "title": "[LLM Runtime] Correct GCC targe options",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-16T07:18:46Z",
    "closed_at": "2023-11-16T08:54:46Z",
    "merged_at": "2023-11-16T08:54:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/697"
  },
  {
    "number": 696,
    "title": "[LLM Runtime] Update README",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-16T07:03:34Z",
    "closed_at": "2023-11-17T05:37:28Z",
    "merged_at": "2023-11-17T05:37:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/696"
  },
  {
    "number": 695,
    "title": "Smoothquant support ipex.optimize_transformers feature",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-16T04:14:47Z",
    "closed_at": "2023-11-21T05:24:07Z",
    "merged_at": "2023-11-21T05:24:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/695"
  },
  {
    "number": 692,
    "title": "[NeuralChat] Rollback predict stream stats format for Cnvg.io",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T15:32:51Z",
    "closed_at": "2024-01-05T07:45:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/692"
  },
  {
    "number": 691,
    "title": "Update requirements.txt",
    "user": "snirbenyosef",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T15:30:52Z",
    "closed_at": "2023-12-08T11:32:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/691"
  },
  {
    "number": 690,
    "title": "[Infra] update docker related readme",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T08:57:37Z",
    "closed_at": "2023-11-16T00:47:20Z",
    "merged_at": "2023-11-16T00:47:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/690"
  },
  {
    "number": 689,
    "title": "[NeuralChat] Integrate PhotoAI backend into NeuralChat",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T07:08:53Z",
    "closed_at": "2023-11-20T11:18:34Z",
    "merged_at": "2023-11-20T11:18:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/689"
  },
  {
    "number": 688,
    "title": "[LLM Runtime] Support load_in_nbit in llm runtime",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T06:35:27Z",
    "closed_at": "2023-11-15T11:36:50Z",
    "merged_at": "2023-11-15T11:36:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/688"
  },
  {
    "number": 687,
    "title": "[LLM Example] add save for mixedprecision model",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T06:33:24Z",
    "closed_at": "2023-11-15T07:18:10Z",
    "merged_at": "2023-11-15T07:18:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/687"
  },
  {
    "number": 686,
    "title": "fix typo : graph_developer_document branch no longer exists",
    "user": "park12sj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T06:13:54Z",
    "closed_at": "2023-11-15T06:27:32Z",
    "merged_at": "2023-11-15T06:27:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/686"
  },
  {
    "number": 685,
    "title": "[LLM Runtime] Add Script for PPL Evaluation",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T06:00:38Z",
    "closed_at": "2023-11-20T01:53:37Z",
    "merged_at": "2023-11-20T01:53:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/685"
  },
  {
    "number": 684,
    "title": "[LLM Runtime] add python api for mistral",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T05:40:42Z",
    "closed_at": "2023-11-15T05:55:21Z",
    "merged_at": "2023-11-15T05:55:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/684"
  },
  {
    "number": 683,
    "title": "enable mistral in habana, fix issue in generate.py",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T03:16:29Z",
    "closed_at": "2023-11-17T01:29:01Z",
    "merged_at": "2023-11-17T01:29:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/683"
  },
  {
    "number": 682,
    "title": "[LLM Runtime] Enable whisper new app",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T03:16:25Z",
    "closed_at": "2023-11-29T06:12:27Z",
    "merged_at": "2023-11-29T06:12:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/682"
  },
  {
    "number": 681,
    "title": "[LLM Runtime] Compare cpp logits with pytorch",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T03:03:51Z",
    "closed_at": "2023-11-17T01:30:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/681"
  },
  {
    "number": 680,
    "title": "Add skip for UT",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-15T02:33:18Z",
    "closed_at": "2023-11-15T04:13:20Z",
    "merged_at": "2023-11-15T04:13:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/680"
  },
  {
    "number": 679,
    "title": "Fix ChatGLM2 llm runtime int4 issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-14T14:29:06Z",
    "closed_at": "2023-11-15T11:42:52Z",
    "merged_at": "2023-11-15T11:42:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/679"
  },
  {
    "number": 677,
    "title": "Add assisted generation example",
    "user": "danielkorat",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-14T12:31:29Z",
    "closed_at": "2023-11-22T08:14:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/677"
  },
  {
    "number": 676,
    "title": "[others]refine gitpod",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-14T10:16:53Z",
    "closed_at": "2023-11-14T10:18:18Z",
    "merged_at": "2023-11-14T10:18:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/676"
  },
  {
    "number": 674,
    "title": "[LLM Runtime] LLAMA & MPT Beam Search",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-14T09:07:03Z",
    "closed_at": "2023-12-20T01:41:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/674"
  },
  {
    "number": 673,
    "title": "[Engine] Apply the STS task to bge models",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-14T06:04:24Z",
    "closed_at": "2023-11-29T07:56:50Z",
    "merged_at": "2023-11-29T07:56:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/673"
  },
  {
    "number": 671,
    "title": "[LLM Runtime]Fix gptneox bug",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-14T03:04:18Z",
    "closed_at": "2023-11-14T03:25:56Z",
    "merged_at": "2023-11-14T03:25:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/671"
  },
  {
    "number": 669,
    "title": "[LLM Runtime] enable qwen graph",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T10:03:35Z",
    "closed_at": "2023-11-24T02:51:17Z",
    "merged_at": "2023-11-24T02:51:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/669"
  },
  {
    "number": 668,
    "title": "[LLM] text-generation example support peft model quantization",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T09:47:30Z",
    "closed_at": "2023-11-13T10:04:13Z",
    "merged_at": "2023-11-13T10:04:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/668"
  },
  {
    "number": 667,
    "title": "[Doc] add readme of tutorials",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T06:28:49Z",
    "closed_at": "2023-11-14T09:59:26Z",
    "merged_at": "2023-11-14T09:59:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/667"
  },
  {
    "number": 666,
    "title": "update ut value due to gcc change",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T03:28:24Z",
    "closed_at": "2023-11-13T03:33:28Z",
    "merged_at": "2023-11-13T03:33:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/666"
  },
  {
    "number": 665,
    "title": "[LLM Runtime] Refine Python API",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-13T00:46:06Z",
    "closed_at": "2023-11-15T02:56:08Z",
    "merged_at": "2023-11-15T02:56:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/665"
  },
  {
    "number": 663,
    "title": "[Doc]add readme",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-10T12:14:29Z",
    "closed_at": "2023-11-12T07:25:01Z",
    "merged_at": "2023-11-12T07:25:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/663"
  },
  {
    "number": 662,
    "title": "[LLM Runtime] disable bf16 scale for jblas",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-10T06:52:50Z",
    "closed_at": "2023-11-13T09:41:00Z",
    "merged_at": "2023-11-13T09:41:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/662"
  },
  {
    "number": 661,
    "title": "fixed bf16 error in convert_llama.py",
    "user": "akarX23",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-10T06:14:15Z",
    "closed_at": "2023-11-10T06:25:13Z",
    "merged_at": "2023-11-10T06:25:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/661"
  },
  {
    "number": 660,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-10T04:39:26Z",
    "closed_at": "2023-11-13T01:57:19Z",
    "merged_at": "2023-11-13T01:57:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/660"
  },
  {
    "number": 659,
    "title": "[NeuralChat] Enable retrieval with url as inputs",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-10T03:44:24Z",
    "closed_at": "2023-11-22T00:45:36Z",
    "merged_at": "2023-11-22T00:45:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/659"
  },
  {
    "number": 658,
    "title": "[Example] BAAI/bge-small-en-v1.5 example add",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-10T01:32:42Z",
    "closed_at": "2023-11-10T01:47:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/658"
  },
  {
    "number": 657,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-10T01:30:28Z",
    "closed_at": "2023-11-10T02:15:53Z",
    "merged_at": "2023-11-10T02:15:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/657"
  },
  {
    "number": 656,
    "title": "[NeuralChat] Fix the bug in prompt generation",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-09T15:02:56Z",
    "closed_at": "2023-11-12T12:30:39Z",
    "merged_at": "2023-11-12T12:30:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/656"
  },
  {
    "number": 655,
    "title": "[LLM Runtime] Allow CompileBF16 on GCC11",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-09T09:16:40Z",
    "closed_at": "2023-11-10T03:11:00Z",
    "merged_at": "2023-11-10T03:11:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/655"
  },
  {
    "number": 654,
    "title": "[Neural Chat] Fix neuralchat ut path",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-09T06:29:15Z",
    "closed_at": "2023-11-13T09:43:06Z",
    "merged_at": "2023-11-13T09:43:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/654"
  },
  {
    "number": 653,
    "title": "GHA chatbot cpu inference CI",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-09T02:53:35Z",
    "closed_at": "2023-11-09T08:58:26Z",
    "merged_at": "2023-11-09T08:58:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/653"
  },
  {
    "number": 651,
    "title": "add noise reducer and amplifier",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T10:10:27Z",
    "closed_at": "2023-11-09T06:27:08Z",
    "merged_at": "2023-11-09T06:27:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/651"
  },
  {
    "number": 650,
    "title": "[NeuralChat] support return error code",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T08:58:12Z",
    "closed_at": "2023-12-12T12:03:52Z",
    "merged_at": "2023-12-12T12:03:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/650"
  },
  {
    "number": 649,
    "title": "[NeuralChat] Added StarCoder, CodeLlama, Falcon and Mistral finetuning example in NeuralChat",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T08:36:37Z",
    "closed_at": "2023-11-09T08:06:40Z",
    "merged_at": "2023-11-09T08:06:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/649"
  },
  {
    "number": 648,
    "title": "[Neural Chat] Update dockerfile for internal test",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T07:10:29Z",
    "closed_at": "2023-11-09T05:46:58Z",
    "merged_at": "2023-11-09T05:46:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/648"
  },
  {
    "number": 647,
    "title": "Fix latest chromadb version issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T06:53:41Z",
    "closed_at": "2023-11-08T08:13:07Z",
    "merged_at": "2023-11-08T08:13:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/647"
  },
  {
    "number": 646,
    "title": "[LLM Runtime] Multi-Round chat with chatglm2",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T05:27:18Z",
    "closed_at": "2023-11-09T07:01:20Z",
    "merged_at": "2023-11-09T07:01:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/646"
  },
  {
    "number": 645,
    "title": "fix spelling",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T03:54:54Z",
    "closed_at": "2023-11-08T03:58:50Z",
    "merged_at": "2023-11-08T03:58:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/645"
  },
  {
    "number": 644,
    "title": "Fix ONNXRT session bug with upgraded optimum 1.14.0",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T03:41:59Z",
    "closed_at": "2023-11-09T13:22:26Z",
    "merged_at": "2023-11-09T13:22:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/644"
  },
  {
    "number": 643,
    "title": "[Example] instruction_tuning_sd examples add",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-08T03:22:31Z",
    "closed_at": "2023-11-10T03:50:01Z",
    "merged_at": "2023-11-10T03:50:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/643"
  },
  {
    "number": 642,
    "title": "Fix llm runtime int4 multi-turn inference issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T14:15:00Z",
    "closed_at": "2023-11-08T05:19:01Z",
    "merged_at": "2023-11-08T05:19:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/642"
  },
  {
    "number": 641,
    "title": "update docker readme",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T09:02:50Z",
    "closed_at": "2023-11-07T09:22:18Z",
    "merged_at": "2023-11-07T09:22:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/641"
  },
  {
    "number": 640,
    "title": "Fix textbot chatglm model conversation issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T08:50:57Z",
    "closed_at": "2023-11-08T02:02:39Z",
    "merged_at": "2023-11-08T02:02:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/640"
  },
  {
    "number": 639,
    "title": "[LLM Runtime] Add jblas split weight interface and support jblas models",
    "user": "ClarkChin08",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T08:39:12Z",
    "closed_at": "2023-11-25T01:37:28Z",
    "merged_at": "2023-11-25T01:37:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/639"
  },
  {
    "number": 638,
    "title": "[LLM] text-generation example support chatglm2&3",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T08:29:10Z",
    "closed_at": "2023-11-13T05:31:56Z",
    "merged_at": "2023-11-13T05:31:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/638"
  },
  {
    "number": 637,
    "title": "Add fine-tuning with Deepspeed example",
    "user": "Liangliang-Ma",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T06:49:10Z",
    "closed_at": "2023-11-13T04:35:55Z",
    "merged_at": "2023-11-13T04:35:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/637"
  },
  {
    "number": 636,
    "title": "[Infra] replace llama with llama2",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T05:53:11Z",
    "closed_at": "2023-11-08T02:15:34Z",
    "merged_at": "2023-11-08T02:15:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/636"
  },
  {
    "number": 635,
    "title": "[LLM Runtime] Update example README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T02:40:35Z",
    "closed_at": "2023-11-07T03:41:50Z",
    "merged_at": "2023-11-07T03:41:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/635"
  },
  {
    "number": 634,
    "title": "add initial version of rl_training",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T02:02:56Z",
    "closed_at": "2023-11-23T07:07:08Z",
    "merged_at": "2023-11-23T07:07:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/634"
  },
  {
    "number": 633,
    "title": "Add providers for onnx whisper example",
    "user": "mengniwang95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-07T01:35:46Z",
    "closed_at": "2023-11-08T02:36:44Z",
    "merged_at": "2023-11-08T02:36:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/633"
  },
  {
    "number": 632,
    "title": "Fix benchmark cmd for onnx whisper example",
    "user": "mengniwang95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T08:13:38Z",
    "closed_at": "2023-11-06T12:15:00Z",
    "merged_at": "2023-11-06T12:15:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/632"
  },
  {
    "number": 631,
    "title": "[Neural Chat] Fix neuralchat woq UT",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T07:17:13Z",
    "closed_at": "2023-11-06T09:20:36Z",
    "merged_at": "2023-11-06T09:20:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/631"
  },
  {
    "number": 630,
    "title": "fix : remove model load before woq llm runtime",
    "user": "park12sj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T07:12:13Z",
    "closed_at": "2023-11-06T14:48:31Z",
    "merged_at": "2023-11-06T14:48:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/630"
  },
  {
    "number": 629,
    "title": "Update requirment",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T06:53:05Z",
    "closed_at": "2023-11-06T08:57:51Z",
    "merged_at": "2023-11-06T08:57:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/629"
  },
  {
    "number": 628,
    "title": "loose version for security check",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T06:45:06Z",
    "closed_at": "2023-11-06T07:20:11Z",
    "merged_at": "2023-11-06T07:20:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/628"
  },
  {
    "number": 627,
    "title": "[Third-Party] restrain transformers version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T06:29:52Z",
    "closed_at": "2023-11-07T03:36:03Z",
    "merged_at": "2023-11-07T03:36:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/627"
  },
  {
    "number": 626,
    "title": "Move photo AI to deployment directory",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T06:14:23Z",
    "closed_at": "2023-11-06T09:10:12Z",
    "merged_at": "2023-11-06T09:10:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/626"
  },
  {
    "number": 625,
    "title": "[LLM Runtime] Fix LLaMA after discarding KV-cache",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T03:33:06Z",
    "closed_at": "2023-11-06T05:43:12Z",
    "merged_at": "2023-11-06T05:43:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/625"
  },
  {
    "number": 624,
    "title": "[NeuralChat] Refine int4 notebook",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T02:29:51Z",
    "closed_at": "2023-11-06T09:20:49Z",
    "merged_at": "2023-11-06T09:20:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/624"
  },
  {
    "number": 623,
    "title": "[Document] update llm runtime readme",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-06T01:32:35Z",
    "closed_at": "2023-11-06T05:20:39Z",
    "merged_at": "2023-11-06T05:20:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/623"
  },
  {
    "number": 622,
    "title": "Update README.md",
    "user": "eltociear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-04T05:59:56Z",
    "closed_at": "2023-11-06T04:40:41Z",
    "merged_at": "2023-11-06T04:40:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/622"
  },
  {
    "number": 621,
    "title": "[Example] BAAI/bge-large-en-v1.5 & BAAI/bge-base-en-v1.5 README update",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-03T13:34:37Z",
    "closed_at": "2023-11-03T14:16:56Z",
    "merged_at": "2023-11-03T14:16:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/621"
  },
  {
    "number": 620,
    "title": "Fix bloom ffn fusion",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-03T07:21:37Z",
    "closed_at": "2023-11-03T08:06:53Z",
    "merged_at": "2023-11-03T08:06:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/620"
  },
  {
    "number": 619,
    "title": "[Example] BAAI/bge-large-en-v1.5 & BAAI/bge-base-en-v1.5 examples add",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-03T03:23:55Z",
    "closed_at": "2023-11-03T12:41:13Z",
    "merged_at": "2023-11-03T12:41:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/619"
  },
  {
    "number": 618,
    "title": "[UT]fix tf autodistill ut fail",
    "user": "n1ck-guo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-03T01:50:28Z",
    "closed_at": "2023-11-03T03:16:00Z",
    "merged_at": "2023-11-03T03:16:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/618"
  },
  {
    "number": 617,
    "title": "[LLM] Update lm-eval commit id",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T14:24:55Z",
    "closed_at": "2023-11-06T01:22:23Z",
    "merged_at": "2023-11-06T01:22:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/617"
  },
  {
    "number": 616,
    "title": "[NeuralChat] Fix compatibility issue caused by the latest version of Pydantic",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T07:28:30Z",
    "closed_at": "2023-11-02T15:30:43Z",
    "merged_at": "2023-11-02T15:30:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/616"
  },
  {
    "number": 615,
    "title": "Added script of merging peft adapter for quantization of llm with peft",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T06:57:00Z",
    "closed_at": "2023-11-02T09:13:31Z",
    "merged_at": "2023-11-02T09:13:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/615"
  },
  {
    "number": 614,
    "title": "fix text-generation requirements.txt",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T06:53:45Z",
    "closed_at": "2023-11-02T06:54:35Z",
    "merged_at": "2023-11-02T06:54:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/614"
  },
  {
    "number": 613,
    "title": "Fix neuralchat examples path change issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T06:46:24Z",
    "closed_at": "2023-11-02T08:29:21Z",
    "merged_at": "2023-11-02T08:29:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/613"
  },
  {
    "number": 612,
    "title": "Combine docker",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T06:27:12Z",
    "closed_at": "2023-11-02T08:33:20Z",
    "merged_at": "2023-11-02T08:33:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/612"
  },
  {
    "number": 611,
    "title": "[LLM Runtime] Enable GPTQ models",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T06:08:44Z",
    "closed_at": "2023-11-24T06:22:07Z",
    "merged_at": "2023-11-24T06:22:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/611"
  },
  {
    "number": 610,
    "title": "Wenjiao/warning fix",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T06:06:03Z",
    "closed_at": "2023-11-02T07:30:00Z",
    "merged_at": "2023-11-02T07:30:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/610"
  },
  {
    "number": 609,
    "title": "[DOC] add LLM Runtime developer document",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T02:12:39Z",
    "closed_at": "2023-11-03T13:58:43Z",
    "merged_at": "2023-11-03T13:58:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/609"
  },
  {
    "number": 608,
    "title": "[LLM Runtime] Shift-RoPE-based Streaming-LLM for Fused-Attention ",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-02T01:46:00Z",
    "closed_at": "2023-11-06T16:31:54Z",
    "merged_at": "2023-11-06T16:31:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/608"
  },
  {
    "number": 607,
    "title": "[NeuralChat] fix video path",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-01T08:24:18Z",
    "closed_at": "2023-11-02T11:47:03Z",
    "merged_at": "2023-11-02T11:47:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/607"
  },
  {
    "number": 606,
    "title": "[NeuralChat] add reward modeling handle for PPO",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-01T07:09:53Z",
    "closed_at": "2023-11-03T10:00:55Z",
    "merged_at": "2023-11-03T10:00:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/606"
  },
  {
    "number": 605,
    "title": "Support peft model in server mode",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-01T05:24:25Z",
    "closed_at": "2023-11-02T03:26:14Z",
    "merged_at": "2023-11-02T03:26:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/605"
  },
  {
    "number": 604,
    "title": "[LLM] Add op_name_dict attribute to SmoothQuantConfig",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-01T03:51:39Z",
    "closed_at": "2023-11-01T05:25:56Z",
    "merged_at": "2023-11-01T05:25:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/604"
  },
  {
    "number": 603,
    "title": "[NeuralChat] Refine askdoc API and retrieval logic",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-11-01T03:08:30Z",
    "closed_at": "2023-11-03T06:10:42Z",
    "merged_at": "2023-11-03T06:10:42Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/603"
  },
  {
    "number": 602,
    "title": "Create autocomment.yml",
    "user": "shraddha761",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T16:52:05Z",
    "closed_at": "2023-11-01T03:28:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/602"
  },
  {
    "number": 601,
    "title": "[NeuralChat] Fix askdoc server port conflict issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T15:44:29Z",
    "closed_at": "2023-11-09T03:53:43Z",
    "merged_at": "2023-11-09T03:53:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/601"
  },
  {
    "number": 600,
    "title": "[UT] fix tf ut",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T15:38:07Z",
    "closed_at": "2023-10-31T15:48:50Z",
    "merged_at": "2023-10-31T15:48:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/600"
  },
  {
    "number": 599,
    "title": "Fix workflow generate.py import issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T15:25:48Z",
    "closed_at": "2023-11-01T01:02:30Z",
    "merged_at": "2023-11-01T01:02:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/599"
  },
  {
    "number": 598,
    "title": "[Doc] Typos fix in mulitple docs files",
    "user": "ghost",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T15:06:15Z",
    "closed_at": "2023-11-01T13:13:25Z",
    "merged_at": "2023-11-01T13:13:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/598"
  },
  {
    "number": 597,
    "title": "[NeuralChat] Fix latest fastapi version issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T14:44:44Z",
    "closed_at": "2023-11-02T22:19:41Z",
    "merged_at": "2023-11-02T22:19:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/597"
  },
  {
    "number": 596,
    "title": "[Doc] change the structure of llm runtime readme",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T12:11:23Z",
    "closed_at": "2023-11-01T07:20:30Z",
    "merged_at": "2023-11-01T07:20:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/596"
  },
  {
    "number": 595,
    "title": "[LLM] Adapt to transformers 4.34",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T08:53:22Z",
    "closed_at": "2023-10-31T11:58:19Z",
    "merged_at": "2023-10-31T11:58:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/595"
  },
  {
    "number": 594,
    "title": "[NeuralChat] Record user requests into DB",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T07:52:07Z",
    "closed_at": "2023-11-03T06:10:25Z",
    "merged_at": "2023-11-03T06:10:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/594"
  },
  {
    "number": 593,
    "title": "[NeuralChat] update path in audio utils",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T07:34:28Z",
    "closed_at": "2023-11-01T01:35:26Z",
    "merged_at": "2023-11-01T01:35:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/593"
  },
  {
    "number": 592,
    "title": "[Infra] fix chatbot workflow name",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T06:11:24Z",
    "closed_at": "2023-10-31T14:25:31Z",
    "merged_at": "2023-10-31T14:25:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/592"
  },
  {
    "number": 591,
    "title": "Fix frontend directory symbolic Link issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T05:32:40Z",
    "closed_at": "2023-10-31T11:36:28Z",
    "merged_at": "2023-10-31T11:36:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/591"
  },
  {
    "number": 590,
    "title": "[ADDED]: Auto comment feature",
    "user": "Killer2OP",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T03:13:13Z",
    "closed_at": "2023-11-01T03:30:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/590"
  },
  {
    "number": 589,
    "title": "Use shared disk",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-31T03:05:47Z",
    "closed_at": "2023-10-31T03:39:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/589"
  },
  {
    "number": 588,
    "title": "Added license section",
    "user": "dalvishruti14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-30T16:18:54Z",
    "closed_at": "2023-11-06T01:25:04Z",
    "merged_at": "2023-11-06T01:25:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/588"
  },
  {
    "number": 585,
    "title": "[NeuralChat] add finetuning or rag notebook.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-30T15:05:26Z",
    "closed_at": "2023-10-31T11:38:32Z",
    "merged_at": "2023-10-31T11:38:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/585"
  },
  {
    "number": 584,
    "title": "[NeuralChat] update notebook on cpu, hpu, xpu, nv_a100",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-30T07:51:34Z",
    "closed_at": "2023-11-24T01:03:27Z",
    "merged_at": "2023-11-24T01:03:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/584"
  },
  {
    "number": 583,
    "title": "[LLM Runtime] Unify KV_cache and Support Batch-dim Process in Beam Search",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-30T07:34:54Z",
    "closed_at": "2023-11-10T02:11:47Z",
    "merged_at": "2023-11-10T02:11:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/583"
  },
  {
    "number": 582,
    "title": "[Infra] Split UT cases into CI and nightly for pre-CI speedup",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-30T05:37:24Z",
    "closed_at": "2023-11-05T13:54:32Z",
    "merged_at": "2023-11-05T13:54:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/582"
  },
  {
    "number": 581,
    "title": "[Example] Add WOQ to code-generation example",
    "user": "violetch24",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-30T04:15:30Z",
    "closed_at": "2023-11-14T02:10:04Z",
    "merged_at": "2023-11-14T02:10:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/581"
  },
  {
    "number": 580,
    "title": "[LLM Runtime] Streaming-LLM based on shift RoPE",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-30T01:25:24Z",
    "closed_at": "2023-10-31T03:39:21Z",
    "merged_at": "2023-10-31T03:39:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/580"
  },
  {
    "number": 579,
    "title": "added .gitpod.yml  for config",
    "user": "PentesterPriyanshu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-29T19:48:22Z",
    "closed_at": "2023-11-14T10:00:00Z",
    "merged_at": "2023-11-14T10:00:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/579"
  },
  {
    "number": 578,
    "title": "fix-minor-typo",
    "user": "Ayaan49",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-29T14:04:46Z",
    "closed_at": "2023-10-30T09:44:06Z",
    "merged_at": "2023-10-30T09:44:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/578"
  },
  {
    "number": 577,
    "title": "Updated Contributors Section in readme.md",
    "user": "mohitd404",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-29T12:32:44Z",
    "closed_at": "2023-10-29T23:31:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/577"
  },
  {
    "number": 576,
    "title": "[LLM] support dpo on habana/gaudi.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-29T12:26:31Z",
    "closed_at": "2023-11-03T15:48:15Z",
    "merged_at": "2023-11-03T15:48:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/576"
  },
  {
    "number": 574,
    "title": "Update README.md - fix two grammar mistakes.",
    "user": "sonijogesh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-28T13:21:49Z",
    "closed_at": "2023-10-29T13:10:46Z",
    "merged_at": "2023-10-29T13:10:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/574"
  },
  {
    "number": 573,
    "title": "Optimize the structure of NeuralChat example directories",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T12:25:25Z",
    "closed_at": "2023-10-27T13:14:31Z",
    "merged_at": "2023-10-27T13:14:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/573"
  },
  {
    "number": 572,
    "title": "migrate neuralchat UT to azure",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T09:38:25Z",
    "closed_at": "2023-10-27T12:19:16Z",
    "merged_at": "2023-10-27T12:19:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/572"
  },
  {
    "number": 571,
    "title": "support top 10 embedding models on the huggingface leaderboard",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T09:09:35Z",
    "closed_at": "2023-10-29T13:09:41Z",
    "merged_at": "2023-10-29T13:09:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/571"
  },
  {
    "number": 570,
    "title": "Fix README frontend link issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T08:15:11Z",
    "closed_at": "2023-10-27T09:22:58Z",
    "merged_at": "2023-10-27T09:22:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/570"
  },
  {
    "number": 569,
    "title": "[CI]fix ut model name",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T07:20:06Z",
    "closed_at": "2023-10-27T09:15:55Z",
    "merged_at": "2023-10-27T09:15:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/569"
  },
  {
    "number": 568,
    "title": "Enable example_inputs dtype dict as default",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T07:18:46Z",
    "closed_at": "2023-10-30T12:55:43Z",
    "merged_at": "2023-10-30T12:55:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/568"
  },
  {
    "number": 567,
    "title": "[LLM Runtime] enable MHA fusion for gptneox&dolly&starcoder&llama2-70b",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T07:13:30Z",
    "closed_at": "2023-11-01T03:45:31Z",
    "merged_at": "2023-11-01T03:45:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/567"
  },
  {
    "number": 566,
    "title": "clean CI code",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T03:50:18Z",
    "closed_at": "2023-10-27T03:53:50Z",
    "merged_at": "2023-10-27T03:53:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/566"
  },
  {
    "number": 565,
    "title": "[LLM Runtime] integrate AVX_VNNI",
    "user": "yuchengliu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T03:17:53Z",
    "closed_at": "2023-11-09T03:37:18Z",
    "merged_at": "2023-11-09T03:37:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/565"
  },
  {
    "number": 564,
    "title": "[NeuralChat] add optimized SadTalker to Video plugin in NeuralChat",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T02:17:55Z",
    "closed_at": "2023-11-22T00:58:41Z",
    "merged_at": "2023-11-22T00:58:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/564"
  },
  {
    "number": 563,
    "title": "fix qbits backend get wrong workspace malloc size when using Release compile Flag",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-27T00:54:12Z",
    "closed_at": "2023-10-27T05:06:49Z",
    "merged_at": "2023-10-27T05:06:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/563"
  },
  {
    "number": 562,
    "title": "Fix issues for cloning voice feature",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T15:55:37Z",
    "closed_at": "2023-10-30T05:34:48Z",
    "merged_at": "2023-10-30T05:34:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/562"
  },
  {
    "number": 561,
    "title": "Fix example wrong name and past_kv shape",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T11:27:05Z",
    "closed_at": "2023-10-31T09:40:20Z",
    "merged_at": "2023-10-31T09:40:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/561"
  },
  {
    "number": 560,
    "title": "Typo Fix",
    "user": "sudhanshu-77",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T09:29:40Z",
    "closed_at": "2023-10-26T12:49:08Z",
    "merged_at": "2023-10-26T12:49:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/560"
  },
  {
    "number": 559,
    "title": "[NeuralChat] Update talkingbot backend and frontend",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T09:27:40Z",
    "closed_at": "2023-10-31T11:35:13Z",
    "merged_at": "2023-10-31T11:35:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/559"
  },
  {
    "number": 558,
    "title": "[NeuralChat] Supported dockerfile for NV GPU.",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T07:39:59Z",
    "closed_at": "2023-10-30T05:35:55Z",
    "merged_at": "2023-10-30T05:35:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/558"
  },
  {
    "number": 557,
    "title": "Update README.md",
    "user": "RS-labhub",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T07:16:29Z",
    "closed_at": "2023-10-27T07:13:39Z",
    "merged_at": "2023-10-27T07:13:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/557"
  },
  {
    "number": 556,
    "title": "Update README.md",
    "user": "RS-labhub",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T07:05:50Z",
    "closed_at": "2023-10-26T07:07:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/556"
  },
  {
    "number": 555,
    "title": "docs/README.md",
    "user": "RS-labhub",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T05:00:45Z",
    "closed_at": "2023-10-26T07:11:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/555"
  },
  {
    "number": 554,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T03:29:52Z",
    "closed_at": "2023-10-26T04:57:15Z",
    "merged_at": "2023-10-26T04:57:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/554"
  },
  {
    "number": 553,
    "title": "remove_llm_language_modeling",
    "user": "xin3he",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T02:56:45Z",
    "closed_at": "2023-10-26T03:33:13Z",
    "merged_at": "2023-10-26T03:33:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/553"
  },
  {
    "number": 552,
    "title": "[LLM Runtime] Enable Mistral-7b",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-26T02:17:11Z",
    "closed_at": "2023-10-26T04:58:41Z",
    "merged_at": "2023-10-26T04:58:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/552"
  },
  {
    "number": 551,
    "title": "Updated documentation for better user experience",
    "user": "Arcturus22",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T14:08:53Z",
    "closed_at": "2023-10-26T04:58:08Z",
    "merged_at": "2023-10-26T04:58:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/551"
  },
  {
    "number": 550,
    "title": "[Documentation] refine Acknowledgements",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T09:55:56Z",
    "closed_at": "2023-10-26T13:12:10Z",
    "merged_at": "2023-10-26T13:12:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/550"
  },
  {
    "number": 549,
    "title": "run inference with habana 1.12 and deepspeed",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T09:10:31Z",
    "closed_at": "2023-10-26T05:59:42Z",
    "merged_at": "2023-10-26T05:59:42Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/549"
  },
  {
    "number": 548,
    "title": "[LLM Runtime] Enable interactive mode of python api",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T08:06:07Z",
    "closed_at": "2023-10-27T05:16:38Z",
    "merged_at": "2023-10-27T05:16:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/548"
  },
  {
    "number": 547,
    "title": "use multi nodes on same server",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T04:03:53Z",
    "closed_at": "2023-10-30T01:54:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/547"
  },
  {
    "number": 546,
    "title": "Added code of conduct file.",
    "user": "Kris248",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T04:00:50Z",
    "closed_at": "2023-10-25T08:44:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/546"
  },
  {
    "number": 545,
    "title": "[NeuralChat] Update test_cache.py",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T03:07:15Z",
    "closed_at": "2023-10-25T05:21:37Z",
    "merged_at": "2023-10-25T05:21:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/545"
  },
  {
    "number": 544,
    "title": "[LLM Runtime] initlize the qwen",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T02:05:17Z",
    "closed_at": "2023-11-03T03:37:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/544"
  },
  {
    "number": 543,
    "title": "[NeuralChat] Support optimization for server mode and add ut cases",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-25T01:36:04Z",
    "closed_at": "2023-11-09T02:33:12Z",
    "merged_at": "2023-11-09T02:33:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/543"
  },
  {
    "number": 542,
    "title": "Add tokenizer name support for NeuralChat textbot",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T14:45:35Z",
    "closed_at": "2023-10-24T14:52:36Z",
    "merged_at": "2023-10-24T14:52:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/542"
  },
  {
    "number": 541,
    "title": "support NeuralChat textbot ipex int8",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T14:05:17Z",
    "closed_at": "2023-10-24T14:21:33Z",
    "merged_at": "2023-10-24T14:21:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/541"
  },
  {
    "number": 540,
    "title": "fix minor issue",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T12:04:01Z",
    "closed_at": "2023-10-24T12:13:35Z",
    "merged_at": "2023-10-24T12:13:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/540"
  },
  {
    "number": 539,
    "title": "optimized augmentation.py",
    "user": "bhargavshirin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T11:35:50Z",
    "closed_at": "2023-10-29T19:43:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/539"
  },
  {
    "number": 538,
    "title": "[NeuralChat] Add test_mpt_trace.py",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T09:45:59Z",
    "closed_at": "2023-10-30T01:35:04Z",
    "merged_at": "2023-10-30T01:35:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/538"
  },
  {
    "number": 537,
    "title": "Support smooth quant alpha auto tune",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T09:44:52Z",
    "closed_at": "2023-10-24T11:51:23Z",
    "merged_at": "2023-10-24T11:51:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/537"
  },
  {
    "number": 536,
    "title": "[Example] stable diffusion modified run qat int8",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T08:08:57Z",
    "closed_at": "2023-11-02T08:04:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/536"
  },
  {
    "number": 535,
    "title": "Habana 1.12",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T07:42:21Z",
    "closed_at": "2023-10-26T01:46:49Z",
    "merged_at": "2023-10-26T01:46:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/535"
  },
  {
    "number": 534,
    "title": "[Example] add generation inference script for deepspeed",
    "user": "dc3671",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T06:51:20Z",
    "closed_at": "2023-11-08T06:59:09Z",
    "merged_at": "2023-11-08T06:59:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/534"
  },
  {
    "number": 533,
    "title": "[Documentation] upload streaming llm video",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T04:25:44Z",
    "closed_at": "2023-10-24T06:54:37Z",
    "merged_at": "2023-10-24T06:54:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/533"
  },
  {
    "number": 532,
    "title": "Enable hpu again",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-24T02:16:53Z",
    "closed_at": "2023-10-24T02:17:25Z",
    "merged_at": "2023-10-24T02:17:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/532"
  },
  {
    "number": 530,
    "title": "optimized test_data_augmentation.py",
    "user": "bhargavshirin",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-23T16:44:13Z",
    "closed_at": "2023-10-24T11:31:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/530"
  },
  {
    "number": 529,
    "title": "added a missing CODE OF CONDUCT file ",
    "user": "romitp4l",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-23T14:35:46Z",
    "closed_at": "2023-10-23T16:51:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/529"
  },
  {
    "number": 528,
    "title": "[LLM] Support ipex 2.1 optimize_transformers feature",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-23T10:51:08Z",
    "closed_at": "2023-11-16T04:16:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/528"
  },
  {
    "number": 526,
    "title": "Update text/code generation example readme to use IPEX 2.1",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-23T06:02:59Z",
    "closed_at": "2023-10-23T07:53:37Z",
    "merged_at": "2023-10-23T07:53:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/526"
  },
  {
    "number": 525,
    "title": "[LLM Runtime] update python api readme",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-23T05:10:40Z",
    "closed_at": "2023-10-23T08:13:51Z",
    "merged_at": "2023-10-23T08:13:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/525"
  },
  {
    "number": 524,
    "title": "fix main branch",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-23T02:45:35Z",
    "closed_at": "2023-10-23T03:53:11Z",
    "merged_at": "2023-10-23T03:53:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/524"
  },
  {
    "number": 523,
    "title": "update python api readme",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-23T02:24:30Z",
    "closed_at": "2023-10-23T03:48:19Z",
    "merged_at": "2023-10-23T03:48:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/523"
  },
  {
    "number": 522,
    "title": "fixed typos",
    "user": "Smoothieewastaken",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-22T10:09:39Z",
    "closed_at": "2023-10-23T00:20:12Z",
    "merged_at": "2023-10-23T00:20:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/522"
  },
  {
    "number": 521,
    "title": "reduce unnecessary tests",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-22T09:56:24Z",
    "closed_at": "2023-10-23T07:52:58Z",
    "merged_at": "2023-10-23T07:52:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/521"
  },
  {
    "number": 520,
    "title": "docs: fix typos in question answering of pytorch",
    "user": "shresthasurav",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-22T08:20:35Z",
    "closed_at": "2023-10-22T23:58:52Z",
    "merged_at": "2023-10-22T23:58:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/520"
  },
  {
    "number": 519,
    "title": "Update README.md -fixed two small grammar bugs in README.md",
    "user": "ayushrakesh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-22T05:06:36Z",
    "closed_at": "2023-10-22T23:58:08Z",
    "merged_at": "2023-10-22T23:58:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/519"
  },
  {
    "number": 518,
    "title": "Update README.md",
    "user": "Shivam250702",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-22T05:06:16Z",
    "closed_at": "2023-11-01T20:38:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/518"
  },
  {
    "number": 517,
    "title": "docs: Corrected all the grammatical errors and typos in README.md",
    "user": "alienishi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-22T03:28:36Z",
    "closed_at": "2023-10-23T00:21:04Z",
    "merged_at": "2023-10-23T00:21:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/517"
  },
  {
    "number": 516,
    "title": "Fix typo in README.md",
    "user": "eltociear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-21T10:04:17Z",
    "closed_at": "2023-10-21T10:34:30Z",
    "merged_at": "2023-10-21T10:34:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/516"
  },
  {
    "number": 515,
    "title": "Update README.md for fast token issue",
    "user": "louie-tsai",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T23:21:08Z",
    "closed_at": "2023-10-21T10:07:54Z",
    "merged_at": "2023-10-21T10:07:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/515"
  },
  {
    "number": 514,
    "title": "[NeuralChat] Add askdoc retrieval api & example",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T08:30:41Z",
    "closed_at": "2023-10-27T02:07:45Z",
    "merged_at": "2023-10-27T02:07:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/514"
  },
  {
    "number": 513,
    "title": "[Optimization] Text-generation support qwen",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T05:59:51Z",
    "closed_at": "2023-10-23T14:50:10Z",
    "merged_at": "2023-10-23T14:50:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/513"
  },
  {
    "number": 512,
    "title": "[Test] improve tf distill ut effiecnty",
    "user": "n1ck-guo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T05:40:02Z",
    "closed_at": "2023-10-23T13:43:34Z",
    "merged_at": "2023-10-23T13:43:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/512"
  },
  {
    "number": 511,
    "title": "improve Avx2 ",
    "user": "yuchengliu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-20T03:54:08Z",
    "closed_at": "2023-10-21T14:08:23Z",
    "merged_at": "2023-10-21T14:08:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/511"
  },
  {
    "number": 510,
    "title": "Fix ChatGLM2 model loading issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T15:03:54Z",
    "closed_at": "2023-10-19T23:17:43Z",
    "merged_at": "2023-10-19T23:17:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/510"
  },
  {
    "number": 509,
    "title": "Remove OneDNN graph env setting for BF16 inference",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T14:12:42Z",
    "closed_at": "2023-10-20T00:23:22Z",
    "merged_at": "2023-10-20T00:23:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/509"
  },
  {
    "number": 508,
    "title": "Fix starcoder issues for IPEX int8 and Weight Only int4",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T13:59:42Z",
    "closed_at": "2023-10-23T23:13:12Z",
    "merged_at": "2023-10-23T23:13:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/508"
  },
  {
    "number": 507,
    "title": "Minor fix",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T13:44:18Z",
    "closed_at": "2023-10-19T13:50:22Z",
    "merged_at": "2023-10-19T13:50:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/507"
  },
  {
    "number": 506,
    "title": "[CI] Use shared disk",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T13:15:41Z",
    "closed_at": "2023-10-31T01:45:11Z",
    "merged_at": "2023-10-31T01:45:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/506"
  },
  {
    "number": 505,
    "title": "Restrain ipex version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T11:44:00Z",
    "closed_at": "2023-10-20T03:09:48Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/505"
  },
  {
    "number": 504,
    "title": "update python api readme",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T10:20:19Z",
    "closed_at": "2023-10-20T09:13:05Z",
    "merged_at": "2023-10-20T09:13:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/504"
  },
  {
    "number": 503,
    "title": "Reduce UT time",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T08:47:43Z",
    "closed_at": "2023-10-24T00:20:02Z",
    "merged_at": "2023-10-24T00:20:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/503"
  },
  {
    "number": 502,
    "title": "[NeuralChat] Add neuralchat UT for cache and memory",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T08:21:13Z",
    "closed_at": "2023-10-24T02:50:52Z",
    "merged_at": "2023-10-24T02:50:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/502"
  },
  {
    "number": 501,
    "title": "[RUNTIME] Enabing streaming llm for Runtime",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T07:49:04Z",
    "closed_at": "2023-10-19T09:17:54Z",
    "merged_at": "2023-10-19T09:17:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/501"
  },
  {
    "number": 500,
    "title": "use INC 2.3.1",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T05:57:19Z",
    "closed_at": "2023-10-19T08:45:35Z",
    "merged_at": "2023-10-19T08:45:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/500"
  },
  {
    "number": 499,
    "title": "reduce ut time consumption",
    "user": "xin3he",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T05:33:00Z",
    "closed_at": "2023-10-20T08:18:19Z",
    "merged_at": "2023-10-20T08:18:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/499"
  },
  {
    "number": 498,
    "title": "Reduce the UT evaluation time",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T04:11:32Z",
    "closed_at": "2023-10-19T13:22:46Z",
    "merged_at": "2023-10-19T13:22:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/498"
  },
  {
    "number": 497,
    "title": "[LLM Runtime] Baichuan FFN & MHA support",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-19T02:36:06Z",
    "closed_at": "2023-10-23T09:23:07Z",
    "merged_at": "2023-10-23T09:23:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/497"
  },
  {
    "number": 496,
    "title": "Add docker setup session for neuralchat finetuning sample",
    "user": "louie-tsai",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T19:48:53Z",
    "closed_at": "2023-10-20T09:40:11Z",
    "merged_at": "2023-10-20T09:40:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/496"
  },
  {
    "number": 495,
    "title": "restrain oneccl version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T14:11:20Z",
    "closed_at": "2023-10-19T04:12:08Z",
    "merged_at": "2023-10-19T04:12:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/495"
  },
  {
    "number": 494,
    "title": "Fix NeuralChat starcoder mha fusion env issue",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T11:48:18Z",
    "closed_at": "2023-10-19T01:40:24Z",
    "merged_at": "2023-10-19T01:40:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/494"
  },
  {
    "number": 493,
    "title": "support Avx2",
    "user": "yuchengliu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T09:02:32Z",
    "closed_at": "2023-10-20T03:14:12Z",
    "merged_at": "2023-10-20T03:14:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/493"
  },
  {
    "number": 492,
    "title": "[Infra] add dependency package",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T03:34:12Z",
    "closed_at": "2023-11-26T07:21:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/492"
  },
  {
    "number": 491,
    "title": "Update release data",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-18T02:35:03Z",
    "closed_at": "2023-10-19T03:42:32Z",
    "merged_at": "2023-10-19T03:42:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/491"
  },
  {
    "number": 490,
    "title": "try workflow run conclusion",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-17T12:54:51Z",
    "closed_at": "2023-10-18T03:12:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/490"
  },
  {
    "number": 489,
    "title": "Enable hpu again",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-17T09:59:09Z",
    "closed_at": "2023-10-24T04:01:34Z",
    "merged_at": "2023-10-24T04:01:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/489"
  },
  {
    "number": 488,
    "title": "[LLM Runtime] ChatGLM MHA Enabling",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-17T08:15:31Z",
    "closed_at": "2023-11-03T03:37:45Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/488"
  },
  {
    "number": 487,
    "title": "Fixed setuptools version limitation for build",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-17T07:29:17Z",
    "closed_at": "2023-10-17T22:40:30Z",
    "merged_at": "2023-10-17T22:40:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/487"
  },
  {
    "number": 486,
    "title": "NeuralChat support IPEX int8 model",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-17T04:20:56Z",
    "closed_at": "2023-10-17T22:44:33Z",
    "merged_at": "2023-10-17T22:44:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/486"
  },
  {
    "number": 485,
    "title": "[NeuralChat] Refine neuralchat docker",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-17T02:31:48Z",
    "closed_at": "2023-10-23T08:24:55Z",
    "merged_at": "2023-10-23T08:24:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/485"
  },
  {
    "number": 484,
    "title": "update requirement",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-17T01:45:43Z",
    "closed_at": "2023-10-17T03:42:09Z",
    "merged_at": "2023-10-17T03:42:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/484"
  },
  {
    "number": 483,
    "title": "Fix main branch unit test fail issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-17T01:42:48Z",
    "closed_at": "2023-10-17T02:08:24Z",
    "merged_at": "2023-10-17T02:08:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/483"
  },
  {
    "number": 482,
    "title": "add missing packages for neural_chat example",
    "user": "louie-tsai",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-17T01:07:19Z",
    "closed_at": "2023-10-17T05:57:04Z",
    "merged_at": "2023-10-17T05:57:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/482"
  },
  {
    "number": 481,
    "title": "[LLM Runtime] Implement Streaming-LLM",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-16T16:58:14Z",
    "closed_at": "2023-11-14T00:46:07Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/481"
  },
  {
    "number": 479,
    "title": "Add mistralai model support for text-generation",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-16T09:45:13Z",
    "closed_at": "2023-10-25T09:10:08Z",
    "merged_at": "2023-10-25T09:10:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/479"
  },
  {
    "number": 478,
    "title": "[NeuralChat] Integrate photoai backend into restful API",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-16T08:58:48Z",
    "closed_at": "2023-10-30T06:01:25Z",
    "merged_at": "2023-10-30T06:01:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/478"
  },
  {
    "number": 477,
    "title": "Fix the msvc compile issue of the Baichuan",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-16T08:20:56Z",
    "closed_at": "2023-10-17T00:27:24Z",
    "merged_at": "2023-10-17T00:27:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/477"
  },
  {
    "number": 476,
    "title": "fix post process with topk topp of python api",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-16T08:14:54Z",
    "closed_at": "2023-10-17T08:16:08Z",
    "merged_at": "2023-10-17T08:16:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/476"
  },
  {
    "number": 475,
    "title": "fix UT test_model",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-16T07:53:49Z",
    "closed_at": "2023-10-16T14:24:32Z",
    "merged_at": "2023-10-16T14:24:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/475"
  },
  {
    "number": 474,
    "title": "only for test.",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-16T03:13:14Z",
    "closed_at": "2023-10-17T08:34:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/474"
  },
  {
    "number": 473,
    "title": "utils: add LLM carbon emission calculator",
    "user": "thuang6",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-16T02:12:06Z",
    "closed_at": "2023-10-16T13:57:58Z",
    "merged_at": "2023-10-16T13:57:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/473"
  },
  {
    "number": 472,
    "title": "[NeuralChat] Add a notebook for text chatbot with caching ",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-16T01:47:12Z",
    "closed_at": "2023-11-08T05:30:10Z",
    "merged_at": "2023-11-08T05:30:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/472"
  },
  {
    "number": 471,
    "title": "refine perf table",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-16T01:09:22Z",
    "closed_at": "2023-10-24T04:03:16Z",
    "merged_at": "2023-10-24T04:03:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/471"
  },
  {
    "number": 470,
    "title": "update release data",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-15T08:27:32Z",
    "closed_at": "2023-10-16T14:17:46Z",
    "merged_at": "2023-10-16T14:17:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/470"
  },
  {
    "number": 469,
    "title": "[Infra] combine docker codegen and chatbot",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-15T06:28:39Z",
    "closed_at": "2023-11-07T08:44:21Z",
    "merged_at": "2023-11-07T08:44:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/469"
  },
  {
    "number": 468,
    "title": "[NeuralChat] Optimize the code for the enterprise chatbot",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-14T13:23:08Z",
    "closed_at": "2023-11-27T05:51:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/468"
  },
  {
    "number": 467,
    "title": "unified chat frontend",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T08:49:50Z",
    "closed_at": "2023-10-25T08:47:42Z",
    "merged_at": "2023-10-25T08:47:42Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/467"
  },
  {
    "number": 466,
    "title": "add neuralchat ut for audio util",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T08:17:26Z",
    "closed_at": "2023-10-20T05:47:46Z",
    "merged_at": "2023-10-20T05:47:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/466"
  },
  {
    "number": 465,
    "title": "[CPP Graph] Opt qbits dequant",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T08:04:45Z",
    "closed_at": "2023-10-19T05:22:38Z",
    "merged_at": "2023-10-19T05:22:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/465"
  },
  {
    "number": 464,
    "title": "Update code-generation benchmark info",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T06:53:02Z",
    "closed_at": "2023-10-13T07:05:56Z",
    "merged_at": "2023-10-13T07:05:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/464"
  },
  {
    "number": 463,
    "title": "read special token id from tokenizer",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T06:52:01Z",
    "closed_at": "2023-10-14T22:53:20Z",
    "merged_at": "2023-10-14T22:53:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/463"
  },
  {
    "number": 462,
    "title": "[NeuralChat] Enable finetune for Qwen-7b-chat on CPU (WIP)",
    "user": "huiyan2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T05:21:10Z",
    "closed_at": "2023-11-30T03:55:05Z",
    "merged_at": "2023-11-30T03:55:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/462"
  },
  {
    "number": 461,
    "title": "Set limitation for model mapping",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T03:53:54Z",
    "closed_at": "2023-10-31T02:12:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/461"
  },
  {
    "number": 460,
    "title": "Upgrade qbits backend",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T03:21:46Z",
    "closed_at": "2023-10-13T08:23:18Z",
    "merged_at": "2023-10-13T08:23:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/460"
  },
  {
    "number": 458,
    "title": "enable hpu in new gaudi2 node",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T02:32:03Z",
    "closed_at": "2023-10-25T03:51:36Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/458"
  },
  {
    "number": 457,
    "title": "[NeuralChat] add UT and simplify logics of ASR/TTS",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T01:40:31Z",
    "closed_at": "2023-11-03T01:31:27Z",
    "merged_at": "2023-11-03T01:31:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/457"
  },
  {
    "number": 456,
    "title": "GitHub Action Workflows speedup",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-13T01:20:28Z",
    "closed_at": "2023-10-13T12:01:11Z",
    "merged_at": "2023-10-13T12:01:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/456"
  },
  {
    "number": 455,
    "title": "add neuralchat finetune notebook.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-12T15:26:46Z",
    "closed_at": "2023-10-13T06:39:15Z",
    "merged_at": "2023-10-13T06:39:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/455"
  },
  {
    "number": 454,
    "title": "fix AutoModelForCausalLM mapping",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-12T13:40:21Z",
    "closed_at": "2023-10-12T13:47:14Z",
    "merged_at": "2023-10-12T13:47:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/454"
  },
  {
    "number": 453,
    "title": "[CPP Graph] MPT MHA support",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-12T11:39:53Z",
    "closed_at": "2023-10-13T05:39:00Z",
    "merged_at": "2023-10-13T05:39:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/453"
  },
  {
    "number": 452,
    "title": "Add starcoder evaluation support when finetuing",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-12T09:35:18Z",
    "closed_at": "2023-10-13T03:21:21Z",
    "merged_at": "2023-10-13T03:21:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/452"
  },
  {
    "number": 451,
    "title": "Migrate chatbot devcloud",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-12T08:29:18Z",
    "closed_at": "2023-10-31T09:12:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/451"
  },
  {
    "number": 450,
    "title": "Add INT4 ONNX whisper example ",
    "user": "mengniwang95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-12T07:04:19Z",
    "closed_at": "2023-10-12T07:37:40Z",
    "merged_at": "2023-10-12T07:37:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/450"
  },
  {
    "number": 449,
    "title": "[Cpp Graph] Beam Search Pybind (model archs: gptj and gptneox)",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-12T06:10:34Z",
    "closed_at": "2023-10-17T01:29:18Z",
    "merged_at": "2023-10-17T01:29:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/449"
  },
  {
    "number": 448,
    "title": "Support starcoder MHA fusion",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-12T05:03:09Z",
    "closed_at": "2023-10-12T10:15:10Z",
    "merged_at": "2023-10-12T10:15:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/448"
  },
  {
    "number": 447,
    "title": "Loading finetuned models",
    "user": "srinarayan-srikanthan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T14:02:49Z",
    "closed_at": "2023-10-12T03:21:00Z",
    "merged_at": "2023-10-12T03:21:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/447"
  },
  {
    "number": 446,
    "title": "update compilere",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T11:30:39Z",
    "closed_at": "2023-10-12T01:53:35Z",
    "merged_at": "2023-10-12T01:53:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/446"
  },
  {
    "number": 445,
    "title": "Refine notebook and fix restful api issues",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T10:46:18Z",
    "closed_at": "2023-10-15T14:19:46Z",
    "merged_at": "2023-10-15T14:19:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/445"
  },
  {
    "number": 444,
    "title": "add docker for code-generation",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T10:07:38Z",
    "closed_at": "2023-10-12T06:21:30Z",
    "merged_at": "2023-10-12T06:21:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/444"
  },
  {
    "number": 443,
    "title": "[CPP Graph] Fused Attention Doc",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T09:14:11Z",
    "closed_at": "2023-10-24T01:22:53Z",
    "merged_at": "2023-10-24T01:22:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/443"
  },
  {
    "number": 442,
    "title": "[LLM] Support QLoRA on CPU device",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T08:50:14Z",
    "closed_at": "2023-11-21T06:08:00Z",
    "merged_at": "2023-11-21T06:08:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/442"
  },
  {
    "number": 441,
    "title": "[LLM Runtime] Support weight only on intel gpu",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T07:48:49Z",
    "closed_at": "2024-01-22T02:49:57Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/441"
  },
  {
    "number": 440,
    "title": "Add audio sample inference code for ONNX whisper model",
    "user": "mengniwang95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T06:00:17Z",
    "closed_at": "2023-10-12T06:47:44Z",
    "merged_at": "2023-10-12T06:47:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/440"
  },
  {
    "number": 439,
    "title": "Update CODEOWNERS",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T05:23:34Z",
    "closed_at": "2023-10-11T11:49:37Z",
    "merged_at": "2023-10-11T11:49:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/439"
  },
  {
    "number": 438,
    "title": "update requirement for docker",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T03:43:06Z",
    "closed_at": "2023-10-11T05:26:39Z",
    "merged_at": "2023-10-11T05:26:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/438"
  },
  {
    "number": 437,
    "title": "Set the fixed transformer version",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T02:19:11Z",
    "closed_at": "2023-10-11T08:18:21Z",
    "merged_at": "2023-10-11T08:18:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/437"
  },
  {
    "number": 436,
    "title": "Add NER plugin to neural chat",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-11T01:52:47Z",
    "closed_at": "2023-10-17T22:41:47Z",
    "merged_at": "2023-10-17T22:41:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/436"
  },
  {
    "number": 435,
    "title": "[CPP Graph] ChatGLM2 MHA support",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-10T12:11:08Z",
    "closed_at": "2023-10-11T03:26:40Z",
    "merged_at": "2023-10-11T03:26:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/435"
  },
  {
    "number": 434,
    "title": "update tests requirement of pytorch",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-10T09:10:56Z",
    "closed_at": "2023-10-10T09:12:17Z",
    "merged_at": "2023-10-10T09:12:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/434"
  },
  {
    "number": 433,
    "title": "update jblas",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-10T06:18:27Z",
    "closed_at": "2023-10-11T04:34:42Z",
    "merged_at": "2023-10-11T04:34:42Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/433"
  },
  {
    "number": 432,
    "title": "Enable Qwen-7B-Chat",
    "user": "huiyan2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-10T04:27:58Z",
    "closed_at": "2023-10-13T03:31:47Z",
    "merged_at": "2023-10-13T03:31:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/432"
  },
  {
    "number": 431,
    "title": "Refined multi-node finetuning notebook on spr",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-10T03:41:17Z",
    "closed_at": "2023-10-10T06:58:56Z",
    "merged_at": "2023-10-10T06:58:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/431"
  },
  {
    "number": 430,
    "title": "update post process with num_beams and do_sample",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-10T02:35:59Z",
    "closed_at": "2023-10-11T04:31:10Z",
    "merged_at": "2023-10-11T04:31:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/430"
  },
  {
    "number": 429,
    "title": "[pre-commit.ci] pre-commit autoupdate",
    "user": "pre-commit-ci[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-10-09T21:01:12Z",
    "closed_at": "2023-10-13T03:31:30Z",
    "merged_at": "2023-10-13T03:31:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/429"
  },
  {
    "number": 428,
    "title": "Update README.md; finetune_model returns NONE, and the peft model is ",
    "user": "qgao007",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-09T19:50:40Z",
    "closed_at": "2023-10-10T03:10:17Z",
    "merged_at": "2023-10-10T03:10:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/428"
  },
  {
    "number": 427,
    "title": "fix chatbot docker build repo input",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-09T14:38:42Z",
    "closed_at": "2023-10-10T01:37:04Z",
    "merged_at": "2023-10-10T01:37:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/427"
  },
  {
    "number": 426,
    "title": "upgrade torch to 2.0.1",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-09T11:28:44Z",
    "closed_at": "2023-10-09T13:24:32Z",
    "merged_at": "2023-10-09T13:24:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/426"
  },
  {
    "number": 425,
    "title": "Add NeuralChat UT",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-09T09:31:02Z",
    "closed_at": "2023-10-15T06:07:07Z",
    "merged_at": "2023-10-15T06:07:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/425"
  },
  {
    "number": 424,
    "title": "gelu support",
    "user": "yuchengliu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-09T06:22:00Z",
    "closed_at": "2023-10-16T03:31:41Z",
    "merged_at": "2023-10-16T03:31:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/424"
  },
  {
    "number": 423,
    "title": "enable whisper for ITREX",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-09T06:01:39Z",
    "closed_at": "2023-10-09T06:04:40Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/423"
  },
  {
    "number": 422,
    "title": "[CPP Graph] Falcon MHA support",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-09T05:31:14Z",
    "closed_at": "2023-10-09T08:13:11Z",
    "merged_at": "2023-10-09T08:13:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/422"
  },
  {
    "number": 421,
    "title": "Restrain langchain version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-09T05:15:55Z",
    "closed_at": "2023-10-09T06:50:28Z",
    "merged_at": "2023-10-09T06:50:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/421"
  },
  {
    "number": 420,
    "title": "[LLM] Fixed awq issue for dataloader",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-09T03:42:44Z",
    "closed_at": "2023-11-26T07:20:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/420"
  },
  {
    "number": 419,
    "title": "add sidebyside ui",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-09T03:05:05Z",
    "closed_at": "2023-10-13T06:07:22Z",
    "merged_at": "2023-10-13T06:07:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/419"
  },
  {
    "number": 418,
    "title": "add readme of doc",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-08T07:53:08Z",
    "closed_at": "2023-10-11T03:26:55Z",
    "merged_at": "2023-10-11T03:26:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/418"
  },
  {
    "number": 417,
    "title": "fix init issue of langchain chroma ",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-08T03:40:30Z",
    "closed_at": "2023-10-09T09:14:54Z",
    "merged_at": "2023-10-09T09:14:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/417"
  },
  {
    "number": 416,
    "title": "restrain langchain version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-08T03:24:40Z",
    "closed_at": "2023-10-09T04:22:33Z",
    "merged_at": "2023-10-09T04:22:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/416"
  },
  {
    "number": 415,
    "title": "Set version of ipex and torch, fix setup_text_chatbot_service_on_spr.i",
    "user": "huiyan2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-07T07:41:40Z",
    "closed_at": "2023-10-10T03:30:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/415"
  },
  {
    "number": 414,
    "title": "[Cpp Graph] Update Falcon HF para and support Falcon-180B",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-07T05:50:54Z",
    "closed_at": "2023-10-12T08:42:40Z",
    "merged_at": "2023-10-12T08:42:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/414"
  },
  {
    "number": 413,
    "title": "reinit cpp model and infinite text generation",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-10-07T05:09:57Z",
    "closed_at": "2023-10-09T08:13:35Z",
    "merged_at": "2023-10-09T08:13:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/413"
  },
  {
    "number": 410,
    "title": "Update setup_text_chatbot_service_on_spr.ipynb",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-28T23:00:38Z",
    "closed_at": "2023-09-28T23:24:47Z",
    "merged_at": "2023-09-28T23:24:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/410"
  },
  {
    "number": 409,
    "title": "[LLM] Upgrade lm_eval integration and usage",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-28T06:21:52Z",
    "closed_at": "2023-11-02T14:29:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/409"
  },
  {
    "number": 408,
    "title": "migrate chatbot CI to Azure node",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-28T05:23:07Z",
    "closed_at": "2023-10-12T08:33:28Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/408"
  },
  {
    "number": 407,
    "title": "add Acknowledgements",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-28T01:42:25Z",
    "closed_at": "2023-10-09T14:25:13Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/407"
  },
  {
    "number": 406,
    "title": "fixes to Neural Chat SparseBM25Retriever pipeline",
    "user": "kminhta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-27T17:11:46Z",
    "closed_at": "2023-10-10T02:47:21Z",
    "merged_at": "2023-10-10T02:47:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/406"
  },
  {
    "number": 405,
    "title": "fixes to Neural Chat SparseBM25Retriever pipeline",
    "user": "kminhta",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-27T16:48:33Z",
    "closed_at": "2023-09-27T16:53:06Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/405"
  },
  {
    "number": 404,
    "title": "fix install from source",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-27T15:21:57Z",
    "closed_at": "2023-09-27T23:12:05Z",
    "merged_at": "2023-09-27T23:12:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/404"
  },
  {
    "number": 403,
    "title": "Update build_chatbot_on_xpu.ipynb",
    "user": "huiyan2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-27T07:53:57Z",
    "closed_at": "2023-09-27T23:38:52Z",
    "merged_at": "2023-09-27T23:38:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/403"
  },
  {
    "number": 402,
    "title": "Fixed bug for woq with AWQ",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-27T05:24:59Z",
    "closed_at": "2023-09-28T07:28:20Z",
    "merged_at": "2023-09-28T07:28:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/402"
  },
  {
    "number": 401,
    "title": "not compiling python api in cpp graph by default",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-27T04:59:42Z",
    "closed_at": "2023-09-27T05:58:34Z",
    "merged_at": "2023-09-27T05:58:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/401"
  },
  {
    "number": 400,
    "title": "update(requirement) : accelerate module",
    "user": "park12sj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-27T04:03:23Z",
    "closed_at": "2023-09-27T06:39:55Z",
    "merged_at": "2023-09-27T06:39:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/400"
  },
  {
    "number": 399,
    "title": "Add itrex llm runtime graph int4 notebook",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T14:59:17Z",
    "closed_at": "2023-10-27T02:14:45Z",
    "merged_at": "2023-10-27T02:14:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/399"
  },
  {
    "number": 398,
    "title": "Enable build_chatbot_on_xpu.ipynb",
    "user": "huiyan2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T13:24:16Z",
    "closed_at": "2023-09-26T15:17:23Z",
    "merged_at": "2023-09-26T15:17:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/398"
  },
  {
    "number": 397,
    "title": "Fix double import issue and update pip install itrex",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T12:55:48Z",
    "closed_at": "2023-09-26T13:52:56Z",
    "merged_at": "2023-09-26T13:52:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/397"
  },
  {
    "number": 396,
    "title": "remove denpendency of fairscale.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T12:22:26Z",
    "closed_at": "2023-09-26T12:28:59Z",
    "merged_at": "2023-09-26T12:28:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/396"
  },
  {
    "number": 395,
    "title": "Refine LLM runtime readme",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T12:06:08Z",
    "closed_at": "2023-09-26T12:31:43Z",
    "merged_at": "2023-09-26T12:31:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/395"
  },
  {
    "number": 393,
    "title": "Refine Chatbot readme",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T08:16:10Z",
    "closed_at": "2023-09-26T08:29:06Z",
    "merged_at": "2023-09-26T08:29:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/393"
  },
  {
    "number": 392,
    "title": "add notebook entry",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T07:58:40Z",
    "closed_at": "2023-09-26T08:14:17Z",
    "merged_at": "2023-09-26T08:14:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/392"
  },
  {
    "number": 391,
    "title": "Update README.md for llama2 70B",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T07:33:42Z",
    "closed_at": "2023-09-26T07:40:43Z",
    "merged_at": "2023-09-26T07:40:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/391"
  },
  {
    "number": 390,
    "title": "Refine chatbot ci",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T03:17:32Z",
    "closed_at": "2023-09-26T05:54:47Z",
    "merged_at": "2023-09-26T05:54:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/390"
  },
  {
    "number": 389,
    "title": "refine trigger condition",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T02:31:23Z",
    "closed_at": "2023-09-26T02:35:18Z",
    "merged_at": "2023-09-26T02:35:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/389"
  },
  {
    "number": 388,
    "title": "Use transfomers tokenizer and streamer for python api",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-26T01:37:24Z",
    "closed_at": "2023-09-26T13:49:54Z",
    "merged_at": "2023-09-26T13:49:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/388"
  },
  {
    "number": 387,
    "title": "refine sparseGPT example & scripts",
    "user": "WeiweiZhang1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-25T17:15:53Z",
    "closed_at": "2023-09-26T02:28:26Z",
    "merged_at": "2023-09-26T02:28:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/387"
  },
  {
    "number": 386,
    "title": "add readme for llm kernels",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-25T09:21:48Z",
    "closed_at": "2023-09-26T07:33:03Z",
    "merged_at": "2023-09-26T07:33:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/386"
  },
  {
    "number": 385,
    "title": "improve text-generaion example typing",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-25T09:18:12Z",
    "closed_at": "2023-09-25T09:28:50Z",
    "merged_at": "2023-09-25T09:28:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/385"
  },
  {
    "number": 384,
    "title": "improve CI",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-25T09:04:17Z",
    "closed_at": "2023-09-25T12:46:10Z",
    "merged_at": "2023-09-25T12:46:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/384"
  },
  {
    "number": 383,
    "title": "Refined documentation for setting up an end to end talkingbot",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-25T08:59:20Z",
    "closed_at": "2023-09-25T23:09:56Z",
    "merged_at": "2023-09-25T23:09:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/383"
  },
  {
    "number": 382,
    "title": "fix python api bug",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-25T07:00:19Z",
    "closed_at": "2023-09-26T03:01:31Z",
    "merged_at": "2023-09-26T03:01:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/382"
  },
  {
    "number": 381,
    "title": "Qg/docs: Update the # of pubs to 11 to reflect the added Hand-On Lab at Intel Innovation 2023. ",
    "user": "qgao007",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-25T05:50:54Z",
    "closed_at": "2023-09-25T08:17:11Z",
    "merged_at": "2023-09-25T08:17:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/381"
  },
  {
    "number": 380,
    "title": "Add Hands-On Lab at Intel Innovation 2023 to the publication section",
    "user": "qgao007",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-25T05:29:15Z",
    "closed_at": "2023-09-25T05:43:33Z",
    "merged_at": "2023-09-25T05:43:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/380"
  },
  {
    "number": 379,
    "title": "Enable \"neuralchat predict\" for Intel GPU",
    "user": "huiyan2021",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-23T13:39:25Z",
    "closed_at": "2023-09-25T00:53:19Z",
    "merged_at": "2023-09-25T00:53:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/379"
  },
  {
    "number": 377,
    "title": "add talkingbot-pc code",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T10:29:34Z",
    "closed_at": "2023-09-26T07:43:16Z",
    "merged_at": "2023-09-26T07:43:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/377"
  },
  {
    "number": 376,
    "title": "[CPP Graph] Baichuan & Baichuan2 Enabling",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T09:30:28Z",
    "closed_at": "2023-10-13T07:21:49Z",
    "merged_at": "2023-10-13T07:21:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/376"
  },
  {
    "number": 375,
    "title": "Add an end to end talkingbot example and notebook",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T08:48:57Z",
    "closed_at": "2023-09-25T00:49:30Z",
    "merged_at": "2023-09-25T00:49:30Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/375"
  },
  {
    "number": 374,
    "title": "support Avx2",
    "user": "yuchengliu1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T07:17:01Z",
    "closed_at": "2023-10-18T09:04:02Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/374"
  },
  {
    "number": 373,
    "title": "Fix the Woq config serialization issue",
    "user": "yiliu30",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T06:03:02Z",
    "closed_at": "2023-09-23T00:16:32Z",
    "merged_at": "2023-09-23T00:16:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/373"
  },
  {
    "number": 372,
    "title": "Added notebook examples for TangoBERT and SWEET",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T05:48:45Z",
    "closed_at": "2023-09-25T04:52:26Z",
    "merged_at": "2023-09-25T04:52:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/372"
  },
  {
    "number": 371,
    "title": "Support lm-eval for merged decoder ONNX models",
    "user": "yuwenzho",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T05:44:35Z",
    "closed_at": "2023-09-22T07:50:45Z",
    "merged_at": "2023-09-22T07:50:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/371"
  },
  {
    "number": 370,
    "title": "Refactor prompt template API",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T05:25:59Z",
    "closed_at": "2023-09-26T00:15:17Z",
    "merged_at": "2023-09-26T00:15:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/370"
  },
  {
    "number": 369,
    "title": "[CPP Graph] KV-Update Optimization",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T03:40:46Z",
    "closed_at": "2023-09-25T02:25:03Z",
    "merged_at": "2023-09-25T02:25:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/369"
  },
  {
    "number": 368,
    "title": "try aspll spellingcheck",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T03:29:06Z",
    "closed_at": "2023-09-24T03:07:59Z",
    "merged_at": "2023-09-24T03:07:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/368"
  },
  {
    "number": 367,
    "title": "Refined documentation for setting up an end to end text chatbot",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-22T02:37:11Z",
    "closed_at": "2023-09-22T04:30:21Z",
    "merged_at": "2023-09-22T04:30:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/367"
  },
  {
    "number": 366,
    "title": "add dpo code.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T11:45:23Z",
    "closed_at": "2023-09-26T09:18:17Z",
    "merged_at": "2023-09-26T09:18:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/366"
  },
  {
    "number": 365,
    "title": "Xwang update doc",
    "user": "xiguiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T09:09:59Z",
    "closed_at": "2023-09-21T22:37:10Z",
    "merged_at": "2023-09-21T22:37:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/365"
  },
  {
    "number": 364,
    "title": "fix dtype issue for chatGLM example",
    "user": "violetch24",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T06:52:39Z",
    "closed_at": "2023-09-21T06:58:55Z",
    "merged_at": "2023-09-21T06:58:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/364"
  },
  {
    "number": 363,
    "title": "fix neuralchat docker readme",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T05:41:17Z",
    "closed_at": "2023-09-21T11:37:29Z",
    "merged_at": "2023-09-21T11:37:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/363"
  },
  {
    "number": 362,
    "title": "update llm runtime parameters",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T03:38:08Z",
    "closed_at": "2023-09-21T10:05:26Z",
    "merged_at": "2023-09-21T10:05:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/362"
  },
  {
    "number": 361,
    "title": "support attention block TP and add gptj llama model",
    "user": "ClarkChin08",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T03:24:17Z",
    "closed_at": "2023-10-25T05:59:50Z",
    "merged_at": "2023-10-25T05:59:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/361"
  },
  {
    "number": 360,
    "title": "add check dict",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T03:22:31Z",
    "closed_at": "2023-09-21T03:23:34Z",
    "merged_at": "2023-09-21T03:23:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/360"
  },
  {
    "number": 359,
    "title": "Temperately disable hpu test",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T02:56:14Z",
    "closed_at": "2023-09-21T03:49:26Z",
    "merged_at": "2023-09-21T03:49:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/359"
  },
  {
    "number": 358,
    "title": "enable the user to load the local database to avoid unnecessary data preprocessing",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T02:45:12Z",
    "closed_at": "2023-09-26T09:22:54Z",
    "merged_at": "2023-09-26T09:22:54Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/358"
  },
  {
    "number": 357,
    "title": "Remove invalid code",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-21T01:52:28Z",
    "closed_at": "2023-09-21T11:35:35Z",
    "merged_at": "2023-09-21T11:35:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/357"
  },
  {
    "number": 356,
    "title": "update publication and mailto",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-20T16:49:05Z",
    "closed_at": "2023-09-20T16:53:59Z",
    "merged_at": "2023-09-20T16:53:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/356"
  },
  {
    "number": 355,
    "title": "fix discord link",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-20T15:21:36Z",
    "closed_at": "2023-09-21T06:32:43Z",
    "merged_at": "2023-09-21T06:32:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/355"
  },
  {
    "number": 354,
    "title": "Support load_in_4bit and load_in_8bit on CPU device",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-20T11:50:45Z",
    "closed_at": "2023-09-21T12:04:57Z",
    "merged_at": "2023-09-21T12:04:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/354"
  },
  {
    "number": 353,
    "title": "Fixed error for replace -100s in predictions by the pad token",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-20T09:21:56Z",
    "closed_at": "2023-09-20T12:47:34Z",
    "merged_at": "2023-09-20T12:47:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/353"
  },
  {
    "number": 352,
    "title": "jupyter notebook update",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-20T07:04:56Z",
    "closed_at": "2023-09-21T08:20:55Z",
    "merged_at": "2023-09-21T08:20:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/352"
  },
  {
    "number": 351,
    "title": "fix ut test config for neural chat ",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-20T03:45:10Z",
    "closed_at": "2023-09-20T05:56:43Z",
    "merged_at": "2023-09-20T05:56:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/351"
  },
  {
    "number": 350,
    "title": "Restrict onnxruntime version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-20T02:58:46Z",
    "closed_at": "2023-09-20T16:36:31Z",
    "merged_at": "2023-09-20T16:36:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/350"
  },
  {
    "number": 349,
    "title": "Fix text-generation logger info",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-20T02:40:53Z",
    "closed_at": "2023-09-20T07:21:03Z",
    "merged_at": "2023-09-20T07:21:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/349"
  },
  {
    "number": 348,
    "title": "improve the logger info.",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-20T02:34:14Z",
    "closed_at": "2023-09-20T02:34:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/348"
  },
  {
    "number": 347,
    "title": "Add dnnl_dim_t cast to fix executor windows failure",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-20T02:01:00Z",
    "closed_at": "2023-09-21T05:34:41Z",
    "merged_at": "2023-09-21T05:34:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/347"
  },
  {
    "number": 346,
    "title": "Jupyter notebook and example for setting up an end to end text chatbot",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-19T16:09:37Z",
    "closed_at": "2023-09-20T14:30:17Z",
    "merged_at": "2023-09-20T14:30:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/346"
  },
  {
    "number": 345,
    "title": "update text-generation scripts",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-19T08:31:40Z",
    "closed_at": "2023-09-19T23:57:17Z",
    "merged_at": "2023-09-19T23:57:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/345"
  },
  {
    "number": 344,
    "title": "Update ChatGLM-6B to README.md",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-19T08:16:11Z",
    "closed_at": "2023-09-19T08:46:46Z",
    "merged_at": "2023-09-19T08:46:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/344"
  },
  {
    "number": 343,
    "title": "New avx512_vnni kernel",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-19T08:00:07Z",
    "closed_at": "2023-09-20T06:31:34Z",
    "merged_at": "2023-09-20T06:31:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/343"
  },
  {
    "number": 342,
    "title": "Replace sample files",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-19T07:50:02Z",
    "closed_at": "2023-09-19T23:57:51Z",
    "merged_at": "2023-09-19T23:57:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/342"
  },
  {
    "number": 341,
    "title": "fix ordinals and conjunctions in tts normalizer",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-19T07:11:56Z",
    "closed_at": "2023-09-20T03:15:12Z",
    "merged_at": "2023-09-20T03:15:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/341"
  },
  {
    "number": 340,
    "title": "change mainpage",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-19T06:18:54Z",
    "closed_at": "2023-09-26T04:10:10Z",
    "merged_at": "2023-09-26T04:10:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/340"
  },
  {
    "number": 339,
    "title": "fix broken link in document",
    "user": "xiguiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-19T03:56:20Z",
    "closed_at": "2023-09-19T07:11:37Z",
    "merged_at": "2023-09-19T07:11:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/339"
  },
  {
    "number": 338,
    "title": "Update text-generation build_env.sh",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-19T02:48:36Z",
    "closed_at": "2023-09-19T02:54:19Z",
    "merged_at": "2023-09-19T02:54:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/338"
  },
  {
    "number": 337,
    "title": "Support streaming mode for neuralchat server",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-18T14:48:57Z",
    "closed_at": "2023-09-19T01:13:33Z",
    "merged_at": "2023-09-19T01:13:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/337"
  },
  {
    "number": 336,
    "title": "fix checkmarx issue",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-18T13:36:39Z",
    "closed_at": "2023-09-19T02:40:46Z",
    "merged_at": "2023-09-19T02:40:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/336"
  },
  {
    "number": 335,
    "title": "fix checkmarx issue",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-18T11:55:02Z",
    "closed_at": "2023-09-18T13:31:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/335"
  },
  {
    "number": 334,
    "title": "Fix Habana finetuning issues",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-18T08:10:07Z",
    "closed_at": "2023-09-18T14:08:13Z",
    "merged_at": "2023-09-18T14:08:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/334"
  },
  {
    "number": 333,
    "title": "fix distributed_state not found error",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-18T04:57:34Z",
    "closed_at": "2023-09-18T05:41:29Z",
    "merged_at": "2023-09-18T05:41:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/333"
  },
  {
    "number": 332,
    "title": "update onednn to v3.3-pc",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-18T02:07:59Z",
    "closed_at": "2023-09-18T05:47:03Z",
    "merged_at": "2023-09-18T05:47:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/332"
  },
  {
    "number": 330,
    "title": "Fix IntelON notebook demo issues",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T11:06:08Z",
    "closed_at": "2023-09-15T14:26:21Z",
    "merged_at": "2023-09-15T14:26:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/330"
  },
  {
    "number": 329,
    "title": "Refine NeuralChat notebook",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T10:10:18Z",
    "closed_at": "2023-09-15T12:42:16Z",
    "merged_at": "2023-09-15T12:42:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/329"
  },
  {
    "number": 328,
    "title": "add whisper",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T09:13:34Z",
    "closed_at": "2023-10-13T03:08:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/328"
  },
  {
    "number": 327,
    "title": "fix position_ids issue, add lm_eval",
    "user": "WeiweiZhang1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T09:04:53Z",
    "closed_at": "2023-09-15T15:01:15Z",
    "merged_at": "2023-09-15T15:01:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/327"
  },
  {
    "number": 326,
    "title": "fix the dolly failed",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T09:02:29Z",
    "closed_at": "2023-09-18T05:50:49Z",
    "merged_at": "2023-09-18T05:50:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/326"
  },
  {
    "number": 325,
    "title": "notebook update",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T08:59:07Z",
    "closed_at": "2023-09-15T12:42:53Z",
    "merged_at": "2023-09-15T12:42:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/325"
  },
  {
    "number": 324,
    "title": "fix speaker embedding path issue",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T06:33:55Z",
    "closed_at": "2023-09-15T07:30:59Z",
    "merged_at": "2023-09-15T07:30:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/324"
  },
  {
    "number": 323,
    "title": "add ut to override parm PR",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T06:15:00Z",
    "closed_at": "2023-09-15T06:15:20Z",
    "merged_at": "2023-09-15T06:15:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/323"
  },
  {
    "number": 322,
    "title": "[Cpp Graph] Align Cpp Beam Search ",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T05:59:02Z",
    "closed_at": "2023-09-27T04:12:00Z",
    "merged_at": "2023-09-27T04:12:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/322"
  },
  {
    "number": 321,
    "title": "add ut for checking the accuracy of the RAG to avoid the other changes influence the retrieval and the generation result",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T05:56:40Z",
    "closed_at": "2023-09-19T02:41:02Z",
    "merged_at": "2023-09-19T02:41:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/321"
  },
  {
    "number": 320,
    "title": "Refine Script and args for Cpp Graph",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T05:08:38Z",
    "closed_at": "2023-09-20T12:17:28Z",
    "merged_at": "2023-09-20T12:17:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/320"
  },
  {
    "number": 319,
    "title": "fix bugs in RAG code for converting the prompt",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-15T04:01:49Z",
    "closed_at": "2023-09-15T04:21:22Z",
    "merged_at": "2023-09-15T04:21:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/319"
  },
  {
    "number": 318,
    "title": "Enable text-generation with new API",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T11:00:54Z",
    "closed_at": "2023-09-18T23:05:36Z",
    "merged_at": "2023-09-18T23:05:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/318"
  },
  {
    "number": 317,
    "title": "Fix habana notebook issues",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T09:58:39Z",
    "closed_at": "2023-09-14T13:57:10Z",
    "merged_at": "2023-09-14T13:57:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/317"
  },
  {
    "number": 316,
    "title": "enable the user to load the local database to avoid unnecessary data preprocessing ",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T09:27:19Z",
    "closed_at": "2023-09-21T02:45:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/316"
  },
  {
    "number": 315,
    "title": "Refactored inference API, and extracted prompt template as individual module",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T08:30:22Z",
    "closed_at": "2023-09-18T06:02:16Z",
    "merged_at": "2023-09-18T06:02:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/315"
  },
  {
    "number": 314,
    "title": "Add Notebooks for NeuralChat",
    "user": "letonghan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T07:50:54Z",
    "closed_at": "2023-09-14T12:23:40Z",
    "merged_at": "2023-09-14T12:23:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/314"
  },
  {
    "number": 313,
    "title": "update QuLAT example transformers version",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T07:29:38Z",
    "closed_at": "2023-09-14T12:25:28Z",
    "merged_at": "2023-09-14T12:25:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/313"
  },
  {
    "number": 312,
    "title": "[Graph] windows build",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T07:11:56Z",
    "closed_at": "2023-09-14T08:08:58Z",
    "merged_at": "2023-09-14T08:08:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/312"
  },
  {
    "number": 311,
    "title": "fix intel neural-chat-7b model config",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T07:03:17Z",
    "closed_at": "2023-09-14T08:45:38Z",
    "merged_at": "2023-09-14T08:45:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/311"
  },
  {
    "number": 310,
    "title": "fix normalizer: year, punctuation after number, end token",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T04:12:55Z",
    "closed_at": "2023-09-14T12:25:46Z",
    "merged_at": "2023-09-14T12:25:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/310"
  },
  {
    "number": 309,
    "title": "Add Notebooks for finetuning chatbot on various platforms",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T03:38:48Z",
    "closed_at": "2023-09-14T05:01:37Z",
    "merged_at": "2023-09-14T05:01:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/309"
  },
  {
    "number": 307,
    "title": "update transformers version",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T03:18:27Z",
    "closed_at": "2023-09-15T08:57:22Z",
    "merged_at": "2023-09-15T08:57:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/307"
  },
  {
    "number": 306,
    "title": "[CPP Graph] Asym model",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-14T02:25:10Z",
    "closed_at": "2023-09-15T09:15:36Z",
    "merged_at": "2023-09-15T09:15:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/306"
  },
  {
    "number": 305,
    "title": "Fix None response bug for security checker",
    "user": "qgao007",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-13T18:31:49Z",
    "closed_at": "2023-09-14T02:22:56Z",
    "merged_at": "2023-09-14T02:22:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/305"
  },
  {
    "number": 304,
    "title": "Merge finetuning and inference docker into one for notebook usage",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-13T14:27:32Z",
    "closed_at": "2023-09-14T05:00:53Z",
    "merged_at": "2023-09-14T05:00:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/304"
  },
  {
    "number": 303,
    "title": "Build wheel from cached dnnl local",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-13T09:42:19Z",
    "closed_at": "2023-09-14T01:28:15Z",
    "merged_at": "2023-09-14T01:28:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/303"
  },
  {
    "number": 302,
    "title": "Support NeuralChatV2 model and add hf_access_token",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-13T06:12:39Z",
    "closed_at": "2023-09-13T08:00:41Z",
    "merged_at": "2023-09-13T08:00:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/302"
  },
  {
    "number": 301,
    "title": "Fix llama model tokenizer issue",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-12T09:37:23Z",
    "closed_at": "2023-09-12T23:34:05Z",
    "merged_at": "2023-09-12T23:34:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/301"
  },
  {
    "number": 300,
    "title": "fix the bloom and dolly ffn fusion error (#284)",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-12T08:55:08Z",
    "closed_at": "2023-09-12T08:55:24Z",
    "merged_at": "2023-09-12T08:55:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/300"
  },
  {
    "number": 299,
    "title": "update jblas",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-12T08:37:21Z",
    "closed_at": "2023-09-13T04:55:25Z",
    "merged_at": "2023-09-13T04:55:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/299"
  },
  {
    "number": 298,
    "title": "add invitation of channel",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-12T08:16:56Z",
    "closed_at": "2023-09-12T23:34:26Z",
    "merged_at": "2023-09-12T23:34:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/298"
  },
  {
    "number": 297,
    "title": "Add weight_only support for PyTorch framework",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-12T07:29:22Z",
    "closed_at": "2023-09-15T10:58:16Z",
    "merged_at": "2023-09-15T10:58:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/297"
  },
  {
    "number": 296,
    "title": "fix readme model path",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-12T06:49:07Z",
    "closed_at": "2023-09-12T09:11:27Z",
    "merged_at": "2023-09-12T09:11:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/296"
  },
  {
    "number": 295,
    "title": "update calculate method",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-12T05:31:49Z",
    "closed_at": "2023-09-12T05:40:11Z",
    "merged_at": "2023-09-12T05:40:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/295"
  },
  {
    "number": 294,
    "title": "Upgrade QBits backend",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-12T02:23:59Z",
    "closed_at": "2023-09-13T07:42:33Z",
    "merged_at": "2023-09-13T07:42:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/294"
  },
  {
    "number": 293,
    "title": "fix CI and add Readme",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T15:53:06Z",
    "closed_at": "2023-09-12T02:19:27Z",
    "merged_at": "2023-09-12T02:19:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/293"
  },
  {
    "number": 292,
    "title": "Improve text-generation example ipex installation",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T14:04:38Z",
    "closed_at": "2023-09-12T05:13:59Z",
    "merged_at": "2023-09-12T05:13:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/292"
  },
  {
    "number": 291,
    "title": "Refine NeuralChat UI readme",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T13:34:17Z",
    "closed_at": "2023-09-11T14:00:18Z",
    "merged_at": "2023-09-11T14:00:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/291"
  },
  {
    "number": 290,
    "title": "Support HPU on ASR/TTS",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T09:08:04Z",
    "closed_at": "2023-09-13T13:32:53Z",
    "merged_at": "2023-09-13T13:32:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/290"
  },
  {
    "number": 289,
    "title": "[CPP Graph] Fix q4_0 gptj with MHA fusion enabled",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T08:06:41Z",
    "closed_at": "2023-09-11T08:23:46Z",
    "merged_at": "2023-09-11T08:23:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/289"
  },
  {
    "number": 288,
    "title": "Disable building OneDNN examples & tests",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T08:01:32Z",
    "closed_at": "2023-09-12T03:26:27Z",
    "merged_at": "2023-09-12T03:26:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/288"
  },
  {
    "number": 287,
    "title": "Refine NeuralChat docker files",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T07:31:25Z",
    "closed_at": "2023-09-11T13:58:06Z",
    "merged_at": "2023-09-11T13:58:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/287"
  },
  {
    "number": 286,
    "title": "Allow caching dnnl caching & avoid building tests",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T07:17:57Z",
    "closed_at": "2023-09-14T01:40:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/286"
  },
  {
    "number": 285,
    "title": "[CPP Graph] Fix q4_0 gptj with MHA fusion enabled",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T05:59:23Z",
    "closed_at": "2023-09-11T07:39:02Z",
    "merged_at": "2023-09-11T07:39:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/285"
  },
  {
    "number": 284,
    "title": "fix the bloom and dolly ffn fusion error",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T05:43:37Z",
    "closed_at": "2023-09-12T07:19:09Z",
    "merged_at": "2023-09-12T07:19:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/284"
  },
  {
    "number": 283,
    "title": "add icons to main page",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T04:00:16Z",
    "closed_at": "2023-09-11T04:53:20Z",
    "merged_at": "2023-09-11T04:53:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/283"
  },
  {
    "number": 282,
    "title": "enhance mpi logging",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T03:21:34Z",
    "closed_at": "2023-09-11T04:12:07Z",
    "merged_at": "2023-09-11T04:12:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/282"
  },
  {
    "number": 281,
    "title": "add general voice-cloning finetuning code",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T03:07:49Z",
    "closed_at": "2023-09-11T07:56:57Z",
    "merged_at": "2023-09-11T07:56:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/281"
  },
  {
    "number": 280,
    "title": "support avx2 prologue",
    "user": "zhewang1-intc",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T02:47:17Z",
    "closed_at": "2023-09-12T01:46:03Z",
    "merged_at": "2023-09-12T01:46:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/280"
  },
  {
    "number": 279,
    "title": "update quantization.md",
    "user": "xiguiw",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-11T02:00:41Z",
    "closed_at": "2023-09-11T05:18:09Z",
    "merged_at": "2023-09-11T05:18:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/279"
  },
  {
    "number": 278,
    "title": "[CPP Graph] ChatGLM Enabling and ChatGLM-2 Issues Fix",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-10T14:25:36Z",
    "closed_at": "2023-09-14T02:20:23Z",
    "merged_at": "2023-09-14T02:20:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/278"
  },
  {
    "number": 277,
    "title": "Jupyter Notebooks for NeuralChat",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-10T14:06:15Z",
    "closed_at": "2023-09-14T23:23:55Z",
    "merged_at": "2023-09-14T23:23:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/277"
  },
  {
    "number": 276,
    "title": "Add neuralchat server unit test cases",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-10T08:37:46Z",
    "closed_at": "2023-09-13T12:15:28Z",
    "merged_at": "2023-09-13T12:15:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/276"
  },
  {
    "number": 275,
    "title": "fix tab",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T11:27:39Z",
    "closed_at": "2023-09-08T11:30:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/275"
  },
  {
    "number": 274,
    "title": "Revert \"Refine mpt text-generation example\"",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T11:13:13Z",
    "closed_at": "2023-09-08T13:32:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/274"
  },
  {
    "number": 273,
    "title": "update pylint and cpp graph",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T10:14:52Z",
    "closed_at": "2023-09-08T14:15:41Z",
    "merged_at": "2023-09-08T14:15:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/273"
  },
  {
    "number": 272,
    "title": "add debug-statements checker",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T08:54:43Z",
    "closed_at": "2023-09-08T10:22:13Z",
    "merged_at": "2023-09-08T10:22:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/272"
  },
  {
    "number": 271,
    "title": "Q4 perchannel",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T08:34:14Z",
    "closed_at": "2023-09-08T09:57:04Z",
    "merged_at": "2023-09-08T09:57:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/271"
  },
  {
    "number": 270,
    "title": "upload talking bot front_end code",
    "user": "WenjiaoYue",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T06:51:30Z",
    "closed_at": "2023-09-10T23:04:59Z",
    "merged_at": "2023-09-10T23:04:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/270"
  },
  {
    "number": 269,
    "title": "test only",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T06:25:30Z",
    "closed_at": "2023-09-08T09:04:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/269"
  },
  {
    "number": 268,
    "title": "Support dynamic plugin loading",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T05:59:59Z",
    "closed_at": "2023-09-12T01:36:53Z",
    "merged_at": "2023-09-12T01:36:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/268"
  },
  {
    "number": 267,
    "title": "Fuzz test",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T03:29:11Z",
    "closed_at": "2023-09-08T03:56:04Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/267"
  },
  {
    "number": 266,
    "title": "Revert \"[CPP Graph] ChatGLM-2 Enabling (#210)\"",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T02:38:59Z",
    "closed_at": "2023-09-08T03:30:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/266"
  },
  {
    "number": 265,
    "title": "reduce hpu test triggers",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T02:22:20Z",
    "closed_at": "2023-09-15T09:17:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/265"
  },
  {
    "number": 264,
    "title": "Migrated finetuning and inference code of workflow chatbot to use NeuralChat API",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T02:19:20Z",
    "closed_at": "2023-09-22T02:37:28Z",
    "merged_at": "2023-09-22T02:37:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/264"
  },
  {
    "number": 263,
    "title": " Update SD README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T01:53:14Z",
    "closed_at": "2023-09-08T02:27:07Z",
    "merged_at": "2023-09-08T02:27:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/263"
  },
  {
    "number": 262,
    "title": "[CPP Graph] fix broken format",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-08T01:35:38Z",
    "closed_at": "2023-09-08T05:51:41Z",
    "merged_at": "2023-09-08T05:51:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/262"
  },
  {
    "number": 261,
    "title": "add xlsx requirements for neuralchat",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-07T12:35:15Z",
    "closed_at": "2023-09-07T12:35:34Z",
    "merged_at": "2023-09-07T12:35:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/261"
  },
  {
    "number": 260,
    "title": "[CPP Graph] Fix models without jblas-based kvcache support",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-07T09:45:39Z",
    "closed_at": "2023-09-07T15:03:44Z",
    "merged_at": "2023-09-07T15:03:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/260"
  },
  {
    "number": 259,
    "title": "Update transformers version",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-07T09:38:56Z",
    "closed_at": "2023-09-07T15:05:27Z",
    "merged_at": "2023-09-07T15:05:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/259"
  },
  {
    "number": 258,
    "title": "Fix retrieval example issue",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-07T08:31:02Z",
    "closed_at": "2023-09-07T23:34:32Z",
    "merged_at": "2023-09-07T23:34:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/258"
  },
  {
    "number": 257,
    "title": "Refine mpt text-generation example",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-07T08:04:06Z",
    "closed_at": "2023-09-08T08:22:20Z",
    "merged_at": "2023-09-08T08:22:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/257"
  },
  {
    "number": 256,
    "title": "Add TTS finetuning interface",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-07T07:49:45Z",
    "closed_at": "2023-09-08T14:32:24Z",
    "merged_at": "2023-09-08T14:32:24Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/256"
  },
  {
    "number": 255,
    "title": "update tensorflow tests and cpp graph",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-07T06:19:05Z",
    "closed_at": "2023-09-08T02:18:06Z",
    "merged_at": "2023-09-08T02:18:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/255"
  },
  {
    "number": 254,
    "title": "[CPP Graph]fix the error of convert bloom and opt",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-07T05:27:50Z",
    "closed_at": "2023-09-07T07:21:22Z",
    "merged_at": "2023-09-07T07:21:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/254"
  },
  {
    "number": 253,
    "title": "Fix a link typo in NeuralChat README",
    "user": "ftian1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-07T05:17:08Z",
    "closed_at": "2023-09-07T06:09:28Z",
    "merged_at": "2023-09-07T06:09:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/253"
  },
  {
    "number": 252,
    "title": "Python api for cpp model",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-07T01:17:30Z",
    "closed_at": "2023-09-20T01:45:09Z",
    "merged_at": "2023-09-20T01:45:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/252"
  },
  {
    "number": 251,
    "title": "Fix retrieval plugin readme and add one necessary package dependency",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-06T09:00:28Z",
    "closed_at": "2023-09-06T09:58:48Z",
    "merged_at": "2023-09-06T09:58:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/251"
  },
  {
    "number": 250,
    "title": "Jianyuzh fix for adobe analytics",
    "user": "NeoZhangJianyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-06T08:01:57Z",
    "closed_at": "2023-09-06T08:44:48Z",
    "merged_at": "2023-09-06T08:44:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/250"
  },
  {
    "number": 249,
    "title": "support adobe analytics, fix anchor issues in examples, add repo link in list",
    "user": "NeoZhangJianyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-06T00:48:09Z",
    "closed_at": "2023-09-06T02:04:18Z",
    "merged_at": "2023-09-06T02:04:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/249"
  },
  {
    "number": 248,
    "title": "Fix tensorflow import issue",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T14:23:36Z",
    "closed_at": "2023-09-07T07:04:03Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/248"
  },
  {
    "number": 247,
    "title": "Add readme for audio and cache plugins",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T14:16:38Z",
    "closed_at": "2023-09-06T07:45:02Z",
    "merged_at": "2023-09-06T07:45:02Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/247"
  },
  {
    "number": 246,
    "title": "Hpu pre script",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T13:46:02Z",
    "closed_at": "2023-09-06T05:35:28Z",
    "merged_at": "2023-09-06T05:35:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/246"
  },
  {
    "number": 245,
    "title": "test pre-script",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T13:18:38Z",
    "closed_at": "2023-09-05T13:32:30Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/245"
  },
  {
    "number": 244,
    "title": "add README for retrieval, safety checker plugins",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T12:02:40Z",
    "closed_at": "2023-09-05T13:21:31Z",
    "merged_at": "2023-09-05T13:21:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/244"
  },
  {
    "number": 243,
    "title": "Update NeuralChat README/Tutorial/Jupyter Notebook documents",
    "user": "ftian1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T11:50:08Z",
    "closed_at": "2023-09-06T08:08:31Z",
    "merged_at": "2023-09-06T08:08:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/243"
  },
  {
    "number": 242,
    "title": "Fixed lazy import issue for PyTorch in intel_extension_for_transformers",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T08:50:05Z",
    "closed_at": "2023-09-07T05:13:55Z",
    "merged_at": "2023-09-07T05:13:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/242"
  },
  {
    "number": 241,
    "title": "fix docker image build dir for inference",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T08:24:17Z",
    "closed_at": "2023-09-05T09:08:59Z",
    "merged_at": "2023-09-05T09:08:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/241"
  },
  {
    "number": 240,
    "title": "Added notebook example of NeuralChat finetuning for LLaMA2 and MPT",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T07:09:07Z",
    "closed_at": "2023-09-06T03:38:40Z",
    "merged_at": "2023-09-06T03:38:40Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/240"
  },
  {
    "number": 239,
    "title": "fix3rdparty--rebase",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T05:47:54Z",
    "closed_at": "2023-09-08T08:51:06Z",
    "merged_at": "2023-09-08T08:51:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/239"
  },
  {
    "number": 238,
    "title": "rename configs for woq",
    "user": "violetch24",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T04:17:50Z",
    "closed_at": "2023-09-08T14:36:52Z",
    "merged_at": "2023-09-08T14:36:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/238"
  },
  {
    "number": 237,
    "title": "Add latest update in readme",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T03:58:54Z",
    "closed_at": "2023-09-05T10:55:14Z",
    "merged_at": "2023-09-05T10:55:14Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/237"
  },
  {
    "number": 236,
    "title": "[CPP Graph] Rename LLM chat application",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T02:56:54Z",
    "closed_at": "2023-09-06T09:56:16Z",
    "merged_at": "2023-09-06T09:56:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/236"
  },
  {
    "number": 235,
    "title": "add sparseGPT example",
    "user": "WeiweiZhang1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T02:26:56Z",
    "closed_at": "2023-09-08T07:35:28Z",
    "merged_at": "2023-09-08T07:35:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/235"
  },
  {
    "number": 234,
    "title": "Add weight_only support for PyTorch framework",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-05T01:05:15Z",
    "closed_at": "2023-09-08T14:46:41Z",
    "merged_at": "2023-09-08T14:46:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/234"
  },
  {
    "number": 233,
    "title": "Fix the code structure and the plugin name in NeuralChat",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-04T13:44:53Z",
    "closed_at": "2023-09-05T07:39:11Z",
    "merged_at": "2023-09-05T07:39:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/233"
  },
  {
    "number": 232,
    "title": "Add NeuralChat tutorial document",
    "user": "ftian1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-04T12:22:36Z",
    "closed_at": "2023-09-05T13:28:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/232"
  },
  {
    "number": 231,
    "title": "update rag readme",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-04T08:34:05Z",
    "closed_at": "2023-09-04T12:20:56Z",
    "merged_at": "2023-09-04T12:20:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/231"
  },
  {
    "number": 230,
    "title": "[Cpp Graph]fix q8 pern QKV fusion of vnni",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-04T06:44:42Z",
    "closed_at": "2023-09-06T03:32:48Z",
    "merged_at": "2023-09-06T03:32:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/230"
  },
  {
    "number": 229,
    "title": "optimize ASR audio to waveform loading",
    "user": "Spycsh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-04T05:50:35Z",
    "closed_at": "2023-09-04T06:23:55Z",
    "merged_at": "2023-09-04T06:23:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/229"
  },
  {
    "number": 228,
    "title": "fix CI",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-04T04:33:11Z",
    "closed_at": "2023-09-04T04:50:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/228"
  },
  {
    "number": 227,
    "title": "Fix checkmarx security issues",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-03T15:24:08Z",
    "closed_at": "2023-09-04T02:42:53Z",
    "merged_at": "2023-09-04T02:42:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/227"
  },
  {
    "number": 226,
    "title": "Fix the bug in rag example (Neural Chat)",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T13:55:47Z",
    "closed_at": "2023-09-04T03:02:57Z",
    "merged_at": "2023-09-04T03:02:57Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/226"
  },
  {
    "number": 225,
    "title": "skip UT for CI",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T08:50:25Z",
    "closed_at": "2023-09-02T15:30:08Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/225"
  },
  {
    "number": 224,
    "title": "fix scan",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T07:21:55Z",
    "closed_at": "2023-09-02T15:15:23Z",
    "merged_at": "2023-09-02T15:15:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/224"
  },
  {
    "number": 223,
    "title": "add TP and gptj model support",
    "user": "ClarkChin08",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T06:29:05Z",
    "closed_at": "2023-09-07T12:37:05Z",
    "merged_at": "2023-09-07T12:37:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/223"
  },
  {
    "number": 222,
    "title": "Refined NeuralChat finetuning config",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T06:24:59Z",
    "closed_at": "2023-09-01T13:18:00Z",
    "merged_at": "2023-09-01T13:18:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/222"
  },
  {
    "number": 221,
    "title": "[CPP Graph] Fix Graph Model quantization on AVX2-only Platforms",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T03:31:30Z",
    "closed_at": "2023-09-01T11:27:49Z",
    "merged_at": "2023-09-01T11:27:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/221"
  },
  {
    "number": 220,
    "title": "update docker clean",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T03:22:25Z",
    "closed_at": "2023-09-01T10:35:49Z",
    "merged_at": "2023-09-01T10:35:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/220"
  },
  {
    "number": 219,
    "title": "fix Dockerfile permissions",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T02:50:53Z",
    "closed_at": "2023-09-07T05:18:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/219"
  },
  {
    "number": 218,
    "title": "change docker image build dir for hpu",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T02:33:04Z",
    "closed_at": "2023-09-01T02:47:05Z",
    "merged_at": "2023-09-01T02:47:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/218"
  },
  {
    "number": 217,
    "title": "neural_chat support torch.float32",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T02:19:15Z",
    "closed_at": "2023-09-01T08:24:29Z",
    "merged_at": "2023-09-01T08:24:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/217"
  },
  {
    "number": 216,
    "title": "add description in neuralchat",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-09-01T02:16:42Z",
    "closed_at": "2023-09-01T04:16:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/216"
  },
  {
    "number": 215,
    "title": "Support int8 llama2 (for demo)",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T14:48:42Z",
    "closed_at": "2023-09-20T03:10:00Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/215"
  },
  {
    "number": 214,
    "title": "Refine Inference Workflow Readme",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T13:59:08Z",
    "closed_at": "2023-08-31T23:37:19Z",
    "merged_at": "2023-08-31T23:37:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/214"
  },
  {
    "number": 213,
    "title": "[CPP Graph] Enable llama2-70b",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T10:07:12Z",
    "closed_at": "2023-09-26T07:31:34Z",
    "merged_at": "2023-09-26T07:31:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/213"
  },
  {
    "number": 212,
    "title": "add SKIP_RUNTIME and RUNTIME_ONLY in setup",
    "user": "xin3he",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T09:35:48Z",
    "closed_at": "2023-09-01T06:17:00Z",
    "merged_at": "2023-09-01T06:17:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/212"
  },
  {
    "number": 211,
    "title": "refine reademe",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T09:28:53Z",
    "closed_at": "2023-09-01T11:38:27Z",
    "merged_at": "2023-09-01T11:38:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/211"
  },
  {
    "number": 210,
    "title": "[CPP Graph] ChatGLM-2 Enabling",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T09:01:07Z",
    "closed_at": "2023-09-08T02:22:05Z",
    "merged_at": "2023-09-08T02:22:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/210"
  },
  {
    "number": 209,
    "title": "refine example for SQ and WOQ",
    "user": "xin3he",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T08:35:35Z",
    "closed_at": "2023-09-08T01:47:22Z",
    "merged_at": "2023-09-08T01:47:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/209"
  },
  {
    "number": 208,
    "title": "fix3rdpartyscan",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T08:27:09Z",
    "closed_at": "2023-09-07T05:18:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/208"
  },
  {
    "number": 207,
    "title": "support bloom for cpp",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T07:39:49Z",
    "closed_at": "2023-09-02T15:46:10Z",
    "merged_at": "2023-09-02T15:46:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/207"
  },
  {
    "number": 206,
    "title": "move neural engine to deprecated",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T06:20:59Z",
    "closed_at": "2023-09-04T09:48:36Z",
    "merged_at": "2023-09-04T09:48:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/206"
  },
  {
    "number": 205,
    "title": "Force CMake to add --std=cxx/c++xx",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T05:35:45Z",
    "closed_at": "2023-09-01T05:27:52Z",
    "merged_at": "2023-09-01T05:27:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/205"
  },
  {
    "number": 204,
    "title": "add finetuning test for mpt-7b-chat with hpu",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T02:15:00Z",
    "closed_at": "2023-08-31T09:51:52Z",
    "merged_at": "2023-08-31T09:51:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/204"
  },
  {
    "number": 203,
    "title": "add one-click script for cpp graph running",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T01:59:21Z",
    "closed_at": "2023-09-08T07:37:44Z",
    "merged_at": "2023-09-08T07:37:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/203"
  },
  {
    "number": 202,
    "title": "llama 70b AutoTP inference",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T01:50:36Z",
    "closed_at": "2023-08-31T14:19:50Z",
    "merged_at": "2023-08-31T14:19:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/202"
  },
  {
    "number": 201,
    "title": "add inference test for llama-2-7b-chat-hf and mpt-7b-chat with hpu",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-31T00:40:50Z",
    "closed_at": "2023-09-01T00:57:41Z",
    "merged_at": "2023-09-01T00:57:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/201"
  },
  {
    "number": 200,
    "title": "Refine reademe of llm runtime",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T14:19:15Z",
    "closed_at": "2023-08-30T15:12:22Z",
    "merged_at": "2023-08-30T15:12:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/200"
  },
  {
    "number": 199,
    "title": "Ut parallal",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T08:25:35Z",
    "closed_at": "2023-09-03T13:17:19Z",
    "merged_at": "2023-09-03T13:17:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/199"
  },
  {
    "number": 198,
    "title": "Update README.md",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T08:09:03Z",
    "closed_at": "2023-08-30T12:12:48Z",
    "merged_at": "2023-08-30T12:12:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/198"
  },
  {
    "number": 197,
    "title": "Update NeuralChat inference with Docker",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T07:24:48Z",
    "closed_at": "2023-09-01T12:03:31Z",
    "merged_at": "2023-09-01T12:03:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/197"
  },
  {
    "number": 196,
    "title": "Update inference.py and generate.py in NeuralChat",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T07:16:09Z",
    "closed_at": "2023-09-08T15:00:17Z",
    "merged_at": "2023-09-08T15:00:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/196"
  },
  {
    "number": 195,
    "title": "Enable NeuralChat Unit Test process",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T07:12:22Z",
    "closed_at": "2023-09-06T15:43:25Z",
    "merged_at": "2023-09-06T15:43:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/195"
  },
  {
    "number": 194,
    "title": "update inference.py and generate.py in NeuralChat",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T07:05:28Z",
    "closed_at": "2023-08-30T07:05:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/194"
  },
  {
    "number": 193,
    "title": "update inference.py and generate.py in NeuralChat",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T06:40:38Z",
    "closed_at": "2023-08-30T07:00:21Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/193"
  },
  {
    "number": 192,
    "title": "refine code-generation example",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T06:36:20Z",
    "closed_at": "2023-08-30T23:27:23Z",
    "merged_at": "2023-08-30T23:27:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/192"
  },
  {
    "number": 191,
    "title": "Fixed import issue for seq2seq examples",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T06:04:37Z",
    "closed_at": "2023-09-05T09:10:39Z",
    "merged_at": "2023-09-05T09:10:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/191"
  },
  {
    "number": 190,
    "title": "update generate.py and readme",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T05:58:19Z",
    "closed_at": "2023-08-30T06:16:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/190"
  },
  {
    "number": 189,
    "title": "add publication and modify UT path",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T05:38:29Z",
    "closed_at": "2023-09-04T04:50:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/189"
  },
  {
    "number": 188,
    "title": "Update README.md",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T05:03:49Z",
    "closed_at": "2023-08-30T23:24:05Z",
    "merged_at": "2023-08-30T23:24:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/188"
  },
  {
    "number": 187,
    "title": "update onednn to v3.3-pc",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T03:17:51Z",
    "closed_at": "2023-09-07T06:07:27Z",
    "merged_at": "2023-09-07T06:07:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/187"
  },
  {
    "number": 186,
    "title": "upgrade transformers/hpu docker image/optimum habana version",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T02:23:27Z",
    "closed_at": "2023-08-30T08:47:35Z",
    "merged_at": "2023-08-30T08:47:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/186"
  },
  {
    "number": 185,
    "title": "update readme path and copy hidden files",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T02:05:23Z",
    "closed_at": "2023-08-30T04:17:19Z",
    "merged_at": "2023-08-30T04:17:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/185"
  },
  {
    "number": 184,
    "title": "Update README.md",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-30T00:02:20Z",
    "closed_at": "2023-08-30T00:54:16Z",
    "merged_at": "2023-08-30T00:54:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/184"
  },
  {
    "number": 182,
    "title": "add SKIP_RUNTIME and RUNTIME_ONLY in setup",
    "user": "xin3he",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-29T07:03:07Z",
    "closed_at": "2023-08-31T06:36:01Z",
    "merged_at": "2023-08-31T06:36:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/182"
  },
  {
    "number": 181,
    "title": "[CPP Graph] add s8 perchannel quant and kernel.",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-29T01:59:16Z",
    "closed_at": "2023-09-01T05:24:19Z",
    "merged_at": "2023-09-01T05:24:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/181"
  },
  {
    "number": 180,
    "title": "Refine transformers model API and update LLM example",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-28T14:36:15Z",
    "closed_at": "2023-08-28T14:40:09Z",
    "merged_at": "2023-08-28T14:40:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/180"
  },
  {
    "number": 179,
    "title": "[CPP Graph] AMX-BF16 MHA with KV update",
    "user": "DDEle",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-28T09:04:11Z",
    "closed_at": "2023-09-07T06:59:18Z",
    "merged_at": "2023-09-07T06:59:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/179"
  },
  {
    "number": 178,
    "title": "add finetuning test for mosaicml-mpt-7b-chat with mpi",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-28T03:23:09Z",
    "closed_at": "2023-08-30T03:00:23Z",
    "merged_at": "2023-08-30T03:00:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/178"
  },
  {
    "number": 177,
    "title": "return correct input len without pad to user",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-28T03:19:27Z",
    "closed_at": "2023-08-30T01:48:15Z",
    "merged_at": "2023-08-30T01:48:15Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/177"
  },
  {
    "number": 176,
    "title": "Fix issues and update README for NeuralChat backend",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-28T02:43:16Z",
    "closed_at": "2023-08-30T01:40:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/176"
  },
  {
    "number": 175,
    "title": "[CPP Graph] Falcon 40B",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-25T08:47:32Z",
    "closed_at": "2023-08-30T07:22:53Z",
    "merged_at": "2023-08-30T07:22:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/175"
  },
  {
    "number": 174,
    "title": "Added QLoRA support in NeuralChat finetuning and refined NeuralChat optimization API.",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-25T06:01:08Z",
    "closed_at": "2023-09-04T07:34:05Z",
    "merged_at": "2023-09-04T07:34:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/174"
  },
  {
    "number": 173,
    "title": "[CPP Graph] Enhance beam search (length_penalty + min_new_tokens)",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-25T03:15:47Z",
    "closed_at": "2023-09-05T02:08:38Z",
    "merged_at": "2023-09-05T02:08:38Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/173"
  },
  {
    "number": 172,
    "title": "fix mpt not support left padding issue.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-25T02:21:43Z",
    "closed_at": "2023-08-25T02:38:06Z",
    "merged_at": "2023-08-25T02:38:06Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/172"
  },
  {
    "number": 171,
    "title": "Revise the plugin structure of neuralchat",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-25T02:15:58Z",
    "closed_at": "2023-09-01T08:09:23Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/171"
  },
  {
    "number": 170,
    "title": "refact folder stracture",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-24T08:47:10Z",
    "closed_at": "2023-08-29T10:33:09Z",
    "merged_at": "2023-08-29T10:33:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/170"
  },
  {
    "number": 169,
    "title": "Improve starcoder README",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-24T06:11:06Z",
    "closed_at": "2023-08-24T06:26:44Z",
    "merged_at": "2023-08-24T06:26:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/169"
  },
  {
    "number": 168,
    "title": "fix double remove issue.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-23T10:15:02Z",
    "closed_at": "2023-08-23T12:37:59Z",
    "merged_at": "2023-08-23T12:37:59Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/168"
  },
  {
    "number": 167,
    "title": "add tutorial document",
    "user": "ftian1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-23T09:55:04Z",
    "closed_at": "2023-09-04T12:20:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/167"
  },
  {
    "number": 166,
    "title": "add finetuning test for mosaicml-mpt-7b-chat",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-23T08:52:21Z",
    "closed_at": "2023-08-25T04:59:19Z",
    "merged_at": "2023-08-25T04:59:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/166"
  },
  {
    "number": 165,
    "title": "Upgrade jblas",
    "user": "luoyu-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-23T07:04:21Z",
    "closed_at": "2023-08-24T08:41:58Z",
    "merged_at": "2023-08-24T08:41:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/165"
  },
  {
    "number": 164,
    "title": "update ci paths",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-23T03:43:13Z",
    "closed_at": "2023-08-25T02:12:46Z",
    "merged_at": "2023-08-25T02:12:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/164"
  },
  {
    "number": 163,
    "title": "asymmetric quantization",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-23T03:09:32Z",
    "closed_at": "2023-08-23T08:21:05Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/163"
  },
  {
    "number": 162,
    "title": "Added bitsandbytes support in NeuralChat optimization and supported inference on NV GPU device.",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-23T02:26:11Z",
    "closed_at": "2023-08-23T03:17:48Z",
    "merged_at": "2023-08-23T03:17:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/162"
  },
  {
    "number": 161,
    "title": "Unify scripts for converting, quantizing and chatting",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-23T02:13:28Z",
    "closed_at": "2023-08-30T06:17:35Z",
    "merged_at": "2023-08-30T06:17:35Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/161"
  },
  {
    "number": 160,
    "title": "[CPP Graph]Enable FFN fusion",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-23T01:57:17Z",
    "closed_at": "2023-09-07T07:02:23Z",
    "merged_at": "2023-09-07T07:02:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/160"
  },
  {
    "number": 159,
    "title": "fix starcoder quantization bug",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-23T01:18:12Z",
    "closed_at": "2023-08-30T01:43:13Z",
    "merged_at": "2023-08-30T01:43:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/159"
  },
  {
    "number": 158,
    "title": "remove hard print.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-22T06:43:29Z",
    "closed_at": "2023-08-22T06:52:20Z",
    "merged_at": "2023-08-22T06:52:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/158"
  },
  {
    "number": 157,
    "title": "add inference test for mosaicml-mpt-7b-chat",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-22T02:13:34Z",
    "closed_at": "2023-08-23T01:35:52Z",
    "merged_at": "2023-08-23T01:35:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/157"
  },
  {
    "number": 156,
    "title": "Update NeuralChat plugins API",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-21T14:00:24Z",
    "closed_at": "2023-08-22T13:28:47Z",
    "merged_at": "2023-08-22T13:28:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/156"
  },
  {
    "number": 155,
    "title": "Enhance text-generation example",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-21T10:40:29Z",
    "closed_at": "2023-08-22T02:48:16Z",
    "merged_at": "2023-08-22T02:48:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/155"
  },
  {
    "number": 154,
    "title": "NeuralChat README refinement",
    "user": "ftian1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-21T09:20:19Z",
    "closed_at": "2023-08-22T08:38:08Z",
    "merged_at": "2023-08-22T08:38:08Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/154"
  },
  {
    "number": 153,
    "title": "Support PEFT model",
    "user": "mengniwang95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-21T09:16:37Z",
    "closed_at": "2023-08-22T08:28:34Z",
    "merged_at": "2023-08-22T08:28:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/153"
  },
  {
    "number": 152,
    "title": "Updated 3rd party versions for compliance",
    "user": "SamanwaySadhu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-18T20:11:47Z",
    "closed_at": "2023-08-21T06:48:13Z",
    "merged_at": "2023-08-21T06:48:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/152"
  },
  {
    "number": 151,
    "title": "Neural Chat API python SDK",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-18T13:56:21Z",
    "closed_at": "2023-08-18T15:07:41Z",
    "merged_at": "2023-08-18T15:07:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/151"
  },
  {
    "number": 150,
    "title": "add RAG (chat with retrieval ) example in NeuralChat",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-18T08:55:49Z",
    "closed_at": "2023-08-18T12:18:28Z",
    "merged_at": "2023-08-18T12:18:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/150"
  },
  {
    "number": 149,
    "title": "Fixthirdparty scan",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-18T01:57:47Z",
    "closed_at": "2023-09-05T05:48:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/149"
  },
  {
    "number": 148,
    "title": "add the sensitive dict and stop dict",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-17T19:17:56Z",
    "closed_at": "2023-08-17T19:19:04Z",
    "merged_at": "2023-08-17T19:19:04Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/148"
  },
  {
    "number": 147,
    "title": "add finetuning lm_eval metric",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-17T12:14:43Z",
    "closed_at": "2023-08-22T04:53:42Z",
    "merged_at": "2023-08-22T04:53:42Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/147"
  },
  {
    "number": 146,
    "title": "Yuxiang/neural chat api",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-17T03:19:08Z",
    "closed_at": "2023-08-17T05:46:46Z",
    "merged_at": "2023-08-17T05:46:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/146"
  },
  {
    "number": 145,
    "title": "make minor revision on the class reference",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-17T02:57:26Z",
    "closed_at": "2023-08-17T03:00:27Z",
    "merged_at": "2023-08-17T03:00:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/145"
  },
  {
    "number": 144,
    "title": "add inference test for llama-2-7b-chat-hf",
    "user": "jiafuzha",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-17T02:50:15Z",
    "closed_at": "2023-08-21T13:10:45Z",
    "merged_at": "2023-08-21T13:10:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/144"
  },
  {
    "number": 143,
    "title": "revision on the file names for NeuralChat and add code description",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-17T01:35:28Z",
    "closed_at": "2023-08-17T01:37:58Z",
    "merged_at": "2023-08-17T01:37:58Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/143"
  },
  {
    "number": 142,
    "title": "add register",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-16T15:07:22Z",
    "closed_at": "2023-08-16T15:16:47Z",
    "merged_at": "2023-08-16T15:16:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/142"
  },
  {
    "number": 141,
    "title": "Fix crash",
    "user": "itayariel1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-16T12:55:04Z",
    "closed_at": "2023-08-18T01:22:11Z",
    "merged_at": "2023-08-18T01:22:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/141"
  },
  {
    "number": 140,
    "title": "support llama2-7b-chat inference with ipex",
    "user": "mengniwang95",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-16T09:59:48Z",
    "closed_at": "2023-08-18T08:40:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/140"
  },
  {
    "number": 139,
    "title": "Update Dockerfile",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-16T08:27:52Z",
    "closed_at": "2023-08-16T08:30:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/139"
  },
  {
    "number": 138,
    "title": "fix bug in benchmark",
    "user": "xin3he",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-16T05:13:19Z",
    "closed_at": "2023-08-16T06:12:05Z",
    "merged_at": "2023-08-16T06:12:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/138"
  },
  {
    "number": 137,
    "title": "[CPP Graph] unify quant and main in application",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-16T03:37:11Z",
    "closed_at": "2023-08-24T02:05:23Z",
    "merged_at": "2023-08-24T02:05:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/137"
  },
  {
    "number": 136,
    "title": "[Graph] avoid duplicated quantization",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-16T03:17:06Z",
    "closed_at": "2023-08-16T08:00:41Z",
    "merged_at": "2023-08-16T08:00:41Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/136"
  },
  {
    "number": 135,
    "title": "add prompt generator and memory",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-16T02:04:19Z",
    "closed_at": "2023-08-16T02:08:07Z",
    "merged_at": "2023-08-16T02:08:07Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/135"
  },
  {
    "number": 134,
    "title": "[Engine] Llama-2-13b-hf Enabling",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-15T08:08:53Z",
    "closed_at": "2023-09-05T02:03:27Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/134"
  },
  {
    "number": 133,
    "title": "[CPP Graph] add opt cpp graph and chat application",
    "user": "zhentaoyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-15T07:41:14Z",
    "closed_at": "2023-09-06T10:20:12Z",
    "merged_at": "2023-09-06T10:20:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/133"
  },
  {
    "number": 132,
    "title": "Neural chat api",
    "user": "Liangyx2",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-15T07:12:30Z",
    "closed_at": "2023-08-15T07:27:31Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/132"
  },
  {
    "number": 131,
    "title": "Fix dockerfile",
    "user": "CeciliaWwq",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-15T02:43:41Z",
    "closed_at": "2023-08-18T08:26:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/131"
  },
  {
    "number": 130,
    "title": "Support bloom for cpp model",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-14T07:44:16Z",
    "closed_at": "2023-09-07T08:18:10Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/130"
  },
  {
    "number": 129,
    "title": "fix pre-ci syntax",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-14T06:08:29Z",
    "closed_at": "2023-08-14T07:52:52Z",
    "merged_at": "2023-08-14T07:52:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/129"
  },
  {
    "number": 128,
    "title": "[pre-ci] remove echo github params",
    "user": "XuehaoSun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-14T04:15:46Z",
    "closed_at": "2023-08-14T04:30:39Z",
    "merged_at": "2023-08-14T04:30:39Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/128"
  },
  {
    "number": 127,
    "title": "add eval_func for alpha tuning and new load for weight-only",
    "user": "xin3he",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-14T04:05:25Z",
    "closed_at": "2023-08-16T04:57:27Z",
    "merged_at": "2023-08-16T04:57:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/127"
  },
  {
    "number": 126,
    "title": "[Graph]Add AMX_BF16 and AMX_INT8 kernels (#1239)",
    "user": "airMeng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-14T02:18:28Z",
    "closed_at": "2023-08-15T00:35:45Z",
    "merged_at": "2023-08-15T00:35:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/126"
  },
  {
    "number": 125,
    "title": "Neural chat api",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-11T10:05:22Z",
    "closed_at": "2023-08-11T10:05:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/125"
  },
  {
    "number": 124,
    "title": "New feature",
    "user": "XuhuiRen",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-11T10:04:48Z",
    "closed_at": "2023-08-11T10:05:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/124"
  },
  {
    "number": 123,
    "title": "amend starcoder input model path",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-11T06:49:43Z",
    "closed_at": "2023-08-15T04:27:44Z",
    "merged_at": "2023-08-15T04:27:44Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/123"
  },
  {
    "number": 122,
    "title": "Update text2text task script",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-11T03:53:24Z",
    "closed_at": "2023-08-15T08:25:17Z",
    "merged_at": "2023-08-15T08:25:17Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/122"
  },
  {
    "number": 121,
    "title": "update contricution.md",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-10T16:48:59Z",
    "closed_at": "2023-08-11T06:26:52Z",
    "merged_at": "2023-08-11T06:26:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/121"
  },
  {
    "number": 120,
    "title": "enhance chatbot code",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-10T12:43:37Z",
    "closed_at": "2023-08-15T02:47:33Z",
    "merged_at": "2023-08-15T02:47:33Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/120"
  },
  {
    "number": 119,
    "title": "fix cpu dockfile problem",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-10T10:10:21Z",
    "closed_at": "2023-08-11T06:53:00Z",
    "merged_at": "2023-08-11T06:53:00Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/119"
  },
  {
    "number": 118,
    "title": "[Engine] ChatGLM-1 & 2 Enabling",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-10T06:10:31Z",
    "closed_at": "2023-09-05T02:03:41Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/118"
  },
  {
    "number": 117,
    "title": "Fixed error of loading model for text2text generation task",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-10T05:25:43Z",
    "closed_at": "2023-08-11T03:18:09Z",
    "merged_at": "2023-08-11T03:18:09Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/117"
  },
  {
    "number": 116,
    "title": "fix oasst dataset issue.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-10T03:00:01Z",
    "closed_at": "2023-08-10T06:55:53Z",
    "merged_at": "2023-08-10T06:55:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/116"
  },
  {
    "number": 115,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-10T00:32:22Z",
    "closed_at": "2023-08-10T01:09:05Z",
    "merged_at": "2023-08-10T01:09:05Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/115"
  },
  {
    "number": 114,
    "title": "support quantized model for generation task in chat",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-09T09:13:27Z",
    "closed_at": "2023-09-06T02:11:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/114"
  },
  {
    "number": 113,
    "title": "[CPP Graph] enable opt series  cpp models",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-09T05:13:26Z",
    "closed_at": "2023-08-15T07:44:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/113"
  },
  {
    "number": 112,
    "title": "Support neural-chat-7b model for chatbot",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-08T14:49:23Z",
    "closed_at": "2023-08-11T12:46:01Z",
    "merged_at": "2023-08-11T12:46:01Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/112"
  },
  {
    "number": 111,
    "title": "Update dockerfile and README for chatbot demo",
    "user": "lvliang-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-08T13:52:13Z",
    "closed_at": "2023-09-08T15:01:32Z",
    "merged_at": "2023-09-08T15:01:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/111"
  },
  {
    "number": 110,
    "title": "fix nb type",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-08T07:43:37Z",
    "closed_at": "2023-08-17T07:14:14Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/110"
  },
  {
    "number": 109,
    "title": "[Engine] Llama-2-7b-chat Enabling",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-08T01:56:03Z",
    "closed_at": "2023-08-08T05:34:12Z",
    "merged_at": "2023-08-08T05:34:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/109"
  },
  {
    "number": 108,
    "title": "update falcon with jblas",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-07T06:35:46Z",
    "closed_at": "2023-08-09T06:40:10Z",
    "merged_at": "2023-08-09T06:40:10Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/108"
  },
  {
    "number": 107,
    "title": "[Engine] ChatGLM-1 Enabling",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-07T01:15:03Z",
    "closed_at": "2023-09-10T14:24:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/107"
  },
  {
    "number": 106,
    "title": "fix benchmark bug of weight-only",
    "user": "xin3he",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-04T08:15:57Z",
    "closed_at": "2023-08-04T09:02:03Z",
    "merged_at": "2023-08-04T09:02:03Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/106"
  },
  {
    "number": 105,
    "title": "enable dolly-v2-3b cpp model",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-04T06:31:51Z",
    "closed_at": "2023-08-09T06:39:31Z",
    "merged_at": "2023-08-09T06:39:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/105"
  },
  {
    "number": 104,
    "title": "refactor chatbot structure and add getting started",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-04T06:29:47Z",
    "closed_at": "2023-08-09T09:17:25Z",
    "merged_at": "2023-08-09T09:17:25Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/104"
  },
  {
    "number": 103,
    "title": "update starcoder with jblas",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-04T05:32:13Z",
    "closed_at": "2023-08-07T07:22:18Z",
    "merged_at": "2023-08-07T07:22:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/103"
  },
  {
    "number": 102,
    "title": "add perf stats in predict_stream",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-04T04:11:51Z",
    "closed_at": "2023-08-09T02:37:49Z",
    "merged_at": "2023-08-09T02:37:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/102"
  },
  {
    "number": 101,
    "title": "Fixed typo for text2text example test script",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-04T03:51:41Z",
    "closed_at": "2023-08-10T01:33:48Z",
    "merged_at": "2023-08-10T01:33:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/101"
  },
  {
    "number": 100,
    "title": "Add Mlperf example minilm_l8 data generate and accuracy test",
    "user": "LJ-underdog",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-04T03:35:04Z",
    "closed_at": "2023-08-04T05:12:37Z",
    "merged_at": "2023-08-04T05:12:37Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/100"
  },
  {
    "number": 99,
    "title": "[LLM] Add UT for INCModelForSeq2SeqLM",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-04T03:26:15Z",
    "closed_at": "2023-11-29T13:40:27Z",
    "merged_at": "2023-11-29T13:40:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/99"
  },
  {
    "number": 98,
    "title": "GPTQ example support",
    "user": "YIYANGCAI",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-03T13:46:20Z",
    "closed_at": "2023-08-31T05:38:31Z",
    "merged_at": "2023-08-31T05:38:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/98"
  },
  {
    "number": 97,
    "title": "fix nctx and scratch buffer for neox and mpt",
    "user": "zhenwei-intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-03T08:37:34Z",
    "closed_at": "2023-08-04T03:37:22Z",
    "merged_at": "2023-08-04T03:37:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/97"
  },
  {
    "number": 96,
    "title": "Create a new package Q_bits for weight_only with JBLAS kernel",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-03T06:53:33Z",
    "closed_at": "2023-09-07T01:01:16Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/96"
  },
  {
    "number": 95,
    "title": "[Engine]QAT Stable Diffusiomn Enabling",
    "user": "Zhenzhong1",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-03T06:43:17Z",
    "closed_at": "2023-08-10T07:17:36Z",
    "merged_at": "2023-08-10T07:17:36Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/95"
  },
  {
    "number": 94,
    "title": "fix symbol.",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-03T03:34:55Z",
    "closed_at": "2023-08-03T12:13:56Z",
    "merged_at": "2023-08-03T12:13:55Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/94"
  },
  {
    "number": 93,
    "title": "Add code finetuning",
    "user": "lkk12014402",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-03T02:44:12Z",
    "closed_at": "2023-08-03T03:23:27Z",
    "merged_at": "2023-08-03T03:23:27Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/93"
  },
  {
    "number": 92,
    "title": "move some operation from main to load_model",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-02T11:00:38Z",
    "closed_at": "2023-08-03T02:59:23Z",
    "merged_at": "2023-08-03T02:59:23Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/92"
  },
  {
    "number": 91,
    "title": "refine example for weigth_only",
    "user": "xin3he",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-02T08:26:42Z",
    "closed_at": "2023-08-04T02:49:47Z",
    "merged_at": "2023-08-04T02:49:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/91"
  },
  {
    "number": 90,
    "title": "Fixed summarization example issue for new version transformers",
    "user": "PenghuiCheng",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-02T03:14:08Z",
    "closed_at": "2023-08-09T06:54:21Z",
    "merged_at": "2023-08-09T06:54:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/90"
  },
  {
    "number": 89,
    "title": "improve code-genereation benchmark",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-02T01:54:19Z",
    "closed_at": "2023-08-02T03:31:47Z",
    "merged_at": "2023-08-02T03:31:47Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/89"
  },
  {
    "number": 88,
    "title": "fix get_train/eval_dataloader batch_size is None",
    "user": "xin3he",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-01T13:45:26Z",
    "closed_at": "2023-08-07T05:28:28Z",
    "merged_at": "2023-08-07T05:28:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/88"
  },
  {
    "number": 87,
    "title": "update publication",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-01T12:30:25Z",
    "closed_at": "2023-08-03T12:07:50Z",
    "merged_at": "2023-08-03T12:07:49Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/87"
  },
  {
    "number": 86,
    "title": "Sync with internal",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-08-01T01:32:09Z",
    "closed_at": "2023-08-01T12:25:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/86"
  },
  {
    "number": 83,
    "title": "Gha test",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-07-28T13:01:36Z",
    "closed_at": "2023-07-31T13:22:32Z",
    "merged_at": "2023-07-31T13:22:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/83"
  },
  {
    "number": 77,
    "title": "Fixd the typo in pruning.md",
    "user": "hardikkamboj",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-07-05T07:13:01Z",
    "closed_at": "2023-09-07T09:47:53Z",
    "merged_at": "2023-09-07T09:47:53Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/77"
  },
  {
    "number": 74,
    "title": "Update requirements.txt",
    "user": "hshen14",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-07-03T23:15:46Z",
    "closed_at": "2023-07-04T01:29:11Z",
    "merged_at": "2023-07-04T01:29:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/74"
  },
  {
    "number": 73,
    "title": "[Engine] Fix typo in model.cpp",
    "user": "eltociear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-07-03T18:32:31Z",
    "closed_at": "2023-08-01T12:33:20Z",
    "merged_at": "2023-08-01T12:33:20Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/73"
  },
  {
    "number": 68,
    "title": "Refactor chatbot code (#972)",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-06-01T03:18:07Z",
    "closed_at": "2023-06-01T03:22:19Z",
    "merged_at": "2023-06-01T03:22:19Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/68"
  },
  {
    "number": 66,
    "title": "Fix the issue of RecursionError when use fast tokenizer",
    "user": "Qianshui-Jiang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-05-25T17:30:29Z",
    "closed_at": "2023-09-13T12:57:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/66"
  },
  {
    "number": 65,
    "title": "[Engine]Refine the gpt-j example code",
    "user": "a32543254",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-05-13T12:58:07Z",
    "closed_at": "2023-05-13T14:48:50Z",
    "merged_at": "2023-05-13T14:48:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/65"
  },
  {
    "number": 64,
    "title": "add example for llm text generation for ipex and torch",
    "user": "mengfei25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-05-11T08:53:13Z",
    "closed_at": "2023-05-12T02:08:35Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/64"
  },
  {
    "number": 63,
    "title": "Create python-package-conda.yml",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-19T09:08:30Z",
    "closed_at": "2023-04-26T01:47:37Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/63"
  },
  {
    "number": 62,
    "title": "update requirement",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-18T01:23:35Z",
    "closed_at": "2023-04-18T01:38:12Z",
    "merged_at": "2023-04-18T01:38:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/62"
  },
  {
    "number": 61,
    "title": "Sync with internal repo",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-10T05:38:23Z",
    "closed_at": "2023-04-12T13:46:16Z",
    "merged_at": "2023-04-12T13:46:16Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/61"
  },
  {
    "number": 60,
    "title": "Refined data load code",
    "user": "XinyuYe-Intel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-10T05:33:32Z",
    "closed_at": "2023-04-10T12:58:35Z",
    "merged_at": "2023-04-10T12:58:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/60"
  },
  {
    "number": 58,
    "title": "Clean data",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-04T16:17:50Z",
    "closed_at": "2023-04-04T16:25:22Z",
    "merged_at": "2023-04-04T16:25:22Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/58"
  },
  {
    "number": 57,
    "title": "update Readme",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-04T12:17:35Z",
    "closed_at": "2023-04-04T15:33:26Z",
    "merged_at": "2023-04-04T15:33:26Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/57"
  },
  {
    "number": 56,
    "title": "V1.0 final update",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-04T06:37:04Z",
    "closed_at": "2023-04-04T09:36:45Z",
    "merged_at": "2023-04-04T09:36:45Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/56"
  },
  {
    "number": 55,
    "title": "clean code for 1.0 release",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-03T12:05:51Z",
    "closed_at": "2023-04-04T06:22:34Z",
    "merged_at": "2023-04-04T06:22:34Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/55"
  },
  {
    "number": 54,
    "title": "delete redundant files for pytorch pruner",
    "user": "n1ck-guo",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-03T07:25:37Z",
    "closed_at": "2023-04-03T08:01:12Z",
    "merged_at": "2023-04-03T08:01:12Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/54"
  },
  {
    "number": 53,
    "title": "V1.0",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-02T16:07:24Z",
    "closed_at": "2023-04-02T17:08:43Z",
    "merged_at": "2023-04-02T17:08:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/53"
  },
  {
    "number": 52,
    "title": "V1.0",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-04-02T15:47:31Z",
    "closed_at": "2023-04-02T15:48:32Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/52"
  },
  {
    "number": 48,
    "title": "Origin/nightly update rebased",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-03-03T02:37:33Z",
    "closed_at": "2023-03-06T03:04:22Z",
    "merged_at": "2023-03-06T03:04:21Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/48"
  },
  {
    "number": 47,
    "title": "Nightly docs update",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-23T06:52:11Z",
    "closed_at": "2023-02-23T07:08:43Z",
    "merged_at": "2023-02-23T07:08:43Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/47"
  },
  {
    "number": 46,
    "title": "add casual language modeling for GPTJ",
    "user": "changwangss",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-15T06:57:49Z",
    "closed_at": "2023-04-26T01:22:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/46"
  },
  {
    "number": 45,
    "title": "add bloom and minor fix",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-15T05:05:00Z",
    "closed_at": "2023-02-15T08:45:50Z",
    "merged_at": "2023-02-15T08:45:50Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/45"
  },
  {
    "number": 44,
    "title": "Update README.md",
    "user": "yao-matrix",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-15T03:29:43Z",
    "closed_at": "2023-02-15T03:42:52Z",
    "merged_at": "2023-02-15T03:42:52Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/44"
  },
  {
    "number": 43,
    "title": "add warmup and greedy to gptj",
    "user": "mengfei25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-14T08:54:36Z",
    "closed_at": "2023-02-14T11:09:31Z",
    "merged_at": "2023-02-14T11:09:31Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/43"
  },
  {
    "number": 42,
    "title": "add inference case for bloom and update readme",
    "user": "sywangyi",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-14T07:26:05Z",
    "closed_at": "2023-02-14T13:37:56Z",
    "merged_at": "2023-02-14T13:37:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/42"
  },
  {
    "number": 41,
    "title": "modify run_gptj.py to setup default params",
    "user": "mengfei25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-14T05:26:22Z",
    "closed_at": "2023-02-14T05:32:32Z",
    "merged_at": "2023-02-14T05:32:32Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/41"
  },
  {
    "number": 40,
    "title": "add gpt-j max tokens settting",
    "user": "mengfei25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-14T03:52:21Z",
    "closed_at": "2023-02-14T04:44:56Z",
    "merged_at": "2023-02-14T04:44:56Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/40"
  },
  {
    "number": 39,
    "title": "update main page",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-13T16:48:19Z",
    "closed_at": "2023-02-14T05:20:55Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/39"
  },
  {
    "number": 38,
    "title": "Update readme",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-13T16:33:40Z",
    "closed_at": "2023-02-14T05:20:51Z",
    "merged_at": "2023-02-14T05:20:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/38"
  },
  {
    "number": 37,
    "title": "add pytorch gptj inference scripts",
    "user": "mengfei25",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-12T16:34:17Z",
    "closed_at": "2023-02-13T06:24:18Z",
    "merged_at": "2023-02-13T06:24:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/37"
  },
  {
    "number": 36,
    "title": "Nightly update",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-12T16:03:41Z",
    "closed_at": "2023-02-13T01:26:13Z",
    "merged_at": "2023-02-13T01:26:13Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/36"
  },
  {
    "number": 35,
    "title": "refine docs links",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-09T05:23:21Z",
    "closed_at": "2023-02-09T06:06:18Z",
    "merged_at": "2023-02-09T06:06:18Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/35"
  },
  {
    "number": 34,
    "title": "Nightly update",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2023-02-09T04:39:12Z",
    "closed_at": "2023-02-09T05:06:29Z",
    "merged_at": "2023-02-09T05:06:29Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/34"
  },
  {
    "number": 33,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/sst2/distilbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:10:14Z",
    "closed_at": "2023-02-13T02:41:50Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/33"
  },
  {
    "number": 32,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/squad/bert_large",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:50Z",
    "closed_at": "2023-02-13T02:40:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/32"
  },
  {
    "number": 31,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/mrpc/distilbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:46Z",
    "closed_at": "2023-02-13T02:40:44Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/31"
  },
  {
    "number": 30,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/sst2/bert_mini",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:43Z",
    "closed_at": "2023-02-13T02:41:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/30"
  },
  {
    "number": 29,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/mrpc/roberta_base",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:41Z",
    "closed_at": "2023-02-13T02:41:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/29"
  },
  {
    "number": 28,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/imagenet/vit",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:38Z",
    "closed_at": "2023-02-13T02:42:34Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/28"
  },
  {
    "number": 27,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/sparse/bert_mini",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:31Z",
    "closed_at": "2023-02-13T02:41:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/27"
  },
  {
    "number": 26,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/sst2/minilm_l6_h384_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:31Z",
    "closed_at": "2023-02-13T02:41:22Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/26"
  },
  {
    "number": 25,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/mrpc/bert_base_cased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:30Z",
    "closed_at": "2023-02-13T02:41:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/25"
  },
  {
    "number": 24,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/emotion/distilbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:30Z",
    "closed_at": "2023-02-13T02:41:11Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/24"
  },
  {
    "number": 23,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/squad/length_adaptive_transformer",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:28Z",
    "closed_at": "2023-02-13T02:40:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/23"
  },
  {
    "number": 22,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/mrpc/bert_mini",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:27Z",
    "closed_at": "2023-02-13T02:40:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/22"
  },
  {
    "number": 21,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/sparse/distilbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:27Z",
    "closed_at": "2023-02-13T02:41:49Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/21"
  },
  {
    "number": 20,
    "title": "Bump onnx from 1.12.0 to 1.13.0 in /examples/deployment/neural_engine/mrpc/bert_base",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-27T01:09:26Z",
    "closed_at": "2023-02-13T02:40:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/20"
  },
  {
    "number": 19,
    "title": "Bump torch from 1.13.0 to 1.13.1 in /examples/deployment/neural_engine/sst2/distilbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-17T09:03:01Z",
    "closed_at": "2023-02-09T05:09:25Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/19"
  },
  {
    "number": 18,
    "title": "Bump torch from 1.12.0 to 1.13.1 in /examples/deployment/ipex/squad/bert_large",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-17T09:02:51Z",
    "closed_at": "2023-02-09T05:09:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/18"
  },
  {
    "number": 17,
    "title": "Bump torch from 1.12.0 to 1.13.1 in /examples/deployment/ipex/squad/distillbert_base_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-17T09:02:50Z",
    "closed_at": "2023-02-09T05:09:19Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/17"
  },
  {
    "number": 16,
    "title": "Bump torch from 1.13.0 to 1.13.1 in /examples/deployment/neural_engine/squad/bert_large",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-17T09:02:46Z",
    "closed_at": "2023-02-09T05:08:58Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/16"
  },
  {
    "number": 15,
    "title": "Bump torch from 1.12.0 to 1.13.1 in /examples/deployment/ipex/squad/distillbert_base_uncased_sparse",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-17T09:02:45Z",
    "closed_at": "2023-02-09T05:08:47Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/15"
  },
  {
    "number": 14,
    "title": "Bump torch from 1.13.0 to 1.13.1 in /examples/deployment/neural_engine/imagenet/vit",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-17T09:02:44Z",
    "closed_at": "2023-02-09T05:09:20Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/14"
  },
  {
    "number": 13,
    "title": "Bump torch from 1.11.0 to 1.13.1 in /examples/deployment/neural_engine/squad/length_adaptive_transformer",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-17T09:02:43Z",
    "closed_at": "2023-02-09T05:09:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/13"
  },
  {
    "number": 12,
    "title": "Bump torch from 1.13.0 to 1.13.1 in /examples/deployment/neural_engine/sst2/bert_mini",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-17T09:02:42Z",
    "closed_at": "2023-02-09T05:09:01Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/12"
  },
  {
    "number": 11,
    "title": "Bump torch from 1.13.0 to 1.13.1 in /examples/deployment/neural_engine/sst2/minilm_l6_h384_uncased",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-17T09:02:41Z",
    "closed_at": "2023-02-09T05:08:51Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/11"
  },
  {
    "number": 10,
    "title": "Bump torch from 1.13.0 to 1.13.1 in /examples/optimization/pytorch/huggingface/question-answering/dynamic",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2023-01-17T09:02:41Z",
    "closed_at": "2023-02-09T05:08:43Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/10"
  },
  {
    "number": 9,
    "title": "rm unused files",
    "user": "NeoZhangJianyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2022-12-22T08:20:05Z",
    "closed_at": "2022-12-22T08:20:11Z",
    "merged_at": "2022-12-22T08:20:11Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/9"
  },
  {
    "number": 8,
    "title": "clear gh-pages",
    "user": "NeoZhangJianyu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2022-12-22T08:17:15Z",
    "closed_at": "2022-12-22T08:17:28Z",
    "merged_at": "2022-12-22T08:17:28Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/8"
  },
  {
    "number": 7,
    "title": "Bump tensorflow from 2.10.0 to 2.10.1 in /intel_extension_for_transformers/backends/neural_engine/test/pytest",
    "user": "dependabot[bot]",
    "user_type": "Bot",
    "is_human": false,
    "created_at": "2022-12-10T14:50:36Z",
    "closed_at": "2022-12-11T14:05:26Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/7"
  },
  {
    "number": 5,
    "title": "update blog in README",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2022-12-06T13:11:58Z",
    "closed_at": "2022-12-06T13:12:51Z",
    "merged_at": "2022-12-06T13:12:51Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/5"
  },
  {
    "number": 4,
    "title": "update example link",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2022-11-30T08:12:51Z",
    "closed_at": "2022-11-30T08:14:46Z",
    "merged_at": "2022-11-30T08:14:46Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/4"
  },
  {
    "number": 2,
    "title": "V0.4rc2",
    "user": "kevinintel",
    "user_type": "User",
    "is_human": true,
    "created_at": "2022-11-24T12:08:31Z",
    "closed_at": "2022-11-24T12:15:56Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/2"
  },
  {
    "number": 1,
    "title": "fix docs format issue",
    "user": "VincyZhang",
    "user_type": "User",
    "is_human": true,
    "created_at": "2022-11-24T11:58:23Z",
    "closed_at": "2022-11-24T11:58:48Z",
    "merged_at": "2022-11-24T11:58:48Z",
    "state": "closed",
    "html_url": "https://github.com/intel/intel-extension-for-transformers/pull/1"
  }
]