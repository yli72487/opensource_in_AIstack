[
  {
    "number": 143,
    "title": "upgrade vllm to v0.4.1",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-17T06:16:30Z",
    "closed_at": "2024-05-17T07:11:43Z",
    "merged_at": "2024-05-17T07:11:43Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/143"
  },
  {
    "number": 142,
    "title": "address apiserver to standalone file",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-15T01:50:52Z",
    "closed_at": "2024-05-17T07:12:30Z",
    "merged_at": "2024-05-17T07:12:30Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/142"
  },
  {
    "number": 141,
    "title": "add description for TODO to avoid lost the context",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-14T02:36:03Z",
    "closed_at": "2024-05-14T02:41:11Z",
    "merged_at": "2024-05-14T02:41:11Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/141"
  },
  {
    "number": 140,
    "title": "fix ui concurrency setting",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-14T02:21:43Z",
    "closed_at": "2024-05-14T02:25:28Z",
    "merged_at": "2024-05-14T02:25:28Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/140"
  },
  {
    "number": 139,
    "title": "upgrade to pydantic v2 -_-",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-14T01:37:02Z",
    "closed_at": "2024-05-14T01:45:28Z",
    "merged_at": "2024-05-14T01:45:28Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/139"
  },
  {
    "number": 138,
    "title": "Update opt-125m default autoscaler parameters to help understand for end user",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-09T14:59:14Z",
    "closed_at": "2024-05-14T02:44:10Z",
    "merged_at": "2024-05-14T02:44:10Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/138"
  },
  {
    "number": 136,
    "title": "Update files of auto scaling on k8s ",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-08T09:01:41Z",
    "closed_at": "2024-05-08T09:07:54Z",
    "merged_at": "2024-05-08T09:07:54Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/136"
  },
  {
    "number": 135,
    "title": "Serve run in thread",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-08T03:27:23Z",
    "closed_at": "2024-05-08T03:36:48Z",
    "merged_at": "2024-05-08T03:36:48Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/135"
  },
  {
    "number": 133,
    "title": "Auto load models when api server start ",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-07T06:58:46Z",
    "closed_at": "2024-05-08T00:27:39Z",
    "merged_at": "2024-05-08T00:27:39Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/133"
  },
  {
    "number": 131,
    "title": "Upgrade ray 2.20.0",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-06T06:38:22Z",
    "closed_at": "2024-05-06T06:47:27Z",
    "merged_at": "2024-05-06T06:47:27Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/131"
  },
  {
    "number": 129,
    "title": "lock vllm and xformers version to fix conflict",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-02T03:03:21Z",
    "closed_at": "2024-05-06T01:19:42Z",
    "merged_at": "2024-05-06T01:19:42Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/129"
  },
  {
    "number": 127,
    "title": "Add csg wukong model",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-30T01:44:10Z",
    "closed_at": "2024-04-30T05:37:53Z",
    "merged_at": "2024-04-30T05:37:53Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/127"
  },
  {
    "number": 126,
    "title": "Enable tensor paramlism for deepspeed",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-29T05:21:34Z",
    "closed_at": "2024-04-29T05:52:51Z",
    "merged_at": "2024-04-29T05:52:51Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/126"
  },
  {
    "number": 125,
    "title": "Add new api in openai style",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-27T14:46:32Z",
    "closed_at": "2024-04-28T23:42:14Z",
    "merged_at": "2024-04-28T23:42:14Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/125"
  },
  {
    "number": 124,
    "title": "correct generated metric",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-25T07:53:57Z",
    "closed_at": "2024-04-25T14:53:45Z",
    "merged_at": "2024-04-25T14:53:45Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/124"
  },
  {
    "number": 122,
    "title": "add llama3-8b from csghub",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-24T06:04:36Z",
    "closed_at": "2024-04-24T06:10:31Z",
    "merged_at": "2024-04-24T06:10:31Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/122"
  },
  {
    "number": 121,
    "title": "avoid to invoke hf to speed up deployment process",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-24T05:36:49Z",
    "closed_at": "2024-04-24T06:11:16Z",
    "merged_at": "2024-04-24T06:11:16Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/121"
  },
  {
    "number": 119,
    "title": "Load path model issue",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-23T08:02:40Z",
    "closed_at": "2024-04-23T10:50:42Z",
    "merged_at": "2024-04-23T10:50:42Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/119"
  },
  {
    "number": 118,
    "title": "the pipeline integration cannot address pad_token/eos_token absent",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-23T03:01:03Z",
    "closed_at": "2024-04-23T03:12:02Z",
    "merged_at": "2024-04-23T03:12:02Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/118"
  },
  {
    "number": 115,
    "title": "support vllm on-fly generate params",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-22T08:52:54Z",
    "closed_at": "2024-04-23T01:04:26Z",
    "merged_at": "2024-04-23T01:04:26Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/115"
  },
  {
    "number": 114,
    "title": "update doc for load model from local path",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-22T01:41:41Z",
    "closed_at": "2024-04-22T02:16:00Z",
    "merged_at": "2024-04-22T02:16:00Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/114"
  },
  {
    "number": 113,
    "title": "suport on-fly generate params",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-21T10:28:15Z",
    "closed_at": "2024-04-22T01:06:34Z",
    "merged_at": "2024-04-22T01:06:34Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/113"
  },
  {
    "number": 112,
    "title": "change default static batch setting",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-20T23:45:48Z",
    "closed_at": "2024-04-21T02:37:56Z",
    "merged_at": "2024-04-21T02:37:56Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/112"
  },
  {
    "number": 111,
    "title": "single prompt will failed in streming",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-18T14:18:17Z",
    "closed_at": "2024-04-19T01:32:19Z",
    "merged_at": "2024-04-19T01:32:19Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/111"
  },
  {
    "number": 110,
    "title": "simplify readme",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-18T08:16:16Z",
    "closed_at": "2024-04-18T13:07:14Z",
    "merged_at": "2024-04-18T13:07:14Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/110"
  },
  {
    "number": 109,
    "title": "recover the text-classification and summarization downstream task sup…",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T11:35:51Z",
    "closed_at": "2024-04-18T13:05:55Z",
    "merged_at": "2024-04-18T13:05:55Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/109"
  },
  {
    "number": 108,
    "title": "question-answer downstream task not work since the input-output format wrong",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T08:51:23Z",
    "closed_at": "2024-04-17T10:19:42Z",
    "merged_at": "2024-04-17T10:19:42Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/108"
  },
  {
    "number": 107,
    "title": "fix llamacpp(gguf) broked by \"revision\"",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T08:02:06Z",
    "closed_at": "2024-04-17T10:17:51Z",
    "merged_at": "2024-04-17T10:17:51Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/107"
  },
  {
    "number": 106,
    "title": "Refine model config yamls",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T07:49:33Z",
    "closed_at": "2024-04-17T10:17:15Z",
    "merged_at": "2024-04-17T10:17:15Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/106"
  },
  {
    "number": 105,
    "title": "translation model broken since wrong handling of its output",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T05:43:32Z",
    "closed_at": "2024-04-17T10:18:25Z",
    "merged_at": "2024-04-17T10:18:25Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/105"
  },
  {
    "number": 102,
    "title": "deepspeed cannot work, since the input token not addressed on right device",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-16T12:55:43Z",
    "closed_at": "2024-04-16T14:00:53Z",
    "merged_at": "2024-04-16T14:00:53Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/102"
  },
  {
    "number": 101,
    "title": "update quickstart.md to remove evaluate",
    "user": "wanggxa",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-16T07:32:50Z",
    "closed_at": "2024-04-16T07:35:27Z",
    "merged_at": "2024-04-16T07:35:27Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/101"
  },
  {
    "number": 96,
    "title": "fix llm-serve list",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-15T12:42:02Z",
    "closed_at": "2024-04-16T01:25:20Z",
    "merged_at": "2024-04-16T01:25:20Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/96"
  },
  {
    "number": 95,
    "title": "refine cli, make cli self-explanatory",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-15T11:51:01Z",
    "closed_at": "2024-04-15T12:13:31Z",
    "merged_at": "2024-04-15T12:13:31Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/95"
  },
  {
    "number": 94,
    "title": "support revision to aviod download latest version of mode",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-15T07:29:53Z",
    "closed_at": "2024-04-15T09:34:34Z",
    "merged_at": "2024-04-15T09:34:34Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/94"
  },
  {
    "number": 93,
    "title": "remove deprecated params: stream",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-14T13:51:16Z",
    "closed_at": "2024-04-15T09:32:55Z",
    "merged_at": "2024-04-15T09:32:55Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/93"
  },
  {
    "number": 91,
    "title": "Support streaming in vllm integration",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-13T14:30:30Z",
    "closed_at": "2024-04-14T01:16:03Z",
    "merged_at": "2024-04-14T01:16:03Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/91"
  },
  {
    "number": 90,
    "title": "UI not support static batch",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-12T08:06:59Z",
    "closed_at": "2024-04-14T01:15:10Z",
    "merged_at": "2024-04-14T01:15:10Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/90"
  },
  {
    "number": 89,
    "title": "fix issue: loading from local folder",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-12T03:34:15Z",
    "closed_at": "2024-04-12T04:34:09Z",
    "merged_at": "2024-04-12T04:34:09Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/89"
  },
  {
    "number": 88,
    "title": "fix issue: vllm cannot address runtime_env",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-11T07:51:11Z",
    "closed_at": "2024-04-12T04:32:50Z",
    "merged_at": "2024-04-12T04:32:50Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/88"
  },
  {
    "number": 86,
    "title": "Refine description of repo",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-11T03:26:22Z",
    "closed_at": "2024-04-11T03:27:29Z",
    "merged_at": "2024-04-11T03:27:28Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/86"
  },
  {
    "number": 85,
    "title": "adopt streaming for ui with text-generation downstream task",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-10T14:24:45Z",
    "closed_at": "2024-04-11T01:46:34Z",
    "merged_at": "2024-04-11T01:46:34Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/85"
  },
  {
    "number": 84,
    "title": "fix issue: non-support streaming pipeline cannot work when call it as stream",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-10T05:36:54Z",
    "closed_at": "2024-04-10T05:57:45Z",
    "merged_at": "2024-04-10T05:57:45Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/84"
  },
  {
    "number": 83,
    "title": "enhance llamacpp integration to share soma logic between streaming and predict",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-10T05:08:19Z",
    "closed_at": "2024-04-10T05:31:34Z",
    "merged_at": "2024-04-10T05:31:34Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/83"
  },
  {
    "number": 82,
    "title": "Refactor streaming",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-09T07:59:31Z",
    "closed_at": "2024-04-09T08:24:54Z",
    "merged_at": "2024-04-09T08:24:54Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/82"
  },
  {
    "number": 81,
    "title": "Fix prompt is not string bug",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-08T02:33:33Z",
    "closed_at": "2024-04-08T06:38:35Z",
    "merged_at": "2024-04-08T06:38:35Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/81"
  },
  {
    "number": 80,
    "title": "fix issue: stream generation is slow",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-07T08:07:05Z",
    "closed_at": "2024-04-07T08:12:35Z",
    "merged_at": "2024-04-07T08:12:35Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/80"
  },
  {
    "number": 79,
    "title": "enhance name of router for comparation scenario",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-07T06:22:29Z",
    "closed_at": "2024-04-07T06:23:54Z",
    "merged_at": "2024-04-07T06:23:54Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/79"
  },
  {
    "number": 78,
    "title": "Fix path params issue, make interface consistent",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-07T06:06:37Z",
    "closed_at": "2024-04-07T06:07:27Z",
    "merged_at": "2024-04-07T06:07:27Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/78"
  },
  {
    "number": 77,
    "title": "update log",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-07T05:38:30Z",
    "closed_at": "2024-04-07T05:39:57Z",
    "merged_at": "2024-04-07T05:39:57Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/77"
  },
  {
    "number": 76,
    "title": "Updata logs",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-07T05:21:10Z",
    "closed_at": "2024-04-07T05:34:42Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/76"
  },
  {
    "number": 75,
    "title": "Fix stream without prompt format",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-07T04:44:52Z",
    "closed_at": "2024-04-07T05:05:50Z",
    "merged_at": "2024-04-07T05:05:50Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/75"
  },
  {
    "number": 74,
    "title": "fix  generate bug for stream  api of llamacpp",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-06T14:41:04Z",
    "closed_at": "2024-04-07T03:07:11Z",
    "merged_at": "2024-04-07T03:07:11Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/74"
  },
  {
    "number": 73,
    "title": "correct vllm version",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-06T13:46:19Z",
    "closed_at": "2024-04-06T14:41:34Z",
    "merged_at": "2024-04-06T14:41:34Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/73"
  },
  {
    "number": 71,
    "title": "add Qwen1.5-72B-GGUF yaml and fix load json input error",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-05T03:48:00Z",
    "closed_at": "2024-04-06T08:56:08Z",
    "merged_at": "2024-04-06T08:56:08Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/71"
  },
  {
    "number": 70,
    "title": "Make scale out policy consistent between deployments",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-03T13:57:05Z",
    "closed_at": "2024-04-04T03:24:31Z",
    "merged_at": "2024-04-04T03:24:31Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/70"
  },
  {
    "number": 69,
    "title": "keep removing deprecated stuff",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-03T09:32:35Z",
    "closed_at": "2024-04-03T10:48:20Z",
    "merged_at": "2024-04-03T10:48:20Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/69"
  },
  {
    "number": 66,
    "title": "add streaming API support",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T13:05:18Z",
    "closed_at": "2024-04-01T13:38:24Z",
    "merged_at": "2024-04-01T13:38:24Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/66"
  },
  {
    "number": 65,
    "title": "Enable chat template applied for vllm integration",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T07:53:03Z",
    "closed_at": "2024-04-01T09:18:56Z",
    "merged_at": "2024-04-01T09:18:56Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/65"
  },
  {
    "number": 64,
    "title": "update Qwen1.5-72B yaml",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T06:08:56Z",
    "closed_at": "2024-04-01T06:10:16Z",
    "merged_at": "2024-04-01T06:10:16Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/64"
  },
  {
    "number": 63,
    "title": "Fix json format issue for \"transformerpipeline\"",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T05:34:28Z",
    "closed_at": "2024-04-01T05:35:30Z",
    "merged_at": "2024-04-01T05:35:30Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/63"
  },
  {
    "number": 62,
    "title": "fix load json data with '\\n' failed",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T03:05:47Z",
    "closed_at": "2024-04-01T03:14:11Z",
    "merged_at": "2024-04-01T03:14:11Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/62"
  },
  {
    "number": 61,
    "title": "Remove the original implements for vllm integration",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T00:26:19Z",
    "closed_at": "2024-04-01T00:31:39Z",
    "merged_at": "2024-04-01T00:31:39Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/61"
  },
  {
    "number": 60,
    "title": "Refactor the solution of vllm integration",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-31T09:08:08Z",
    "closed_at": "2024-03-31T09:17:54Z",
    "merged_at": "2024-03-31T09:17:54Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/60"
  },
  {
    "number": 58,
    "title": "remove useless stuff",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T14:44:18Z",
    "closed_at": "2024-03-28T14:48:09Z",
    "merged_at": "2024-03-28T14:48:08Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/58"
  },
  {
    "number": 57,
    "title": "enable prompt template for gguf format inference",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T08:27:59Z",
    "closed_at": "2024-03-28T14:47:22Z",
    "merged_at": "2024-03-28T14:47:22Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/57"
  },
  {
    "number": 56,
    "title": "Update ray to 2.9.3",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T14:03:37Z",
    "closed_at": "2024-03-27T23:46:42Z",
    "merged_at": "2024-03-27T23:46:42Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/56"
  },
  {
    "number": 54,
    "title": "Enable chat template for huggingface transformer",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-27T07:07:04Z",
    "closed_at": "2024-03-27T07:13:12Z",
    "merged_at": "2024-03-27T07:13:12Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/54"
  },
  {
    "number": 51,
    "title": "enable \"use_bettertransformer\" and \"torch_compile\"",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-26T09:19:08Z",
    "closed_at": "2024-03-26T11:00:57Z",
    "merged_at": "2024-03-26T11:00:57Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/51"
  },
  {
    "number": 50,
    "title": "fix output issue 4 ui",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-26T05:33:08Z",
    "closed_at": "2024-03-26T11:00:17Z",
    "merged_at": "2024-03-26T11:00:17Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/50"
  },
  {
    "number": 49,
    "title": "fix max-token conflict w/ DS",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-26T01:11:17Z",
    "closed_at": "2024-03-26T10:59:30Z",
    "merged_at": "2024-03-26T10:59:30Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/49"
  },
  {
    "number": 47,
    "title": "enable deepspeed inference",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T05:57:29Z",
    "closed_at": "2024-03-25T05:59:58Z",
    "merged_at": "2024-03-25T05:59:57Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/47"
  },
  {
    "number": 46,
    "title": "add parameter for timeout",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T05:52:25Z",
    "closed_at": "2024-03-25T05:53:59Z",
    "merged_at": "2024-03-25T05:53:58Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/46"
  },
  {
    "number": 44,
    "title": "Add Qwen1.5-72B-chat",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-25T04:36:05Z",
    "closed_at": "2024-03-25T05:36:10Z",
    "merged_at": "2024-03-25T05:36:10Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/44"
  },
  {
    "number": 43,
    "title": "Update deepseek yaml file",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-24T04:22:22Z",
    "closed_at": "2024-03-24T04:33:56Z",
    "merged_at": "2024-03-24T04:33:56Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/43"
  },
  {
    "number": 42,
    "title": "Update config.py for new model",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-22T13:03:58Z",
    "closed_at": "2024-03-22T13:07:40Z",
    "merged_at": "2024-03-22T13:07:40Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/42"
  },
  {
    "number": 41,
    "title": "fix issue caused by huggingface pipeline with text-generation",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-22T08:26:30Z",
    "closed_at": "2024-03-22T08:39:41Z",
    "merged_at": "2024-03-22T08:39:41Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/41"
  },
  {
    "number": 40,
    "title": "remove some abandoned implements",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-21T00:51:25Z",
    "closed_at": "2024-03-21T00:53:09Z",
    "merged_at": "2024-03-21T00:53:09Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/40"
  },
  {
    "number": 39,
    "title": "fix broken yamls",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T13:07:23Z",
    "closed_at": "2024-03-20T13:08:59Z",
    "merged_at": "2024-03-20T13:08:59Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/39"
  },
  {
    "number": 38,
    "title": "add opencsg-deepseek-coder-1.3b",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T11:39:19Z",
    "closed_at": "2024-03-20T13:09:14Z",
    "merged_at": "2024-03-20T13:09:14Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/38"
  },
  {
    "number": 36,
    "title": "refactor the \"defaulttransformers\" to meet the common design of class \"pipeline\"",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-20T02:56:40Z",
    "closed_at": "2024-03-20T11:18:16Z",
    "merged_at": "2024-03-20T11:18:16Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/36"
  },
  {
    "number": 35,
    "title": "refine config for model: deepseek-coder-1.3b",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-19T12:11:31Z",
    "closed_at": "2024-03-20T11:17:44Z",
    "merged_at": "2024-03-20T11:17:44Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/35"
  },
  {
    "number": 34,
    "title": "devicemap not work on mps, since put data in wrong device, fix it",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-18T23:51:54Z",
    "closed_at": "2024-03-19T00:40:57Z",
    "merged_at": "2024-03-19T00:40:57Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/34"
  },
  {
    "number": 33,
    "title": "Device map not work with macos",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-18T13:50:43Z",
    "closed_at": "2024-03-18T14:11:46Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/33"
  },
  {
    "number": 32,
    "title": "fix bug for set pad_token",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-18T13:22:26Z",
    "closed_at": "2024-03-18T13:27:49Z",
    "merged_at": "2024-03-18T13:27:49Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/32"
  },
  {
    "number": 31,
    "title": "Break non text generation model by warmup",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-18T07:16:51Z",
    "closed_at": "2024-03-18T07:30:03Z",
    "merged_at": "2024-03-18T07:30:03Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/31"
  },
  {
    "number": 30,
    "title": "Enable warmup for defaulttransformers",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-17T09:48:53Z",
    "closed_at": "2024-03-18T02:00:14Z",
    "merged_at": "2024-03-18T02:00:14Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/30"
  },
  {
    "number": 29,
    "title": "Output pipeline format not work",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-17T07:26:27Z",
    "closed_at": "2024-03-17T07:34:59Z",
    "merged_at": "2024-03-17T07:34:59Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/29"
  },
  {
    "number": 28,
    "title": "Fix issue from_pretrain has no parameter device",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-17T02:51:20Z",
    "closed_at": "2024-03-17T08:29:28Z",
    "merged_at": "2024-03-17T08:29:28Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/28"
  },
  {
    "number": 27,
    "title": "Warmup mechanism",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-16T09:19:08Z",
    "closed_at": "2024-03-16T13:25:50Z",
    "merged_at": "2024-03-16T13:25:50Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/27"
  },
  {
    "number": 26,
    "title": "Remove initializer: transformerpipeline",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-16T03:55:52Z",
    "closed_at": "2024-03-16T09:13:29Z",
    "merged_at": "2024-03-16T09:13:29Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/26"
  },
  {
    "number": 25,
    "title": "Push image to opencsg registry",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-16T01:47:25Z",
    "closed_at": "2024-03-16T01:57:42Z",
    "merged_at": "2024-03-16T01:57:42Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/25"
  },
  {
    "number": 24,
    "title": "Refine some parameters for initialization(will keep refining...) and fix Qwen issues",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-15T12:54:35Z",
    "closed_at": "2024-03-16T02:04:47Z",
    "merged_at": "2024-03-16T02:04:47Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/24"
  },
  {
    "number": 22,
    "title": "Update document for download model from OpenCSG model hub",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-15T08:05:06Z",
    "closed_at": "2024-03-15T08:07:45Z",
    "merged_at": "2024-03-15T08:07:45Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/22"
  },
  {
    "number": 21,
    "title": "Remove default creation of ddp, since have no idea if it's a case for…",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T07:54:35Z",
    "closed_at": "2024-03-16T02:06:15Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/21"
  },
  {
    "number": 19,
    "title": "Enable multiple workers cooperated on batch prompt",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-13T01:07:20Z",
    "closed_at": "2024-03-13T07:39:43Z",
    "merged_at": "2024-03-13T07:39:43Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/19"
  },
  {
    "number": 18,
    "title": "add model opencsg-deepseek-coder-1.3b-v0.1",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-12T13:58:51Z",
    "closed_at": "2024-03-13T07:46:06Z",
    "merged_at": "2024-03-13T07:46:06Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/18"
  },
  {
    "number": 17,
    "title": "Fix default pipeline output problem",
    "user": "jasonhe258",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-11T07:28:09Z",
    "closed_at": "2024-03-11T07:34:21Z",
    "merged_at": "2024-03-11T07:34:21Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/17"
  },
  {
    "number": 13,
    "title": "Remove useless code about fine tuning",
    "user": "jasonhe258",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-10T07:19:03Z",
    "closed_at": "2024-03-10T07:48:40Z",
    "merged_at": "2024-03-10T07:48:40Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/13"
  },
  {
    "number": 12,
    "title": "format python code using autopep8 and add pylint",
    "user": "jasonhe258",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-09T10:23:12Z",
    "closed_at": "2024-03-11T00:57:27Z",
    "merged_at": "2024-03-11T00:57:27Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/12"
  },
  {
    "number": 11,
    "title": "update project url in readme",
    "user": "jasonhe258",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-08T09:53:26Z",
    "closed_at": "2024-03-08T09:54:01Z",
    "merged_at": "2024-03-08T09:54:01Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/11"
  },
  {
    "number": 10,
    "title": "Add Developer Guide",
    "user": "jasonhe258",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-08T07:14:29Z",
    "closed_at": "2024-03-09T10:15:24Z",
    "merged_at": "2024-03-09T10:15:24Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/10"
  },
  {
    "number": 9,
    "title": "Fix README_cn.md translation",
    "user": "pulltheflower",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-07T10:20:38Z",
    "closed_at": "2024-03-07T13:06:30Z",
    "merged_at": "2024-03-07T13:06:30Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/9"
  },
  {
    "number": 7,
    "title": "Add cn readme and license",
    "user": "jasonhe258",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-05T11:39:33Z",
    "closed_at": "2024-03-05T11:39:40Z",
    "merged_at": "2024-03-05T11:39:40Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/7"
  },
  {
    "number": 6,
    "title": "Fix output issue for default transformers pipeline",
    "user": "jasonhe258",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-01T09:46:12Z",
    "closed_at": "2024-03-01T10:12:40Z",
    "merged_at": "2024-03-01T10:12:40Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/6"
  },
  {
    "number": 5,
    "title": "Fix loading issue for non text-generation models",
    "user": "depenglee1707",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T13:24:41Z",
    "closed_at": "2024-03-01T10:11:46Z",
    "merged_at": "2024-03-01T10:11:46Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/5"
  },
  {
    "number": 4,
    "title": "enhance model if for cli",
    "user": "jasonhe258",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T09:26:42Z",
    "closed_at": "2024-02-29T09:34:30Z",
    "merged_at": "2024-02-29T09:34:30Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/4"
  },
  {
    "number": 2,
    "title": "replace / to -- in model id",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T07:45:44Z",
    "closed_at": "2024-02-29T07:48:42Z",
    "merged_at": "2024-02-29T07:48:42Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/2"
  },
  {
    "number": 1,
    "title": "Set min replica to 1 for opt-125m",
    "user": "HaiHui886",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-02-29T03:16:14Z",
    "closed_at": "2024-02-29T03:22:18Z",
    "merged_at": "2024-02-29T03:22:18Z",
    "state": "closed",
    "html_url": "https://github.com/OpenCSGs/llm-inference/pull/1"
  }
]