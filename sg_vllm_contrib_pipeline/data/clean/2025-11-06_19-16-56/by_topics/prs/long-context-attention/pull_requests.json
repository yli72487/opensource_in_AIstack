[
  {
    "number": 164,
    "title": "Fix assert message error",
    "user": "Xiangyanglikecode",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-10-17T06:51:04Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/164"
  },
  {
    "number": 162,
    "title": "Adapted for Huawei Ascend npu",
    "user": "endymion-ni",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-30T08:38:39Z",
    "closed_at": "2025-10-14T03:23:19Z",
    "merged_at": "2025-10-14T03:23:19Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/162"
  },
  {
    "number": 159,
    "title": "Add AttnType.AITER",
    "user": "kTorp",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-09-15T06:41:59Z",
    "closed_at": "2025-09-19T03:33:53Z",
    "merged_at": "2025-09-19T03:33:53Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/159"
  },
  {
    "number": 157,
    "title": "cudnn attention for pytorch",
    "user": "zhumakhan",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-28T09:41:02Z",
    "closed_at": "2025-09-25T17:49:33Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/157"
  },
  {
    "number": 156,
    "title": "Fix backward gradient count mismatch",
    "user": "MartinPernus",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-08-25T12:30:57Z",
    "closed_at": "2025-09-09T11:46:49Z",
    "merged_at": "2025-09-09T11:46:49Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/156"
  },
  {
    "number": 153,
    "title": "[bugfix] correct the operator alias in attention.py",
    "user": "houchen-li",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-18T11:21:55Z",
    "closed_at": "2025-07-18T12:57:16Z",
    "merged_at": "2025-07-18T12:57:16Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/153"
  },
  {
    "number": 152,
    "title": "[feature] adapt for Moore Threads graphics processing unit",
    "user": "houchen-li",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-07-17T12:02:19Z",
    "closed_at": "2025-07-18T01:46:21Z",
    "merged_at": "2025-07-18T01:46:21Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/152"
  },
  {
    "number": 151,
    "title": "fix: flashattention3 call",
    "user": "yuyu5333",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-26T04:25:22Z",
    "closed_at": "2025-05-27T01:43:47Z",
    "merged_at": "2025-05-27T01:43:47Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/151"
  },
  {
    "number": 150,
    "title": "fix: remove redundant code to avoid error in specific env",
    "user": "ZDJeffrey",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-05-15T04:11:16Z",
    "closed_at": "2025-05-27T01:43:18Z",
    "merged_at": "2025-05-27T01:43:18Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/150"
  },
  {
    "number": 148,
    "title": "fix import flashinfer error on AMD GPUs",
    "user": "akaitsuki-ii",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-25T04:06:05Z",
    "closed_at": "2025-05-28T07:41:18Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/148"
  },
  {
    "number": 147,
    "title": "bump to 0.6.3.post1",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-21T01:51:49Z",
    "closed_at": "2025-04-21T01:51:55Z",
    "merged_at": "2025-04-21T01:51:55Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/147"
  },
  {
    "number": 146,
    "title": "fix sage_fp16_triton repeat",
    "user": "ITerydh",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T12:19:51Z",
    "closed_at": "2025-04-21T01:49:40Z",
    "merged_at": "2025-04-21T01:49:40Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/146"
  },
  {
    "number": 144,
    "title": "bump to version 0.6.3",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T05:39:16Z",
    "closed_at": "2025-04-18T05:39:25Z",
    "merged_at": "2025-04-18T05:39:25Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/144"
  },
  {
    "number": 143,
    "title": "add more sage attn impl.",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-18T05:32:53Z",
    "closed_at": "2025-04-18T05:36:25Z",
    "merged_at": "2025-04-18T05:36:25Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/143"
  },
  {
    "number": 142,
    "title": "Fix flash attn call",
    "user": "Edenzzzz",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-15T15:22:31Z",
    "closed_at": null,
    "merged_at": null,
    "state": "open",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/142"
  },
  {
    "number": 141,
    "title": "Add an option for using a triton kernel for sageattention",
    "user": "intervitens",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-09T20:10:25Z",
    "closed_at": "2025-04-14T05:59:14Z",
    "merged_at": "2025-04-14T05:59:13Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/141"
  },
  {
    "number": 140,
    "title": "bump version to 0.6.2",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T08:31:25Z",
    "closed_at": "2025-04-08T08:31:51Z",
    "merged_at": "2025-04-08T08:31:51Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/140"
  },
  {
    "number": 139,
    "title": "feat: support flashinfer for ring attention",
    "user": "ZDJeffrey",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-08T06:50:02Z",
    "closed_at": "2025-04-08T08:04:44Z",
    "merged_at": "2025-04-08T08:04:44Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/139"
  },
  {
    "number": 138,
    "title": "rename test args for sparse sage",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-07T03:28:10Z",
    "closed_at": "2025-04-07T03:28:38Z",
    "merged_at": "2025-04-07T03:28:38Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/138"
  },
  {
    "number": 136,
    "title": "feat: support sparse sage attn, passed unit test",
    "user": "Eigensystem",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-06T18:01:45Z",
    "closed_at": "2025-04-07T02:48:00Z",
    "merged_at": "2025-04-07T02:48:00Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/136"
  },
  {
    "number": 134,
    "title": "remove useless tests and benchmarks",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T09:58:43Z",
    "closed_at": "2025-04-04T09:59:04Z",
    "merged_at": "2025-04-04T09:59:04Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/134"
  },
  {
    "number": 133,
    "title": "add unitest for sparse_sage",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T09:48:28Z",
    "closed_at": "2025-04-07T02:48:09Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/133"
  },
  {
    "number": 132,
    "title": "(WIP) feat: support sparse_sage_attn",
    "user": "Eigensystem",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T05:08:42Z",
    "closed_at": "2025-04-04T09:31:53Z",
    "merged_at": "2025-04-04T09:31:53Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/132"
  },
  {
    "number": 131,
    "title": "add fp8 comm.",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-04-04T04:05:07Z",
    "closed_at": "2025-04-07T04:49:54Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/131"
  },
  {
    "number": 130,
    "title": "bump version to 0.6.1",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-20T01:31:21Z",
    "closed_at": "2025-03-20T01:31:36Z",
    "merged_at": "2025-03-20T01:31:36Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/130"
  },
  {
    "number": 129,
    "title": "fix(sage attention): layout bug",
    "user": "Eigensystem",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T15:21:55Z",
    "closed_at": "2025-03-20T01:28:05Z",
    "merged_at": "2025-03-20T01:28:05Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/129"
  },
  {
    "number": 128,
    "title": "fix sageattention lse return shape error",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T13:09:18Z",
    "closed_at": "2025-03-19T13:09:37Z",
    "merged_at": "2025-03-19T13:09:37Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/128"
  },
  {
    "number": 127,
    "title": "fix sageattention bugs",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T12:50:54Z",
    "closed_at": "2025-03-19T12:52:56Z",
    "merged_at": "2025-03-19T12:52:56Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/127"
  },
  {
    "number": 126,
    "title": "feat: support sage attention",
    "user": "Eigensystem",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-03-19T10:52:40Z",
    "closed_at": "2025-03-19T11:45:40Z",
    "merged_at": "2025-03-19T11:45:40Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/126"
  },
  {
    "number": 124,
    "title": "updated pytorch attention to flash attention",
    "user": "FrankLeeeee",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-19T07:14:07Z",
    "closed_at": "2025-02-19T07:19:29Z",
    "merged_at": "2025-02-19T07:19:29Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/124"
  },
  {
    "number": 123,
    "title": "[Fix] remove unnecessary print",
    "user": "xibosun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2025-02-10T03:22:22Z",
    "closed_at": "2025-02-13T09:45:15Z",
    "merged_at": "2025-02-13T09:45:15Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/123"
  },
  {
    "number": 121,
    "title": " fix qkvpacked_attn backward bugs",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-30T14:24:47Z",
    "closed_at": "2024-12-30T15:26:36Z",
    "merged_at": "2024-12-30T15:26:36Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/121"
  },
  {
    "number": 119,
    "title": "fix license classifier error",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-26T07:32:32Z",
    "closed_at": "2024-12-26T07:32:46Z",
    "merged_at": "2024-12-26T07:32:46Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/119"
  },
  {
    "number": 118,
    "title": "bump to 0.6.0",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-26T07:26:50Z",
    "closed_at": "2024-12-26T07:26:55Z",
    "merged_at": "2024-12-26T07:26:55Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/118"
  },
  {
    "number": 117,
    "title": "FlashAttentionImpl -> AttnType",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-26T07:17:16Z",
    "closed_at": "2024-12-26T07:17:32Z",
    "merged_at": "2024-12-26T07:17:32Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/117"
  },
  {
    "number": 116,
    "title": "version bump to 0.5.1",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-26T06:29:12Z",
    "closed_at": "2024-12-26T06:29:26Z",
    "merged_at": "2024-12-26T06:29:26Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/116"
  },
  {
    "number": 115,
    "title": "pytorch ring attention",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-26T03:29:46Z",
    "closed_at": "2024-12-26T06:23:41Z",
    "merged_at": "2024-12-26T06:23:41Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/115"
  },
  {
    "number": 113,
    "title": "test with flash_attn 2.6.3 and more flexible test",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-16T09:43:39Z",
    "closed_at": "2024-12-16T09:44:18Z",
    "merged_at": "2024-12-16T09:44:18Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/113"
  },
  {
    "number": 111,
    "title": "update readme pictures",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-11T10:06:48Z",
    "closed_at": "2024-12-11T11:09:10Z",
    "merged_at": "2024-12-11T11:09:10Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/111"
  },
  {
    "number": 110,
    "title": "bump to 0.5.0",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-11T10:01:10Z",
    "closed_at": "2024-12-11T10:02:02Z",
    "merged_at": "2024-12-11T10:02:02Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/110"
  },
  {
    "number": 109,
    "title": "update readme with fav3 and no fa usage",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-11T08:08:44Z",
    "closed_at": "2024-12-11T08:10:29Z",
    "merged_at": "2024-12-11T08:10:29Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/109"
  },
  {
    "number": 108,
    "title": "support fa 2.7.0 and support torch only ulysses",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-11T07:13:33Z",
    "closed_at": "2024-12-11T07:15:59Z",
    "merged_at": "2024-12-11T07:15:59Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/108"
  },
  {
    "number": 107,
    "title": "support flash_attn 2.7.0",
    "user": "xibosun",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-12-02T03:43:36Z",
    "closed_at": "2024-12-04T09:47:45Z",
    "merged_at": "2024-12-04T09:47:45Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/107"
  },
  {
    "number": 106,
    "title": "version to 0.4.2",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-19T02:10:56Z",
    "closed_at": "2024-11-19T02:11:02Z",
    "merged_at": "2024-11-19T02:11:02Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/106"
  },
  {
    "number": 105,
    "title": "flash_attn3 not directly import",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-19T02:08:04Z",
    "closed_at": "2024-11-19T02:08:24Z",
    "merged_at": "2024-11-19T02:08:24Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/105"
  },
  {
    "number": 104,
    "title": "ulysses in benchmark",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-15T13:19:12Z",
    "closed_at": "2024-11-15T13:19:41Z",
    "merged_at": "2024-11-15T13:19:40Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/104"
  },
  {
    "number": 103,
    "title": "feat: add use_sync switch to ulysses",
    "user": "Eigensystem",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-15T08:18:09Z",
    "closed_at": "2024-11-15T08:34:14Z",
    "merged_at": "2024-11-15T08:34:14Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/103"
  },
  {
    "number": 102,
    "title": "dump to 0.4.0",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-15T08:14:19Z",
    "closed_at": "2024-11-15T08:15:09Z",
    "merged_at": "2024-11-15T08:15:09Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/102"
  },
  {
    "number": 101,
    "title": "dump version to 0.4.1",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-15T08:07:15Z",
    "closed_at": "2024-11-15T08:07:28Z",
    "merged_at": "2024-11-15T08:07:28Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/101"
  },
  {
    "number": 100,
    "title": "version to 0.4.0",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-15T07:20:24Z",
    "closed_at": "2024-11-15T07:21:10Z",
    "merged_at": "2024-11-15T07:21:10Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/100"
  },
  {
    "number": 99,
    "title": "update readme for FA3",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-15T07:08:34Z",
    "closed_at": "2024-11-15T07:10:22Z",
    "merged_at": "2024-11-15T07:10:22Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/99"
  },
  {
    "number": 98,
    "title": "1114v2",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-14T03:27:03Z",
    "closed_at": "2024-11-15T06:54:00Z",
    "merged_at": "2024-11-15T06:54:00Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/98"
  },
  {
    "number": 97,
    "title": "add dit benchmark script",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-14T01:43:53Z",
    "closed_at": "2024-11-14T01:44:02Z",
    "merged_at": "2024-11-14T01:44:02Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/97"
  },
  {
    "number": 96,
    "title": "FA3: update to the latest FA3 API.",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-14T01:31:55Z",
    "closed_at": "2024-11-14T01:32:09Z",
    "merged_at": "2024-11-14T01:32:09Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/96"
  },
  {
    "number": 95,
    "title": "flash attention 3",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-13T08:17:43Z",
    "closed_at": "2024-11-13T15:34:49Z",
    "merged_at": "2024-11-13T15:34:49Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/95"
  },
  {
    "number": 94,
    "title": "version 0.3.7",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-11T03:30:59Z",
    "closed_at": "2024-11-11T03:31:34Z",
    "merged_at": "2024-11-11T03:31:34Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/94"
  },
  {
    "number": 93,
    "title": "sync after all2all for device memory saving",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-11T03:19:04Z",
    "closed_at": "2024-11-11T03:29:40Z",
    "merged_at": "2024-11-11T03:29:40Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/93"
  },
  {
    "number": 92,
    "title": "prevent dispatch issues in SequenceParallel to improve performance",
    "user": "uclalch",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-07T10:13:12Z",
    "closed_at": "2024-11-11T02:52:47Z",
    "merged_at": "2024-11-11T02:52:47Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/92"
  },
  {
    "number": 91,
    "title": "Chitu for Ulysses",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-11-01T12:10:46Z",
    "closed_at": "2025-03-20T10:52:38Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/91"
  },
  {
    "number": 90,
    "title": "add int8flashattention and sageattention for ulysses",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-25T05:59:32Z",
    "closed_at": "2024-11-01T12:03:12Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/90"
  },
  {
    "number": 89,
    "title": "upgrade version to 0.3.6",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-16T08:19:06Z",
    "closed_at": "2024-10-16T08:19:25Z",
    "merged_at": "2024-10-16T08:19:25Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/89"
  },
  {
    "number": 87,
    "title": "update all_to_all",
    "user": "Lay2000",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-10-14T11:24:55Z",
    "closed_at": "2024-10-14T11:30:51Z",
    "merged_at": "2024-10-14T11:30:51Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/87"
  },
  {
    "number": 84,
    "title": "version 0.3.5",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-19T15:21:34Z",
    "closed_at": "2024-09-19T15:22:20Z",
    "merged_at": "2024-09-19T15:22:20Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/84"
  },
  {
    "number": 83,
    "title": "revert version 0.3.2",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-19T14:56:39Z",
    "closed_at": "2024-09-19T14:56:47Z",
    "merged_at": "2024-09-19T14:56:47Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/83"
  },
  {
    "number": 82,
    "title": "version 0.3.3",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-19T14:26:25Z",
    "closed_at": "2024-09-19T14:26:39Z",
    "merged_at": "2024-09-19T14:26:39Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/82"
  },
  {
    "number": 81,
    "title": "polish publish workflow",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-19T13:50:46Z",
    "closed_at": "2024-09-19T13:51:03Z",
    "merged_at": "2024-09-19T13:51:03Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/81"
  },
  {
    "number": 80,
    "title": "version to 0.3.2",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-16T15:04:06Z",
    "closed_at": "2024-09-16T15:04:19Z",
    "merged_at": "2024-09-16T15:04:19Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/80"
  },
  {
    "number": 79,
    "title": "remove useless workflow",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-14T09:00:27Z",
    "closed_at": "2024-09-14T09:00:41Z",
    "merged_at": "2024-09-14T09:00:41Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/79"
  },
  {
    "number": 78,
    "title": "version 0.3.2",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-14T05:56:19Z",
    "closed_at": "2024-09-14T05:56:50Z",
    "merged_at": "2024-09-14T05:56:50Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/78"
  },
  {
    "number": 77,
    "title": "auto publish python package when release on github",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-14T05:50:57Z",
    "closed_at": "2024-09-14T05:51:25Z",
    "merged_at": "2024-09-14T05:51:25Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/77"
  },
  {
    "number": 76,
    "title": "remove amd installation to an individual doc",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-14T03:54:42Z",
    "closed_at": "2024-09-14T04:13:09Z",
    "merged_at": "2024-09-14T04:13:09Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/76"
  },
  {
    "number": 75,
    "title": "0914v2",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-14T03:46:05Z",
    "closed_at": "2024-09-14T03:47:29Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/75"
  },
  {
    "number": 74,
    "title": "use extract_local for test_hybrid_attn.py",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-09-14T03:30:03Z",
    "closed_at": "2024-09-14T03:30:33Z",
    "merged_at": "2024-09-14T03:30:33Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/74"
  },
  {
    "number": 72,
    "title": "improve readability and potential numerical stability of ring attention",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-28T03:09:08Z",
    "closed_at": "2024-08-28T03:09:23Z",
    "merged_at": "2024-08-28T03:09:23Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/72"
  },
  {
    "number": 71,
    "title": "[AMD GPU] Add amd gpu suppport",
    "user": "yiakwy-xpu-ml-framework-team",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-26T07:42:57Z",
    "closed_at": "2024-09-03T15:16:34Z",
    "merged_at": "2024-09-03T15:16:34Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/71"
  },
  {
    "number": 70,
    "title": "feat: add support for flash_attn>=2.6.0",
    "user": "Eigensystem",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-23T10:06:57Z",
    "closed_at": "2024-08-26T14:10:35Z",
    "merged_at": "2024-08-26T14:10:35Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/70"
  },
  {
    "number": 69,
    "title": "add license",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-08-22T03:46:31Z",
    "closed_at": "2024-08-22T03:46:38Z",
    "merged_at": "2024-08-22T03:46:38Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/69"
  },
  {
    "number": 63,
    "title": "support multiple node",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-27T07:02:54Z",
    "closed_at": "2024-06-27T07:03:11Z",
    "merged_at": "2024-06-27T07:03:11Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/63"
  },
  {
    "number": 62,
    "title": "add reference to this project",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-26T07:17:56Z",
    "closed_at": "2024-06-26T07:18:03Z",
    "merged_at": "2024-06-26T07:18:03Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/62"
  },
  {
    "number": 61,
    "title": "release V0.2",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-25T03:05:38Z",
    "closed_at": "2024-06-25T03:05:54Z",
    "merged_at": "2024-06-25T03:05:54Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/61"
  },
  {
    "number": 60,
    "title": "add tesla supports for ulysses",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-21T07:41:41Z",
    "closed_at": "2024-06-21T07:43:14Z",
    "merged_at": "2024-06-21T07:43:14Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/60"
  },
  {
    "number": 59,
    "title": "ulysses do not use flash_attn on T4 GPU",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-06-20T08:31:40Z",
    "closed_at": "2024-06-20T08:31:56Z",
    "merged_at": "2024-06-20T08:31:56Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/59"
  },
  {
    "number": 58,
    "title": "add loss curve",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-28T08:01:16Z",
    "closed_at": "2024-05-28T08:01:24Z",
    "merged_at": "2024-05-28T08:01:24Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/58"
  },
  {
    "number": 57,
    "title": "Fix r_rank/u_rank in extract local to be compatible with DP",
    "user": "ShomyLiu",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-24T09:06:35Z",
    "closed_at": "2024-05-27T08:11:59Z",
    "merged_at": "2024-05-27T08:11:59Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/57"
  },
  {
    "number": 55,
    "title": "update readme",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-16T02:31:30Z",
    "closed_at": "2024-05-16T02:31:38Z",
    "merged_at": "2024-05-16T02:31:37Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/55"
  },
  {
    "number": 54,
    "title": "update readme",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-05-08T01:58:27Z",
    "closed_at": "2024-05-08T01:58:37Z",
    "merged_at": "2024-05-08T01:58:37Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/54"
  },
  {
    "number": 52,
    "title": "add patches",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-19T08:06:27Z",
    "closed_at": "2024-04-19T08:06:35Z",
    "merged_at": "2024-04-19T08:06:35Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/52"
  },
  {
    "number": 51,
    "title": "add megatron-deepspeed patch",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-19T08:04:35Z",
    "closed_at": "2024-04-19T08:04:43Z",
    "merged_at": "2024-04-19T08:04:43Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/51"
  },
  {
    "number": 50,
    "title": "set_seq_parallel_pg compatible with DP process group",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-19T06:36:40Z",
    "closed_at": "2024-04-19T06:37:27Z",
    "merged_at": "2024-04-19T06:37:27Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/50"
  },
  {
    "number": 49,
    "title": "hotfix import error",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-18T08:19:13Z",
    "closed_at": "2024-04-18T08:43:14Z",
    "merged_at": "2024-04-18T08:43:14Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/49"
  },
  {
    "number": 48,
    "title": "update comprehensive benchmark results",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-18T06:42:22Z",
    "closed_at": "2024-04-18T06:42:31Z",
    "merged_at": "2024-04-18T06:42:31Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/48"
  },
  {
    "number": 47,
    "title": "fix qkvpacked strip and zigzag test error",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-17T09:35:48Z",
    "closed_at": "2024-04-18T01:56:36Z",
    "merged_at": "2024-04-18T01:56:36Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/47"
  },
  {
    "number": 45,
    "title": "remove useless code",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-16T07:36:33Z",
    "closed_at": "2024-04-16T07:51:33Z",
    "merged_at": "2024-04-16T07:51:33Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/45"
  },
  {
    "number": 44,
    "title": "async hybrid attention forward only",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-16T07:05:07Z",
    "closed_at": "2024-04-16T07:05:30Z",
    "merged_at": "2024-04-16T07:05:30Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/44"
  },
  {
    "number": 43,
    "title": "initial async ulysses attn",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-15T12:24:46Z",
    "closed_at": "2024-04-15T12:25:11Z",
    "merged_at": "2024-04-15T12:25:11Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/43"
  },
  {
    "number": 42,
    "title": "move all_to_all to ./comm",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-15T08:58:37Z",
    "closed_at": "2024-04-15T08:58:44Z",
    "merged_at": "2024-04-15T08:58:44Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/42"
  },
  {
    "number": 41,
    "title": "add torch profiler",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-15T08:40:27Z",
    "closed_at": "2024-04-15T08:41:56Z",
    "merged_at": "2024-04-15T08:41:56Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/41"
  },
  {
    "number": 38,
    "title": "add gqa support and benchmark results in readme",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-11T08:28:30Z",
    "closed_at": "2024-04-11T08:29:12Z",
    "merged_at": "2024-04-11T08:29:12Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/38"
  },
  {
    "number": 36,
    "title": "warmup longctx no pack benchmark",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-10T07:51:27Z",
    "closed_at": "2024-04-10T08:05:58Z",
    "merged_at": "2024-04-10T08:05:58Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/36"
  },
  {
    "number": 35,
    "title": "use global process group",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-10T03:30:16Z",
    "closed_at": "2024-04-10T03:37:08Z",
    "merged_at": "2024-04-10T03:37:07Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/35"
  },
  {
    "number": 34,
    "title": "0409v3",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-10T03:26:34Z",
    "closed_at": "2024-04-10T03:27:39Z",
    "merged_at": null,
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/34"
  },
  {
    "number": 32,
    "title": "reorganize the directories",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-09T09:04:06Z",
    "closed_at": "2024-04-09T09:04:14Z",
    "merged_at": "2024-04-09T09:04:14Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/32"
  },
  {
    "number": 31,
    "title": "fix ulysses bugs",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-09T08:06:00Z",
    "closed_at": "2024-04-09T08:06:07Z",
    "merged_at": "2024-04-09T08:06:07Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/31"
  },
  {
    "number": 30,
    "title": "update benchmark scripts",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-09T07:38:58Z",
    "closed_at": "2024-04-09T07:39:07Z",
    "merged_at": "2024-04-09T07:39:07Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/30"
  },
  {
    "number": 28,
    "title": "add use_ulysses_low flag",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T08:52:35Z",
    "closed_at": "2024-04-01T08:52:50Z",
    "merged_at": "2024-04-01T08:52:50Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/28"
  },
  {
    "number": 27,
    "title": "update readme",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T08:18:43Z",
    "closed_at": "2024-04-01T08:18:50Z",
    "merged_at": "2024-04-01T08:18:50Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/27"
  },
  {
    "number": 26,
    "title": "update readme using more ring-attn-impl for LongContextAttention",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T08:13:40Z",
    "closed_at": "2024-04-01T08:13:48Z",
    "merged_at": "2024-04-01T08:13:48Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/26"
  },
  {
    "number": 25,
    "title": "add ring-attn impl type",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-04-01T03:37:07Z",
    "closed_at": "2024-04-01T03:37:25Z",
    "merged_at": "2024-04-01T03:37:25Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/25"
  },
  {
    "number": 23,
    "title": "polish readme",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T10:08:02Z",
    "closed_at": "2024-03-29T10:08:11Z",
    "merged_at": "2024-03-29T10:08:11Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/23"
  },
  {
    "number": 22,
    "title": "polish readme",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T09:57:03Z",
    "closed_at": "2024-03-29T09:57:14Z",
    "merged_at": "2024-03-29T09:57:14Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/22"
  },
  {
    "number": 21,
    "title": "update readme",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T09:55:01Z",
    "closed_at": "2024-03-29T09:55:16Z",
    "merged_at": "2024-03-29T09:55:16Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/21"
  },
  {
    "number": 20,
    "title": "add media",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T09:43:04Z",
    "closed_at": "2024-03-29T09:43:43Z",
    "merged_at": "2024-03-29T09:43:43Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/20"
  },
  {
    "number": 19,
    "title": "polish benchmark flags",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T09:23:35Z",
    "closed_at": "2024-03-29T09:23:43Z",
    "merged_at": "2024-03-29T09:23:43Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/19"
  },
  {
    "number": 18,
    "title": "revert benchmark flags",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T08:33:32Z",
    "closed_at": "2024-03-29T08:33:41Z",
    "merged_at": "2024-03-29T08:33:41Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/18"
  },
  {
    "number": 17,
    "title": "add flags to benchmark script",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T08:07:42Z",
    "closed_at": "2024-03-29T08:07:50Z",
    "merged_at": "2024-03-29T08:07:50Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/17"
  },
  {
    "number": 16,
    "title": "remove useless print",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T07:42:23Z",
    "closed_at": "2024-03-29T07:42:32Z",
    "merged_at": "2024-03-29T07:42:32Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/16"
  },
  {
    "number": 15,
    "title": "long context attn qkvpacked",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T07:18:11Z",
    "closed_at": "2024-03-29T07:18:18Z",
    "merged_at": "2024-03-29T07:18:18Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/15"
  },
  {
    "number": 14,
    "title": "add longctxattn benchmark",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T06:08:27Z",
    "closed_at": "2024-03-29T06:08:40Z",
    "merged_at": "2024-03-29T06:08:40Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/14"
  },
  {
    "number": 13,
    "title": "add LongContextAttention",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T05:44:25Z",
    "closed_at": "2024-03-29T05:44:32Z",
    "merged_at": "2024-03-29T05:44:32Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/13"
  },
  {
    "number": 12,
    "title": "fix process group init bug",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-29T03:42:10Z",
    "closed_at": "2024-03-29T03:42:42Z",
    "merged_at": "2024-03-29T03:42:42Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/12"
  },
  {
    "number": 11,
    "title": "add use_ring_low_dim flag in hybrid test",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T11:43:49Z",
    "closed_at": "2024-03-28T11:44:04Z",
    "merged_at": "2024-03-28T11:44:04Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/11"
  },
  {
    "number": 10,
    "title": "polish readme",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T09:34:02Z",
    "closed_at": "2024-03-28T09:34:14Z",
    "merged_at": "2024-03-28T09:34:14Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/10"
  },
  {
    "number": 9,
    "title": "update readme for hybrid attention",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T09:32:29Z",
    "closed_at": "2024-03-28T09:32:38Z",
    "merged_at": "2024-03-28T09:32:38Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/9"
  },
  {
    "number": 8,
    "title": "add hybrid sequence parallel",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T09:15:33Z",
    "closed_at": "2024-03-28T09:15:42Z",
    "merged_at": "2024-03-28T09:15:42Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/8"
  },
  {
    "number": 7,
    "title": "polish ulysses attn test print",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T08:49:01Z",
    "closed_at": "2024-03-28T08:49:10Z",
    "merged_at": "2024-03-28T08:49:10Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/7"
  },
  {
    "number": 6,
    "title": "check ulysses attention backward pass correctness",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T08:45:16Z",
    "closed_at": "2024-03-28T08:45:30Z",
    "merged_at": "2024-03-28T08:45:30Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/6"
  },
  {
    "number": 5,
    "title": "check ulysses forward pass correctness",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T08:05:26Z",
    "closed_at": "2024-03-28T08:06:01Z",
    "merged_at": "2024-03-28T08:06:01Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/5"
  },
  {
    "number": 4,
    "title": "fix a bug in all-to-all args and check ulysses output",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T07:48:29Z",
    "closed_at": "2024-03-28T07:49:17Z",
    "merged_at": "2024-03-28T07:49:17Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/4"
  },
  {
    "number": 3,
    "title": "test torch mha version",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T06:27:32Z",
    "closed_at": "2024-03-28T06:30:10Z",
    "merged_at": "2024-03-28T06:30:10Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/3"
  },
  {
    "number": 2,
    "title": "refactor 4D tensor all-to-all",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T06:12:12Z",
    "closed_at": "2024-03-28T06:12:53Z",
    "merged_at": "2024-03-28T06:12:53Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/2"
  },
  {
    "number": 1,
    "title": "add ring flash attn as local attn",
    "user": "feifeibear",
    "user_type": "User",
    "is_human": true,
    "created_at": "2024-03-28T03:31:25Z",
    "closed_at": "2024-03-28T03:31:54Z",
    "merged_at": "2024-03-28T03:31:54Z",
    "state": "closed",
    "html_url": "https://github.com/feifeibear/long-context-attention/pull/1"
  }
]